# Arxiv Papers in cs.CV on 2021-09-13
### Unsupervised domain adaptation for cross-modality liver segmentation via joint adversarial learning and self-learning
- **Arxiv ID**: http://arxiv.org/abs/2109.05664v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.05664v3)
- **Published**: 2021-09-13 01:46:28+00:00
- **Updated**: 2022-02-24 07:45:21+00:00
- **Authors**: Jin Hong, Simon Chun-Ho Yu, Weitian Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Liver segmentation on images acquired using computed tomography (CT) and magnetic resonance imaging (MRI) plays an important role in clinical management of liver diseases. Compared to MRI, CT images of liver are more abundant and readily available. However, MRI can provide richer quantitative information of the liver compared to CT. Thus, it is desirable to achieve unsupervised domain adaptation for transferring the learned knowledge from the source domain containing labeled CT images to the target domain containing unlabeled MR images. In this work, we report a novel unsupervised domain adaptation framework for cross-modality liver segmentation via joint adversarial learning and self-learning. We propose joint semantic-aware and shape-entropy-aware adversarial learning with post-situ identification manner to implicitly align the distribution of task-related features extracted from the target domain with those from the source domain. In proposed framework, a network is trained with the above two adversarial losses in an unsupervised manner, and then a mean completer of pseudo-label generation is employed to produce pseudo-labels to train the next network (desired model). Additionally, semantic-aware adversarial learning and two self-learning methods, including pixel-adaptive mask refinement and student-to-partner learning, are proposed to train the desired model. To improve the robustness of the desired model, a low-signal augmentation function is proposed to transform MRI images as the input of the desired model to handle hard samples. Using the public data sets, our experiments demonstrated the proposed unsupervised domain adaptation framework reached four supervised learning methods with a Dice score 0.912 plus or minus 0.037 (mean plus or minus standard deviation).



### CANS: Communication Limited Camera Network Self-Configuration for Intelligent Industrial Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2109.05665v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.05665v1)
- **Published**: 2021-09-13 01:54:33+00:00
- **Updated**: 2021-09-13 01:54:33+00:00
- **Authors**: Jingzheng Tu, Qimin Xu, Cailian Chen
- **Comment**: 6 pages, 11 figures
- **Journal**: None
- **Summary**: Realtime and intelligent video surveillance via camera networks involve computation-intensive vision detection tasks with massive video data, which is crucial for safety in the edge-enabled industrial Internet of Things (IIoT). Multiple video streams compete for limited communication resources on the link between edge devices and camera networks, resulting in considerable communication congestion. It postpones the completion time and degrades the accuracy of vision detection tasks. Thus, achieving high accuracy of vision detection tasks under the communication constraints and vision task deadline constraints is challenging. Previous works focus on single camera configuration to balance the tradeoff between accuracy and processing time of detection tasks by setting video quality parameters. In this paper, an adaptive camera network self-configuration method (CANS) of video surveillance is proposed to cope with multiple video streams of heterogeneous quality of service (QoS) demands for edge-enabled IIoT. Moreover, it adapts to video content and network dynamics. Specifically, the tradeoff between two key performance metrics, \emph{i.e.,} accuracy and latency, is formulated as an NP-hard optimization problem with latency constraints. Simulation on real-world surveillance datasets demonstrates that the proposed CANS method achieves low end-to-end latency (13 ms on average) with high accuracy (92\% on average) with network dynamics. The results validate the effectiveness of the CANS.



### UMPNet: Universal Manipulation Policy Network for Articulated Objects
- **Arxiv ID**: http://arxiv.org/abs/2109.05668v4
- **DOI**: 10.1109/LRA.2022.3142397
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.05668v4)
- **Published**: 2021-09-13 02:01:00+00:00
- **Updated**: 2022-02-10 22:38:52+00:00
- **Authors**: Zhenjia Xu, Zhanpeng He, Shuran Song
- **Comment**: RA-L/ICRA 2022. Project page: https://ump-net.cs.columbia.edu/
- **Journal**: None
- **Summary**: We introduce the Universal Manipulation Policy Network (UMPNet) -- a single image-based policy network that infers closed-loop action sequences for manipulating arbitrary articulated objects. To infer a wide range of action trajectories, the policy supports 6DoF action representation and varying trajectory length. To handle a diverse set of objects, the policy learns from objects with different articulation structures and generalizes to unseen objects or categories. The policy is trained with self-guided exploration without any human demonstrations, scripted policy, or pre-defined goal conditions. To support effective multi-step interaction, we introduce a novel Arrow-of-Time action attribute that indicates whether an action will change the object state back to the past or forward into the future. With the Arrow-of-Time inference at each interaction step, the learned policy is able to select actions that consistently lead towards or away from a given state, thereby, enabling both effective state exploration and goal-conditioned manipulation. Video is available at https://youtu.be/KqlvcL9RqKM



### Shape-Biased Domain Generalization via Shock Graph Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2109.05671v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.05671v1)
- **Published**: 2021-09-13 02:10:40+00:00
- **Updated**: 2021-09-13 02:10:40+00:00
- **Authors**: Maruthi Narayanan, Vickram Rajendran, Benjamin Kimia
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: There is an emerging sense that the vulnerability of Image Convolutional Neural Networks (CNN), i.e., sensitivity to image corruptions, perturbations, and adversarial attacks, is connected with Texture Bias. This relative lack of Shape Bias is also responsible for poor performance in Domain Generalization (DG). The inclusion of a role of shape alleviates these vulnerabilities and some approaches have achieved this by training on negative images, images endowed with edge maps, or images with conflicting shape and texture information. This paper advocates an explicit and complete representation of shape using a classical computer vision approach, namely, representing the shape content of an image with the shock graph of its contour map. The resulting graph and its descriptor is a complete representation of contour content and is classified using recent Graph Neural Network (GNN) methods. The experimental results on three domain shift datasets, Colored MNIST, PACS, and VLCS demonstrate that even without using appearance the shape-based approach exceeds classical Image CNN based methods in domain generalization.



### FaceGuard: Proactive Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.05673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.05673v1)
- **Published**: 2021-09-13 02:36:25+00:00
- **Updated**: 2021-09-13 02:36:25+00:00
- **Authors**: Yuankun Yang, Chenyue Liang, Hongyu He, Xiaoyu Cao, Neil Zhenqiang Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Existing deepfake-detection methods focus on passive detection, i.e., they detect fake face images via exploiting the artifacts produced during deepfake manipulation. A key limitation of passive detection is that it cannot detect fake faces that are generated by new deepfake generation methods. In this work, we propose FaceGuard, a proactive deepfake-detection framework. FaceGuard embeds a watermark into a real face image before it is published on social media. Given a face image that claims to be an individual (e.g., Nicolas Cage), FaceGuard extracts a watermark from it and predicts the face image to be fake if the extracted watermark does not match well with the individual's ground truth one. A key component of FaceGuard is a new deep-learning-based watermarking method, which is 1) robust to normal image post-processing such as JPEG compression, Gaussian blurring, cropping, and resizing, but 2) fragile to deepfake manipulation. Our evaluation on multiple datasets shows that FaceGuard can detect deepfakes accurately and outperforms existing methods.



### Real-Time EMG Signal Classification via Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.05674v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.05674v1)
- **Published**: 2021-09-13 02:36:44+00:00
- **Updated**: 2021-09-13 02:36:44+00:00
- **Authors**: Reza Bagherian Azhiri, Mohammad Esmaeili, Mehrdad Nourani
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time classification of Electromyography signals is the most challenging part of controlling a prosthetic hand. Achieving a high classification accuracy of EMG signals in a short delay time is still challenging. Recurrent neural networks (RNNs) are artificial neural network architectures that are appropriate for sequential data such as EMG. In this paper, after extracting features from a hybrid time-frequency domain (discrete Wavelet transform), we utilize a set of recurrent neural network-based architectures to increase the classification accuracy and reduce the prediction delay time. The performances of these architectures are compared and in general outperform other state-of-the-art methods by achieving 96% classification accuracy in 600 msec.



### Online Unsupervised Learning of Visual Representations and Categories
- **Arxiv ID**: http://arxiv.org/abs/2109.05675v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2109.05675v4)
- **Published**: 2021-09-13 02:38:23+00:00
- **Updated**: 2022-05-28 15:06:13+00:00
- **Authors**: Mengye Ren, Tyler R. Scott, Michael L. Iuzzolino, Michael C. Mozer, Richard Zemel
- **Comment**: Technical report, 32 pages
- **Journal**: None
- **Summary**: Real world learning scenarios involve a nonstationary distribution of classes with sequential dependencies among the samples, in contrast to the standard machine learning formulation of drawing samples independently from a fixed, typically uniform distribution. Furthermore, real world interactions demand learning on-the-fly from few or no class labels. In this work, we propose an unsupervised model that simultaneously performs online visual representation learning and few-shot learning of new categories without relying on any class labels. Our model is a prototype-based memory network with a control component that determines when to form a new class prototype. We formulate it as an online mixture model, where components are created with only a single new example, and assignments do not have to be balanced, which permits an approximation to natural imbalanced distributions from uncurated raw data. Learning includes a contrastive loss that encourages different views of the same image to be assigned to the same prototype. The result is a mechanism that forms categorical representations of objects in nonstationary environments. Experiments show that our method can learn from an online stream of visual input data and its learned representations are significantly better at category recognition compared to state-of-the-art self-supervised learning methods.



### Domain and Content Adaptive Convolution based Multi-Source Domain Generalization for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.05676v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.05676v2)
- **Published**: 2021-09-13 02:41:38+00:00
- **Updated**: 2022-09-25 09:57:29+00:00
- **Authors**: Shishuai Hu, Zehui Liao, Jianpeng Zhang, Yong Xia
- **Comment**: IEEE-TMI
- **Journal**: None
- **Summary**: The domain gap caused mainly by variable medical image quality renders a major obstacle on the path between training a segmentation model in the lab and applying the trained model to unseen clinical data. To address this issue, domain generalization methods have been proposed, which however usually use static convolutions and are less flexible. In this paper, we propose a multi-source domain generalization model based on the domain and content adaptive convolution (DCAC) for the segmentation of medical images across different modalities. Specifically, we design the domain adaptive convolution (DAC) module and content adaptive convolution (CAC) module and incorporate both into an encoder-decoder backbone. In the DAC module, a dynamic convolutional head is conditioned on the predicted domain code of the input to make our model adapt to the unseen target domain. In the CAC module, a dynamic convolutional head is conditioned on the global image features to make our model adapt to the test image. We evaluated the DCAC model against the baseline and four state-of-the-art domain generalization methods on the prostate segmentation, COVID-19 lesion segmentation, and optic cup/optic disc segmentation tasks. Our results not only indicate that the proposed DCAC model outperforms all competing methods on each segmentation task but also demonstrate the effectiveness of the DAC and CAC modules. Code is available at \url{https://git.io/DCAC}.



### Spatial and Semantic Consistency Regularizations for Pedestrian Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.05686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05686v1)
- **Published**: 2021-09-13 03:36:44+00:00
- **Updated**: 2021-09-13 03:36:44+00:00
- **Authors**: Jian Jia, Xiaotang Chen, Kaiqi Huang
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: While recent studies on pedestrian attribute recognition have shown remarkable progress in leveraging complicated networks and attention mechanisms, most of them neglect the inter-image relations and an important prior: spatial consistency and semantic consistency of attributes under surveillance scenarios. The spatial locations of the same attribute should be consistent between different pedestrian images, \eg, the ``hat" attribute and the ``boots" attribute are always located at the top and bottom of the picture respectively. In addition, the inherent semantic feature of the ``hat" attribute should be consistent, whether it is a baseball cap, beret, or helmet. To fully exploit inter-image relations and aggregate human prior in the model learning process, we construct a Spatial and Semantic Consistency (SSC) framework that consists of two complementary regularizations to achieve spatial and semantic consistency for each attribute. Specifically, we first propose a spatial consistency regularization to focus on reliable and stable attribute-related regions. Based on the precise attribute locations, we further propose a semantic consistency regularization to extract intrinsic and discriminative semantic features. We conduct extensive experiments on popular benchmarks including PA100K, RAP, and PETA. Results show that the proposed method performs favorably against state-of-the-art methods without increasing parameters.



### PAT: Pseudo-Adversarial Training For Detecting Adversarial Videos
- **Arxiv ID**: http://arxiv.org/abs/2109.05695v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2109.05695v1)
- **Published**: 2021-09-13 04:05:46+00:00
- **Updated**: 2021-09-13 04:05:46+00:00
- **Authors**: Nupur Thakur, Baoxin Li
- **Comment**: 7 pages,5 figures
- **Journal**: None
- **Summary**: Extensive research has demonstrated that deep neural networks (DNNs) are prone to adversarial attacks. Although various defense mechanisms have been proposed for image classification networks, fewer approaches exist for video-based models that are used in security-sensitive applications like surveillance. In this paper, we propose a novel yet simple algorithm called Pseudo-Adversarial Training (PAT), to detect the adversarial frames in a video without requiring knowledge of the attack. Our approach generates `transition frames' that capture critical deviation from the original frames and eliminate the components insignificant to the detection task. To avoid the necessity of knowing the attack model, we produce `pseudo perturbations' to train our detection network. Adversarial detection is then achieved through the use of the detected frames. Experimental results on UCF-101 and 20BN-Jester datasets show that PAT can detect the adversarial video frames and videos with a high detection rate. We also unveil the potential reasons for the effectiveness of the transition frames and pseudo perturbations through extensive experiments.



### Rethinking Lightweight Convolutional Neural Networks for Efficient and High-quality Pavement Crack Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.05707v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05707v2)
- **Published**: 2021-09-13 05:01:34+00:00
- **Updated**: 2023-01-31 15:48:41+00:00
- **Authors**: Kai Li, Jie Yang, Siwei Ma, Bo Wang, Shanshe Wang, Yingjie Tian, Zhiquan Qi
- **Comment**: 19 pages, 14 figures, 12 tables
- **Journal**: None
- **Summary**: Pixel-level road crack detection has always been a challenging task in intelligent transportation systems. Due to the external environments, such as weather, light, and other factors, pavement cracks often present low contrast, poor continuity, and different sizes in length and width. However, most of the existing studies pay less attention to crack data under different situations. Meanwhile, recent algorithms based on deep convolutional neural networks (DCNNs) have promoted the development of cutting-edge models for crack detection. Nevertheless, they usually focus on complex models for good performance, but ignore detection efficiency in practical applications. In this article, to address the first issue, we collected two new databases (i.e. Rain365 and Sun520) captured in rainy and sunny days respectively, which enrich the data of the open source community. For the second issue, we reconsider how to improve detection efficiency with excellent performance, and then propose our lightweight encoder-decoder architecture termed CarNet. Specifically, we introduce a novel olive-shaped structure for the encoder network, a light-weight multi-scale block and a new up-sampling method in the decoder network. Numerous experiments show that our model can better balance detection performance and efficiency compared with previous models. Especially, on the Sun520 dataset, our CarNet significantly advances the state-of-the-art performance with ODS F-score from 0.488 to 0.514. Meanwhile, it does so with an improved detection speed (104 frame per second) which is orders of magnitude faster than some recent DCNNs-based algorithms specially designed for crack detection.



### Fine-Grained Few Shot Learning with Foreground Object Transformation
- **Arxiv ID**: http://arxiv.org/abs/2109.05719v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05719v1)
- **Published**: 2021-09-13 05:58:20+00:00
- **Updated**: 2021-09-13 05:58:20+00:00
- **Authors**: Chaofei Wang, Shiji Song, Qisen Yang, Xiang Li, Gao Huang
- **Comment**: Accepted by Neurocomputing
- **Journal**: None
- **Summary**: Traditional fine-grained image classification generally requires abundant labeled samples to deal with the low inter-class variance but high intra-class variance problem. However, in many scenarios we may have limited samples for some novel sub-categories, leading to the fine-grained few shot learning (FG-FSL) setting. To address this challenging task, we propose a novel method named foreground object transformation (FOT), which is composed of a foreground object extractor and a posture transformation generator. The former aims to remove image background, which tends to increase the difficulty of fine-grained image classification as it amplifies the intra-class variance while reduces inter-class variance. The latter transforms the posture of the foreground object to generate additional samples for the novel sub-category. As a data augmentation method, FOT can be conveniently applied to any existing few shot learning algorithm and greatly improve its performance on FG-FSL tasks. In particular, in combination with FOT, simple fine-tuning baseline methods can be competitive with the state-of-the-art methods both in inductive setting and transductive setting. Moreover, FOT can further boost the performances of latest excellent methods and bring them up to the new state-of-the-art. In addition, we also show the effectiveness of FOT on general FSL tasks.



### Low-Shot Validation: Active Importance Sampling for Estimating Classifier Performance on Rare Categories
- **Arxiv ID**: http://arxiv.org/abs/2109.05720v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.05720v1)
- **Published**: 2021-09-13 06:01:16+00:00
- **Updated**: 2021-09-13 06:01:16+00:00
- **Authors**: Fait Poms, Vishnu Sarukkai, Ravi Teja Mullapudi, Nimit S. Sohoni, William R. Mark, Deva Ramanan, Kayvon Fatahalian
- **Comment**: Accepted to ICCV 2021; 12 pages, 12 figures
- **Journal**: None
- **Summary**: For machine learning models trained with limited labeled training data, validation stands to become the main bottleneck to reducing overall annotation costs. We propose a statistical validation algorithm that accurately estimates the F-score of binary classifiers for rare categories, where finding relevant examples to evaluate on is particularly challenging. Our key insight is that simultaneous calibration and importance sampling enables accurate estimates even in the low-sample regime (< 300 samples). Critically, we also derive an accurate single-trial estimator of the variance of our method and demonstrate that this estimator is empirically accurate at low sample counts, enabling a practitioner to know how well they can trust a given low-sample estimate. When validating state-of-the-art semi-supervised models on ImageNet and iNaturalist2017, our method achieves the same estimates of model performance with up to 10x fewer labels than competing approaches. In particular, we can estimate model F1 scores with a variance of 0.005 using as few as 100 labels.



### ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/2109.05721v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05721v2)
- **Published**: 2021-09-13 06:05:28+00:00
- **Updated**: 2022-12-19 15:12:48+00:00
- **Authors**: Yangyu Huang, Hao Yang, Chong Li, Jongyoo Kim, Fangyun Wei
- **Comment**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision. 2021 (ICCV 2021)
- **Journal**: None
- **Summary**: The recent progress of CNN has dramatically improved face alignment performance. However, few works have paid attention to the error-bias with respect to error distribution of facial landmarks. In this paper, we investigate the error-bias issue in face alignment, where the distributions of landmark errors tend to spread along the tangent line to landmark curves. This error-bias is not trivial since it is closely connected to the ambiguous landmark labeling task. Inspired by this observation, we seek a way to leverage the error-bias property for better convergence of CNN model. To this end, we propose anisotropic direction loss (ADL) and anisotropic attention module (AAM) for coordinate and heatmap regression, respectively. ADL imposes strong binding force in normal direction for each landmark point on facial boundaries. On the other hand, AAM is an attention module which can get anisotropic attention mask focusing on the region of point and its local edge connected by adjacent points, it has a stronger response in tangent than in normal, which means relaxed constraints in the tangent. These two methods work in a complementary manner to learn both facial structures and texture details. Finally, we integrate them into an optimized end-to-end training pipeline named ADNet. Our ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which demonstrates the effectiveness and robustness.



### Robust Multi-Domain Mitosis Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.15092v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.15092v1)
- **Published**: 2021-09-13 06:25:15+00:00
- **Updated**: 2021-09-13 06:25:15+00:00
- **Authors**: Mustaffa Hussain, Ritesh Gangnani, Sasidhar Kadiyala
- **Comment**: 3 pages, 2 figures
- **Journal**: None
- **Summary**: Domain variability is a common bottle neck in developing generalisable algorithms for various medical applications. Motivated by the observation that the domain variability of the medical images is to some extent compact, we propose to learn a target representative feature space through unpaired image to image translation (CycleGAN). We comprehensively evaluate the performanceand usefulness by utilising the transformation to mitosis detection with candidate proposal and classification. This work presents a simple yet effective multi-step mitotic figure detection algorithm developed as a baseline for the MIDOG challenge. On the preliminary test set, the algorithm scoresan F1 score of 0.52.



### Effective Tensor Completion via Element-wise Weighted Low-rank Tensor Train with Overlapping Ket Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.05736v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05736v3)
- **Published**: 2021-09-13 06:50:37+00:00
- **Updated**: 2022-01-24 05:17:44+00:00
- **Authors**: Yang Zhang, Yao Wang, Zhi Han, Xi'ai Chen, Yandong Tang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, there have been an increasing number of applications of tensor completion based on the tensor train (TT) format because of its efficiency and effectiveness in dealing with higher-order tensor data. However, existing tensor completion methods using TT decomposition have two obvious drawbacks. One is that they only consider mode weights according to the degree of mode balance, even though some elements are recovered better in an unbalanced mode. The other is that serious blocking artifacts appear when the missing element rate is relatively large. To remedy such two issues, in this work, we propose a novel tensor completion approach via the element-wise weighted technique. Accordingly, a novel formulation for tensor completion and an effective optimization algorithm, called as tensor completion by parallel weighted matrix factorization via tensor train (TWMac-TT), is proposed. In addition, we specifically consider the recovery quality of edge elements from adjacent blocks. Different from traditional reshaping and ket augmentation, we utilize a new tensor augmentation technique called overlapping ket augmentation, which can further avoid blocking artifacts. We then conduct extensive performance evaluations on synthetic data and several real image data sets. Our experimental results demonstrate that the proposed algorithm TWMac-TT outperforms several other competing tensor completion methods.



### HCDG: A Hierarchical Consistency Framework for Domain Generalization on Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.05742v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05742v4)
- **Published**: 2021-09-13 07:07:23+00:00
- **Updated**: 2023-08-24 07:31:59+00:00
- **Authors**: Yijun Yang, Shujun Wang, Lei Zhu, Lequan Yu
- **Comment**: this paper is currently not published
- **Journal**: None
- **Summary**: Modern deep neural networks struggle to transfer knowledge and generalize across diverse domains when deployed to real-world applications. Currently, domain generalization (DG) is introduced to learn a universal representation from multiple domains to improve the network generalization ability on unseen domains. However, previous DG methods only focus on the data-level consistency scheme without considering the synergistic regularization among different consistency schemes. In this paper, we present a novel Hierarchical Consistency framework for Domain Generalization (HCDG) by integrating Extrinsic Consistency and Intrinsic Consistency synergistically. Particularly, for the Extrinsic Consistency, we leverage the knowledge across multiple source domains to enforce data-level consistency. To better enhance such consistency, we design a novel Amplitude Gaussian-mixing strategy into Fourier-based data augmentation called DomainUp. For the Intrinsic Consistency, we perform task-level consistency for the same instance under the dual-task scenario. We evaluate the proposed HCDG framework on two medical image segmentation tasks, i.e., optic cup/disc segmentation on fundus images and prostate MRI segmentation. Extensive experimental results manifest the effectiveness and versatility of our HCDG framework.



### Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation
- **Arxiv ID**: http://arxiv.org/abs/2109.05743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.05743v1)
- **Published**: 2021-09-13 07:08:46+00:00
- **Updated**: 2021-09-13 07:08:46+00:00
- **Authors**: Zechen Bai, Yuta Nakashima, Noa Garcia
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Have you ever looked at a painting and wondered what is the story behind it? This work presents a framework to bring art closer to people by generating comprehensive descriptions of fine-art paintings. Generating informative descriptions for artworks, however, is extremely challenging, as it requires to 1) describe multiple aspects of the image such as its style, content, or composition, and 2) provide background and contextual knowledge about the artist, their influences, or the historical period. To address these challenges, we introduce a multi-topic and knowledgeable art description framework, which modules the generated sentences according to three artistic topics and, additionally, enhances each description with external knowledge. The framework is validated through an exhaustive analysis, both quantitative and qualitative, as well as a comparative human evaluation, demonstrating outstanding results in terms of both topic diversity and information veracity.



### ChangeChip: A Reference-Based Unsupervised Change Detection for PCB Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.05746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05746v1)
- **Published**: 2021-09-13 07:10:07+00:00
- **Updated**: 2021-09-13 07:10:07+00:00
- **Authors**: Yehonatan Fridman, Matan Rusanovsky, Gal Oren
- **Comment**: 8 pages, 5 figures, " The sources of ChangeChip, as well as CD-PCB,
  are available at: https://github.com/Scientific-Computing-Lab-NRCN/ChangeChip
  "
- **Journal**: None
- **Summary**: The usage of electronic devices increases, and becomes predominant in most aspects of life. Surface Mount Technology (SMT) is the most common industrial method for manufacturing electric devices in which electrical components are mounted directly onto the surface of a Printed Circuit Board (PCB). Although the expansion of electronic devices affects our lives in a productive way, failures or defects in the manufacturing procedure of those devices might also be counterproductive and even harmful in some cases. It is therefore desired and sometimes crucial to ensure zero-defect quality in electronic devices and their production. While traditional Image Processing (IP) techniques are not sufficient to produce a complete solution, other promising methods like Deep Learning (DL) might also be challenging for PCB inspection, mainly because such methods require big adequate datasets which are missing, not available or not updated in the rapidly growing field of PCBs. Thus, PCB inspection is conventionally performed manually by human experts. Unsupervised Learning (UL) methods may potentially be suitable for PCB inspection, having learning capabilities on the one hand, while not relying on large datasets on the other. In this paper, we introduce ChangeChip, an automated and integrated change detection system for defect detection in PCBs, from soldering defects to missing or misaligned electronic elements, based on Computer Vision (CV) and UL. We achieve good quality defect detection by applying an unsupervised change detection between images of a golden PCB (reference) and the inspected PCB under various setting. In this work, we also present CD-PCB, a synthesized labeled dataset of 20 pairs of PCB images for evaluation of defect detection algorithms.



### Meta Navigator: Search for a Good Adaptation Policy for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.05749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05749v1)
- **Published**: 2021-09-13 07:20:01+00:00
- **Updated**: 2021-09-13 07:20:01+00:00
- **Authors**: Chi Zhang, Henghui Ding, Guosheng Lin, Ruibo Li, Changhu Wang, Chunhua Shen
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: Few-shot learning aims to adapt knowledge learned from previous tasks to novel tasks with only a limited amount of labeled data. Research literature on few-shot learning exhibits great diversity, while different algorithms often excel at different few-shot learning scenarios. It is therefore tricky to decide which learning strategies to use under different task conditions. Inspired by the recent success in Automated Machine Learning literature (AutoML), in this paper, we present Meta Navigator, a framework that attempts to solve the aforementioned limitation in few-shot learning by seeking a higher-level strategy and proffer to automate the selection from various few-shot learning designs. The goal of our work is to search for good parameter adaptation policies that are applied to different stages in the network for few-shot classification. We present a search space that covers many popular few-shot learning algorithms in the literature and develop a differentiable searching and decoding algorithm based on meta-learning that supports gradient-based optimization. We demonstrate the effectiveness of our searching-based method on multiple benchmark datasets. Extensive experiments show that our approach significantly outperforms baselines and demonstrates performance advantages over many state-of-the-art methods. Code and models will be made publicly available.



### Spatial-Separated Curve Rendering Network for Efficient and High-Resolution Image Harmonization
- **Arxiv ID**: http://arxiv.org/abs/2109.05750v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05750v4)
- **Published**: 2021-09-13 07:20:16+00:00
- **Updated**: 2021-11-30 15:23:46+00:00
- **Authors**: Jingtang Liang, Xiaodong Cun, Chi-Man Pun, Jue Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image harmonization aims to modify the color of the composited region with respect to the specific background. Previous works model this task as a pixel-wise image-to-image translation using UNet family structures. However, the model size and computational cost limit the ability of their models on edge devices and higher-resolution images. To this end, we propose a novel spatial-separated curve rendering network(S$^2$CRNet) for efficient and high-resolution image harmonization for the first time. In S$^2$CRNet, we firstly extract the spatial-separated embeddings from the thumbnails of the masked foreground and background individually. Then, we design a curve rendering module(CRM), which learns and combines the spatial-specific knowledge using linear layers to generate the parameters of the piece-wise curve mapping in the foreground region. Finally, we directly render the original high-resolution images using the learned color curve. Besides, we also make two extensions of the proposed framework via the Cascaded-CRM and Semantic-CRM for cascaded refinement and semantic guidance, respectively. Experiments show that the proposed method reduces more than 90% parameters compared with previous methods but still achieves the state-of-the-art performance on both synthesized iHarmony4 and real-world DIH test sets. Moreover, our method can work smoothly on higher resolution images(eg., $2048\times2048$) in 0.1 seconds with much lower GPU computational resources than all existing methods. The code will be made available at \url{http://github.com/stefanLeong/S2CRNet}.



### Adversarially Trained Object Detector for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.05751v2
- **DOI**: 10.1109/ACCESS.2022.3180344
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.05751v2)
- **Published**: 2021-09-13 07:21:28+00:00
- **Updated**: 2021-11-25 10:35:25+00:00
- **Authors**: Kazuma Fujii, Hiroshi Kera, Kazuhiko Kawamoto
- **Comment**: 10 pages, 6 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
- **Journal**: None
- **Summary**: Unsupervised domain adaptation, which involves transferring knowledge from a label-rich source domain to an unlabeled target domain, can be used to substantially reduce annotation costs in the field of object detection. In this study, we demonstrate that adversarial training in the source domain can be employed as a new approach for unsupervised domain adaptation. Specifically, we establish that adversarially trained detectors achieve improved detection performance in target domains that are significantly shifted from source domains. This phenomenon is attributed to the fact that adversarially trained detectors can be used to extract robust features that are in alignment with human perception and worth transferring across domains while discarding domain-specific non-robust features. In addition, we propose a method that combines adversarial training and feature alignment to ensure the improved alignment of robust features with the target domain. We conduct experiments on four benchmark datasets and confirm the effectiveness of our proposed approach on large domain shifts from real to artistic images. Compared to the baseline models, the adversarially trained detectors improve the mean average precision by up to 7.7%, and further by up to 11.8% when feature alignments are incorporated. Although our method degrades performance for small domain shifts, quantification of the domain shift based on the Frechet distance allows us to determine whether adversarial training should be conducted.



### Global-Local Dynamic Feature Alignment Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2109.05759v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05759v2)
- **Published**: 2021-09-13 07:53:36+00:00
- **Updated**: 2022-02-03 09:17:06+00:00
- **Authors**: Zhangqiang Ming, Yong Yang, Xiaoyong Wei, Jianrong Yan, Xiangkun Wang, Fengjie Wang, Min Zhu
- **Comment**: 28 pages, 8 figures
- **Journal**: None
- **Summary**: The misalignment of human images caused by bounding box detection errors or partial occlusions is one of the main challenges in person Re-Identification (Re-ID) tasks. Previous local-based methods mainly focus on learning local features in predefined semantic regions of pedestrians. These methods usually use local hard alignment methods or introduce auxiliary information such as key human pose points to match local features, which are often not applicable when large scene differences are encountered. To solve these problems, we propose a simple and efficient Local Sliding Alignment (LSA) strategy to dynamically align the local features of two images by setting a sliding window on the local stripes of the pedestrian. LSA can effectively suppress spatial misalignment and does not need to introduce extra supervision information. Then, we design a Global-Local Dynamic Feature Alignment Network (GLDFA-Net) framework, which contains both global and local branches. We introduce LSA into the local branch of GLDFA-Net to guide the computation of distance metrics, which can further improve the accuracy of the testing phase. Evaluation experiments on several mainstream evaluation datasets including Market-1501, DukeMTMC-reID, CUHK03 and MSMT17 show that our method has competitive accuracy over the several state-of-the-art person Re-ID methods. Specifically, it achieves 86.1% mAP and 94.8% Rank-1 accuracy on Market1501.



### Learnable Discrete Wavelet Pooling (LDW-Pooling) For Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.06638v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06638v4)
- **Published**: 2021-09-13 08:02:38+00:00
- **Updated**: 2021-10-20 07:24:13+00:00
- **Authors**: Bor-Shiun Wang, Jun-Wei Hsieh, Ming-Ching Chang, Ping-Yang Chen, Lipeng Ke, Siwei Lyu
- **Comment**: Accepted by BMVC 2021
- **Journal**: None
- **Summary**: Pooling is a simple but essential layer in modern deep CNN architectures for feature aggregation and extraction. Typical CNN design focuses on the conv layers and activation functions, while leaving the pooling layers with fewer options. We introduce the Learning Discrete Wavelet Pooling (LDW-Pooling) that can be applied universally to replace standard pooling operations to better extract features with improved accuracy and efficiency. Motivated from the wavelet theory, we adopt the low-pass (L) and high-pass (H) filters horizontally and vertically for pooling on a 2D feature map. Feature signals are decomposed into four (LL, LH, HL, HH) subbands to retain features better and avoid information dropping. The wavelet transform ensures features after pooling can be fully preserved and recovered. We next adopt an energy-based attention learning to fine-select crucial and representative features. LDW-Pooling is effective and efficient when compared with other state-of-the-art pooling techniques such as WaveletPooling and LiftPooling. Extensive experimental validation shows that LDW-Pooling can be applied to a wide range of standard CNN architectures and consistently outperform standard (max, mean, mixed, and stochastic) pooling operations.



### DHA: End-to-End Joint Optimization of Data Augmentation Policy, Hyper-parameter and Architecture
- **Arxiv ID**: http://arxiv.org/abs/2109.05765v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.05765v2)
- **Published**: 2021-09-13 08:12:50+00:00
- **Updated**: 2022-11-03 12:33:19+00:00
- **Authors**: Kaichen Zhou, Lanqing Hong, Shoukang Hu, Fengwei Zhou, Binxin Ru, Jiashi Feng, Zhenguo Li
- **Comment**: None
- **Journal**: Transactions on Machine Learning Research 2022
- **Summary**: Automated machine learning (AutoML) usually involves several crucial components, such as Data Augmentation (DA) policy, Hyper-Parameter Optimization (HPO), and Neural Architecture Search (NAS). Although many strategies have been developed for automating these components in separation, joint optimization of these components remains challenging due to the largely increased search dimension and the variant input types of each component. In parallel to this, the common practice of searching for the optimal architecture first and then retraining it before deployment in NAS often suffers from low performance correlation between the searching and retraining stages. An end-to-end solution that integrates the AutoML components and returns a ready-to-use model at the end of the search is desirable. In view of these, we propose DHA, which achieves joint optimization of Data augmentation policy, Hyper-parameter and Architecture. Specifically, end-to-end NAS is achieved in a differentiable manner by optimizing a compressed lower-dimensional feature space, while DA policy and HPO are regarded as dynamic schedulers, which adapt themselves to the update of network parameters and network architecture at the same time. Experiments show that DHA achieves state-of-the-art (SOTA) results on various datasets and search spaces. To the best of our knowledge, we are the first to efficiently and jointly optimize DA policy, NAS, and HPO in an end-to-end manner without retraining.



### Learning to Predict Diverse Human Motions from a Single Image via Mixture Density Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.05776v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05776v2)
- **Published**: 2021-09-13 08:49:33+00:00
- **Updated**: 2022-07-22 13:52:57+00:00
- **Authors**: Chunzhi Gu, Yan Zhao, Chao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion prediction, which plays a key role in computer vision, generally requires a past motion sequence as input. However, in real applications, a complete and correct past motion sequence can be too expensive to achieve. In this paper, we propose a novel approach to predicting future human motions from a much weaker condition, i.e., a single image, with mixture density networks (MDN) modeling. Contrary to most existing deep human motion prediction approaches, the multimodal nature of MDN enables the generation of diverse future motion hypotheses, which well compensates for the strong stochastic ambiguity aggregated by the single input and human motion uncertainty. In designing the loss function, we further introduce the energy-based formulation to flexibly impose prior losses over the learnable parameters of MDN to maintain motion coherence as well as improve the prediction accuracy by customizing the energy functions. Our trained model directly takes an image as input and generates multiple plausible motions that satisfy the given condition. Extensive experiments on two standard benchmark datasets demonstrate the effectiveness of our method in terms of prediction diversity and accuracy.



### Deep Joint Source-Channel Coding for Multi-Task Network
- **Arxiv ID**: http://arxiv.org/abs/2109.05779v2
- **DOI**: 10.1109/LSP.2021.3113827
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.05779v2)
- **Published**: 2021-09-13 08:58:02+00:00
- **Updated**: 2021-09-27 06:32:21+00:00
- **Authors**: Mengyang Wang, Zhicong Zhang, Jiahui Li, Mengyao Ma, Xiaopeng Fan
- **Comment**: Accpeted by IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: Multi-task learning (MTL) is an efficient way to improve the performance of related tasks by sharing knowledge. However, most existing MTL networks run on a single end and are not suitable for collaborative intelligence (CI) scenarios. In this work, we propose an MTL network with a deep joint source-channel coding (JSCC) framework, which allows operating under CI scenarios. We first propose a feature fusion based MTL network (FFMNet) for joint object detection and semantic segmentation. Compared with other MTL networks, FFMNet gets higher performance with fewer parameters. Then FFMNet is split into two parts, which run on a mobile device and an edge server respectively. The feature generated by the mobile device is transmitted through the wireless channel to the edge server. To reduce the transmission overhead of the intermediate feature, a deep JSCC network is designed. By combining two networks together, the whole model achieves 512x compression for the intermediate feature and a performance loss within 2% on both tasks. At last, by training with noise, the FFMNet with JSCC is robust to various channel conditions and outperforms the separate source and channel coding scheme.



### The State of the Art when using GPUs in Devising Image Generation Methods Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.05783v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.05783v1)
- **Published**: 2021-09-13 09:00:29+00:00
- **Updated**: 2021-09-13 09:00:29+00:00
- **Authors**: Yasuko Kawahata
- **Comment**: 13 pages
- **Journal**: Research Report (2016/03)
- **Summary**: Deep learning is a technique for machine learning using multi-layer neural networks. It has been used for image synthesis and image recognition, but in recent years, it has also been used for various social detection and social labeling. In this analysis, we compared (1) the number of Iterations per minute between the GPU and CPU when using the VGG model and the NIN model, and (2) the number of Iterations per minute by the number of pixels when using the VGG model, using an image with 128 pixels. When the number of pixels was 64 or 128, the processing time was almost the same when using the GPU, but when the number of pixels was changed to 256, the number of iterations per minute decreased and the processing time increased by about three times. In this case study, since the number of pixels becomes core dumping when the number of pixels is 512 or more, we can consider that we should consider improvement in the vector calculation part. If we aim to achieve 8K highly saturated computer graphics using neural networks, we will need to consider an environment that allows computation even when the size of the image becomes even more highly saturated and massive, and parallel computation when performing image recognition and tuning.



### DSNet: A Dual-Stream Framework for Weakly-Supervised Gigapixel Pathology Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2109.05788v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05788v2)
- **Published**: 2021-09-13 09:10:43+00:00
- **Updated**: 2022-03-09 08:15:53+00:00
- **Authors**: Tiange Xiang, Yang Song, Chaoyi Zhang, Dongnan Liu, Mei Chen, Fan Zhang, Heng Huang, Lauren O'Donnell, Weidong Cai
- **Comment**: IEEE TMI 2022
- **Journal**: None
- **Summary**: We present a novel weakly-supervised framework for classifying whole slide images (WSIs). WSIs, due to their gigapixel resolution, are commonly processed by patch-wise classification with patch-level labels. However, patch-level labels require precise annotations, which is expensive and usually unavailable on clinical data. With image-level labels only, patch-wise classification would be sub-optimal due to inconsistency between the patch appearance and image-level label. To address this issue, we posit that WSI analysis can be effectively conducted by integrating information at both high magnification (local) and low magnification (regional) levels. We auto-encode the visual signals in each patch into a latent embedding vector representing local information, and down-sample the raw WSI to hardware-acceptable thumbnails representing regional information. The WSI label is then predicted with a Dual-Stream Network (DSNet), which takes the transformed local patch embeddings and multi-scale thumbnail images as inputs and can be trained by the image-level label only. Experiments conducted on two large-scale public datasets demonstrate that our method outperforms all recent state-of-the-art weakly-supervised WSI classification methods.



### MLFW: A Database for Face Recognition on Masked Faces
- **Arxiv ID**: http://arxiv.org/abs/2109.05804v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05804v2)
- **Published**: 2021-09-13 09:30:10+00:00
- **Updated**: 2021-09-15 11:20:46+00:00
- **Authors**: Chengrui Wang, Han Fang, Yaoyao Zhong, Weihong Deng
- **Comment**: None
- **Journal**: None
- **Summary**: As more and more people begin to wear masks due to current COVID-19 pandemic, existing face recognition systems may encounter severe performance degradation when recognizing masked faces. To figure out the impact of masks on face recognition model, we build a simple but effective tool to generate masked faces from unmasked faces automatically, and construct a new database called Masked LFW (MLFW) based on Cross-Age LFW (CALFW) database. The mask on the masked face generated by our method has good visual consistency with the original face. Moreover, we collect various mask templates, covering most of the common styles appeared in the daily life, to achieve diverse generation effects. Considering realistic scenarios, we design three kinds of combinations of face pairs. The recognition accuracy of SOTA models declines 5%-16% on MLFW database compared with the accuracy on the original images. MLFW database can be viewed and downloaded at \url{http://whdeng.cn/mlfw}.



### Leveraging Clinical Characteristics for Improved Deep Learning-Based Kidney Tumor Segmentation on CT
- **Arxiv ID**: http://arxiv.org/abs/2109.05816v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.05816v1)
- **Published**: 2021-09-13 09:38:22+00:00
- **Updated**: 2021-09-13 09:38:22+00:00
- **Authors**: Christina B. Lund, Bas H. M. van der Velden
- **Comment**: Submitted to the Kidney and Kidney Tumor Segmentation Challenge
  (KiTS21), MICCAI 2021
- **Journal**: None
- **Summary**: This paper assesses whether using clinical characteristics in addition to imaging can improve automated segmentation of kidney cancer on contrast-enhanced computed tomography (CT). A total of 300 kidney cancer patients with contrast-enhanced CT scans and clinical characteristics were included. A baseline segmentation of the kidney cancer was performed using a 3D U-Net. Input to the U-Net were the contrast-enhanced CT images, output were segmentations of kidney, kidney tumors, and kidney cysts. A cognizant sampling strategy was used to leverage clinical characteristics for improved segmentation. To this end, a Least Absolute Shrinkage and Selection Operator (LASSO) was used. Segmentations were evaluated using Dice and Surface Dice. Improvement in segmentation was assessed using Wilcoxon signed rank test. The baseline 3D U-Net showed a segmentation performance of 0.90 for kidney and kidney masses, i.e., kidney, tumor, and cyst, 0.29 for kidney masses, and 0.28 for kidney tumor, while the 3D U-Net trained with cognizant sampling enhanced the segmentation performance and reached Dice scores of 0.90, 0.39, and 0.38 respectively. To conclude, the cognizant sampling strategy leveraging the clinical characteristics significantly improved kidney cancer segmentation.



### Self supervised learning improves dMMR/MSI detection from histology slides across multiple cancers
- **Arxiv ID**: http://arxiv.org/abs/2109.05819v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.05819v1)
- **Published**: 2021-09-13 09:43:12+00:00
- **Updated**: 2021-09-13 09:43:12+00:00
- **Authors**: Charlie Saillard, Olivier Dehaene, Tanguy Marchand, Olivier Moindrot, Aurlie Kamoun, Benoit Schmauch, Simon Jegou
- **Comment**: Accepted for poster and oral presentation at the MICCAI 2021 COMPAY
  Workshop (submitted the 19th of July 2021)
- **Journal**: Proceedings of the MICCAI Workshop on Computational Pathology,
  156, 2021, 191-205
- **Summary**: Microsatellite instability (MSI) is a tumor phenotype whose diagnosis largely impacts patient care in colorectal cancers (CRC), and is associated with response to immunotherapy in all solid tumors. Deep learning models detecting MSI tumors directly from H&E stained slides have shown promise in improving diagnosis of MSI patients. Prior deep learning models for MSI detection have relied on neural networks pretrained on ImageNet dataset, which does not contain any medical image. In this study, we leverage recent advances in self-supervised learning by training neural networks on histology images from the TCGA dataset using MoCo V2. We show that these networks consistently outperform their counterparts pretrained using ImageNet and obtain state-of-the-art results for MSI detection with AUCs of 0.92 and 0.83 for CRC and gastric tumors, respectively. These models generalize well on an external CRC cohort (0.97 AUC on PAIP) and improve transfer from one organ to another. Finally we show that predictive image regions exhibit meaningful histological patterns, and that the use of MoCo features highlighted more relevant patterns according to an expert pathologist.



### Improving the Robustness of Adversarial Attacks Using an Affine-Invariant Gradient Estimator
- **Arxiv ID**: http://arxiv.org/abs/2109.05820v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.05820v2)
- **Published**: 2021-09-13 09:43:17+00:00
- **Updated**: 2022-04-22 07:06:17+00:00
- **Authors**: Wenzhao Xiang, Hang Su, Chang Liu, Yandong Guo, Shibao Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: As designers of artificial intelligence try to outwit hackers, both sides continue to hone in on AI's inherent vulnerabilities. Designed and trained from certain statistical distributions of data, AI's deep neural networks (DNNs) remain vulnerable to deceptive inputs that violate a DNN's statistical, predictive assumptions. Before being fed into a neural network, however, most existing adversarial examples cannot maintain malicious functionality when applied to an affine transformation. For practical purposes, maintaining that malicious functionality serves as an important measure of the robustness of adversarial attacks. To help DNNs learn to defend themselves more thoroughly against attacks, we propose an affine-invariant adversarial attack, which can consistently produce more robust adversarial examples over affine transformations. For efficiency, we propose to disentangle current affine-transformation strategies from the Euclidean geometry coordinate plane with its geometric translations, rotations and dilations; we reformulate the latter two in polar coordinates. Afterwards, we construct an affine-invariant gradient estimator by convolving the gradient at the original image with derived kernels, which can be integrated with any gradient-based attack methods. Extensive experiments on ImageNet, including some experiments under physical condition, demonstrate that our method can significantly improve the affine invariance of adversarial examples and, as a byproduct, improve the transferability of adversarial examples, compared with alternative state-of-the-art methods.



### Variational Disentanglement for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2109.05826v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05826v3)
- **Published**: 2021-09-13 09:55:32+00:00
- **Updated**: 2023-05-16 10:28:02+00:00
- **Authors**: Yufei Wang, Haoliang Li, Hao Cheng, Bihan Wen, Lap-Pui Chau, Alex C. Kot
- **Comment**: Accepted to TMLR 2022
- **Journal**: None
- **Summary**: Domain generalization aims to learn an invariant model that can generalize well to the unseen target domain. In this paper, we propose to tackle the problem of domain generalization by delivering an effective framework named Variational Disentanglement Network (VDN), which is capable of disentangling the domain-specific features and task-specific features, where the task-specific features are expected to be better generalized to unseen but related test data. We further show the rationale of our proposed method by proving that our proposed framework is equivalent to minimize the evidence upper bound of the divergence between the distribution of task-specific features and its invariant ground truth derived from variational inference. We conduct extensive experiments to verify our method on three benchmarks, and both quantitative and qualitative results illustrate the effectiveness of our method.



### Adversarial Bone Length Attack on Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.05830v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.05830v2)
- **Published**: 2021-09-13 09:59:44+00:00
- **Updated**: 2022-03-25 10:21:50+00:00
- **Authors**: Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto
- **Comment**: 12 pages, 8 figures, accepted to AAAI2022
- **Journal**: None
- **Summary**: Skeleton-based action recognition models have recently been shown to be vulnerable to adversarial attacks. Compared to adversarial attacks on images, perturbations to skeletons are typically bounded to a lower dimension of approximately 100 per frame. This lower-dimensional setting makes it more difficult to generate imperceptible perturbations. Existing attacks resolve this by exploiting the temporal structure of the skeleton motion so that the perturbation dimension increases to thousands. In this paper, we show that adversarial attacks can be performed on skeleton-based action recognition models, even in a significantly low-dimensional setting without any temporal manipulation. Specifically, we restrict the perturbations to the lengths of the skeleton's bones, which allows an adversary to manipulate only approximately 30 effective dimensions. We conducted experiments on the NTU RGB+D and HDM05 datasets and demonstrate that the proposed attack successfully deceived models with sometimes greater than 90% success rate by small perturbations. Furthermore, we discovered an interesting phenomenon: in our low-dimensional setting, the adversarial training with the bone length attack shares a similar property with data augmentation, and it not only improves the adversarial robustness but also improves the classification accuracy on the original data. This is an interesting counterexample of the trade-off between adversarial robustness and clean accuracy, which has been widely observed in studies on adversarial training in the high-dimensional regime.



### IceNet for Interactive Contrast Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2109.05838v2
- **DOI**: 10.1109/ACCESS.2021.3137993
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.05838v2)
- **Published**: 2021-09-13 10:07:28+00:00
- **Updated**: 2021-12-25 13:04:10+00:00
- **Authors**: Keunsoo Ko, Chang-Su Kim
- **Comment**: 11 pages, 9 figures, 3 tables. This paper has been accepted for
  publication in IEEE Access. Copyright may change without notice
- **Journal**: None
- **Summary**: A CNN-based interactive contrast enhancement algorithm, called IceNet, is proposed in this work, which enables a user to adjust image contrast easily according to his or her preference. Specifically, a user provides a parameter for controlling the global brightness and two types of scribbles to darken or brighten local regions in an image. Then, given these annotations, IceNet estimates a gamma map for the pixel-wise gamma correction. Finally, through color restoration, an enhanced image is obtained. The user may provide annotations iteratively to obtain a satisfactory image. IceNet is also capable of producing a personalized enhanced image automatically, which can serve as a basis for further adjustment if so desired. Moreover, to train IceNet effectively and reliably, we propose three differentiable losses. Extensive experiments show that IceNet can provide users with satisfactorily enhanced images.



### Conditional MoCoGAN for Zero-Shot Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2109.05864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05864v1)
- **Published**: 2021-09-13 11:05:45+00:00
- **Updated**: 2021-09-13 11:05:45+00:00
- **Authors**: Shun Kimura, Kazuhiko Kawamoto
- **Comment**: 5 pages, 4 figure
- **Journal**: None
- **Summary**: We propose a conditional generative adversarial network (GAN) model for zero-shot video generation. In this study, we have explored zero-shot conditional generation setting. In other words, we generate unseen videos from training samples with missing classes. The task is an extension of conditional data generation. The key idea is to learn disentangled representations in the latent space of a GAN. To realize this objective, we base our model on the motion and content decomposed GAN and conditional GAN for image generation. We build the model to find better-disentangled representations and to generate good-quality videos. We demonstrate the effectiveness of our proposed model through experiments on the Weizmann action database and the MUG facial expression database.



### Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images
- **Arxiv ID**: http://arxiv.org/abs/2109.05885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05885v1)
- **Published**: 2021-09-13 11:44:07+00:00
- **Updated**: 2021-09-13 11:44:07+00:00
- **Authors**: Size Wu, Sheng Jin, Wentao Liu, Lei Bai, Chen Qian, Dong Liu, Wanli Ouyang
- **Comment**: Accepted by ICCV'2021
- **Journal**: None
- **Summary**: This paper studies the task of estimating the 3D human poses of multiple persons from multiple calibrated camera views. Following the top-down paradigm, we decompose the task into two stages, i.e. person localization and pose estimation. Both stages are processed in coarse-to-fine manners. And we propose three task-specific graph neural networks for effective message passing. For 3D person localization, we first use Multi-view Matching Graph Module (MMG) to learn the cross-view association and recover coarse human proposals. The Center Refinement Graph Module (CRG) further refines the results via flexible point-based prediction. For 3D pose estimation, the Pose Regression Graph Module (PRG) learns both the multi-view geometry and structural relations between human joints. Our approach achieves state-of-the-art performance on CMU Panoptic and Shelf datasets with significantly lower computation complexity.



### Nonlocal Patch-Based Fully-Connected Tensor Network Decomposition for Remote Sensing Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2109.05889v1
- **DOI**: 10.1109/LGRS.2021.3124804
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05889v1)
- **Published**: 2021-09-13 11:49:29+00:00
- **Updated**: 2021-09-13 11:49:29+00:00
- **Authors**: Wen-Jie Zheng, Xi-Le Zhao, Yu-Bang Zheng, Zhi-Feng Pang
- **Comment**: None
- **Journal**: IEEE Geoscience and Remote Sensing Letters, 2021
- **Summary**: Remote sensing image (RSI) inpainting plays an important role in real applications. Recently, fully-connected tensor network (FCTN) decomposition has been shown the remarkable ability to fully characterize the global correlation. Considering the global correlation and the nonlocal self-similarity (NSS) of RSIs, this paper introduces the FCTN decomposition to the whole RSI and its NSS groups, and proposes a novel nonlocal patch-based FCTN (NL-FCTN) decomposition for RSI inpainting. Different from other nonlocal patch-based methods, the NL-FCTN decomposition-based method, which increases tensor order by stacking similar small-sized patches to NSS groups, cleverly leverages the remarkable ability of FCTN decomposition to deal with higher-order tensors. Besides, we propose an efficient proximal alternating minimization-based algorithm to solve the proposed NL-FCTN decomposition-based model with a theoretical convergence guarantee. Extensive experiments on RSIs demonstrate that the proposed method achieves the state-of-the-art inpainting performance in all compared methods.



### WeakSTIL: Weak whole-slide image level stromal tumor infiltrating lymphocyte scores are all you need
- **Arxiv ID**: http://arxiv.org/abs/2109.05892v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.05892v1)
- **Published**: 2021-09-13 11:55:28+00:00
- **Updated**: 2021-09-13 11:55:28+00:00
- **Authors**: Yoni Schirris, Mendel Engelaer, Andreas Panteli, Hugo Mark Horlings, Efstratios Gavves, Jonas Teuwen
- **Comment**: 8 pages, 8 figures, 1 table, 4 pages supplementary
- **Journal**: None
- **Summary**: We present WeakSTIL, an interpretable two-stage weak label deep learning pipeline for scoring the percentage of stromal tumor infiltrating lymphocytes (sTIL%) in H&E-stained whole-slide images (WSIs) of breast cancer tissue. The sTIL% score is a prognostic and predictive biomarker for many solid tumor types. However, due to the high labeling efforts and high intra- and interobserver variability within and between expert annotators, this biomarker is currently not used in routine clinical decision making. WeakSTIL compresses tiles of a WSI using a feature extractor pre-trained with self-supervised learning on unlabeled histopathology data and learns to predict precise sTIL% scores for each tile in the tumor bed by using a multiple instance learning regressor that only requires a weak WSI-level label. By requiring only a weak label, we overcome the large annotation efforts required to train currently existing TIL detection methods. We show that WeakSTIL is at least as good as other TIL detection methods when predicting the WSI-level sTIL% score, reaching a coefficient of determination of $0.45\pm0.15$ when compared to scores generated by an expert pathologist, and an AUC of $0.89\pm0.05$ when treating it as the clinically interesting sTIL-high vs sTIL-low classification task. Additionally, we show that the intermediate tile-level predictions of WeakSTIL are highly interpretable, which suggests that WeakSTIL pays attention to latent features related to the number of TILs and the tissue type. In the future, WeakSTIL may be used to provide consistent and interpretable sTIL% predictions to stratify breast cancer patients into targeted therapy arms.



### Evolving Architectures with Gradient Misalignment toward Low Adversarial Transferability
- **Arxiv ID**: http://arxiv.org/abs/2109.05919v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2109.05919v1)
- **Published**: 2021-09-13 12:41:53+00:00
- **Updated**: 2021-09-13 12:41:53+00:00
- **Authors**: Kevin Richard G. Operiano, Wanchalerm Pora, Hitoshi Iba, Hiroshi Kera
- **Comment**: 23 pages, 4 figures
- **Journal**: None
- **Summary**: Deep neural network image classifiers are known to be susceptible not only to adversarial examples created for them but even those created for others. This phenomenon poses a potential security risk in various black-box systems relying on image classifiers. The reason behind such transferability of adversarial examples is not yet fully understood and many studies have proposed training methods to obtain classifiers with low transferability. In this study, we address this problem from a novel perspective through investigating the contribution of the network architecture to transferability. Specifically, we propose an architecture searching framework that employs neuroevolution to evolve network architectures and the gradient misalignment loss to encourage networks to converge into dissimilar functions after training. Our experiments show that the proposed framework successfully discovers architectures that reduce transferability from four standard networks including ResNet and VGG, while maintaining a good accuracy on unperturbed images. In addition, the evolved networks trained with gradient misalignment exhibit significantly lower transferability compared to standard networks trained with gradient misalignment, which indicates that the network architecture plays an important role in reducing transferability. This study demonstrates that designing or exploring proper network architectures is a promising approach to tackle the transferability issue and train adversarially robust image classifiers.



### Low-Light Image Enhancement with Normalizing Flow
- **Arxiv ID**: http://arxiv.org/abs/2109.05923v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.05923v1)
- **Published**: 2021-09-13 12:45:08+00:00
- **Updated**: 2021-09-13 12:45:08+00:00
- **Authors**: Yufei Wang, Renjie Wan, Wenhan Yang, Haoliang Li, Lap-Pui Chau, Alex C. Kot
- **Comment**: None
- **Journal**: None
- **Summary**: To enhance low-light images to normally-exposed ones is highly ill-posed, namely that the mapping relationship between them is one-to-many. Previous works based on the pixel-wise reconstruction losses and deterministic processes fail to capture the complex conditional distribution of normally exposed images, which results in improper brightness, residual noise, and artifacts. In this paper, we investigate to model this one-to-many relationship via a proposed normalizing flow model. An invertible network that takes the low-light images/features as the condition and learns to map the distribution of normally exposed images into a Gaussian distribution. In this way, the conditional distribution of the normally exposed images can be well modeled, and the enhancement process, i.e., the other inference direction of the invertible network, is equivalent to being constrained by a loss function that better describes the manifold structure of natural images during the training. The experimental results on the existing benchmark datasets show our method achieves better quantitative and qualitative results, obtaining better-exposed illumination, less noise and artifact, and richer colors.



### Vision-based system identification and 3D keypoint discovery using dynamics constraints
- **Arxiv ID**: http://arxiv.org/abs/2109.05928v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2109.05928v1)
- **Published**: 2021-09-13 12:51:14+00:00
- **Updated**: 2021-09-13 12:51:14+00:00
- **Authors**: Miguel Jaques, Martin Asenov, Michael Burke, Timothy Hospedales
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces V-SysId, a novel method that enables simultaneous keypoint discovery, 3D system identification, and extrinsic camera calibration from an unlabeled video taken from a static camera, using only the family of equations of motion of the object of interest as weak supervision. V-SysId takes keypoint trajectory proposals and alternates between maximum likelihood parameter estimation and extrinsic camera calibration, before applying a suitable selection criterion to identify the track of interest. This is then used to train a keypoint tracking model using supervised learning. Results on a range of settings (robotics, physics, physiology) highlight the utility of this approach.



### Task Guided Compositional Representation Learning for ZDA
- **Arxiv ID**: http://arxiv.org/abs/2109.05934v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05934v1)
- **Published**: 2021-09-13 13:02:20+00:00
- **Updated**: 2021-09-13 13:02:20+00:00
- **Authors**: Shuang Liu, Mete Ozay
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot domain adaptation (ZDA) methods aim to transfer knowledge about a task learned in a source domain to a target domain, while data from target domain are not available. In this work, we address learning feature representations which are invariant to and shared among different domains considering task characteristics for ZDA. To this end, we propose a method for task-guided ZDA (TG-ZDA) which employs multi-branch deep neural networks to learn feature representations exploiting their domain invariance and shareability properties. The proposed TG-ZDA models can be trained end-to-end without requiring synthetic tasks and data generated from estimated representations of target domains. The proposed TG-ZDA has been examined using benchmark ZDA tasks on image classification datasets. Experimental results show that our proposed TG-ZDA outperforms state-of-the-art ZDA methods for different domains and tasks.



### Balancing the Budget: Feature Selection and Tracking for Multi-Camera Visual-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/2109.05975v3
- **DOI**: 10.1109/LRA.2021.3137910
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.05975v3)
- **Published**: 2021-09-13 13:53:09+00:00
- **Updated**: 2022-05-12 13:19:46+00:00
- **Authors**: Lintong Zhang, David Wisth, Marco Camurri, Maurice Fallon
- **Comment**: Video at https://youtu.be/cLWeAT72e0U
- **Journal**: IEEE Robotics and Automation Letters ( Volume: 7, Issue: 2, April
  2022)
- **Summary**: We present a multi-camera visual-inertial odometry system based on factor graph optimization which estimates motion by using all cameras simultaneously while retaining a fixed overall feature budget. We focus on motion tracking in challenging environments, such as narrow corridors, dark spaces with aggressive motions, and abrupt lighting changes. These scenarios cause traditional monocular or stereo odometry to fail. While tracking motion with extra cameras should theoretically prevent failures, it leads to additional complexity and computational burden. To overcome these challenges, we introduce two novel methods to improve multi-camera feature tracking. First, instead of tracking features separately in each camera, we track features continuously as they move from one camera to another. This increases accuracy and achieves a more compact factor graph representation. Second, we select a fixed budget of tracked features across the cameras to reduce back-end optimization time. We have found that using a smaller set of informative features can maintain the same tracking accuracy. Our proposed method was extensively tested using a hardware-synchronized device consisting of an IMU and four cameras (a front stereo pair and two lateral) in scenarios including: an underground mine, large open spaces, and building interiors with narrow stairs and corridors. Compared to stereo-only state-of-the-art visual-inertial odometry methods, our approach reduces the drift rate, relative pose error, by up to 80% in translation and 39% in rotation.



### Mutual Supervision for Dense Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.05986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.05986v1)
- **Published**: 2021-09-13 14:04:13+00:00
- **Updated**: 2021-09-13 14:04:13+00:00
- **Authors**: Ziteng Gao, Limin Wang, Gangshan Wu
- **Comment**: ICCV 2021 camera ready version
- **Journal**: None
- **Summary**: The classification and regression head are both indispensable components to build up a dense object detector, which are usually supervised by the same training samples and thus expected to have consistency with each other for detecting objects accurately in the detection pipeline. In this paper, we break the convention of the same training samples for these two heads in dense detectors and explore a novel supervisory paradigm, termed as Mutual Supervision (MuSu), to respectively and mutually assign training samples for the classification and regression head to ensure this consistency. MuSu defines training samples for the regression head mainly based on classification predicting scores and in turn, defines samples for the classification head based on localization scores from the regression head. Experimental results show that the convergence of detectors trained by this mutual supervision is guaranteed and the effectiveness of the proposed method is verified on the challenging MS COCO benchmark. We also find that tiling more anchors at the same location benefits detectors and leads to further improvements under this training scheme. We hope this work can inspire further researches on the interaction of the classification and regression task in detection and the supervision paradigm for detectors, especially separately for these two heads.



### Learning to Ground Visual Objects for Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/2109.06013v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.06013v3)
- **Published**: 2021-09-13 14:48:44+00:00
- **Updated**: 2022-05-31 08:48:57+00:00
- **Authors**: Feilong Chen, Xiuyi Chen, Can Xu, Daxin Jiang
- **Comment**: Findings of the Association for Computational Linguistics: EMNLP 2021
- **Journal**: None
- **Summary**: Visual dialog is challenging since it needs to answer a series of coherent questions based on understanding the visual environment. How to ground related visual objects is one of the key problems. Previous studies utilize the question and history to attend to the image and achieve satisfactory performance, however these methods are not sufficient to locate related visual objects without any guidance. The inappropriate grounding of visual objects prohibits the performance of visual dialog models. In this paper, we propose a novel approach to Learn to Ground visual objects for visual dialog, which employs a novel visual objects grounding mechanism where both prior and posterior distributions over visual objects are used to facilitate visual objects grounding. Specifically, a posterior distribution over visual objects is inferred from both context (history and questions) and answers, and it ensures the appropriate grounding of visual objects during the training process. Meanwhile, a prior distribution, which is inferred from context only, is used to approximate the posterior distribution so that appropriate visual objects can be grounded even without answers during the inference process. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our approach improves the previous strong models in both generative and discriminative settings by a significant margin.



### Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting
- **Arxiv ID**: http://arxiv.org/abs/2109.06061v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06061v2)
- **Published**: 2021-09-13 15:29:03+00:00
- **Updated**: 2021-10-20 16:01:43+00:00
- **Authors**: Zian Wang, Jonah Philion, Sanja Fidler, Jan Kautz
- **Comment**: ICCV 2021 (Oral Presentation)
- **Journal**: None
- **Summary**: In this work, we address the problem of jointly estimating albedo, normals, depth and 3D spatially-varying lighting from a single image. Most existing methods formulate the task as image-to-image translation, ignoring the 3D properties of the scene. However, indoor scenes contain complex 3D light transport where a 2D representation is insufficient. In this paper, we propose a unified, learning-based inverse rendering framework that formulates 3D spatially-varying lighting. Inspired by classic volume rendering techniques, we propose a novel Volumetric Spherical Gaussian representation for lighting, which parameterizes the exitant radiance of the 3D scene surfaces on a voxel grid. We design a physics based differentiable renderer that utilizes our 3D lighting representation, and formulates the energy-conserving image formation process that enables joint training of all intrinsic properties with the re-rendering constraint. Our model ensures physically correct predictions and avoids the need for ground-truth HDR lighting which is not easily accessible. Experiments show that our method outperforms prior works both quantitatively and qualitatively, and is capable of producing photorealistic results for AR applications such as virtual object insertion even for highly specular objects.



### On Pursuit of Designing Multi-modal Transformer for Video Grounding
- **Arxiv ID**: http://arxiv.org/abs/2109.06085v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.06085v2)
- **Published**: 2021-09-13 16:01:19+00:00
- **Updated**: 2022-04-11 09:12:15+00:00
- **Authors**: Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang, Yuexian Zou
- **Comment**: Accepted by Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2021, Oral)
- **Journal**: None
- **Summary**: Video grounding aims to localize the temporal segment corresponding to a sentence query from an untrimmed video. Almost all existing video grounding methods fall into two frameworks: 1) Top-down model: It predefines a set of segment candidates and then conducts segment classification and regression. 2) Bottom-up model: It directly predicts frame-wise probabilities of the referential segment boundaries. However, all these methods are not end-to-end, i.e., they always rely on some time-consuming post-processing steps to refine predictions. To this end, we reformulate video grounding as a set prediction task and propose a novel end-to-end multi-modal Transformer model, dubbed as GTR. Specifically, GTR has two encoders for video and language encoding, and a cross-modal decoder for grounding prediction. To facilitate the end-to-end training, we use a Cubic Embedding layer to transform the raw videos into a set of visual tokens. To better fuse these two modalities in the decoder, we design a new Multi-head Cross-Modal Attention. The whole GTR is optimized via a Many-to-One matching loss. Furthermore, we conduct comprehensive studies to investigate different model design choices. Extensive results on three benchmarks have validated the superiority of GTR. All three typical GTR variants achieve record-breaking performance on all datasets and metrics, with several times faster inference speed.



### Single-stream CNN with Learnable Architecture for Multi-source Remote Sensing Data
- **Arxiv ID**: http://arxiv.org/abs/2109.06094v2
- **DOI**: 10.1109/TGRS.2022.3169163
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06094v2)
- **Published**: 2021-09-13 16:10:41+00:00
- **Updated**: 2022-02-07 04:49:08+00:00
- **Authors**: Yi Yang, Daoye Zhu, Tengteng Qu, Qiangyu Wang, Fuhu Ren, Chengqi Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an efficient and generalizable framework based on deep convolutional neural network (CNN) for multi-source remote sensing data joint classification. While recent methods are mostly based on multi-stream architectures, we use group convolution to construct equivalent network architectures efficiently within a single-stream network. We further adopt and improve dynamic grouping convolution (DGConv) to make group convolution hyperparameters, and thus the overall network architecture, learnable during network training. The proposed method therefore can theoretically adjust any modern CNN models to any multi-source remote sensing data set, and can potentially avoid sub-optimal solutions caused by manually decided architecture hyperparameters. In the experiments, the proposed method is applied to ResNet and UNet, and the adjusted networks are verified on three very diverse benchmark data sets (i.e., Houston2018 data, Berlin data, and MUUFL data). Experimental results demonstrate the effectiveness of the proposed single-stream CNNs, and in particular ResNet18-DGConv improves the state-of-the-art classification overall accuracy (OA) on HS-SAR Berlin data set from $62.23\%$ to $68.21\%$. In the experiments we have two interesting findings. First, using DGConv generally reduces test OA variance. Second, multi-stream is harmful to model performance if imposed to the first few layers, but becomes beneficial if applied to deeper layers. Altogether, the findings imply that multi-stream architecture, instead of being a strictly necessary component in deep learning models for multi-source remote sensing data, essentially plays the role of model regularizer. Our code is publicly available at https://github.com/yyyyangyi/Multi-source-RS-DGConv. We hope our work can inspire novel research in the future.



### The mathematics of adversarial attacks in AI -- Why deep learning is unstable despite the existence of stable neural networks
- **Arxiv ID**: http://arxiv.org/abs/2109.06098v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NA, math.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2109.06098v1)
- **Published**: 2021-09-13 16:19:25+00:00
- **Updated**: 2021-09-13 16:19:25+00:00
- **Authors**: Alexander Bastounis, Anders C Hansen, Verner Vlai
- **Comment**: 29 pages, 1 figure
- **Journal**: None
- **Summary**: The unprecedented success of deep learning (DL) makes it unchallenged when it comes to classification problems. However, it is well established that the current DL methodology produces universally unstable neural networks (NNs). The instability problem has caused an enormous research effort -- with a vast literature on so-called adversarial attacks -- yet there has been no solution to the problem. Our paper addresses why there has been no solution to the problem, as we prove the following mathematical paradox: any training procedure based on training neural networks for classification problems with a fixed architecture will yield neural networks that are either inaccurate or unstable (if accurate) -- despite the provable existence of both accurate and stable neural networks for the same classification problems. The key is that the stable and accurate neural networks must have variable dimensions depending on the input, in particular, variable dimensions is a necessary condition for stability.   Our result points towards the paradox that accurate and stable neural networks exist, however, modern algorithms do not compute them. This yields the question: if the existence of neural networks with desirable properties can be proven, can one also find algorithms that compute them? There are cases in mathematics where provable existence implies computability, but will this be the case for neural networks? The contrary is true, as we demonstrate how neural networks can provably exist as approximate minimisers to standard optimisation problems with standard cost functions, however, no randomised algorithm can compute them with probability better than 1/2.



### Weakly Supervised Person Search with Region Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.06109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06109v1)
- **Published**: 2021-09-13 16:33:27+00:00
- **Updated**: 2021-09-13 16:33:27+00:00
- **Authors**: Chuchu Han, Kai Su, Dongdong Yu, Zehuan Yuan, Changxin Gao, Nong Sang, Yi Yang, Changhu Wang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Supervised learning is dominant in person search, but it requires elaborate labeling of bounding boxes and identities. Large-scale labeled training data is often difficult to collect, especially for person identities. A natural question is whether a good person search model can be trained without the need of identity supervision. In this paper, we present a weakly supervised setting where only bounding box annotations are available. Based on this new setting, we provide an effective baseline model termed Region Siamese Networks (R-SiamNets). Towards learning useful representations for recognition in the absence of identity labels, we supervise the R-SiamNet with instance-level consistency loss and cluster-level contrastive loss. For instance-level consistency learning, the R-SiamNet is constrained to extract consistent features from each person region with or without out-of-region context. For cluster-level contrastive learning, we enforce the aggregation of closest instances and the separation of dissimilar ones in feature space. Extensive experiments validate the utility of our weakly supervised method. Our model achieves the rank-1 of 87.1% and mAP of 86.0% on CUHK-SYSU benchmark, which surpasses several fully supervised methods, such as OIM and MGTS, by a clear margin. More promising performance can be reached by incorporating extra training data. We hope this work could encourage the future research in this field.



### Blood vessel segmentation in en-face OCTA images: a frequency based method
- **Arxiv ID**: http://arxiv.org/abs/2109.06116v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06116v2)
- **Published**: 2021-09-13 16:42:58+00:00
- **Updated**: 2022-01-28 17:15:04+00:00
- **Authors**: Anna Breger, Felix Goldbach, Bianca S. Gerendas, Ursula Schmidt-Erfurth, Martin Ehler
- **Comment**: None
- **Journal**: None
- **Summary**: Optical coherence tomography angiography (OCTA) is a novel noninvasive imaging modality for visualization of retinal blood flow in the human retina. Using specific OCTA imaging biomarkers for the identification of pathologies, automated image segmentations of the blood vessels can improve subsequent analysis and diagnosis. We present a novel segmentation method for vessel density identification based on frequency representations of the image, in particular, using so-called Gabor filter banks. The algorithm is evaluated qualitatively and quantitatively on an OCTA image in-house data set from $10$ eyes acquired by a Cirrus HD-OCT device. Qualitatively, the segmentation outcomes received very good visual evaluation feedback by experts. Quantitatively, we compared the resulting vessel density values with automated in-built values provided by the device. The results underline the visual evaluation. For the evaluation of the FAZ identification substep, manual annotations of $2$ expert graders were used, showing that our results coincide well in visual and quantitative manners. Lastly, we suggest the computation of adaptive local vessel density maps that allow straightforward analysis of retinal blood flow in a local manner.



### Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2109.06122v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.06122v2)
- **Published**: 2021-09-13 16:56:43+00:00
- **Updated**: 2022-11-08 23:42:26+00:00
- **Authors**: Jihyung Kil, Cheng Zhang, Dong Xuan, Wei-Lun Chao
- **Comment**: Accepted to EMNLP 2021
- **Journal**: None
- **Summary**: Visual question answering (VQA) is challenging not only because the model has to handle multi-modal information, but also because it is just so hard to collect sufficient training examples -- there are too many questions one can ask about an image. As a result, a VQA model trained solely on human-annotated examples could easily over-fit specific question styles or image contents that are being asked, leaving the model largely ignorant about the sheer diversity of questions. Existing methods address this issue primarily by introducing an auxiliary task such as visual grounding, cycle consistency, or debiasing. In this paper, we take a drastically different approach. We found that many of the "unknowns" to the learned VQA model are indeed "known" in the dataset implicitly. For instance, questions asking about the same object in different images are likely paraphrases; the number of detected or annotated objects in an image already provides the answer to the "how many" question, even if the question has not been annotated for that image. Building upon these insights, we present a simple data augmentation pipeline SimpleAug to turn this "known" knowledge into training examples for VQA. We show that these augmented examples can notably improve the learned VQA models' performance, not only on the VQA-CP dataset with language prior shifts but also on the VQA v2 dataset without such shifts. Our method further opens up the door to leverage weakly-labeled or unlabeled images in a principled way to enhance VQA models. Our code and data are publicly available at https://github.com/heendung/simpleAUG.



### Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color
- **Arxiv ID**: http://arxiv.org/abs/2109.06129v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.06129v2)
- **Published**: 2021-09-13 17:09:40+00:00
- **Updated**: 2021-09-14 07:10:41+00:00
- **Authors**: Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, Anders Sgaard
- **Comment**: CoNLL 2021
- **Journal**: None
- **Summary**: Pretrained language models have been shown to encode relational information, such as the relations between entities or concepts in knowledge-bases -- (Paris, Capital, France). However, simple relations of this type can often be recovered heuristically and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric.   Using two methods of evaluating the structural alignment of colors in this space with text-derived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming. Further analysis suggests that differences in alignment are, in part, mediated by collocationality and differences in syntactic usage, posing questions as to the relationship between color perception and usage and context.



### DAFNe: A One-Stage Anchor-Free Approach for Oriented Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.06148v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.06148v4)
- **Published**: 2021-09-13 17:37:20+00:00
- **Updated**: 2022-05-30 13:36:56+00:00
- **Authors**: Steven Lang, Fabrizio Ventola, Kristian Kersting
- **Comment**: Main paper: 8 pages, References: 2 pages, Appendix: 7 pages; Main
  paper: 6 figures, Appendix: 6 figures
- **Journal**: None
- **Summary**: We present DAFNe, a Dense one-stage Anchor-Free deep Network for oriented object detection. As a one-stage model, it performs bounding box predictions on a dense grid over the input image, being architecturally simpler in design, as well as easier to optimize than its two-stage counterparts. Furthermore, as an anchor-free model, it reduces the prediction complexity by refraining from employing bounding box anchors. With DAFNe we introduce an orientation-aware generalization of the center-ness function for arbitrarily oriented bounding boxes to down-weight low-quality predictions and a center-to-corner bounding box prediction strategy that improves object localization performance. Our experiments show that DAFNe outperforms all previous one-stage anchor-free models on DOTA 1.0, DOTA 1.5, and UCAS-AOD and is on par with the best models on HRSC2016.



### Image Shape Manipulation from a Single Augmented Training Sample
- **Arxiv ID**: http://arxiv.org/abs/2109.06151v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06151v3)
- **Published**: 2021-09-13 17:44:04+00:00
- **Updated**: 2021-11-25 14:04:32+00:00
- **Authors**: Yael Vinker, Eliahu Horwitz, Nir Zabari, Yedid Hoshen
- **Comment**: The paper is available as arXiv:2007.01289 This paper has been
  withdrawn by the author due to duplication and merged with the existing
  submission. The project page for the paper is
  http://www.vision.huji.ac.il/deepsim/
- **Journal**: None
- **Summary**: In this paper, we present DeepSIM, a generative model for conditional image manipulation based on a single image. We find that extensive augmentation is key for enabling single image training, and incorporate the use of thin-plate-spline (TPS) as an effective augmentation. Our network learns to map between a primitive representation of the image to the image itself. The choice of a primitive representation has an impact on the ease and expressiveness of the manipulations and can be automatic (e.g. edges), manual (e.g. segmentation) or hybrid such as edges on top of segmentations. At manipulation time, our generator allows for making complex image changes by modifying the primitive input representation and mapping it through the network. Our method is shown to achieve remarkable performance on image manipulation tasks.



### Single-Stage Keypoint-Based Category-Level Object Pose Estimation from an RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2109.06161v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.06161v2)
- **Published**: 2021-09-13 17:55:00+00:00
- **Updated**: 2022-05-12 08:50:48+00:00
- **Authors**: Yunzhi Lin, Jonathan Tremblay, Stephen Tyree, Patricio A. Vela, Stan Birchfield
- **Comment**: ICRA 2022. Project page at https://sites.google.com/view/centerpose
- **Journal**: None
- **Summary**: Prior work on 6-DoF object pose estimation has largely focused on instance-level processing, in which a textured CAD model is available for each object being detected. Category-level 6-DoF pose estimation represents an important step toward developing robotic vision systems that operate in unstructured, real-world scenarios. In this work, we propose a single-stage, keypoint-based approach for category-level object pose estimation that operates on unknown object instances within a known category using a single RGB image as input. The proposed network performs 2D object detection, detects 2D keypoints, estimates 6-DoF pose, and regresses relative bounding cuboid dimensions. These quantities are estimated in a sequential fashion, leveraging the recent idea of convGRU for propagating information from easier tasks to those that are more difficult. We favor simplicity in our design choices: generic cuboid vertex coordinates, single-stage network, and monocular RGB input. We conduct extensive experiments on the challenging Objectron benchmark, outperforming state-of-the-art methods on the 3D IoU metric (27.6% higher than the MobilePose single-stage approach and 7.1% higher than the related two-stage approach).



### On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.06163v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06163v2)
- **Published**: 2021-09-13 17:57:24+00:00
- **Updated**: 2021-10-10 18:34:36+00:00
- **Authors**: Zhaoshuo Li, Nathan Drenkow, Hao Ding, Andy S. Ding, Alexander Lu, Francis X. Creighton, Russell H. Taylor, Mathias Unberath
- **Comment**: preprint
- **Journal**: None
- **Summary**: Scene depth estimation from stereo and monocular imagery is critical for extracting 3D information for downstream tasks such as scene understanding. Recently, learning-based methods for depth estimation have received much attention due to their high performance and flexibility in hardware choice. However, collecting ground truth data for supervised training of these algorithms is costly or outright impossible. This circumstance suggests a need for alternative learning approaches that do not require corresponding depth measurements. Indeed, self-supervised learning of depth estimation provides an increasingly popular alternative. It is based on the idea that observed frames can be synthesized from neighboring frames if accurate depth of the scene is known - or in this case, estimated. We show empirically that - contrary to common belief - improvements in image synthesis do not necessitate improvement in depth estimation. Rather, optimizing for image synthesis can result in diverging performance with respect to the main prediction objective - depth. We attribute this diverging phenomenon to aleatoric uncertainties, which originate from data. Based on our experiments on four datasets (spanning street, indoor, and medical) and five architectures (monocular and stereo), we conclude that this diverging phenomenon is independent of the dataset domain and not mitigated by commonly used regularization techniques. To underscore the importance of this finding, we include a survey of methods which use image synthesis, totaling 127 papers over the last six years. This observed divergence has not been previously reported or studied in depth, suggesting room for future improvement of self-supervised approaches which might be impacted the finding.



### CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.06165v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.06165v4)
- **Published**: 2021-09-13 17:59:07+00:00
- **Updated**: 2022-03-19 11:02:21+00:00
- **Authors**: Tongkun Xu, Weihua Chen, Pichao Wang, Fan Wang, Hao Li, Rong Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. Most existing UDA methods focus on learning domain-invariant feature representation, either from the domain level or category level, using convolution neural networks (CNNs)-based frameworks. One fundamental problem for the category level based UDA is the production of pseudo labels for samples in target domain, which are usually too noisy for accurate domain alignment, inevitably compromising the UDA performance. With the success of Transformer in various tasks, we find that the cross-attention in Transformer is robust to the noisy input pairs for better feature alignment, thus in this paper Transformer is adopted for the challenging UDA task. Specifically, to generate accurate input pairs, we design a two-way center-aware labeling algorithm to produce pseudo labels for target samples. Along with the pseudo labels, a weight-sharing triple-branch transformer framework is proposed to apply self-attention and cross-attention for source/target feature learning and source-target domain alignment, respectively. Such design explicitly enforces the framework to learn discriminative domain-specific and domain-invariant representations simultaneously. The proposed method is dubbed CDTrans (cross-domain transformer), and it provides one of the first attempts to solve UDA tasks with a pure transformer solution. Experiments show that our proposed method achieves the best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet. Code and models are available at https://github.com/CDTrans/CDTrans.



### Pose with Style: Detail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2109.06166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06166v1)
- **Published**: 2021-09-13 17:59:33+00:00
- **Updated**: 2021-09-13 17:59:33+00:00
- **Authors**: Badour AlBahar, Jingwan Lu, Jimei Yang, Zhixin Shu, Eli Shechtman, Jia-Bin Huang
- **Comment**: SIGGRAPH Asia 2021. Project page: https://pose-with-style.github.io/
- **Journal**: None
- **Summary**: We present an algorithm for re-rendering a person from a single image under arbitrary poses. Existing methods often have difficulties in hallucinating occluded contents photo-realistically while preserving the identity and fine details in the source image. We first learn to inpaint the correspondence field between the body surface texture and the source image with a human body symmetry prior. The inpainted correspondence field allows us to transfer/warp local features extracted from the source to the target view even under large pose changes. Directly mapping the warped local features to an RGB image using a simple CNN decoder often leads to visible artifacts. Thus, we extend the StyleGAN generator so that it takes pose as input (for controlling poses) and introduces a spatially varying modulation for the latent space using the warped local features (for controlling appearances). We show that our method compares favorably against the state-of-the-art algorithms in both quantitative evaluation and visual comparison.



### Incremental Abstraction in Distributed Probabilistic SLAM Graphs
- **Arxiv ID**: http://arxiv.org/abs/2109.06241v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.06241v2)
- **Published**: 2021-09-13 18:16:36+00:00
- **Updated**: 2022-04-04 22:37:16+00:00
- **Authors**: Joseph Ortiz, Talfan Evans, Edgar Sucar, Andrew J. Davison
- **Comment**: Published at ICRA 2022. Project page:
  https://joeaortiz.github.io/incremental_abstraction/
- **Journal**: None
- **Summary**: Scene graphs represent the key components of a scene in a compact and semantically rich way, but are difficult to build during incremental SLAM operation because of the challenges of robustly identifying abstract scene elements and optimising continually changing, complex graphs. We present a distributed, graph-based SLAM framework for incrementally building scene graphs based on two novel components. First, we propose an incremental abstraction framework in which a neural network proposes abstract scene elements that are incorporated into the factor graph of a feature-based monocular SLAM system. Scene elements are confirmed or rejected through optimisation and incrementally replace the points yielding a more dense, semantic and compact representation. Second, enabled by our novel routing procedure, we use Gaussian Belief Propagation (GBP) for distributed inference on a graph processor. The time per iteration of GBP is structure-agnostic and we demonstrate the speed advantages over direct methods for inference of heterogeneous factor graphs. We run our system on real indoor datasets using planar abstractions and recover the major planes with significant compression.



### Cross-Modality Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.06274v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06274v4)
- **Published**: 2021-09-13 19:24:15+00:00
- **Updated**: 2021-11-08 19:05:50+00:00
- **Authors**: Han Liu, Yubo Fan, Can Cui, Dingjie Su, Andrew McNeil, Benoit M. Dawant
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic methods to segment the vestibular schwannoma (VS) tumors and the cochlea from magnetic resonance imaging (MRI) are critical to VS treatment planning. Although supervised methods have achieved satisfactory performance in VS segmentation, they require full annotations by experts, which is laborious and time-consuming. In this work, we aim to tackle the VS and cochlea segmentation problem in an unsupervised domain adaptation setting. Our proposed method leverages both the image-level domain alignment to minimize the domain divergence and semi-supervised training to further boost the performance. Furthermore, we propose to fuse the labels predicted from multiple models via noisy label correction. Our results on the challenge validation leaderboard showed that our unsupervised method has achieved promising VS and cochlea segmentation performance with mean dice score of 0.8261 $\pm$ 0.0416; The mean dice value for the tumor is 0.8302 $\pm$ 0.0772. This is comparable to the weakly-supervised based method.



### MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks
- **Arxiv ID**: http://arxiv.org/abs/2109.06275v1
- **DOI**: 10.18653/v1/2021.emnlp-main.85
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.06275v1)
- **Published**: 2021-09-13 19:26:19+00:00
- **Updated**: 2021-09-13 19:26:19+00:00
- **Authors**: Cristian-Paul Bara, Sky CH-Wang, Joyce Chai
- **Comment**: None
- **Journal**: None
- **Summary**: An ideal integration of autonomous agents in a human world implies that they are able to collaborate on human terms. In particular, theory of mind plays an important role in maintaining common ground during human collaboration and communication. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners' beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks.



### Monocular Camera Localization for Automated Vehicles Using Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2109.06296v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2109.06296v1)
- **Published**: 2021-09-13 20:12:42+00:00
- **Updated**: 2021-09-13 20:12:42+00:00
- **Authors**: Eunhyek Joa, Francesco Borrelli
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of finding the current position and heading angle of an autonomous vehicle in real-time using a single camera. Compared to methods which require LiDARs and high definition (HD) 3D maps in real-time, the proposed approach is easily scalable and computationally efficient, at the price of lower precision.   The new method combines and adapts existing algorithms in three different fields: image retrieval, mapping database, and particle filtering. The result is a simple, real-time localization method using an image retrieval method whose performance is comparable to other monocular camera localization methods which use a map built with LiDARs.   We evaluate the proposed method using the KITTI odometry dataset and via closed-loop experiments with an indoor 1:10 autonomous vehicle. The tests demonstrate real-time capability and a 10cm level accuracy. Also, experimental results of the closed-loop indoor tests show the presence of a positive feedback loop between the localization error and the control error. Such phenomena is analysed in details at the end of the article.



### Physics Driven Domain Specific Transporter Framework with Attention Mechanism for Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/2109.06346v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.06346v1)
- **Published**: 2021-09-13 22:11:22+00:00
- **Updated**: 2021-09-13 22:11:22+00:00
- **Authors**: Arpan Tripathi, Abhilash Rakkunedeth, Mahesh Raveendranatha Panicker, Jack Zhang, Naveenjyote Boora, Jessica Knight, Jacob Jaremko, Yale Tung Chen, Kiran Vishnu Narayan, Kesavadas C
- **Comment**: 11 pages,18 figures(including supplementary material)
- **Journal**: None
- **Summary**: Most applications of deep learning techniques in medical imaging are supervised and require a large number of labeled data which is expensive and requires many hours of careful annotation by experts. In this paper, we propose an unsupervised, physics driven domain specific transporter framework with an attention mechanism to identify relevant key points with applications in ultrasound imaging. The proposed framework identifies key points that provide a concise geometric representation highlighting regions with high structural variation in ultrasound videos. We incorporate physics driven domain specific information as a feature probability map and use the radon transform to highlight features in specific orientations. The proposed framework has been trained on130 Lung ultrasound (LUS) videos and 113 Wrist ultrasound (WUS) videos and validated on 100 Lung ultrasound (LUS) videos and 58 Wrist ultrasound (WUS) videos acquired from multiple centers across the globe. Images from both datasets were independently assessed by experts to identify clinically relevant features such as A-lines, B-lines and pleura from LUS and radial metaphysis, radial epiphysis and carpal bones from WUS videos. The key points detected from both datasets showed high sensitivity (LUS = 99\% , WUS = 74\%) in detecting the image landmarks identified by experts. Also, on employing for classification of the given lung image into normal and abnormal classes, the proposed approach, even with no prior training, achieved an average accuracy of 97\% and an average F1-score of 95\% respectively on the task of co-classification with 3 fold cross-validation. With the purely unsupervised nature of the proposed approach, we expect the key point detection approach to increase the applicability of ultrasound in various examination performed in emergency and point of care.



### POPCORN: Progressive Pseudo-labeling with Consistency Regularization and Neighboring
- **Arxiv ID**: http://arxiv.org/abs/2109.06361v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06361v1)
- **Published**: 2021-09-13 23:36:36+00:00
- **Updated**: 2021-09-13 23:36:36+00:00
- **Authors**: Reda Abdellah Kamraoui, Vinh-Thong Ta, Nicolas Papadakis, Fanny Compaire, Jos V Manjon, Pierrick Coup
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) uses unlabeled data to compensate for the scarcity of annotated images and the lack of method generalization to unseen domains, two usual problems in medical segmentation tasks. In this work, we propose POPCORN, a novel method combining consistency regularization and pseudo-labeling designed for image segmentation. The proposed framework uses high-level regularization to constrain our segmentation model to use similar latent features for images with similar segmentations. POPCORN estimates a proximity graph to select data from easiest ones to more difficult ones, in order to ensure accurate pseudo-labeling and to limit confirmation bias. Applied to multiple sclerosis lesion segmentation, our method demonstrates competitive results compared to other state-of-the-art SSL strategies.



### Sensor Adversarial Traits: Analyzing Robustness of 3D Object Detection Sensor Fusion Models
- **Arxiv ID**: http://arxiv.org/abs/2109.06363v1
- **DOI**: 10.1109/ICIP42928.2021.9506183
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.06363v1)
- **Published**: 2021-09-13 23:38:42+00:00
- **Updated**: 2021-09-13 23:38:42+00:00
- **Authors**: Won Park, Nan Liu, Qi Alfred Chen, Z. Morley Mao
- **Comment**: None
- **Journal**: 2021 IEEE International Conference on Image Processing (ICIP),
  2021, pp. 484-488
- **Summary**: A critical aspect of autonomous vehicles (AVs) is the object detection stage, which is increasingly being performed with sensor fusion models: multimodal 3D object detection models which utilize both 2D RGB image data and 3D data from a LIDAR sensor as inputs. In this work, we perform the first study to analyze the robustness of a high-performance, open source sensor fusion model architecture towards adversarial attacks and challenge the popular belief that the use of additional sensors automatically mitigate the risk of adversarial attacks. We find that despite the use of a LIDAR sensor, the model is vulnerable to our purposefully crafted image-based adversarial attacks including disappearance, universal patch, and spoofing. After identifying the underlying reason, we explore some potential defenses and provide some recommendations for improved sensor fusion models.



### From Heatmaps to Structural Explanations of Image Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2109.06365v1
- **DOI**: 10.1002/ail2.46
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.06365v1)
- **Published**: 2021-09-13 23:39:57+00:00
- **Updated**: 2021-09-13 23:39:57+00:00
- **Authors**: Li Fuxin, Zhongang Qi, Saeed Khorram, Vivswan Shitole, Prasad Tadepalli, Minsuk Kahng, Alan Fern
- **Comment**: Submitted to Applied AI Letters
- **Journal**: Applied AI Letters.2021;2:e46
- **Summary**: This paper summarizes our endeavors in the past few years in terms of explaining image classifiers, with the aim of including negative results and insights we have gained. The paper starts with describing the explainable neural network (XNN), which attempts to extract and visualize several high-level concepts purely from the deep network, without relying on human linguistic concepts. This helps users understand network classifications that are less intuitive and substantially improves user performance on a difficult fine-grained classification task of discriminating among different species of seagulls.   Realizing that an important missing piece is a reliable heatmap visualization tool, we have developed I-GOS and iGOS++ utilizing integrated gradients to avoid local optima in heatmap generation, which improved the performance across all resolutions. During the development of those visualizations, we realized that for a significant number of images, the classifier has multiple different paths to reach a confident prediction. This has lead to our recent development of structured attention graphs (SAGs), an approach that utilizes beam search to locate multiple coarse heatmaps for a single image, and compactly visualizes a set of heatmaps by capturing how different combinations of image regions impact the confidence of a classifier.   Through the research process, we have learned much about insights in building deep network explanations, the existence and frequency of multiple explanations, and various tricks of the trade that make explanations work. In this paper, we attempt to share those insights and opinions with the readers with the hope that some of them will be informative for future researchers on explainable deep learning.




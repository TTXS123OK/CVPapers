# Arxiv Papers in cs.CV on 2021-06-16
### Multi-scale Neural ODEs for 3D Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2106.08493v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08493v2)
- **Published**: 2021-06-16 00:26:53+00:00
- **Updated**: 2021-06-17 21:05:42+00:00
- **Authors**: Junshen Xu, Eric Z. Chen, Xiao Chen, Terrence Chen, Shanhui Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration plays an important role in medical image analysis. Conventional optimization based methods provide an accurate estimation due to the iterative process at the cost of expensive computation. Deep learning methods such as learn-to-map are much faster but either iterative or coarse-to-fine approach is required to improve accuracy for handling large motions. In this work, we proposed to learn a registration optimizer via a multi-scale neural ODE model. The inference consists of iterative gradient updates similar to a conventional gradient descent optimizer but in a much faster way, because the neural ODE learns from the training data to adapt the gradient efficiently at each iteration. Furthermore, we proposed to learn a modal-independent similarity metric to address image appearance variations across different image contrasts. We performed evaluations through extensive experiments in the context of multi-contrast 3D MR images from both public and private data sources and demonstrate the superior performance of our proposed methods.



### GKNet: grasp keypoint network for grasp candidates detection
- **Arxiv ID**: http://arxiv.org/abs/2106.08497v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08497v3)
- **Published**: 2021-06-16 00:34:55+00:00
- **Updated**: 2021-12-15 00:09:26+00:00
- **Authors**: Ruinian Xu, Fu-Jen Chu, Patricio A. Vela
- **Comment**: 25 pages, 12 figures, 14 tables
- **Journal**: None
- **Summary**: Contemporary grasp detection approaches employ deep learning to achieve robustness to sensor and object model uncertainty. The two dominant approaches design either grasp-quality scoring or anchor-based grasp recognition networks. This paper presents a different approach to grasp detection by treating it as keypoint detection in image-space. The deep network detects each grasp candidate as a pair of keypoints, convertible to the grasp representationg = {x, y, w, {\theta}} T , rather than a triplet or quartet of corner points. Decreasing the detection difficulty by grouping keypoints into pairs boosts performance. To promote capturing dependencies between keypoints, a non-local module is incorporated into the network design. A final filtering strategy based on discrete and continuous orientation prediction removes false correspondences and further improves grasp detection performance. GKNet, the approach presented here, achieves a good balance between accuracy and speed on the Cornell and the abridged Jacquard datasets (96.9% and 98.39% at 41.67 and 23.26 fps). Follow-up experiments on a manipulator evaluate GKNet using 4 types of grasping experiments reflecting different nuisance sources: static grasping, dynamic grasping, grasping at varied camera angles, and bin picking. GKNet outperforms reference baselines in static and dynamic grasping experiments while showing robustness to varied camera viewpoints and moderate clutter. The results confirm the hypothesis that grasp keypoints are an effective output representation for deep grasp networks that provide robustness to expected nuisance factors.



### ICDAR 2021 Competition on Components Segmentation Task of Document Photos
- **Arxiv ID**: http://arxiv.org/abs/2106.08499v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.08499v2)
- **Published**: 2021-06-16 00:49:58+00:00
- **Updated**: 2021-07-09 01:40:34+00:00
- **Authors**: Celso A. M. Lopes Junior, Ricardo B. das Neves Junior, Byron L. D. Bezerra, Alejandro H. Toselli, Donato Impedovo
- **Comment**: 15 pages; 5 figures; Accepted at ICDAR 2021: 16th International
  Conference on Document Analysis and Recognition
- **Journal**: None
- **Summary**: This paper describes the short-term competition on the Components Segmentation Task of Document Photos that was prepared in the context of the 16th International Conference on Document Analysis and Recognition (ICDAR 2021). This competition aims to bring together researchers working in the field of identification document image processing and provides them a suitable benchmark to compare their techniques on the component segmentation task of document images. Three challenge tasks were proposed entailing different segmentation assignments to be performed on a provided dataset. The collected data are from several types of Brazilian ID documents, whose personal information was conveniently replaced. There were 16 participants whose results obtained for some or all the three tasks show different rates for the adopted metrics, like Dice Similarity Coefficient ranging from 0.06 to 0.99. Different Deep Learning models were applied by the entrants with diverse strategies to achieve the best results in each of the tasks. Obtained results show that the currently applied methods for solving one of the proposed tasks (document boundary detection) are already well established. However, for the other two challenge tasks (text zone and handwritten sign detection) research and development of more robust approaches are still required to achieve acceptable results.



### Understanding and Evaluating Racial Biases in Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2106.08503v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08503v2)
- **Published**: 2021-06-16 01:07:24+00:00
- **Updated**: 2021-08-30 15:07:38+00:00
- **Authors**: Dora Zhao, Angelina Wang, Olga Russakovsky
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Image captioning is an important task for benchmarking visual reasoning and for enabling accessibility for people with vision impairments. However, as in many machine learning settings, social biases can influence image captioning in undesirable ways. In this work, we study bias propagation pathways within image captioning, focusing specifically on the COCO dataset. Prior work has analyzed gender bias in captions using automatically-derived gender labels; here we examine racial and intersectional biases using manual annotations. Our first contribution is in annotating the perceived gender and skin color of 28,315 of the depicted people after obtaining IRB approval. Using these annotations, we compare racial biases present in both manual and automatically-generated image captions. We demonstrate differences in caption performance, sentiment, and word choice between images of lighter versus darker-skinned people. Further, we find the magnitude of these differences to be greater in modern captioning systems compared to older ones, thus leading to concerns that without proper consideration and mitigation these differences will only become increasingly prevalent. Code and data is available at https://princetonvisualai.github.io/imagecaptioning-bias .



### Dynamically Grown Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.08505v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08505v1)
- **Published**: 2021-06-16 01:25:51+00:00
- **Updated**: 2021-06-16 01:25:51+00:00
- **Authors**: Lanlan Liu, Yuting Zhang, Jia Deng, Stefano Soatto
- **Comment**: Accepted to AAAI 2021
- **Journal**: None
- **Summary**: Recent work introduced progressive network growing as a promising way to ease the training for large GANs, but the model design and architecture-growing strategy still remain under-explored and needs manual design for different image data. In this paper, we propose a method to dynamically grow a GAN during training, optimizing the network architecture and its parameters together with automation. The method embeds architecture search techniques as an interleaving step with gradient-based training to periodically seek the optimal architecture-growing strategy for the generator and discriminator. It enjoys the benefits of both eased training because of progressive growing and improved performance because of broader architecture design space. Experimental results demonstrate new state-of-the-art of image generation. Observations in the search procedure also provide constructive insights into the GAN model design such as generator-discriminator balance and convolutional layer choices.



### Revisit Visual Representation in Analytics Taxonomy: A Compression Perspective
- **Arxiv ID**: http://arxiv.org/abs/2106.08512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08512v1)
- **Published**: 2021-06-16 01:44:32+00:00
- **Updated**: 2021-06-16 01:44:32+00:00
- **Authors**: Yueyu Hu, Wenhan Yang, Haofeng Huang, Jiaying Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual analytics have played an increasingly critical role in the Internet of Things, where massive visual signals have to be compressed and fed into machines. But facing such big data and constrained bandwidth capacity, existing image/video compression methods lead to very low-quality representations, while existing feature compression techniques fail to support diversified visual analytics applications/tasks with low-bit-rate representations. In this paper, we raise and study the novel problem of supporting multiple machine vision analytics tasks with the compressed visual representation, namely, the information compression problem in analytics taxonomy. By utilizing the intrinsic transferability among different tasks, our framework successfully constructs compact and expressive representations at low bit-rates to support a diversified set of machine vision tasks, including both high-level semantic-related tasks and mid-level geometry analytic tasks. In order to impose compactness in the representations, we propose a codebook-based hyperprior, which helps map the representation into a low-dimensional manifold. As it well fits the signal structure of the deep visual feature, it facilitates more accurate entropy estimation, and results in higher compression efficiency. With the proposed framework and the codebook-based hyperprior, we further investigate the relationship of different task features owning different levels of abstraction granularity. Experimental results demonstrate that with the proposed scheme, a set of diversified tasks can be supported at a significantly lower bit-rate, compared with existing compression schemes.



### Watching Too Much Television is Good: Self-Supervised Audio-Visual Representation Learning from Movies and TV Shows
- **Arxiv ID**: http://arxiv.org/abs/2106.08513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08513v1)
- **Published**: 2021-06-16 02:00:11+00:00
- **Updated**: 2021-06-16 02:00:11+00:00
- **Authors**: Mahdi M. Kalayeh, Nagendra Kamath, Lingyi Liu, Ashok Chandrashekar
- **Comment**: None
- **Journal**: None
- **Summary**: The abundance and ease of utilizing sound, along with the fact that auditory clues reveal so much about what happens in the scene, make the audio-visual space a perfectly intuitive choice for self-supervised representation learning. However, the current literature suggests that training on \textit{uncurated} data yields considerably poorer representations compared to the \textit{curated} alternatives collected in supervised manner, and the gap only narrows when the volume of data significantly increases. Furthermore, the quality of learned representations is known to be heavily influenced by the size and taxonomy of the curated datasets used for self-supervised training. This begs the question of whether we are celebrating too early on catching up with supervised learning when our self-supervised efforts still rely almost exclusively on curated data. In this paper, we study the efficacy of learning from Movies and TV Shows as forms of uncurated data for audio-visual self-supervised learning. We demonstrate that a simple model based on contrastive learning, trained on a collection of movies and TV shows, not only dramatically outperforms more complex methods which are trained on orders of magnitude larger uncurated datasets, but also performs very competitively with the state-of-the-art that learns from large-scale curated data. We identify that audiovisual patterns like the appearance of the main character or prominent scenes and mise-en-sc\`ene which frequently occur through the whole duration of a movie, lead to an overabundance of easy negative instances in the contrastive learning formulation. Capitalizing on such observation, we propose a hierarchical sampling policy, which despite its simplicity, effectively improves the performance, particularly when learning from TV shows which naturally face less semantic diversity.



### ECKPN: Explicit Class Knowledge Propagation Network for Transductive Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.08523v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.08523v1)
- **Published**: 2021-06-16 02:29:43+00:00
- **Updated**: 2021-06-16 02:29:43+00:00
- **Authors**: Chaofan Chen, Xiaoshan Yang, Changsheng Xu, Xuhui Huang, Zhe Ma
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Recently, the transductive graph-based methods have achieved great success in the few-shot classification task. However, most existing methods ignore exploring the class-level knowledge that can be easily learned by humans from just a handful of samples. In this paper, we propose an Explicit Class Knowledge Propagation Network (ECKPN), which is composed of the comparison, squeeze and calibration modules, to address this problem. Specifically, we first employ the comparison module to explore the pairwise sample relations to learn rich sample representations in the instance-level graph. Then, we squeeze the instance-level graph to generate the class-level graph, which can help obtain the class-level visual knowledge and facilitate modeling the relations of different classes. Next, the calibration module is adopted to characterize the relations of the classes explicitly to obtain the more discriminative class-level knowledge representations. Finally, we combine the class-level knowledge with the instance-level sample representations to guide the inference of the query samples. We conduct extensive experiments on four few-shot classification benchmarks, and the experimental results show that the proposed ECKPN significantly outperforms the state-of-the-art methods.



### Listen to Your Favorite Melodies with img2Mxml, Producing MusicXML from Sheet Music Image by Measure-based Multimodal Deep Learning-driven Assembly
- **Arxiv ID**: http://arxiv.org/abs/2106.12037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.12037v1)
- **Published**: 2021-06-16 03:35:33+00:00
- **Updated**: 2021-06-16 03:35:33+00:00
- **Authors**: Tomoyuki Shishido, Fehmiju Fati, Daisuke Tokushige, Yasuhiro Ono
- **Comment**: 19 pages, 7 figures
- **Journal**: None
- **Summary**: Deep learning has recently been applied to optical music recognition (OMR). However, currently OMR processing from various sheet music images still lacks precision to be widely applicable. Here, we present an MMdA (Measure-based Multimodal deep learning (DL)-driven Assembly) method allowing for end-to-end OMR processing from various images including inclined photo images. Using this method, measures are extracted by a deep learning model, aligned, and resized to be used for inference of given musical symbol components by using multiple deep learning models in sequence or in parallel. Use of each standardized measure enables efficient training of the models and accurate adjustment of five staff lines in each measure. Multiple musical symbol component category models with a small number of feature types can represent a diverse set of notes and other musical symbols including chords. This MMdA method provides a solution to end-to-end OMR processing with precision.



### Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions
- **Arxiv ID**: http://arxiv.org/abs/2106.08543v3
- **DOI**: 10.1109/TNNLS.2022.3159990
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08543v3)
- **Published**: 2021-06-16 03:58:21+00:00
- **Updated**: 2022-04-12 09:13:50+00:00
- **Authors**: Sangmin Woo, Junhyug Noh, Kangil Kim
- **Comment**: IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
- **Journal**: None
- **Summary**: In this work, we seek new insights into the underlying challenges of the Scene Graph Generation (SGG) task. Quantitative and qualitative analysis of the Visual Genome dataset implies -- 1) Ambiguity: even if inter-object relationship contains the same object (or predicate), they may not be visually or semantically similar, 2) Asymmetry: despite the nature of the relationship that embodied the direction, it was not well addressed in previous studies, and 3) Higher-order contexts: leveraging the identities of certain graph elements can help to generate accurate scene graphs. Motivated by the analysis, we design a novel SGG framework, Local-to-Global Interaction Networks (LOGIN). Locally, interactions extract the essence between three instances of subject, object, and background, while baking direction awareness into the network by explicitly constraining the input order of subject and object. Globally, interactions encode the contexts between every graph component (i.e., nodes and edges). Finally, Attract & Repel loss is utilized to fine-tune the distribution of predicate embeddings. By design, our framework enables predicting the scene graph in a bottom-up manner, leveraging the possible complementariness. To quantify how much LOGIN is aware of relational direction, a new diagnostic task called Bidirectional Relationship Classification (BRC) is also proposed. Experimental results demonstrate that LOGIN can successfully distinguish relational direction than existing methods (in BRC task), while showing state-of-the-art results on the Visual Genome benchmark (in SGG task).



### Unsupervised-learning-based method for chest MRI-CT transformation using structure constrained unsupervised generative attention networks
- **Arxiv ID**: http://arxiv.org/abs/2106.08557v2
- **DOI**: 10.1038/s41598-022-14677-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08557v2)
- **Published**: 2021-06-16 05:22:27+00:00
- **Updated**: 2022-08-05 01:29:25+00:00
- **Authors**: Hidetoshi Matsuo, Mizuho Nishio, Munenobu Nogami, Feibi Zeng, Takako Kurimoto, Sandeep Kaushik, Florian Wiesinger, Atsushi K Kono, Takamichi Murakami
- **Comment**: 27 pages, 12 figures
- **Journal**: Sci Rep 12, 11090 (2022)
- **Summary**: The integrated positron emission tomography/magnetic resonance imaging (PET/MRI) scanner facilitates the simultaneous acquisition of metabolic information via PET and morphological information with high soft-tissue contrast using MRI. Although PET/MRI facilitates the capture of high-accuracy fusion images, its major drawback can be attributed to the difficulty encountered when performing attenuation correction, which is necessary for quantitative PET evaluation. The combined PET/MRI scanning requires the generation of attenuation-correction maps from MRI owing to no direct relationship between the gamma-ray attenuation information and MRIs. While MRI-based bone-tissue segmentation can be readily performed for the head and pelvis regions, the realization of accurate bone segmentation via chest CT generation remains a challenging task. This can be attributed to the respiratory and cardiac motions occurring in the chest as well as its anatomically complicated structure and relatively thin bone cortex. This paper presents a means to minimise the anatomical structural changes without human annotation by adding structural constraints using a modality-independent neighbourhood descriptor (MIND) to a generative adversarial network (GAN) that can transform unpaired images. The results obtained in this study revealed the proposed U-GAT-IT + MIND approach to outperform all other competing approaches. The findings of this study hint towards possibility of synthesising clinically acceptable CT images from chest MRI without human annotation, thereby minimising the changes in the anatomical structure.



### Detection of Morphed Face Images Using Discriminative Wavelet Sub-bands
- **Arxiv ID**: http://arxiv.org/abs/2106.08565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08565v1)
- **Published**: 2021-06-16 06:03:08+00:00
- **Updated**: 2021-06-16 06:03:08+00:00
- **Authors**: Poorya Aghdaie, Baaria Chaudhary, Sobhan Soleymani, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: This work investigates the well-known problem of morphing attacks, which has drawn considerable attention in the biometrics community. Morphed images have exposed face recognition systems' susceptibility to false acceptance, resulting in dire consequences, especially for national security applications. To detect morphing attacks, we propose a method which is based on a discriminative 2D Discrete Wavelet Transform (2D-DWT). A discriminative wavelet sub-band can highlight inconsistencies between a real and a morphed image. We observe that there is a salient discrepancy between the entropy of a given sub-band in a bona fide image, and the same sub-band's entropy in a morphed sample. Considering this dissimilarity between these two entropy values, we find the Kullback-Leibler divergence between the two distributions, namely the entropy of the bona fide and the corresponding morphed images. The most discriminative wavelet sub-bands are those with the highest corresponding KL-divergence values. Accordingly, 22 sub-bands are selected as the most discriminative ones in terms of morph detection. We show that a Deep Neural Network (DNN) trained on the 22 discriminative sub-bands can detect morphed samples precisely. Most importantly, the effectiveness of our algorithm is validated through experiments on three datasets: VISAPP17, LMA, and MorGAN. We also performed an ablation study on the sub-band selection.



### Anomaly Detection in Video Sequences: A Benchmark and Computational Model
- **Arxiv ID**: http://arxiv.org/abs/2106.08570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08570v1)
- **Published**: 2021-06-16 06:34:38+00:00
- **Updated**: 2021-06-16 06:34:38+00:00
- **Authors**: Boyang Wan, Wenhui Jiang, Yuming Fang, Zhiyuan Luo, Guanqun Ding
- **Comment**: Publication in IET Image Processing
- **Journal**: None
- **Summary**: Anomaly detection has attracted considerable search attention. However, existing anomaly detection databases encounter two major problems. Firstly, they are limited in scale. Secondly, training sets contain only video-level labels indicating the existence of an abnormal event during the full video while lacking annotations of precise time durations. To tackle these problems, we contribute a new Large-scale Anomaly Detection (LAD) database as the benchmark for anomaly detection in video sequences, which is featured in two aspects. 1) It contains 2000 video sequences including normal and abnormal video clips with 14 anomaly categories including crash, fire, violence, etc. with large scene varieties, making it the largest anomaly analysis database to date. 2) It provides the annotation data, including video-level labels (abnormal/normal video, anomaly type) and frame-level labels (abnormal/normal video frame) to facilitate anomaly detection. Leveraging the above benefits from the LAD database, we further formulate anomaly detection as a fully-supervised learning problem and propose a multi-task deep neural network to solve it. We first obtain the local spatiotemporal contextual feature by using an Inflated 3D convolutional (I3D) network. Then we construct a recurrent convolutional neural network fed the local spatiotemporal contextual feature to extract the spatiotemporal contextual feature. With the global spatiotemporal contextual feature, the anomaly type and score can be computed simultaneously by a multi-task neural network. Experimental results show that the proposed method outperforms the state-of-the-art anomaly detection methods on our database and other public databases of anomaly detection. Codes are available at https://github.com/wanboyang/anomaly_detection_LAD2000.



### Learning Implicit Glyph Shape Representation
- **Arxiv ID**: http://arxiv.org/abs/2106.08573v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08573v1)
- **Published**: 2021-06-16 06:42:55+00:00
- **Updated**: 2021-06-16 06:42:55+00:00
- **Authors**: Ying-Tian Liu, Yuan-Chen Guo, Yi-Xiao Li, Chen Wang, Song-Hai Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel implicit glyph shape representation, which models glyphs as shape primitives enclosed by quadratic curves, and naturally enables generating glyph images at arbitrary high resolutions. Experiments on font reconstruction and interpolation tasks verified that this structured implicit representation is suitable for describing both structure and style features of glyphs. Furthermore, based on the proposed representation, we design a simple yet effective disentangled network for the challenging one-shot font style transfer problem, and achieve the best results comparing to state-of-the-art alternatives in both quantitative and qualitative comparisons. Benefit from this representation, our generated glyphs have the potential to be converted to vector fonts through post-processing, reducing the gap between rasterized images and vector graphics. We hope this work can provide a powerful tool for 2D shape analysis and synthesis, and inspire further exploitation in implicit representations for 2D shape modeling.



### Compound Frechet Inception Distance for Quality Assessment of GAN Created Images
- **Arxiv ID**: http://arxiv.org/abs/2106.08575v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08575v1)
- **Published**: 2021-06-16 06:53:27+00:00
- **Updated**: 2021-06-16 06:53:27+00:00
- **Authors**: Eric J. Nunn, Pejman Khadivi, Shadrokh Samavi
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: Generative adversarial networks or GANs are a type of generative modeling framework. GANs involve a pair of neural networks engaged in a competition in iteratively creating fake data, indistinguishable from the real data. One notable application of GANs is developing fake human faces, also known as "deep fakes," due to the deep learning algorithms at the core of the GAN framework. Measuring the quality of the generated images is inherently subjective but attempts to objectify quality using standardized metrics have been made. One example of objective metrics is the Frechet Inception Distance (FID), which measures the difference between distributions of feature vectors for two separate datasets of images. There are situations that images with low perceptual qualities are not assigned appropriate FID scores. We propose to improve the robustness of the evaluation process by integrating lower-level features to cover a wider array of visual defects. Our proposed method integrates three levels of feature abstractions to evaluate the quality of generated images. Experimental evaluations show better performance of the proposed method for distorted images.



### Domain Consistency Regularization for Unsupervised Multi-source Domain Adaptive Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.08590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08590v1)
- **Published**: 2021-06-16 07:29:27+00:00
- **Updated**: 2021-06-16 07:29:27+00:00
- **Authors**: Zhipeng Luo, Xiaobing Zhang, Shijian Lu, Shuai Yi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based multi-source unsupervised domain adaptation (MUDA) has been actively studied in recent years. Compared with single-source unsupervised domain adaptation (SUDA), domain shift in MUDA exists not only between the source and target domains but also among multiple source domains. Most existing MUDA algorithms focus on extracting domain-invariant representations among all domains whereas the task-specific decision boundaries among classes are largely neglected. In this paper, we propose an end-to-end trainable network that exploits domain Consistency Regularization for unsupervised Multi-source domain Adaptive classification (CRMA). CRMA aligns not only the distributions of each pair of source and target domains but also that of all domains. For each pair of source and target domains, we employ an intra-domain consistency to regularize a pair of domain-specific classifiers to achieve intra-domain alignment. In addition, we design an inter-domain consistency that targets joint inter-domain alignment among all domains. To address different similarities between multiple source domains and the target domain, we design an authorization strategy that assigns different authorities to domain-specific classifiers adaptively for optimal pseudo label prediction and self-training. Extensive experiments show that CRMA tackles unsupervised domain adaptation effectively under a multi-source setup and achieves superior adaptation consistently across multiple MUDA datasets.



### Temporal Convolution Networks with Positional Encoding for Evoked Expression Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.08596v1
- **DOI**: 10.1016/j.patrec.2023.07.002
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2106.08596v1)
- **Published**: 2021-06-16 07:49:36+00:00
- **Updated**: 2021-06-16 07:49:36+00:00
- **Authors**: VanThong Huynh, Guee-Sang Lee, Hyung-Jeong Yang, Soo-Huyng Kim
- **Comment**: Oral presentation at AUVi Workshop - CVPR 2021
  (https://sites.google.com/view/auvi-cvpr2021/program). Source code available
  at https://github.com/th2l/EvokedExpression-tcnpe
- **Journal**: None
- **Summary**: This paper presents an approach for Evoked Expressions from Videos (EEV) challenge, which aims to predict evoked facial expressions from video. We take advantage of pre-trained models on large-scale datasets in computer vision and audio signals to extract the deep representation of timestamps in the video. A temporal convolution network, rather than an RNN like architecture, is used to explore temporal relationships due to its advantage in memory consumption and parallelism. Furthermore, to address the missing annotations of some timestamps, positional encoding is employed to ensure continuity of input data when discarding these timestamps during training. We achieved state-of-the-art results on the EEV challenge with a Pearson correlation coefficient of 0.05477, the first ranked performance in the EEV 2021 challenge.



### PatchNet: Unsupervised Object Discovery based on Patch Embedding
- **Arxiv ID**: http://arxiv.org/abs/2106.08599v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.4.10; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2106.08599v1)
- **Published**: 2021-06-16 07:56:19+00:00
- **Updated**: 2021-06-16 07:56:19+00:00
- **Authors**: Hankyu Moon, Heng Hao, Sima Didari, Jae Oh Woo, Patrick Bangert
- **Comment**: None
- **Journal**: None
- **Summary**: We demonstrate that frequently appearing objects can be discovered by training randomly sampled patches from a small number of images (100 to 200) by self-supervision. Key to this approach is the pattern space, a latent space of patterns that represents all possible sub-images of the given image data. The distance structure in the pattern space captures the co-occurrence of patterns due to the frequent objects. The pattern space embedding is learned by minimizing the contrastive loss between randomly generated adjacent patches. To prevent the embedding from learning the background, we modulate the contrastive loss by color-based object saliency and background dissimilarity. The learned distance structure serves as object memory, and the frequent objects are simply discovered by clustering the pattern vectors from the random patches sampled for inference. Our image representation based on image patches naturally handles the position and scale invariance property that is crucial to multi-object discovery. The method has been proven surprisingly effective, and successfully applied to finding multiple human faces and bodies from natural images.



### Federated Semi-supervised Medical Image Classification via Inter-client Relation Matching
- **Arxiv ID**: http://arxiv.org/abs/2106.08600v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08600v1)
- **Published**: 2021-06-16 07:58:00+00:00
- **Updated**: 2021-06-16 07:58:00+00:00
- **Authors**: Quande Liu, Hongzheng Yang, Qi Dou, Pheng-Ann Heng
- **Comment**: Accepted to MICCAI 2021
- **Journal**: None
- **Summary**: Federated learning (FL) has emerged with increasing popularity to collaborate distributed medical institutions for training deep networks. However, despite existing FL algorithms only allow the supervised training setting, most hospitals in realistic usually cannot afford the intricate data labeling due to absence of budget or expertise. This paper studies a practical yet challenging FL problem, named \textit{Federated Semi-supervised Learning} (FSSL), which aims to learn a federated model by jointly utilizing the data from both labeled and unlabeled clients (i.e., hospitals). We present a novel approach for this problem, which improves over traditional consistency regularization mechanism with a new inter-client relation matching scheme. The proposed learning scheme explicitly connects the learning across labeled and unlabeled clients by aligning their extracted disease relationships, thereby mitigating the deficiency of task knowledge at unlabeled clients and promoting discriminative information from unlabeled samples. We validate our method on two large-scale medical image classification datasets. The effectiveness of our method has been demonstrated with the clear improvements over state-of-the-arts as well as the thorough ablation analysis on both tasks\footnote{Code will be made available at \url{https://github.com/liuquande/FedIRM}}.



### Self-Supervised GANs with Label Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.08601v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08601v5)
- **Published**: 2021-06-16 07:58:00+00:00
- **Updated**: 2022-01-15 16:48:56+00:00
- **Authors**: Liang Hou, Huawei Shen, Qi Cao, Xueqi Cheng
- **Comment**: Accepted at NeurIPS 2021
- **Journal**: None
- **Summary**: Recently, transformation-based self-supervised learning has been applied to generative adversarial networks (GANs) to mitigate catastrophic forgetting in the discriminator by introducing a stationary learning environment. However, the separate self-supervised tasks in existing self-supervised GANs cause a goal inconsistent with generative modeling due to the fact that their self-supervised classifiers are agnostic to the generator distribution. To address this problem, we propose a novel self-supervised GAN that unifies the GAN task with the self-supervised task by augmenting the GAN labels (real or fake) via self-supervision of data transformation. Specifically, the original discriminator and self-supervised classifier are unified into a label-augmented discriminator that predicts the augmented labels to be aware of both the generator distribution and the data distribution under every transformation, and then provide the discrepancy between them to optimize the generator. Theoretically, we prove that the optimal generator could converge to replicate the real data distribution. Empirically, we show that the proposed method significantly outperforms previous self-supervised and data augmentation GANs on both generative modeling and representation learning across benchmark datasets.



### Disentangling Semantic-to-visual Confusion for Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.08605v1
- **DOI**: 10.1109/TMM.2021.3089017
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08605v1)
- **Published**: 2021-06-16 08:04:11+00:00
- **Updated**: 2021-06-16 08:04:11+00:00
- **Authors**: Zihan Ye, Fuyuan Hu, Fan Lyu, Linyan Li, Kaizhu Huang
- **Comment**: Accepted by IEEE TRANSACTIONS ON MULTIMEDIA (TMM) in 2021
- **Journal**: None
- **Summary**: Using generative models to synthesize visual features from semantic distribution is one of the most popular solutions to ZSL image classification in recent years. The triplet loss (TL) is popularly used to generate realistic visual distributions from semantics by automatically searching discriminative representations. However, the traditional TL cannot search reliable unseen disentangled representations due to the unavailability of unseen classes in ZSL. To alleviate this drawback, we propose in this work a multi-modal triplet loss (MMTL) which utilizes multimodal information to search a disentangled representation space. As such, all classes can interplay which can benefit learning disentangled class representations in the searched space. Furthermore, we develop a novel model called Disentangling Class Representation Generative Adversarial Network (DCR-GAN) focusing on exploiting the disentangled representations in training, feature synthesis, and final recognition stages. Benefiting from the disentangled representations, DCR-GAN could fit a more realistic distribution over both seen and unseen features. Extensive experiments show that our proposed model can lead to superior performance to the state-of-the-arts on four benchmark datasets. Our code is available at https://github.com/FouriYe/DCRGAN-TMM.



### FastAno: Fast Anomaly Detection via Spatio-temporal Patch Transformation
- **Arxiv ID**: http://arxiv.org/abs/2106.08613v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08613v4)
- **Published**: 2021-06-16 08:14:31+00:00
- **Updated**: 2021-10-25 07:20:46+00:00
- **Authors**: Chaewon Park, MyeongAh Cho, Minhyeok Lee, Sangyoun Lee
- **Comment**: Accepted to IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2022
- **Journal**: None
- **Summary**: Video anomaly detection has gained significant attention due to the increasing requirements of automatic monitoring for surveillance videos. Especially, the prediction based approach is one of the most studied methods to detect anomalies by predicting frames that include abnormal events in the test set after learning with the normal frames of the training set. However, a lot of prediction networks are computationally expensive owing to the use of pre-trained optical flow networks, or fail to detect abnormal situations because of their strong generative ability to predict even the anomalies. To address these shortcomings, we propose spatial rotation transformation (SRT) and temporal mixing transformation (TMT) to generate irregular patch cuboids within normal frame cuboids in order to enhance the learning of normal features. Additionally, the proposed patch transformation is used only during the training phase, allowing our model to detect abnormal frames at fast speed during inference. Our model is evaluated on three anomaly detection benchmarks, achieving competitive accuracy and surpassing all the previous works in terms of speed.



### EdgeConv with Attention Module for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.08615v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08615v3)
- **Published**: 2021-06-16 08:15:20+00:00
- **Updated**: 2021-10-26 04:45:26+00:00
- **Authors**: Minhyeok Lee, Sangwon Hwang, Chaewon Park, Sangyoun Lee
- **Comment**: Accepted to IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2022
- **Journal**: None
- **Summary**: Monocular depth estimation is an especially important task in robotics and autonomous driving, where 3D structural information is essential. However, extreme lighting conditions and complex surface objects make it difficult to predict depth in a single image. Therefore, to generate accurate depth maps, it is important for the model to learn structural information about the scene. We propose a novel Patch-Wise EdgeConv Module (PEM) and EdgeConv Attention Module (EAM) to solve the difficulty of monocular depth estimation. The proposed modules extract structural information by learning the relationship between image patches close to each other in space using edge convolution. Our method is evaluated on two popular datasets, the NYU Depth V2 and the KITTI Eigen split, achieving state-of-the-art performance. We prove that the proposed model predicts depth robustly in challenging scenes through various comparative experiments.



### CMF: Cascaded Multi-model Fusion for Referring Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.08617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08617v1)
- **Published**: 2021-06-16 08:18:39+00:00
- **Updated**: 2021-06-16 08:18:39+00:00
- **Authors**: Jianhua Yang, Yan Huang, Zhanyu Ma, Liang Wang
- **Comment**: Accepted by ICIP 2021
- **Journal**: None
- **Summary**: In this work, we address the task of referring image segmentation (RIS), which aims at predicting a segmentation mask for the object described by a natural language expression. Most existing methods focus on establishing unidirectional or directional relationships between visual and linguistic features to associate two modalities together, while the multi-scale context is ignored or insufficiently modeled. Multi-scale context is crucial to localize and segment those objects that have large scale variations during the multi-modal fusion process. To solve this problem, we propose a simple yet effective Cascaded Multi-modal Fusion (CMF) module, which stacks multiple atrous convolutional layers in parallel and further introduces a cascaded branch to fuse visual and linguistic features. The cascaded branch can progressively integrate multi-scale contextual information and facilitate the alignment of two modalities during the multi-modal fusion process. Experimental results on four benchmark datasets demonstrate that our method outperforms most state-of-the-art methods. Code is available at https://github.com/jianhua2022/CMF-Refseg.



### Structured DropConnect for Uncertainty Inference in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.08624v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 14J60 (Primary) 14F05, 14J26 (Secondary), F.2.2; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2106.08624v2)
- **Published**: 2021-06-16 08:31:14+00:00
- **Updated**: 2021-08-01 08:32:09+00:00
- **Authors**: Wenqing Zheng, Jiyang Xie, Weidong Liu, Zhanyu Ma
- **Comment**: 5 pages,1 figures
- **Journal**: None
- **Summary**: With the complexity of the network structure, uncertainty inference has become an important task to improve the classification accuracy for artificial intelligence systems. For image classification tasks, we propose a structured DropConnect (SDC) framework to model the output of a deep neural network by a Dirichlet distribution. We introduce a DropConnect strategy on weights in the fully connected layers during training. In test, we split the network into several sub-networks, and then model the Dirichlet distribution by match its moments with the mean and variance of the outputs of these sub-networks. The entropy of the estimated Dirichlet distribution is finally utilized for uncertainty inference. In this paper, this framework is implemented on LeNet$5$ and VGG$16$ models for misclassification detection and out-of-distribution detection on MNIST and CIFAR-$10$ datasets. Experimental results show that the performance of the proposed SDC can be comparable to other uncertainty inference methods. Furthermore, the SDC is adapted well to different network structures with certain generalization capabilities and research prospects.



### Shuffle Transformer with Feature Alignment for Video Face Parsing
- **Arxiv ID**: http://arxiv.org/abs/2106.08650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08650v1)
- **Published**: 2021-06-16 09:25:33+00:00
- **Updated**: 2021-06-16 09:25:33+00:00
- **Authors**: Rui Zhang, Yang Han, Zilong Huang, Pei Cheng, Guozhong Luo, Gang Yu, Bin Fu
- **Comment**: technical report
- **Journal**: None
- **Summary**: This is a short technical report introducing the solution of the Team TCParser for Short-video Face Parsing Track of The 3rd Person in Context (PIC) Workshop and Challenge at CVPR 2021. In this paper, we introduce a strong backbone which is cross-window based Shuffle Transformer for presenting accurate face parsing representation. To further obtain the finer segmentation results, especially on the edges, we introduce a Feature Alignment Aggregation (FAA) module. It can effectively relieve the feature misalignment issue caused by multi-resolution feature aggregation. Benefiting from the stronger backbone and better feature aggregation, the proposed method achieves 86.9519% score in the Short-video Face Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge, ranked the first place.



### ParticleAugment: Sampling-Based Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.08693v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08693v3)
- **Published**: 2021-06-16 10:56:02+00:00
- **Updated**: 2021-10-15 14:43:07+00:00
- **Authors**: Alexander Tsaregorodtsev, Vasileios Belagiannis
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We present an automated data augmentation approach for image classification. We formulate the problem as Monte Carlo sampling where our goal is to approximate the optimal augmentation policies. We propose a particle filtering scheme for the policy search where the probability of applying a set of augmentation operations forms the state of the filter. We measure the policy performance based on the loss function difference between a reference and the actual model, which we afterwards use to re-weight the particles and finally update the policy. In our experiments, we show that our formulation for automated augmentation reaches promising results on CIFAR-10, CIFAR-100, and ImageNet datasets using the standard network architectures for this problem. By comparing with the related work, our method reaches a balance between the computational cost of policy search and the model performance. Our code will be made publicly available.



### Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in Real-Time MRI
- **Arxiv ID**: http://arxiv.org/abs/2106.08706v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.HC, cs.LG, cs.SD, eess.AS, I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2106.08706v1)
- **Published**: 2021-06-16 11:20:02+00:00
- **Updated**: 2021-06-16 11:20:02+00:00
- **Authors**: Laxmi Pandey, Ahmed Sabbir Arif
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Speech sounds of spoken language are obtained by varying configuration of the articulators surrounding the vocal tract. They contain abundant information that can be utilized to better understand the underlying mechanism of human speech production. We propose a novel deep neural network-based learning framework that understands acoustic information in the variable-length sequence of vocal tract shaping during speech production, captured by real-time magnetic resonance imaging (rtMRI), and translate it into text. The proposed framework comprises of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. On the USC-TIMIT corpus, the model achieved a 40.6% PER at sentence-level, much better compared to the existing models. To the best of our knowledge, this is the first study that demonstrates the recognition of entire spoken sentence based on an individual's articulatory motions captured by rtMRI video. We also performed an analysis of variations in the geometry of articulation in each sub-regions of the vocal tract (i.e., pharyngeal, velar and dorsal, hard palate, labial constriction region) with respect to different emotions and genders. Results suggest that each sub-regions distortion is affected by both emotion and gender.



### Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2106.08710v1
- **DOI**: 10.1145/3557999
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.08710v1)
- **Published**: 2021-06-16 11:26:37+00:00
- **Updated**: 2021-06-16 11:26:37+00:00
- **Authors**: Jacky Cao, Kit-Yung Lam, Lik-Hang Lee, Xiaoli Liu, Pan Hui, Xiang Su
- **Comment**: This work is currently under review in an international journal
- **Journal**: None
- **Summary**: Mobile Augmented Reality (MAR) integrates computer-generated virtual objects with physical environments for mobile devices. MAR systems enable users to interact with MAR devices, such as smartphones and head-worn wearables, and performs seamless transitions from the physical world to a mixed world with digital entities. These MAR systems support user experiences by using MAR devices to provide universal accessibility to digital contents. Over the past 20 years, a number of MAR systems have been developed, however, the studies and design of MAR frameworks have not yet been systematically reviewed from the perspective of user-centric design. This article presents the first effort of surveying existing MAR frameworks (count: 37) and further discusses the latest studies on MAR through a top-down approach: 1) MAR applications; 2) MAR visualisation techniques adaptive to user mobility and contexts; 3) systematic evaluation of MAR frameworks including supported platforms and corresponding features such as tracking, feature extraction plus sensing capabilities; and 4) underlying machine learning approaches supporting intelligent operations within MAR systems. Finally, we summarise the development of emerging research fields, current state-of-the-art, and discuss the important open challenges and possible theoretical and technical directions. This survey aims to benefit both researchers and MAR system developers alike.



### 2nd Place Solution for Waymo Open Dataset Challenge -- Real-time 2D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.08713v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08713v1)
- **Published**: 2021-06-16 11:32:03+00:00
- **Updated**: 2021-06-16 11:32:03+00:00
- **Authors**: Yueming Zhang, Xiaolin Song, Bing Bai, Tengfei Xing, Chao Liu, Xin Gao, Zhihui Wang, Yawei Wen, Haojin Liao, Guoshan Zhang, Pengfei Xu
- **Comment**: None
- **Journal**: None
- **Summary**: In an autonomous driving system, it is essential to recognize vehicles, pedestrians and cyclists from images. Besides the high accuracy of the prediction, the requirement of real-time running brings new challenges for convolutional network models. In this report, we introduce a real-time method to detect the 2D objects from images. We aggregate several popular one-stage object detectors and train the models of variety input strategies independently, to yield better performance for accurate multi-scale detection of each category, especially for small objects. For model acceleration, we leverage TensorRT to optimize the inference time of our detection pipeline. As shown in the leaderboard, our proposed detection framework ranks the 2nd place with 75.00% L1 mAP and 69.72% L2 mAP in the real-time 2D detection track of the Waymo Open Dataset Challenges, while our framework achieves the latency of 45.8ms/frame on an Nvidia Tesla V100 GPU.



### AtrialGeneral: Domain Generalization for Left Atrial Segmentation of Multi-Center LGE MRIs
- **Arxiv ID**: http://arxiv.org/abs/2106.08727v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08727v3)
- **Published**: 2021-06-16 11:58:11+00:00
- **Updated**: 2021-07-05 03:00:37+00:00
- **Authors**: Lei Li, Veronika A. Zimmer, Julia A. Schnabel, Xiahai Zhuang
- **Comment**: 10 pages, 4 figures, MICCAI2021
- **Journal**: None
- **Summary**: Left atrial (LA) segmentation from late gadolinium enhanced magnetic resonance imaging (LGE MRI) is a crucial step needed for planning the treatment of atrial fibrillation. However, automatic LA segmentation from LGE MRI is still challenging, due to the poor image quality, high variability in LA shapes, and unclear LA boundary. Though deep learning-based methods can provide promising LA segmentation results, they often generalize poorly to unseen domains, such as data from different scanners and/or sites. In this work, we collect 210 LGE MRIs from different centers with different levels of image quality. To evaluate the domain generalization ability of models on the LA segmentation task, we employ four commonly used semantic segmentation networks for the LA segmentation from multi-center LGE MRIs. Besides, we investigate three domain generalization strategies, i.e., histogram matching, mutual information based disentangled representation, and random style transfer, where a simple histogram matching is proved to be most effective.



### Learning to Disentangle GAN Fingerprint for Fake Image Attribution
- **Arxiv ID**: http://arxiv.org/abs/2106.08749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08749v1)
- **Published**: 2021-06-16 12:50:40+00:00
- **Updated**: 2021-06-16 12:50:40+00:00
- **Authors**: Tianyun Yang, Juan Cao, Qiang Sheng, Lei Li, Jiaqi Ji, Xirong Li, Sheng Tang
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Rapid pace of generative models has brought about new threats to visual forensics such as malicious personation and digital copyright infringement, which promotes works on fake image attribution. Existing works on fake image attribution mainly rely on a direct classification framework. Without additional supervision, the extracted features could include many content-relevant components and generalize poorly. Meanwhile, how to obtain an interpretable GAN fingerprint to explain the decision remains an open question. Adopting a multi-task framework, we propose a GAN Fingerprint Disentangling Network (GFD-Net) to simultaneously disentangle the fingerprint from GAN-generated images and produce a content-irrelevant representation for fake image attribution. A series of constraints are provided to guarantee the stability and discriminability of the fingerprint, which in turn helps content-irrelevant feature extraction. Further, we perform comprehensive analysis on GAN fingerprint, providing some clues about the properties of GAN fingerprint and which factors dominate the fingerprint in GAN architecture. Experiments show that our GFD-Net achieves superior fake image attribution performance in both closed-world and open-world testing. We also apply our method in binary fake image detection and exhibit a significant generalization ability on unseen generators.



### Unsupervised Domain Adaptation with Variational Approximation for Cardiac Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.08752v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08752v1)
- **Published**: 2021-06-16 13:00:39+00:00
- **Updated**: 2021-06-16 13:00:39+00:00
- **Authors**: Fuping Wu, Xiahai Zhuang
- **Comment**: accepted by IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Unsupervised domain adaptation is useful in medical image segmentation. Particularly, when ground truths of the target images are not available, domain adaptation can train a target-specific model by utilizing the existing labeled images from other modalities. Most of the reported works mapped images of both the source and target domains into a common latent feature space, and then reduced their discrepancy either implicitly with adversarial training or explicitly by directly minimizing a discrepancy metric. In this work, we propose a new framework, where the latent features of both domains are driven towards a common and parameterized variational form, whose conditional distribution given the image is Gaussian. This is achieved by two networks based on variational auto-encoders (VAEs) and a regularization for this variational approximation. Both of the VAEs, each for one domain, contain a segmentation module, where the source segmentation is trained in a supervised manner, while the target one is trained unsupervisedly. We validated the proposed domain adaptation method using two cardiac segmentation tasks, i.e., the cross-modality (CT and MR) whole heart segmentation and the cross-sequence cardiac MR segmentation. Results show that the proposed method achieved better accuracies compared to two state-of-the-art approaches and demonstrated good potential for cardiac segmentation. Furthermore, the proposed explicit regularization was shown to be effective and efficient in narrowing down the distribution gap between domains, which is useful for unsupervised domain adaptation. Our code and data has been released via https://zmiclab.github.io/projects.html.



### Optimizing Data Augmentation Policy Through Random Unidimensional Search
- **Arxiv ID**: http://arxiv.org/abs/2106.08756v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08756v4)
- **Published**: 2021-06-16 13:07:59+00:00
- **Updated**: 2023-07-14 14:23:46+00:00
- **Authors**: Xiaomeng Dong, Michael Potter, Gaurav Kumar, Yun-Chan Tsai, V. Ratna Saripalli, Theodore Trafalis
- **Comment**: None
- **Journal**: None
- **Summary**: It is no secret amongst deep learning researchers that finding the optimal data augmentation strategy during training can mean the difference between state-of-the-art performance and a run-of-the-mill result. To that end, the community has seen many efforts to automate the process of finding the perfect augmentation procedure for any task at hand. Unfortunately, even recent cutting-edge methods bring massive computational overhead, requiring as many as 100 full model trainings to settle on an ideal configuration. We show how to achieve equivalent performance using just 6 trainings with Random Unidimensional Augmentation. Source code is available at https://github.com/fastestimator/RUA/tree/v1.0



### Toward Affective XAI: Facial Affect Analysis for Understanding Explainable Human-AI Interactions
- **Arxiv ID**: http://arxiv.org/abs/2106.08761v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2106.08761v2)
- **Published**: 2021-06-16 13:14:21+00:00
- **Updated**: 2021-10-15 14:00:02+00:00
- **Authors**: Luke Guerdan, Alex Raymond, Hatice Gunes
- **Comment**: None
- **Journal**: None
- **Summary**: As machine learning approaches are increasingly used to augment human decision-making, eXplainable Artificial Intelligence (XAI) research has explored methods for communicating system behavior to humans. However, these approaches often fail to account for the emotional responses of humans as they interact with explanations. Facial affect analysis, which examines human facial expressions of emotions, is one promising lens for understanding how users engage with explanations. Therefore, in this work, we aim to (1) identify which facial affect features are pronounced when people interact with XAI interfaces, and (2) develop a multitask feature embedding for linking facial affect signals with participants' use of explanations. Our analyses and results show that the occurrence and values of facial AU1 and AU4, and Arousal are heightened when participants fail to use explanations effectively. This suggests that facial affect analysis should be incorporated into XAI to personalize explanations to individuals' interaction styles and to adapt explanations based on the difficulty of the task performed.



### Study of visual processing techniques for dynamic speckles: a comparative analysis
- **Arxiv ID**: http://arxiv.org/abs/2106.15507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15507v1)
- **Published**: 2021-06-16 13:15:04+00:00
- **Updated**: 2021-06-16 13:15:04+00:00
- **Authors**: Amit Chatterjee, Jitendra Dhanotiya, Vimal Bhatia, Shashi Prakash
- **Comment**: None
- **Journal**: OSI ICLLT-2016, Tezpur University
- **Summary**: Main visual techniques used to obtain information from speckle patterns are Fujii method, generalized difference, weighted generalized difference, mean windowed difference, structural function (SF), modified SF, etc. In this work, a comparative analysis of major visual techniques for natural gum sample is carried out. Obtained results conclusively establish SF based method as an optimum tool for visual inspection of dynamic speckle data.



### Shape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving Objects
- **Arxiv ID**: http://arxiv.org/abs/2106.08762v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08762v2)
- **Published**: 2021-06-16 13:18:08+00:00
- **Updated**: 2021-10-26 12:30:35+00:00
- **Authors**: Denys Rozumnyi, Martin R. Oswald, Vittorio Ferrari, Marc Pollefeys
- **Comment**: Accepted to 35th Conference on Neural Information Processing Systems
  (NeurIPS 2021)
- **Journal**: 35th Conference on Neural Information Processing Systems (NeurIPS
  2021)
- **Summary**: We address the novel task of jointly reconstructing the 3D shape, texture, and motion of an object from a single motion-blurred image. While previous approaches address the deblurring problem only in the 2D image domain, our proposed rigorous modeling of all object properties in the 3D domain enables the correct description of arbitrary object motion. This leads to significantly better image decomposition and sharper deblurring results. We model the observed appearance of a motion-blurred object as a combination of the background and a 3D object with constant translation and rotation. Our method minimizes a loss on reconstructing the input image via differentiable rendering with suitable regularizers. This enables estimating the textured 3D mesh of the blurred object with high fidelity. Our method substantially outperforms competing approaches on several benchmarks for fast moving objects deblurring. Qualitative results show that the reconstructed 3D mesh generates high-quality temporal super-resolution and novel views of the deblurred object.



### Robustness of Object Detectors in Degrading Weather Conditions
- **Arxiv ID**: http://arxiv.org/abs/2106.08795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08795v1)
- **Published**: 2021-06-16 13:56:07+00:00
- **Updated**: 2021-06-16 13:56:07+00:00
- **Authors**: Muhammad Jehanzeb Mirza, Cornelius Buerkle, Julio Jarquin, Michael Opitz, Fabian Oboril, Kay-Ulrich Scholl, Horst Bischof
- **Comment**: Accepted for publication at ITSC 2021
- **Journal**: None
- **Summary**: State-of-the-art object detection systems for autonomous driving achieve promising results in clear weather conditions. However, such autonomous safety critical systems also need to work in degrading weather conditions, such as rain, fog and snow. Unfortunately, most approaches evaluate only on the KITTI dataset, which consists only of clear weather scenes. In this paper we address this issue and perform one of the most detailed evaluation on single and dual modality architectures on data captured in real weather conditions. We analyse the performance degradation of these architectures in degrading weather conditions. We demonstrate that an object detection architecture performing good in clear weather might not be able to handle degrading weather conditions. We also perform ablation studies on the dual modality architectures and show their limitations.



### Unsupervised Person Re-identification via Multi-Label Prediction and Classification based on Graph-Structural Insight
- **Arxiv ID**: http://arxiv.org/abs/2106.08798v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.08798v1)
- **Published**: 2021-06-16 14:00:40+00:00
- **Updated**: 2021-06-16 14:00:40+00:00
- **Authors**: Jongmin Yu, Hyeontaek Oh
- **Comment**: submitted to ICCV
- **Journal**: None
- **Summary**: This paper addresses unsupervised person re-identification (Re-ID) using multi-label prediction and classification based on graph-structural insight. Our method extracts features from person images and produces a graph that consists of the features and a pairwise similarity of them as nodes and edges, respectively. Based on the graph, the proposed graph structure based multi-label prediction (GSMLP) method predicts multi-labels by considering the pairwise similarity and the adjacency node distribution of each node. The multi-labels created by GSMLP are applied to the proposed selective multi-label classification (SMLC) loss. SMLC integrates a hard-sample mining scheme and a multi-label classification. The proposed GSMLP and SMLC boost the performance of unsupervised person Re-ID without any pre-labelled dataset. Experimental results justify the superiority of the proposed method in unsupervised person Re-ID by producing state-of-the-art performance. The source code for this paper is publicly available on 'https://github.com/uknownpioneer/GSMLP-SMLC.git'.



### Contrastive Learning with Continuous Proxy Meta-Data for 3D MRI Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.08808v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.08808v1)
- **Published**: 2021-06-16 14:17:04+00:00
- **Updated**: 2021-06-16 14:17:04+00:00
- **Authors**: Benoit Dufumier, Pietro Gori, Julie Victor, Antoine Grigis, Michel Wessa, Paolo Brambilla, Pauline Favre, Mircea Polosan, Colm McDonald, Camille Marie Piguet, Edouard Duchesnay
- **Comment**: MICCAI 2021
- **Journal**: MICCAI 2021
- **Summary**: Traditional supervised learning with deep neural networks requires a tremendous amount of labelled data to converge to a good solution. For 3D medical images, it is often impractical to build a large homogeneous annotated dataset for a specific pathology. Self-supervised methods offer a new way to learn a representation of the images in an unsupervised manner with a neural network. In particular, contrastive learning has shown great promises by (almost) matching the performance of fully-supervised CNN on vision tasks. Nonetheless, this method does not take advantage of available meta-data, such as participant's age, viewed as prior knowledge. Here, we propose to leverage continuous proxy metadata, in the contrastive learning framework, by introducing a new loss called y-Aware InfoNCE loss. Specifically, we improve the positive sampling during pre-training by adding more positive examples with similar proxy meta-data with the anchor, assuming they share similar discriminative semantic features.With our method, a 3D CNN model pre-trained on $10^4$ multi-site healthy brain MRI scans can extract relevant features for three classification tasks: schizophrenia, bipolar diagnosis and Alzheimer's detection. When fine-tuned, it also outperforms 3D CNN trained from scratch on these tasks, as well as state-of-the-art self-supervised methods. Our code is made publicly available here.



### SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking
- **Arxiv ID**: http://arxiv.org/abs/2106.08816v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08816v2)
- **Published**: 2021-06-16 14:28:57+00:00
- **Updated**: 2021-07-30 13:37:43+00:00
- **Authors**: Ziang Cao, Changhong Fu, Junjie Ye, Bowen Li, Yiming Li
- **Comment**: 2021 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
- **Journal**: None
- **Summary**: Recently, the Siamese-based method has stood out from multitudinous tracking methods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to various special challenges in UAV tracking, \textit{e.g.}, severe occlusion and fast motion, most existing Siamese-based trackers hardly combine superior performance with high efficiency. To this concern, in this paper, a novel attentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking. By virtue of the attention mechanism, we conduct a special attentional aggregation network (AAN) consisting of self-AAN and cross-AAN for raising the representation ability of features eventually. The former AAN aggregates and models the self-semantic interdependencies of the single feature map via spatial and channel dimensions. The latter aims to aggregate the cross-interdependencies of two different semantic features including the location information of anchors. In addition, the anchor proposal network based on dual features is proposed to raise its robustness of tracking objects with various scales. Experiments on two well-known authoritative benchmarks are conducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA trackers. Besides, real-world tests onboard a typical embedded platform demonstrate that SiamAPN++ achieves promising tracking results with real-time speed.



### Metamorphic image registration using a semi-Lagrangian scheme
- **Arxiv ID**: http://arxiv.org/abs/2106.08817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08817v1)
- **Published**: 2021-06-16 14:29:08+00:00
- **Updated**: 2021-06-16 14:29:08+00:00
- **Authors**: Anton Franois, Pietro Gori, Joan Glauns
- **Comment**: SEE GSI 2021
- **Journal**: Geometric Science for Information 2021
- **Summary**: In this paper, we propose an implementation of both Large Deformation Diffeomorphic Metric Mapping (LDDMM) and Metamorphosis image registration using a semi-Lagrangian scheme for geodesic shooting. We propose to solve both problems as an inexact matching providing a single and unifying cost function. We demonstrate that for image registration the use of a semi-Lagrangian scheme is more stable than a standard Eulerian scheme. Our GPU implementation is based on PyTorch, which greatly simplifies and accelerates the computations thanks to its powerful automatic differentiation engine. It will be freely available at https://github.com/antonfrancois/Demeter_metamorphosis.



### JRDB-Act: A Large-scale Dataset for Spatio-temporal Action, Social Group and Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.08827v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08827v2)
- **Published**: 2021-06-16 14:43:46+00:00
- **Updated**: 2021-11-24 04:40:27+00:00
- **Authors**: Mahsa Ehsanpour, Fatemeh Saleh, Silvio Savarese, Ian Reid, Hamid Rezatofighi
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of large-scale video action understanding datasets has facilitated advances in the interpretation of visual scenes containing people. However, learning to recognise human actions and their social interactions in an unconstrained real-world environment comprising numerous people, with potentially highly unbalanced and long-tailed distributed action labels from a stream of sensory data captured from a mobile robot platform remains a significant challenge, not least owing to the lack of a reflective large-scale dataset. In this paper, we introduce JRDB-Act, as an extension of the existing JRDB, which is captured by a social mobile manipulator and reflects a real distribution of human daily-life actions in a university campus environment. JRDB-Act has been densely annotated with atomic actions, comprises over 2.8M action labels, constituting a large-scale spatio-temporal action detection dataset. Each human bounding box is labeled with one pose-based action label and multiple~(optional) interaction-based action labels. Moreover JRDB-Act provides social group annotation, conducive to the task of grouping individuals based on their interactions in the scene to infer their social activities~(common activities in each social group). Each annotated label in JRDB-Act is tagged with the annotators' confidence level which contributes to the development of reliable evaluation strategies. In order to demonstrate how one can effectively utilise such annotations, we develop an end-to-end trainable pipeline to learn and infer these tasks, i.e. individual action and social group detection. The data and the evaluation code is publicly available at https://jrdb.erc.monash.edu/.



### A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment Analysis Methods
- **Arxiv ID**: http://arxiv.org/abs/2106.08829v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08829v1)
- **Published**: 2021-06-16 14:44:48+00:00
- **Updated**: 2021-06-16 14:44:48+00:00
- **Authors**: Gullal S. Cheema, Sherzod Hakimov, Eric Mller-Budack, Ralph Ewerth
- **Comment**: Accepted in Workshop on Multi-ModalPre-Training for Multimedia
  Understanding (MMPT 2021), co-located with ICMR 2021
- **Journal**: None
- **Summary**: Opinion and sentiment analysis is a vital task to characterize subjective information in social media posts. In this paper, we present a comprehensive experimental evaluation and comparison with six state-of-the-art methods, from which we have re-implemented one of them. In addition, we investigate different textual and visual feature embeddings that cover different aspects of the content, as well as the recently introduced multimodal CLIP embeddings. Experimental results are presented for two different publicly available benchmark datasets of tweets and corresponding images. In contrast to the evaluation methodology of previous work, we introduce a reproducible and fair evaluation scheme to make results comparable. Finally, we conduct an error analysis to outline the limitations of the methods and possibilities for the future work.



### GelSight Wedge: Measuring High-Resolution 3D Contact Geometry with a Compact Robot Finger
- **Arxiv ID**: http://arxiv.org/abs/2106.08851v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08851v1)
- **Published**: 2021-06-16 15:15:29+00:00
- **Updated**: 2021-06-16 15:15:29+00:00
- **Authors**: Shaoxiong Wang, Yu She, Branden Romero, Edward Adelson
- **Comment**: ICRA 2021
- **Journal**: None
- **Summary**: Vision-based tactile sensors have the potential to provide important contact geometry to localize the objective with visual occlusion. However, it is challenging to measure high-resolution 3D contact geometry for a compact robot finger, to simultaneously meet optical and mechanical constraints. In this work, we present the GelSight Wedge sensor, which is optimized to have a compact shape for robot fingers, while achieving high-resolution 3D reconstruction. We evaluate the 3D reconstruction under different lighting configurations, and extend the method from 3 lights to 1 or 2 lights. We demonstrate the flexibility of the design by shrinking the sensor to the size of a human finger for fine manipulation tasks. We also show the effectiveness and potential of the reconstructed 3D geometry for pose tracking in the 3D space.



### X-MAN: Explaining multiple sources of anomalies in video
- **Arxiv ID**: http://arxiv.org/abs/2106.08856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08856v1)
- **Published**: 2021-06-16 15:25:50+00:00
- **Updated**: 2021-06-16 15:25:50+00:00
- **Authors**: Stanislaw Szymanowicz, James Charles, Roberto Cipolla
- **Comment**: In Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops, June 2021
- **Journal**: None
- **Summary**: Our objective is to detect anomalies in video while also automatically explaining the reason behind the detector's response. In a practical sense, explainability is crucial for this task as the required response to an anomaly depends on its nature and severity. However, most leading methods (based on deep neural networks) are not interpretable and hide the decision making process in uninterpretable feature representations. In an effort to tackle this problem we make the following contributions: (1) we show how to build interpretable feature representations suitable for detecting anomalies with state of the art performance, (2) we propose an interpretable probabilistic anomaly detector which can describe the reason behind it's response using high level concepts, (3) we are the first to directly consider object interactions for anomaly detection and (4) we propose a new task of explaining anomalies and release a large dataset for evaluating methods on this task. Our method competes well with the state of the art on public datasets while also providing anomaly explanation based on objects and their interactions.



### Over-and-Under Complete Convolutional RNN for MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2106.08886v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08886v2)
- **Published**: 2021-06-16 15:56:34+00:00
- **Updated**: 2021-06-25 02:22:42+00:00
- **Authors**: Pengfei Guo, Jeya Maria Jose Valanarasu, Puyang Wang, Jinyuan Zhou, Shanshan Jiang, Vishal M. Patel
- **Comment**: Accepted to MICCAI 2021
- **Journal**: None
- **Summary**: Reconstructing magnetic resonance (MR) images from undersampled data is a challenging problem due to various artifacts introduced by the under-sampling operation. Recent deep learning-based methods for MR image reconstruction usually leverage a generic auto-encoder architecture which captures low-level features at the initial layers and high-level features at the deeper layers. Such networks focus much on global features which may not be optimal to reconstruct the fully-sampled image. In this paper, we propose an Over-and-Under Complete Convolutional Recurrent Neural Network (OUCR), which consists of an overcomplete and an undercomplete Convolutional Recurrent Neural Network(CRNN). The overcomplete branch gives special attention in learning local structures by restraining the receptive field of the network. Combining it with the undercomplete branch leads to a network which focuses more on low-level features without losing out on the global structures. Extensive experiments on two datasets demonstrate that the proposed method achieves significant improvements over the compressed sensing and popular deep learning-based methods with less number of trainable parameters.



### Toward Robotic Weed Control: Detection of Nutsedge Weed in Bermudagrass Turf Using Inaccurate and Insufficient Training Data
- **Arxiv ID**: http://arxiv.org/abs/2106.08897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08897v1)
- **Published**: 2021-06-16 15:58:00+00:00
- **Updated**: 2021-06-16 15:58:00+00:00
- **Authors**: Shuangyu Xie, Chengsong Hu, Muthukumar Bagavathiannan, Dezhen Song
- **Comment**: None
- **Journal**: None
- **Summary**: To enable robotic weed control, we develop algorithms to detect nutsedge weed from bermudagrass turf. Due to the similarity between the weed and the background turf, manual data labeling is expensive and error-prone. Consequently, directly applying deep learning methods for object detection cannot generate satisfactory results. Building on an instance detection approach (i.e. Mask R-CNN), we combine synthetic data with raw data to train the network. We propose an algorithm to generate high fidelity synthetic data, adopting different levels of annotations to reduce labeling cost. Moreover, we construct a nutsedge skeleton-based probabilistic map (NSPM) as the neural network input to reduce the reliance on pixel-wise precise labeling. We also modify loss function from cross entropy to Kullback-Leibler divergence which accommodates uncertainty in the labeling process. We implement the proposed algorithm and compare it with both Faster R-CNN and Mask R-CNN. The results show that our design can effectively overcome the impact of imprecise and insufficient training sample issues and significantly outperform the Faster R-CNN counterpart with a false negative rate of only 0.4%. In particular, our approach also reduces labeling time by 95% while achieving better performance if comparing with the original Mask R-CNN approach.



### Structure First Detail Next: Image Inpainting with Pyramid Generator
- **Arxiv ID**: http://arxiv.org/abs/2106.08905v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08905v2)
- **Published**: 2021-06-16 16:00:16+00:00
- **Updated**: 2021-08-05 03:08:34+00:00
- **Authors**: Shuyi Qu, Zhenxing Niu, Kaizhu Huang, Jianke Zhu, Matan Protter, Gadi Zimerman, Yinghui Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent deep generative models have achieved promising performance in image inpainting. However, it is still very challenging for a neural network to generate realistic image details and textures, due to its inherent spectral bias. By our understanding of how artists work, we suggest to adopt a `structure first detail next' workflow for image inpainting. To this end, we propose to build a Pyramid Generator by stacking several sub-generators, where lower-layer sub-generators focus on restoring image structures while the higher-layer sub-generators emphasize image details. Given an input image, it will be gradually restored by going through the entire pyramid in a bottom-up fashion. Particularly, our approach has a learning scheme of progressively increasing hole size, which allows it to restore large-hole images. In addition, our method could fully exploit the benefits of learning with high-resolution images, and hence is suitable for high-resolution image inpainting. Extensive experimental results on benchmark datasets have validated the effectiveness of our approach compared with state-of-the-art methods.



### $C^3$: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues
- **Arxiv ID**: http://arxiv.org/abs/2106.08914v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08914v2)
- **Published**: 2021-06-16 16:05:27+00:00
- **Updated**: 2023-08-05 08:04:15+00:00
- **Authors**: Hung Le, Nancy F. Chen, Steven C. H. Hoi
- **Comment**: 24th Meeting of the Special Interest Group on Discourse and Dialogue
  (SIGDIAL)
- **Journal**: None
- **Summary**: Video-grounded dialogue systems aim to integrate video understanding and dialogue understanding to generate responses that are relevant to both the dialogue and video context. Most existing approaches employ deep learning models and have achieved remarkable performance, given the relatively small datasets available. However, the results are partly accomplished by exploiting biases in the datasets rather than developing multimodal reasoning, resulting in limited generalization. In this paper, we propose a novel approach of Compositional Counterfactual Contrastive Learning ($C^3$) to develop contrastive training between factual and counterfactual samples in video-grounded dialogues. Specifically, we design factual/counterfactual sampling based on the temporal steps in videos and tokens in dialogues and propose contrastive loss functions that exploit object-level or action-level variance. Different from prior approaches, we focus on contrastive hidden state representations among compositional output tokens to optimize the representation space in a generation setting. We achieved promising performance gains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the benefits of our approach in grounding video and dialogue context.



### Transductive Few-Shot Learning: Clustering is All You Need?
- **Arxiv ID**: http://arxiv.org/abs/2106.09516v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.09516v1)
- **Published**: 2021-06-16 16:14:01+00:00
- **Updated**: 2021-06-16 16:14:01+00:00
- **Authors**: Imtiaz Masud Ziko, Malik Boudiaf, Jose Dolz, Eric Granger, Ismail Ben Ayed
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate a general formulation for clustering and transductive few-shot learning, which integrates prototype-based objectives, Laplacian regularization and supervision constraints from a few labeled data points. We propose a concave-convex relaxation of the problem, and derive a computationally efficient block-coordinate bound optimizer, with convergence guarantee. At each iteration,our optimizer computes independent (parallel) updates for each point-to-cluster assignment. Therefore, it could be trivially distributed for large-scale clustering and few-shot tasks. Furthermore, we provides a thorough convergence analysis based on point-to-set maps. Were port comprehensive clustering and few-shot learning experiments over various data sets, showing that our method yields competitive performances, in term of accuracy and optimization quality, while scaling up to large problems. Using standard training on the base classes, without resorting to complex meta-learning and episodic-training strategies, our approach outperforms state-of-the-art few-shot methods by significant margins, across various models, settings and data sets. Surprisingly, we found that even standard clustering procedures (e.g., K-means), which correspond to particular, non-regularized cases of our general model, already achieve competitive performances in comparison to the state-of-the-art in few-shot learning. These surprising results point to the limitations of the current few-shot benchmarks, and question the viability of a large body of convoluted few-shot learning techniques in the recent literature.



### Differentiable Diffusion for Dense Depth Estimation from Multi-view Images
- **Arxiv ID**: http://arxiv.org/abs/2106.08917v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08917v2)
- **Published**: 2021-06-16 16:17:34+00:00
- **Updated**: 2021-06-29 15:43:24+00:00
- **Authors**: Numair Khan, Min H. Kim, James Tompkin
- **Comment**: None
- **Journal**: CVPR 2021
- **Summary**: We present a method to estimate dense depth by optimizing a sparse set of points such that their diffusion into a depth map minimizes a multi-view reprojection error from RGB supervision. We optimize point positions, depths, and weights with respect to the loss by differential splatting that models points as Gaussians with analytic transmittance. Further, we develop an efficient optimization routine that can simultaneously optimize the 50k+ points required for complex scene reconstruction. We validate our routine using ground truth data and show high reconstruction quality. Then, we apply this to light field and wider baseline images via self supervision, and show improvements in both average and outlier error for depth maps diffused from inaccurate sparse points. Finally, we compare qualitative and quantitative results to image processing and deep learning methods. http://visual.cs.brown.edu/diffdiffdepth



### A Spiking Neural Network for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.08921v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.08921v1)
- **Published**: 2021-06-16 16:23:18+00:00
- **Updated**: 2021-06-16 16:23:18+00:00
- **Authors**: Kinjal Patel, Eric Hunsberger, Sean Batir, Chris Eliasmith
- **Comment**: None
- **Journal**: None
- **Summary**: We seek to investigate the scalability of neuromorphic computing for computer vision, with the objective of replicating non-neuromorphic performance on computer vision tasks while reducing power consumption. We convert the deep Artificial Neural Network (ANN) architecture U-Net to a Spiking Neural Network (SNN) architecture using the Nengo framework. Both rate-based and spike-based models are trained and optimized for benchmarking performance and power, using a modified version of the ISBI 2D EM Segmentation dataset consisting of microscope images of cells. We propose a partitioning method to optimize inter-chip communication to improve speed and energy efficiency when deploying multi-chip networks on the Loihi neuromorphic chip. We explore the advantages of regularizing firing rates of Loihi neurons for converting ANN to SNN with minimum accuracy loss and optimized energy consumption. We propose a percentile based regularization loss function to limit the spiking rate of the neuron between a desired range. The SNN is converted directly from the corresponding ANN, and demonstrates similar semantic segmentation as the ANN using the same number of neurons and weights. However, the neuromorphic implementation on the Intel Loihi neuromorphic chip is over 2x more energy-efficient than conventional hardware (CPU, GPU) when running online (one image at a time). These power improvements are achieved without sacrificing the task performance accuracy of the network, and when all weights (Loihi, CPU, and GPU networks) are quantized to 8 bits.



### Improved CNN-based Learning of Interpolation Filters for Low-Complexity Inter Prediction in Video Coding
- **Arxiv ID**: http://arxiv.org/abs/2106.08936v1
- **DOI**: 10.1109/OJSP.2021.3089439
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.08936v1)
- **Published**: 2021-06-16 16:48:01+00:00
- **Updated**: 2021-06-16 16:48:01+00:00
- **Authors**: Luka Murn, Saverio Blasi, Alan F. Smeaton, Marta Mrak
- **Comment**: IEEE Open Journal of Signal Processing Special Issue on Applied AI
  and Machine Learning for Video Coding and Streaming, June 2021
- **Journal**: None
- **Summary**: The versatility of recent machine learning approaches makes them ideal for improvement of next generation video compression solutions. Unfortunately, these approaches typically bring significant increases in computational complexity and are difficult to interpret into explainable models, affecting their potential for implementation within practical video coding applications. This paper introduces a novel explainable neural network-based inter-prediction scheme, to improve the interpolation of reference samples needed for fractional precision motion compensation. The approach requires a single neural network to be trained from which a full quarter-pixel interpolation filter set is derived, as the network is easily interpretable due to its linear structure. A novel training framework enables each network branch to resemble a specific fractional shift. This practical solution makes it very efficient to use alongside conventional video coding schemes. When implemented in the context of the state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and 2.25% BD-rate savings can be achieved on average for lower resolution sequences under the random access, low-delay B and low-delay P configurations, respectively, while the complexity of the learned interpolation schemes is significantly reduced compared to the interpolation with full CNNs.



### Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch
- **Arxiv ID**: http://arxiv.org/abs/2106.08970v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.08970v3)
- **Published**: 2021-06-16 17:09:55+00:00
- **Updated**: 2022-10-13 16:01:36+00:00
- **Authors**: Hossein Souri, Liam Fowl, Rama Chellappa, Micah Goldblum, Tom Goldstein
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: As the curation of data for machine learning becomes increasingly automated, dataset tampering is a mounting threat. Backdoor attackers tamper with training data to embed a vulnerability in models that are trained on that data. This vulnerability is then activated at inference time by placing a "trigger" into the model's input. Typical backdoor attacks insert the trigger directly into the training data, although the presence of such an attack may be visible upon inspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning without placing a trigger into the training data at all. However, this hidden trigger attack is ineffective at poisoning neural networks trained from scratch. We develop a new hidden trigger attack, Sleeper Agent, which employs gradient matching, data selection, and target model re-training during the crafting process. Sleeper Agent is the first hidden trigger backdoor attack to be effective against neural networks trained from scratch. We demonstrate its effectiveness on ImageNet and in black-box settings. Our implementation code can be found at https://github.com/hsouri/Sleeper-Agent.



### The Oxford Road Boundaries Dataset
- **Arxiv ID**: http://arxiv.org/abs/2106.08983v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.08983v1)
- **Published**: 2021-06-16 17:23:34+00:00
- **Updated**: 2021-06-16 17:23:34+00:00
- **Authors**: Tarlan Suleymanov, Matthew Gadd, Daniele De Martini, Paul Newman
- **Comment**: Accepted for publication at the workshop "3D-DLAD: 3D-Deep Learning
  for Autonomous Driving" (WS15), Intelligent Vehicles Symposium (IV 2021)
- **Journal**: None
- **Summary**: In this paper we present the Oxford Road Boundaries Dataset, designed for training and testing machine-learning-based road-boundary detection and inference approaches. We have hand-annotated two of the 10 km-long forays from the Oxford Robotcar Dataset and generated from other forays several thousand further examples with semi-annotated road-boundary masks. To boost the number of training samples in this way, we used a vision-based localiser to project labels from the annotated datasets to other traversals at different times and weather conditions. As a result, we release 62605 labelled samples, of which 47639 samples are curated. Each of these samples contains both raw and classified masks for left and right lenses. Our data contains images from a diverse set of scenarios such as straight roads, parked cars, junctions, etc. Files for download and tools for manipulating the labelled data are available at: oxford-robotics-institute.github.io/road-boundaries-dataset



### Soft Attention: Does it Actually Help to Learn Social Interactions in Pedestrian Trajectory Prediction?
- **Arxiv ID**: http://arxiv.org/abs/2106.15321v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15321v1)
- **Published**: 2021-06-16 17:39:35+00:00
- **Updated**: 2021-06-16 17:39:35+00:00
- **Authors**: Laurent Boucaud, Daniel Aloise, Nicolas Saunier
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: We consider the problem of predicting the future path of a pedestrian using its motion history and the motion history of the surrounding pedestrians, called social information. Since the seminal paper on Social-LSTM, deep-learning has become the main tool used to model the impact of social interactions on a pedestrian's motion. The demonstration that these models can learn social interactions relies on an ablative study of these models. The models are compared with and without their social interactions module on two standard metrics, the Average Displacement Error and Final Displacement Error. Yet, these complex models were recently outperformed by a simple constant-velocity approach. This questions if they actually allow to model social interactions as well as the validity of the proof. In this paper, we focus on the deep-learning models with a soft-attention mechanism for social interaction modeling and study whether they use social information at prediction time. We conduct two experiments across four state-of-the-art approaches on the ETH and UCY datasets, which were also used in previous work. First, the models are trained by replacing the social information with random noise and compared to model trained with actual social information. Second, we use a gating mechanism along with a $L_0$ penalty, allowing models to shut down their inner components. The models consistently learn to prune their soft-attention mechanism. For both experiments, neither the course of the convergence nor the prediction performance were altered. This demonstrates that the soft-attention mechanism and therefore the social information are ignored by the models.



### On the approximation capability of GNNs in node classification/regression tasks
- **Arxiv ID**: http://arxiv.org/abs/2106.08992v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, F.2.2; G.2.2; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2106.08992v5)
- **Published**: 2021-06-16 17:46:51+00:00
- **Updated**: 2023-06-25 14:04:29+00:00
- **Authors**: Giuseppe Alessio D'Inverno, Monica Bianchini, Maria Lucia Sampoli, Franco Scarselli
- **Comment**: 22 pages, 5 figures
- **Journal**: None
- **Summary**: Graph Neural Networks (GNNs) are a broad class of connectionist models for graph processing. Recent studies have shown that GNNs can approximate any function on graphs, modulo the equivalence relation on graphs defined by the Weisfeiler--Lehman (WL) test. However, these results suffer from some limitations, both because they were derived using the Stone--Weierstrass theorem -- which is existential in nature, -- and because they assume that the target function to be approximated must be continuous. Furthermore, all current results are dedicated to graph classification/regression tasks, where the GNN must produce a single output for the whole graph, while also node classification/regression problems, in which an output is returned for each node, are very common. In this paper, we propose an alternative way to demonstrate the approximation capability of GNNs that overcomes these limitations. Indeed, we show that GNNs are universal approximators in probability for node classification/regression tasks, as they can approximate any measurable function that satisfies the 1--WL equivalence on nodes. The proposed theoretical framework allows the approximation of generic discontinuous target functions and also suggests the GNN architecture that can reach a desired approximation. In addition, we provide a bound on the number of the GNN layers required to achieve the desired degree of approximation, namely $2r-1$, where $r$ is the maximum number of nodes for the graphs in the domain.



### Invertible Attention
- **Arxiv ID**: http://arxiv.org/abs/2106.09003v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09003v2)
- **Published**: 2021-06-16 17:55:02+00:00
- **Updated**: 2021-06-27 13:01:09+00:00
- **Authors**: Jiajun Zha, Yiran Zhong, Jing Zhang, Richard Hartley, Liang Zheng
- **Comment**: 19 pages. The code is available at
  https://github.com/Schwartz-Zha/InvertibleAttention
- **Journal**: None
- **Summary**: Attention has been proved to be an efficient mechanism to capture long-range dependencies. However, so far it has not been deployed in invertible networks. This is due to the fact that in order to make a network invertible, every component within the network needs to be a bijective transformation, but a normal attention block is not. In this paper, we propose invertible attention that can be plugged into existing invertible models. We mathematically and experimentally prove that the invertibility of an attention model can be achieved by carefully constraining its Lipschitz constant. We validate the invertibility of our invertible attention on image reconstruction task with 3 popular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible attention achieves similar performance in comparison with normal non-invertible attention on dense prediction tasks. The code is available at https://github.com/Schwartz-Zha/InvertibleAttention



### Evolving Image Compositions for Feature Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.09011v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2106.09011v2)
- **Published**: 2021-06-16 17:57:18+00:00
- **Updated**: 2022-03-31 19:47:18+00:00
- **Authors**: Paola Cascante-Bonilla, Arshdeep Sekhon, Yanjun Qi, Vicente Ordonez
- **Comment**: Accepted to BMVC 2021. Camera-Ready version. Project page:
  https://paolacascante.com/patchmix/index.html
- **Journal**: None
- **Summary**: Convolutional neural networks for visual recognition require large amounts of training samples and usually benefit from data augmentation. This paper proposes PatchMix, a data augmentation method that creates new samples by composing patches from pairs of images in a grid-like pattern. These new samples are assigned label scores that are proportional to the number of patches borrowed from each image. We then add a set of additional losses at the patch-level to regularize and to encourage good representations at both the patch and image levels. A ResNet-50 model trained on ImageNet using PatchMix exhibits superior transfer learning capabilities across a wide array of benchmarks. Although PatchMix can rely on random pairings and random grid-like patterns for mixing, we explore evolutionary search as a guiding strategy to jointly discover optimal grid-like patterns and image pairings. For this purpose, we conceive a fitness function that bypasses the need to re-train a model to evaluate each possible choice. In this way, PatchMix outperforms a base model on CIFAR-10 (+1.91), CIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16).



### Cascading Modular Network (CAM-Net) for Multimodal Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2106.09015v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.09015v1)
- **Published**: 2021-06-16 17:58:13+00:00
- **Updated**: 2021-06-16 17:58:13+00:00
- **Authors**: Shichong Peng, Alireza Moazeni, Ke Li
- **Comment**: Videos available as ancillary files
- **Journal**: None
- **Summary**: Deep generative models such as GANs have driven impressive advances in conditional image synthesis in recent years. A persistent challenge has been to generate diverse versions of output images from the same input image, due to the problem of mode collapse: because only one ground truth output image is given per input image, only one mode of the conditional distribution is modelled. In this paper, we focus on this problem of multimodal conditional image synthesis and build on the recently proposed technique of Implicit Maximum Likelihood Estimation (IMLE). Prior IMLE-based methods required different architectures for different tasks, which limit their applicability, and were lacking in fine details in the generated images. We propose CAM-Net, a unified architecture that can be applied to a broad range of tasks. Additionally, it is capable of generating convincing high frequency details, achieving a reduction of the Frechet Inception Distance (FID) by up to 45.3% compared to the baseline.



### Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2106.09016v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.09016v1)
- **Published**: 2021-06-16 17:58:21+00:00
- **Updated**: 2021-06-16 17:58:21+00:00
- **Authors**: Yahui Liu, Enver Sangineto, Yajing Chen, Linchao Bao, Haoxian Zhang, Nicu Sebe, Bruno Lepri, Wei Wang, Marco De Nadai
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Image-to-Image (I2I) multi-domain translation models are usually evaluated also using the quality of their semantic interpolation results. However, state-of-the-art models frequently show abrupt changes in the image appearance during interpolation, and usually perform poorly in interpolations across domains. In this paper, we propose a new training protocol based on three specific losses which help a translation network to learn a smooth and disentangled latent style space in which: 1) Both intra- and inter-domain interpolations correspond to gradual changes in the generated images and 2) The content of the source image is better preserved during the translation. Moreover, we propose a novel evaluation metric to properly measure the smoothness of latent style space of I2I translation models. The proposed method can be plugged into existing translation approaches, and our extensive experiments on different datasets show that it can significantly boost the quality of the generated images and the graduality of the interpolations.



### Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2106.09017v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.09017v1)
- **Published**: 2021-06-16 17:58:23+00:00
- **Updated**: 2021-06-16 17:58:23+00:00
- **Authors**: Haoxiang Wang, Han Zhao, Bo Li
- **Comment**: ICML 2021 camera-ready version. Code is released at
  https://github.com/AI-secure/multi-task-learning
- **Journal**: None
- **Summary**: Multi-task learning (MTL) aims to improve the generalization of several related tasks by learning them jointly. As a comparison, in addition to the joint training scheme, modern meta-learning allows unseen tasks with limited labels during the test phase, in the hope of fast adaptation over them. Despite the subtle difference between MTL and meta-learning in the problem formulation, both learning paradigms share the same insight that the shared structure between existing training tasks could lead to better generalization and adaptation. In this paper, we take one important step further to understand the close connection between these two learning paradigms, through both theoretical analysis and empirical investigation. Theoretically, we first demonstrate that MTL shares the same optimization formulation with a class of gradient-based meta-learning (GBML) algorithms. We then prove that for over-parameterized neural networks with sufficient depth, the learned predictive functions of MTL and GBML are close. In particular, this result implies that the predictions given by these two models are similar over the same unseen task. Empirically, we corroborate our theoretical findings by showing that, with proper implementation, MTL is competitive against state-of-the-art GBML algorithms on a set of few-shot image classification benchmarks. Since existing GBML algorithms often involve costly second-order bi-level optimization, our first-order MTL method is an order of magnitude faster on large-scale datasets such as mini-ImageNet. We believe this work could help bridge the gap between these two learning paradigms, and provide a computationally efficient alternative to GBML that also supports fast task adaptation.



### End-to-End Semi-Supervised Object Detection with Soft Teacher
- **Arxiv ID**: http://arxiv.org/abs/2106.09018v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.09018v3)
- **Published**: 2021-06-16 17:59:30+00:00
- **Updated**: 2021-08-06 16:28:39+00:00
- **Authors**: Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, Zicheng Liu
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: This paper presents an end-to-end semi-supervised object detection approach, in contrast to previous more complex multi-stage methods. The end-to-end training gradually improves pseudo label qualities during the curriculum, and the more and more accurate pseudo labels in turn benefit object detection training. We also propose two simple yet effective techniques within this framework: a soft teacher mechanism where the classification loss of each unlabeled bounding box is weighed by the classification score produced by the teacher network; a box jittering approach to select reliable pseudo boxes for the learning of box regression. On the COCO benchmark, the proposed approach outperforms previous methods by a large margin under various labeling ratios, i.e. 1\%, 5\% and 10\%. Moreover, our approach proves to perform also well when the amount of labeled data is relatively large. For example, it can improve a 40.9 mAP baseline detector trained using the full COCO training set by +3.6 mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the state-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev), it can still significantly improve the detection accuracy by +1.5 mAP, reaching 60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching 52.4 mAP. Further incorporating with the Object365 pre-trained model, the detection accuracy reaches 61.3 mAP and the instance segmentation accuracy reaches 53.0 mAP, pushing the new state-of-the-art.



### Regularization of Mixture Models for Robust Principal Graph Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.09035v2
- **DOI**: 10.1109/TPAMI.2021.3124973
- **Categories**: **cs.LG**, cond-mat.dis-nn, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.09035v2)
- **Published**: 2021-06-16 18:00:02+00:00
- **Updated**: 2023-07-10 13:14:18+00:00
- **Authors**: Tony Bonnaire, Aurlien Decelle, Nabila Aghanim
- **Comment**: 12 pages, 6 figures
- **Journal**: in IEEE Transactions on Pattern Analysis and Machine Intelligence,
  vol. 44, no. 12, pp. 9119-9130, 1 Dec. 2022
- **Summary**: A regularized version of Mixture Models is proposed to learn a principal graph from a distribution of $D$-dimensional data points. In the particular case of manifold learning for ridge detection, we assume that the underlying manifold can be modeled as a graph structure acting like a topological prior for the Gaussian clusters turning the problem into a maximum a posteriori estimation. Parameters of the model are iteratively estimated through an Expectation-Maximization procedure making the learning of the structure computationally efficient with guaranteed convergence for any graph prior in a polynomial time. We also embed in the formalism a natural way to make the algorithm robust to outliers of the pattern and heteroscedasticity of the manifold sampling coherently with the graph structure. The method uses a graph prior given by the minimum spanning tree that we extend using random sub-samplings of the dataset to take into account cycles that can be observed in the spatial distribution.



### Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure
- **Arxiv ID**: http://arxiv.org/abs/2106.09051v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.09051v1)
- **Published**: 2021-06-16 18:00:12+00:00
- **Updated**: 2021-06-16 18:00:12+00:00
- **Authors**: Paul Henderson, Christoph H. Lampert, Bernd Bickel
- **Comment**: None
- **Journal**: None
- **Summary**: Our goal in this work is to generate realistic videos given just one initial frame as input. Existing unsupervised approaches to this task do not consider the fact that a video typically shows a 3D environment, and that this should remain coherent from frame to frame even as the camera and objects move. We address this by developing a model that first estimates the latent 3D structure of the scene, including the segmentation of any moving objects. It then predicts future frames by simulating the object and camera dynamics, and rendering the resulting views. Importantly, it is trained end-to-end using only the unsupervised objective of predicting future frames, without any 3D information nor segmentation annotations. Experiments on two challenging datasets of natural videos show that our model can estimate 3D structure and motion segmentation from a single frame, and hence generate plausible and varied predictions.



### Automatic Main Character Recognition for Photographic Studies
- **Arxiv ID**: http://arxiv.org/abs/2106.09064v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.09064v1)
- **Published**: 2021-06-16 18:14:45+00:00
- **Updated**: 2021-06-16 18:14:45+00:00
- **Authors**: Mert Seker, Anssi Mnnist, Alexandros Iosifidis, Jenni Raitoharju
- **Comment**: 6 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Main characters in images are the most important humans that catch the viewer's attention upon first look, and they are emphasized by properties such as size, position, color saturation, and sharpness of focus. Identifying the main character in images plays an important role in traditional photographic studies and media analysis, but the task is performed manually and can be slow and laborious. Furthermore, selection of main characters can be sometimes subjective. In this paper, we analyze the feasibility of solving the main character recognition needed for photographic studies automatically and propose a method for identifying the main characters. The proposed method uses machine learning based human pose estimation along with traditional computer vision approaches for this task. We approach the task as a binary classification problem where each detected human is classified either as a main character or not. To evaluate both the subjectivity of the task and the performance of our method, we collected a dataset of 300 varying images from multiple sources and asked five people, a photographic researcher and four other persons, to annotate the main characters. Our analysis showed a relatively high agreement between different annotators. The proposed method achieved a promising F1 score of 0.83 on the full image set and 0.96 on a subset evaluated as most clear and important cases by the photographic researcher.



### SPeCiaL: Self-Supervised Pretraining for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.09065v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.09065v1)
- **Published**: 2021-06-16 18:15:15+00:00
- **Updated**: 2021-06-16 18:15:15+00:00
- **Authors**: Lucas Caccia, Joelle Pineau
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents SPeCiaL: a method for unsupervised pretraining of representations tailored for continual learning. Our approach devises a meta-learning objective that differentiates through a sequential learning process. Specifically, we train a linear model over the representations to match different augmented views of the same image together, each view presented sequentially. The linear model is then evaluated on both its ability to classify images it just saw, and also on images from previous iterations. This gives rise to representations that favor quick knowledge retention with minimal forgetting. We evaluate SPeCiaL in the Continual Few-Shot Learning setting, and show that it can match or outperform other supervised pretraining approaches.



### Deformation Driven Seq2Seq Longitudinal Tumor and Organs-at-Risk Prediction for Radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2106.09076v3
- **DOI**: 10.1002/mp.15075
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09076v3)
- **Published**: 2021-06-16 18:29:16+00:00
- **Updated**: 2021-08-31 02:36:44+00:00
- **Authors**: Donghoon Lee, Sadegh R Alam, Jue Jiang, Pengpeng Zhang, Saad Nadeem, Yu-Chi Hu
- **Comment**: Medical Physics 2021, Saad Nadeem and Yu-Chi Hu contributed equally
- **Journal**: None
- **Summary**: Purpose: Radiotherapy presents unique challenges and clinical requirements for longitudinal tumor and organ-at-risk (OAR) prediction during treatment. The challenges include tumor inflammation/edema and radiation-induced changes in organ geometry, whereas the clinical requirements demand flexibility in input/output sequence timepoints to update the predictions on rolling basis and the grounding of all predictions in relationship to the pre-treatment imaging information for response and toxicity assessment in adaptive radiotherapy. Methods: To deal with the aforementioned challenges and to comply with the clinical requirements, we present a novel 3D sequence-to-sequence model based on Convolution Long Short Term Memory (ConvLSTM) that makes use of series of deformation vector fields (DVF) between individual timepoints and reference pre-treatment/planning CTs to predict future anatomical deformations and changes in gross tumor volume as well as critical OARs. High-quality DVF training data is created by employing hyper-parameter optimization on the subset of the training data with DICE coefficient and mutual information metric. We validated our model on two radiotherapy datasets: a publicly available head-and-neck dataset (28 patients with manually contoured pre-, mid-, and post-treatment CTs), and an internal non-small cell lung cancer dataset (63 patients with manually contoured planning CT and 6 weekly CBCTs). Results: The use of DVF representation and skip connections overcomes the blurring issue of ConvLSTM prediction with the traditional image representation. The mean and standard deviation of DICE for predictions of lung GTV at week 4, 5, and 6 were 0.83$\pm$0.09, 0.82$\pm$0.08, and 0.81$\pm$0.10, respectively, and for post-treatment ipsilateral and contralateral parotids, were 0.81$\pm$0.06 and 0.85$\pm$0.02.



### Scaling-up Diverse Orthogonal Convolutional Networks with a Paraunitary Framework
- **Arxiv ID**: http://arxiv.org/abs/2106.09121v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2106.09121v1)
- **Published**: 2021-06-16 20:50:59+00:00
- **Updated**: 2021-06-16 20:50:59+00:00
- **Authors**: Jiahao Su, Wonmin Byeon, Furong Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Enforcing orthogonality in neural networks is an antidote for gradient vanishing/exploding problems, sensitivity by adversarial perturbation, and bounding generalization errors. However, many previous approaches are heuristic, and the orthogonality of convolutional layers is not systematically studied: some of these designs are not exactly orthogonal, while others only consider standard convolutional layers and propose specific classes of their realizations. To address this problem, we propose a theoretical framework for orthogonal convolutional layers, which establishes the equivalence between various orthogonal convolutional layers in the spatial domain and the paraunitary systems in the spectral domain. Since there exists a complete spectral factorization of paraunitary systems, any orthogonal convolution layer can be parameterized as convolutions of spatial filters. Our framework endows high expressive power to various convolutional layers while maintaining their exact orthogonality. Furthermore, our layers are memory and computationally efficient for deep networks compared to previous designs. Our versatile framework, for the first time, enables the study of architecture designs for deep orthogonal networks, such as choices of skip connection, initialization, stride, and dilation. Consequently, we scale up orthogonal networks to deep architectures, including ResNet, WideResNet, and ShuffleNet, substantially increasing the performance over the traditional shallow orthogonal networks.



### Probing Image-Language Transformers for Verb Understanding
- **Arxiv ID**: http://arxiv.org/abs/2106.09141v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.09141v1)
- **Published**: 2021-06-16 21:36:36+00:00
- **Updated**: 2021-06-16 21:36:36+00:00
- **Authors**: Lisa Anne Hendricks, Aida Nematzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal image-language transformers have achieved impressive results on a variety of tasks that rely on fine-tuning (e.g., visual question answering and image retrieval). We are interested in shedding light on the quality of their pretrained representations -- in particular, if these models can distinguish different types of verbs or if they rely solely on nouns in a given sentence. To do so, we collect a dataset of image-sentence pairs (in English) consisting of 421 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset). We use this dataset to evaluate pretrained image-language transformers and find that they fail more in situations that require verb understanding compared to other parts of speech. We also investigate what category of verbs are particularly challenging.



### Positional Contrastive Learning for Volumetric Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.09157v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09157v3)
- **Published**: 2021-06-16 22:15:28+00:00
- **Updated**: 2021-09-28 18:01:06+00:00
- **Authors**: Dewen Zeng, Yawen Wu, Xinrong Hu, Xiaowei Xu, Haiyun Yuan, Meiping Huang, Jian Zhuang, Jingtong Hu, Yiyu Shi
- **Comment**: 8 pages, conference
- **Journal**: None
- **Summary**: The success of deep learning heavily depends on the availability of large labeled training sets. However, it is hard to get large labeled datasets in medical image domain because of the strict privacy concern and costly labeling efforts. Contrastive learning, an unsupervised learning technique, has been proved powerful in learning image-level representations from unlabeled data. The learned encoder can then be transferred or fine-tuned to improve the performance of downstream tasks with limited labels. A critical step in contrastive learning is the generation of contrastive data pairs, which is relatively simple for natural image classification but quite challenging for medical image segmentation due to the existence of the same tissue or organ across the dataset. As a result, when applied to medical image segmentation, most state-of-the-art contrastive learning frameworks inevitably introduce a lot of false-negative pairs and result in degraded segmentation quality. To address this issue, we propose a novel positional contrastive learning (PCL) framework to generate contrastive data pairs by leveraging the position information in volumetric medical images. Experimental results on CT and MRI datasets demonstrate that the proposed PCL method can substantially improve the segmentation performance compared to existing methods in both semi-supervised setting and transfer learning setting.



### LiRA: Learning Visual Speech Representations from Audio through Self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2106.09171v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2106.09171v1)
- **Published**: 2021-06-16 23:20:06+00:00
- **Updated**: 2021-06-16 23:20:06+00:00
- **Authors**: Pingchuan Ma, Rodrigo Mira, Stavros Petridis, Bjrn W. Schuller, Maja Pantic
- **Comment**: Accepted for publication at Interspeech 2021
- **Journal**: None
- **Summary**: The large amount of audiovisual content being shared online today has drawn substantial attention to the prospect of audiovisual self-supervised learning. Recent works have focused on each of these modalities separately, while others have attempted to model both simultaneously in a cross-modal fashion. However, comparatively little attention has been given to leveraging one modality as a training objective to learn from the other. In this work, we propose Learning visual speech Representations from Audio via self-supervision (LiRA). Specifically, we train a ResNet+Conformer model to predict acoustic features from unlabelled visual speech. We find that this pre-trained model can be leveraged towards word-level and sentence-level lip-reading through feature extraction and fine-tuning experiments. We show that our approach significantly outperforms other self-supervised methods on the Lip Reading in the Wild (LRW) dataset and achieves state-of-the-art performance on Lip Reading Sentences 2 (LRS2) using only a fraction of the total labelled data.



### Effective Evaluation of Deep Active Learning on Image Classification Tasks
- **Arxiv ID**: http://arxiv.org/abs/2106.15324v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15324v3)
- **Published**: 2021-06-16 23:29:39+00:00
- **Updated**: 2021-11-02 20:25:49+00:00
- **Authors**: Nathan Beck, Durga Sivasubramanian, Apurva Dani, Ganesh Ramakrishnan, Rishabh Iyer
- **Comment**: 10 pages in main paper, 6 figures in main paper, 2 tables in main
  paper. 24 pages in total, 15 figures in total, 13 tables in total
- **Journal**: None
- **Summary**: With the goal of making deep learning more label-efficient, a growing number of papers have been studying active learning (AL) for deep models. However, there are a number of issues in the prevalent experimental settings, mainly stemming from a lack of unified implementation and benchmarking. Issues in the current literature include sometimes contradictory observations on the performance of different AL algorithms, unintended exclusion of important generalization approaches such as data augmentation and SGD for optimization, a lack of study of evaluation facets like the labeling efficiency of AL, and little or no clarity on the scenarios in which AL outperforms random sampling (RS). In this work, we present a unified re-implementation of state-of-the-art AL algorithms in the context of image classification via our new open-source AL toolkit DISTIL, and we carefully study these issues as facets of effective evaluation. On the positive side, we show that AL techniques are $2\times$ to $4\times$ more label-efficient compared to RS with the use of data augmentation. Surprisingly, when data augmentation is included, there is no longer a consistent gain in using BADGE, a state-of-the-art approach, over simple uncertainty sampling. We then do a careful analysis of how existing approaches perform with varying amounts of redundancy and number of examples per class. Finally, we provide several insights for AL practitioners to consider in future work, such as the effect of the AL batch size, the effect of initialization, the importance of retraining the model at every round, and other insights.



### Insights into Data through Model Behaviour: An Explainability-driven Strategy for Data Auditing for Responsible Computer Vision Applications
- **Arxiv ID**: http://arxiv.org/abs/2106.09177v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.09177v1)
- **Published**: 2021-06-16 23:46:39+00:00
- **Updated**: 2021-06-16 23:46:39+00:00
- **Authors**: Alexander Wong, Adam Dorfman, Paul McInnis, Hayden Gunraj
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: In this study, we take a departure and explore an explainability-driven strategy to data auditing, where actionable insights into the data at hand are discovered through the eyes of quantitative explainability on the behaviour of a dummy model prototype when exposed to data. We demonstrate this strategy by auditing two popular medical benchmark datasets, and discover hidden data quality issues that lead deep learning models to make predictions for the wrong reasons. The actionable insights gained from this explainability driven data auditing strategy is then leveraged to address the discovered issues to enable the creation of high-performing deep learning models with appropriate prediction behaviour. The hope is that such an explainability-driven strategy can be complimentary to data-driven strategies to facilitate for more responsible development of machine learning algorithms for computer vision applications.



### The Fishnet Open Images Database: A Dataset for Fish Detection and Fine-Grained Categorization in Fisheries
- **Arxiv ID**: http://arxiv.org/abs/2106.09178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.09178v1)
- **Published**: 2021-06-16 23:53:18+00:00
- **Updated**: 2021-06-16 23:53:18+00:00
- **Authors**: Justin Kay, Matt Merrifield
- **Comment**: In 8th Workshop on Fine-Grained Visual Categorization at CVPR 2021
- **Journal**: None
- **Summary**: Camera-based electronic monitoring (EM) systems are increasingly being deployed onboard commercial fishing vessels to collect essential data for fisheries management and regulation. These systems generate large quantities of video data which must be reviewed on land by human experts. Computer vision can assist this process by automatically detecting and classifying fish species, however the lack of existing public data in this domain has hindered progress. To address this, we present the Fishnet Open Images Database, a large dataset of EM imagery for fish detection and fine-grained categorization onboard commercial fishing vessels. The dataset consists of 86,029 images containing 34 object classes, making it the largest and most diverse public dataset of fisheries EM imagery to-date. It includes many of the characteristic challenges of EM data: visual similarity between species, skewed class distributions, harsh weather conditions, and chaotic crew activity. We evaluate the performance of existing detection and classification algorithms and demonstrate that the dataset can serve as a challenging benchmark for development of computer vision algorithms in fisheries. The dataset is available at https://www.fishnet.ai/.




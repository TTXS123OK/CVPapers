# Arxiv Papers in cs.CV on 2021-06-27
### Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2106.14118v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.14118v4)
- **Published**: 2021-06-27 00:49:02+00:00
- **Updated**: 2021-10-17 17:41:25+00:00
- **Authors**: Anurag Bagchi, Jazib Mahmood, Dolton Fernandes, Ravi Kiran Sarvadevabhatla
- **Comment**: None
- **Journal**: None
- **Summary**: State of the art architectures for untrimmed video Temporal Action Localization (TAL) have only considered RGB and Flow modalities, leaving the information-rich audio modality totally unexploited. Audio fusion has been explored for the related but arguably easier problem of trimmed (clip-level) action recognition. However, TAL poses a unique set of challenges. In this paper, we propose simple but effective fusion-based approaches for TAL. To the best of our knowledge, our work is the first to jointly consider audio and video modalities for supervised TAL. We experimentally show that our schemes consistently improve performance for state of the art video-only TAL approaches. Specifically, they help achieve new state of the art performance on large-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14 (57.18 mAP@0.5). Our experiments include ablations involving multiple fusion schemes, modality combinations and TAL architectures. Our code, models and associated data are available at https://github.com/skelemoa/tal-hmo.



### Attention-guided Progressive Mapping for Profile Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.14124v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14124v2)
- **Published**: 2021-06-27 02:21:41+00:00
- **Updated**: 2021-06-29 10:33:31+00:00
- **Authors**: Junyang Huang, Changxing Ding
- **Comment**: Accepted by IJCB 2021. Code is available
- **Journal**: None
- **Summary**: The past few years have witnessed great progress in the domain of face recognition thanks to advances in deep learning. However, cross pose face recognition remains a significant challenge. It is difficult for many deep learning algorithms to narrow the performance gap caused by pose variations; the main reasons for this relate to the intra-class discrepancy between face images in different poses and the pose imbalances of training datasets. Learning pose-robust features by traversing to the feature space of frontal faces provides an effective and cheap way to alleviate this problem. In this paper, we present a method for progressively transforming profile face representations to the canonical pose with an attentive pair-wise loss. Firstly, to reduce the difficulty of directly transforming the profile face features into a frontal pose, we propose to learn the feature residual between the source pose and its nearby pose in a block-byblock fashion, and thus traversing to the feature space of a smaller pose by adding the learned residual. Secondly, we propose an attentive pair-wise loss to guide the feature transformation progressing in the most effective direction. Finally, our proposed progressive module and attentive pair-wise loss are light-weight and easy to implement, adding only about 7:5% extra parameters. Evaluations on the CFP and CPLFW datasets demonstrate the superiority of our proposed method. Code is available at https://github.com/hjy1312/AGPM.



### Visual Conceptual Blending with Large-scale Language and Vision Models
- **Arxiv ID**: http://arxiv.org/abs/2106.14127v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14127v1)
- **Published**: 2021-06-27 02:48:39+00:00
- **Updated**: 2021-06-27 02:48:39+00:00
- **Authors**: Songwei Ge, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: We ask the question: to what extent can recent large-scale language and image generation models blend visual concepts? Given an arbitrary object, we identify a relevant object and generate a single-sentence description of the blend of the two using a language model. We then generate a visual depiction of the blend using a text-based image generation model. Quantitative and qualitative evaluations demonstrate the superiority of language models over classical methods for conceptual blending, and of recent large-scale image generation models over prior models for the visual depiction.



### Robust Pose Transfer with Dynamic Details using Neural Video Rendering
- **Arxiv ID**: http://arxiv.org/abs/2106.14132v3
- **DOI**: 10.1109/TPAMI.2022.3166989
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2106.14132v3)
- **Published**: 2021-06-27 03:40:22+00:00
- **Updated**: 2023-05-08 14:59:47+00:00
- **Authors**: Yang-tian Sun, Hao-zhi Huang, Xuan Wang, Yu-kun Lai, Wei Liu, Lin Gao
- **Comment**: Video link: https://www.bilibili.com/video/BV1y64y1C7ge/
- **Journal**: None
- **Summary**: Pose transfer of human videos aims to generate a high fidelity video of a target person imitating actions of a source person. A few studies have made great progress either through image translation with deep latent features or neural rendering with explicit 3D features. However, both of them rely on large amounts of training data to generate realistic results, and the performance degrades on more accessible internet videos due to insufficient training frames. In this paper, we demonstrate that the dynamic details can be preserved even trained from short monocular videos. Overall, we propose a neural video rendering framework coupled with an image-translation-based dynamic details generation network (D2G-Net), which fully utilizes both the stability of explicit 3D features and the capacity of learning components. To be specific, a novel texture representation is presented to encode both the static and pose-varying appearance characteristics, which is then mapped to the image space and rendered as a detail-rich frame in the neural rendering stage. Moreover, we introduce a concise temporal loss in the training stage to suppress the detail flickering that is made more visible due to high-quality dynamic details generated by our method. Through extensive comparisons, we demonstrate that our neural human video renderer is capable of achieving both clearer dynamic details and more robust performance even on accessible short videos with only 2k - 4k frames.



### Semi-supervised Semantic Segmentation with Directional Context-aware Consistency
- **Arxiv ID**: http://arxiv.org/abs/2106.14133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14133v1)
- **Published**: 2021-06-27 03:42:40+00:00
- **Updated**: 2021-06-27 03:42:40+00:00
- **Authors**: Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, Hengshuang Zhao, Liwei Wang, Jiaya Jia
- **Comment**: Accepted to CVPR 2021. Code is available at
  https://github.com/dvlab-research/Context-Aware-Consistency
- **Journal**: None
- **Summary**: Semantic segmentation has made tremendous progress in recent years. However, satisfying performance highly depends on a large number of pixel-level annotations. Therefore, in this paper, we focus on the semi-supervised segmentation problem where only a small set of labeled data is provided with a much larger collection of totally unlabeled images. Nevertheless, due to the limited annotations, models may overly rely on the contexts available in the training data, which causes poor generalization to the scenes unseen before. A preferred high-level representation should capture the contextual information while not losing self-awareness. Therefore, we propose to maintain the context-aware consistency between features of the same identity but with different contexts, making the representations robust to the varying environments. Moreover, we present the Directional Contrastive Loss (DC Loss) to accomplish the consistency in a pixel-to-pixel manner, only requiring the feature with lower quality to be aligned towards its counterpart. In addition, to avoid the false-negative samples and filter the uncertain positive samples, we put forward two sampling strategies. Extensive experiments show that our simple yet effective method surpasses current state-of-the-art methods by a large margin and also generalizes well with extra image-level annotations.



### Building a Video-and-Language Dataset with Human Actions for Multimodal Logical Inference
- **Arxiv ID**: http://arxiv.org/abs/2106.14137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14137v1)
- **Published**: 2021-06-27 03:57:36+00:00
- **Updated**: 2021-06-27 03:57:36+00:00
- **Authors**: Riko Suzuki, Hitomi Yanaka, Koji Mineshima, Daisuke Bekki
- **Comment**: Accepted to MMSR I
- **Journal**: None
- **Summary**: This paper introduces a new video-and-language dataset with human actions for multimodal logical inference, which focuses on intentional and aspectual expressions that describe dynamic human actions. The dataset consists of 200 videos, 5,554 action labels, and 1,942 action triplets of the form <subject, predicate, object> that can be translated into logical semantic representations. The dataset is expected to be useful for evaluating multimodal inference systems between videos and semantically complicated sentences including negation and quantification.



### Machine Learning Detection Algorithm for Large Barkhausen Jumps in Cluttered Environment
- **Arxiv ID**: http://arxiv.org/abs/2106.14148v1
- **DOI**: 10.1109/LMAG.2019.2938463
- **Categories**: **cs.LG**, cs.CV, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2106.14148v1)
- **Published**: 2021-06-27 05:37:12+00:00
- **Updated**: 2021-06-27 05:37:12+00:00
- **Authors**: Roger Alimi, Amir Ivry, Elad Fisher, Eyal Weiss
- **Comment**: Accepted to IEEE Magnetics Letters
- **Journal**: pp. 1-5, vol. 10, year 2019
- **Summary**: Modern magnetic sensor arrays conventionally utilize state of the art low power magnetometers such as parallel and orthogonal fluxgates. Low power fluxgates tend to have large Barkhausen jumps that appear as a dc jump in the fluxgate output. This phenomenon deteriorates the signal fidelity and effectively increases the internal sensor noise. Even if sensors that are more prone to dc jumps can be screened during production, the conventional noise measurement does not always catch the dc jump because of its sparsity. Moreover, dc jumps persist in almost all the sensor cores although at a slower but still intolerable rate. Even if dc jumps can be easily detected in a shielded environment, when deployed in presence of natural noise and clutter, it can be hard to positively detect them. This work fills this gap and presents algorithms that distinguish dc jumps embedded in natural magnetic field data. To improve robustness to noise, we developed two machine learning algorithms that employ temporal and statistical physical-based features of a pre-acquired and well-known experimental data set. The first algorithm employs a support vector machine classifier, while the second is based on a neural network architecture. We compare these new approaches to a more classical kernel-based method. To that purpose, the receiver operating characteristic curve is generated, which allows diagnosis ability of the different classifiers by comparing their performances across various operation points. The accuracy of the machine learning-based algorithms over the classic method is highly emphasized. In addition, high generalization and robustness of the neural network can be concluded, based on the rapid convergence of the corresponding receiver operating characteristic curves.



### Image content dependent semi-fragile watermarking with localized tamper detection
- **Arxiv ID**: http://arxiv.org/abs/2106.14150v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.14150v1)
- **Published**: 2021-06-27 05:40:56+00:00
- **Updated**: 2021-06-27 05:40:56+00:00
- **Authors**: Samira Hosseini, Mojtaba Mahdavi
- **Comment**: 32 pages, 11 figures, 5 tables
- **Journal**: None
- **Summary**: Content-independent watermarks and block-wise independency can be considered as vulnerabilities in semi-fragile watermarking methods. In this paper to achieve the objectives of semi-fragile watermarking techniques, a method is proposed to not have the mentioned shortcomings. In the proposed method, the watermark is generated by relying on image content and a key. Furthermore, the embedding scheme causes the watermarked blocks to become dependent on each other, using a key. In the embedding phase, the image is partitioned into non-overlapping blocks. In order to detect and separate the different types of attacks more precisely, the proposed method embeds three copies of each watermark bit into LWT coefficients of each 4x4 block. In the authentication phase, by voting between the extracted bits the error maps are created; these maps indicate image authenticity and reveal the modified regions. Also, in order to automate the authentication, the images are classified into four categories using seven features. Classification accuracy in the experiments is 97.97 percent. It is noted that our experiments demonstrate that the proposed method is robust against JPEG compression and is competitive with a state-of-the-art semi-fragile watermarking method, in terms of robustness and semi-fragility.



### Post-Training Quantization for Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2106.14156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14156v1)
- **Published**: 2021-06-27 06:27:22+00:00
- **Updated**: 2021-06-27 06:27:22+00:00
- **Authors**: Zhenhua Liu, Yunhe Wang, Kai Han, Siwei Ma, Wen Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, transformer has achieved remarkable performance on a variety of computer vision applications. Compared with mainstream convolutional neural networks, vision transformers are often of sophisticated architectures for extracting powerful feature representations, which are more difficult to be developed on mobile devices. In this paper, we present an effective post-training quantization algorithm for reducing the memory storage and computational costs of vision transformers. Basically, the quantization task can be regarded as finding the optimal low-bit quantization intervals for weights and inputs, respectively. To preserve the functionality of the attention mechanism, we introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. Moreover, we thoroughly analyze the relationship between quantization loss of different layers and the feature diversity, and explore a mixed-precision quantization scheme by exploiting the nuclear norm of each attention map and output feature. The effectiveness of the proposed method is verified on several benchmark models and datasets, which outperforms the state-of-the-art post-training quantization algorithms. For instance, we can obtain an 81.29\% top-1 accuracy using DeiT-B model on ImageNet dataset with about 8-bit quantization.



### DenseTNT: Waymo Open Dataset Motion Prediction Challenge 1st Place Solution
- **Arxiv ID**: http://arxiv.org/abs/2106.14160v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.14160v2)
- **Published**: 2021-06-27 07:21:29+00:00
- **Updated**: 2021-09-26 06:29:11+00:00
- **Authors**: Junru Gu, Qiao Sun, Hang Zhao
- **Comment**: This is a technical report. ICCV paper is at arXiv:2108.09640 and
  project page is at https://github.com/Tsinghua-MARS-Lab/DenseTNT
- **Journal**: None
- **Summary**: In autonomous driving, goal-based multi-trajectory prediction methods are proved to be effective recently, where they first score goal candidates, then select a final set of goals, and finally complete trajectories based on the selected goals. However, these methods usually involve goal predictions based on sparse predefined anchors. In this work, we propose an anchor-free model, named DenseTNT, which performs dense goal probability estimation for trajectory prediction. Our model achieves state-of-the-art performance, and ranks 1st on the Waymo Open Dataset Motion Prediction Challenge. Project page is at https://github.com/Tsinghua-MARS-Lab/DenseTNT.



### Few-Shot Domain Expansion for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2106.14162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14162v1)
- **Published**: 2021-06-27 07:38:50+00:00
- **Updated**: 2021-06-27 07:38:50+00:00
- **Authors**: Bowen Yang, Jing Zhang, Zhenfei Yin, Jing Shao
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) is an indispensable and widely used module in face recognition systems. Although high accuracy has been achieved, a FAS system will never be perfect due to the non-stationary applied environments and the potential emergence of new types of presentation attacks in real-world applications. In practice, given a handful of labeled samples from a new deployment scenario (target domain) and abundant labeled face images in the existing source domain, the FAS system is expected to perform well in the new scenario without sacrificing the performance on the original domain. To this end, we identify and address a more practical problem: Few-Shot Domain Expansion for Face Anti-Spoofing (FSDE-FAS). This problem is challenging since with insufficient target domain training samples, the model may suffer from both overfitting to the target domain and catastrophic forgetting of the source domain. To address the problem, this paper proposes a Style transfer-based Augmentation for Semantic Alignment (SASA) framework. We propose to augment the target data by generating auxiliary samples based on photorealistic style transfer. With the assistant of the augmented data, we further propose a carefully designed mechanism to align different domains from both instance-level and distribution-level, and then stabilize the performance on the source domain with a less-forgetting constraint. Two benchmarks are proposed to simulate the FSDE-FAS scenarios, and the experimental results show that the proposed SASA method outperforms state-of-the-art methods.



### Indoor Panorama Planar 3D Reconstruction via Divide and Conquer
- **Arxiv ID**: http://arxiv.org/abs/2106.14166v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14166v2)
- **Published**: 2021-06-27 07:58:29+00:00
- **Updated**: 2021-09-09 11:12:50+00:00
- **Authors**: Cheng Sun, Chi-Wei Hsiao, Ning-Hsu Wang, Min Sun, Hwann-Tzong Chen
- **Comment**: Code at https://github.com/sunset1995/PanoPlane360. Video at
  https://www.youtube.com/watch?v=2uvP0V1oGRo
- **Journal**: None
- **Summary**: Indoor panorama typically consists of human-made structures parallel or perpendicular to gravity. We leverage this phenomenon to approximate the scene in a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this end, we propose an effective divide-and-conquer strategy that divides pixels based on their plane orientation estimation; then, the succeeding instance segmentation module conquers the task of planes clustering more easily in each plane orientation group. Besides, parameters of V-planes depend on camera yaw rotation, but translation-invariant CNNs are less aware of the yaw change. We thus propose a yaw-invariant V-planar reparameterization for CNNs to learn. We create a benchmark for indoor panorama planar reconstruction by extending existing 360 depth datasets with ground truth H\&V-planes (referred to as PanoH&V dataset) and adopt state-of-the-art planar reconstruction methods to predict H\&V-planes as our baselines. Our method outperforms the baselines by a large margin on the proposed dataset.



### A Behavior-aware Graph Convolution Network Model for Video Recommendation
- **Arxiv ID**: http://arxiv.org/abs/2106.15402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15402v1)
- **Published**: 2021-06-27 08:24:45+00:00
- **Updated**: 2021-06-27 08:24:45+00:00
- **Authors**: Wei Zhuo, Kunchi Liu, Taofeng Xue, Beihong Jin, Beibei Li, Xinzhou Dong, He Chen, Wenhai Pan, Xuejian Zhang, Shuo Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Interactions between users and videos are the major data source of performing video recommendation. Despite lots of existing recommendation methods, user behaviors on videos, which imply the complex relations between users and videos, are still far from being fully explored. In the paper, we present a model named Sagittarius. Sagittarius adopts a graph convolutional neural network to capture the influence between users and videos. In particular, Sagittarius differentiates between different user behaviors by weighting and fuses the semantics of user behaviors into the embeddings of users and videos. Moreover, Sagittarius combines multiple optimization objectives to learn user and video embeddings and then achieves the video recommendation by the learned user and video embeddings. The experimental results on multiple datasets show that Sagittarius outperforms several state-of-the-art models in terms of recall, unique recall and NDCG.



### Residual Moment Loss for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.14178v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.14178v1)
- **Published**: 2021-06-27 09:31:49+00:00
- **Updated**: 2021-06-27 09:31:49+00:00
- **Authors**: Quanziang Wang, Renzhen Wang, Yuexiang Li, Kai Ma, Yefeng Zheng, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Location information is proven to benefit the deep learning models on capturing the manifold structure of target objects, and accordingly boosts the accuracy of medical image segmentation. However, most existing methods encode the location information in an implicit way, e.g. the distance transform maps, which describe the relative distance from each pixel to the contour boundary, for the network to learn. These implicit approaches do not fully exploit the position information (i.e. absolute location) of targets. In this paper, we propose a novel loss function, namely residual moment (RM) loss, to explicitly embed the location information of segmentation targets during the training of deep learning networks. Particularly, motivated by image moments, the segmentation prediction map and ground-truth map are weighted by coordinate information. Then our RM loss encourages the networks to maintain the consistency between the two weighted maps, which promotes the segmentation networks to easily locate the targets and extract manifold-structure-related features. We validate the proposed RM loss by conducting extensive experiments on two publicly available datasets, i.e., 2D optic cup and disk segmentation and 3D left atrial segmentation. The experimental results demonstrate the effectiveness of our RM loss, which significantly boosts the accuracy of segmentation networks.



### The Story in Your Eyes: An Individual-difference-aware Model for Cross-person Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.14183v1
- **DOI**: 10.1109/TIP.2022.3171416
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14183v1)
- **Published**: 2021-06-27 10:14:10+00:00
- **Updated**: 2021-06-27 10:14:10+00:00
- **Authors**: Jun Bao, Buyu Liu, Jun Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method on refining cross-person gaze prediction task with eye/face images only by explicitly modelling the person-specific differences. Specifically, we first assume that we can obtain some initial gaze prediction results with existing method, which we refer to as InitNet, and then introduce three modules, the Validity Module (VM), Self-Calibration (SC) and Person-specific Transform (PT)) Module. By predicting the reliability of current eye/face images, our VM is able to identify invalid samples, e.g. eye blinking images, and reduce their effects in our modelling process. Our SC and PT module then learn to compensate for the differences on valid samples only. The former models the translation offsets by bridging the gap between initial predictions and dataset-wise distribution. And the later learns more general person-specific transformation by incorporating the information from existing initial predictions of the same person. We validate our ideas on three publicly available datasets, EVE, XGaze and MPIIGaze and demonstrate that our proposed method outperforms the SOTA methods significantly on all of them, e.g. respectively 21.7%, 36.0% and 32.9% relative performance improvements. We won the GAZE 2021 Competition on the EVE dataset. Our code can be found here https://github.com/bjj9/EVE_SCPT.



### Memory Guided Road Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.14184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14184v1)
- **Published**: 2021-06-27 10:17:02+00:00
- **Updated**: 2021-06-27 10:17:02+00:00
- **Authors**: Praveen Venkatesh, Rwik Rana, Varun Jain
- **Comment**: None
- **Journal**: None
- **Summary**: In self driving car applications, there is a requirement to predict the location of the lane given an input RGB front facing image. In this paper, we propose an architecture that allows us to increase the speed and robustness of road detection without a large hit in accuracy by introducing an underlying shared feature space that is propagated over time, which serves as a flowing dynamic memory. By utilizing the gist of previous frames, we train the network to predict the current road with a greater accuracy and lesser deviation from previous frames.



### An XAI Approach to Deep Learning Models in the Detection of DCIS
- **Arxiv ID**: http://arxiv.org/abs/2106.14186v2
- **DOI**: 10.1007/978-3-031-34171-7_33
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.14186v2)
- **Published**: 2021-06-27 10:22:33+00:00
- **Updated**: 2023-06-02 05:01:57+00:00
- **Authors**: Michele La Ferla
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: The results showed that XAI could indeed be used as a proof of concept to begin discussions on the implementation of assistive AI systems within the clinical community.



### Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic
- **Arxiv ID**: http://arxiv.org/abs/2106.14190v1
- **DOI**: 10.1016/j.patcog.2021.108057.
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.14190v1)
- **Published**: 2021-06-27 10:34:39+00:00
- **Updated**: 2021-06-27 10:34:39+00:00
- **Authors**: Nidhi Gowdra, Roopak Sinha, Stephen MacDonell, Wei Qi Yan
- **Comment**: Journal paper, 14 pages, 3 tables, 3 figures
- **Journal**: Pattern Recognition 119(2021), pp.108057
- **Summary**: Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and ResNeXt-56 are severely over-parameterized, necessitating a consequent increase in the computational resources required for model training which scales exponentially for increments in model depth. In this paper, we propose an Entropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust and simple, yet effective in resolving the problem of over-parameterization with regards to network depth of CNN model. The EBCLE heuristic employs a priori knowledge of the entropic data distribution of input datasets to determine an upper bound for convolutional network depth, beyond which identity transformations are prevalent offering insignificant contributions for enhancing model performance. Restricting depth redundancies by forcing feature compression and abstraction restricts over-parameterization while decreasing training time by 24.99% - 78.59% without degradation in model performance. We present empirical evidence to emphasize the relative effectiveness of broader, yet shallower models trained using the EBCLE heuristic, which maintains or outperforms baseline classification accuracies of narrower yet deeper models. The EBCLE heuristic is architecturally agnostic and EBCLE based CNN models restrict depth redundancies resulting in enhanced utilization of the available computational resources. The proposed EBCLE heuristic is a compelling technique for researchers to analytically justify their HyperParameter (HP) choices for CNNs. Empirical validation of the EBCLE heuristic in training CNN models was established on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10, MNIST) and four network architectures (DenseNet, ResNet, ResNeXt and EfficientNet B0-B2) with appropriate statistical tests employed to infer any conclusive claims presented in this paper.



### Disentangling semantic features of macromolecules in Cryo-Electron Tomography
- **Arxiv ID**: http://arxiv.org/abs/2106.14192v1
- **DOI**: None
- **Categories**: **q-bio.BM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14192v1)
- **Published**: 2021-06-27 10:41:26+00:00
- **Updated**: 2021-06-27 10:41:26+00:00
- **Authors**: Kai Yi, Jianye Pang, Yungeng Zhang, Xiangrui Zeng, Min Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Cryo-electron tomography (Cryo-ET) is a 3D imaging technique that enables the systemic study of shape, abundance, and distribution of macromolecular structures in single cells in near-atomic resolution. However, the systematic and efficient $\textit{de novo}$ recognition and recovery of macromolecular structures captured by Cryo-ET are very challenging due to the structural complexity and imaging limits. Even macromolecules with identical structures have various appearances due to different orientations and imaging limits, such as noise and the missing wedge effect. Explicitly disentangling the semantic features of macromolecules is crucial for performing several downstream analyses on the macromolecules. This paper has addressed the problem by proposing a 3D Spatial Variational Autoencoder that explicitly disentangle the structure, orientation, and shift of macromolecules. Extensive experiments on both synthesized and real cryo-ET datasets and cross-domain evaluations demonstrate the efficacy of our method.



### SAR-Net: Shape Alignment and Recovery Network for Category-level 6D Object Pose and Size Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.14193v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.14193v2)
- **Published**: 2021-06-27 10:41:50+00:00
- **Updated**: 2022-04-11 13:44:23+00:00
- **Authors**: Haitao Lin, Zichang Liu, Chilam Cheang, Yanwei Fu, Guodong Guo, Xiangyang Xue
- **Comment**: accepted by CVPR2022
- **Journal**: None
- **Summary**: Given a single scene image, this paper proposes a method of Category-level 6D Object Pose and Size Estimation (COPSE) from the point cloud of the target object, without external real pose-annotated training data. Specifically, beyond the visual cues in RGB images, we rely on the shape information predominately from the depth (D) channel. The key idea is to explore the shape alignment of each instance against its corresponding category-level template shape, and the symmetric correspondence of each object category for estimating a coarse 3D object shape. Our framework deforms the point cloud of the category-level template shape to align the observed instance point cloud for implicitly representing its 3D rotation. Then we model the symmetric correspondence by predicting symmetric point cloud from the partially observed point cloud. The concatenation of the observed point cloud and symmetric one reconstructs a coarse object shape, thus facilitating object center (3D translation) and 3D size estimation. Extensive experiments on the category-level NOCS benchmark demonstrate that our lightweight model still competes with state-of-the-art approaches that require labeled real-world images. We also deploy our approach to a physical Baxter robot to perform grasping tasks on unseen but category-known instances, and the results further validate the efficacy of our proposed model. Code and pre-trained models are available on the project webpage.



### Learning to solve geometric construction problems from images
- **Arxiv ID**: http://arxiv.org/abs/2106.14195v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG, cs.LG, cs.LO
- **Links**: [PDF](http://arxiv.org/pdf/2106.14195v1)
- **Published**: 2021-06-27 10:47:41+00:00
- **Updated**: 2021-06-27 10:47:41+00:00
- **Authors**: J. Macke, J. Sedlar, M. Olsak, J. Urban, J. Sivic
- **Comment**: 16 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: We describe a purely image-based method for finding geometric constructions with a ruler and compass in the Euclidea geometric game. The method is based on adapting the Mask R-CNN state-of-the-art image processing neural architecture and adding a tree-based search procedure to it. In a supervised setting, the method learns to solve all 68 kinds of geometric construction problems from the first six level packs of Euclidea with an average 92% accuracy. When evaluated on new kinds of problems, the method can solve 31 of the 68 kinds of Euclidea problems. We believe that this is the first time that a purely image-based learning has been trained to solve geometric construction problems of this difficulty.



### A Machine Learning Model for Early Detection of Diabetic Foot using Thermogram Images
- **Arxiv ID**: http://arxiv.org/abs/2106.14207v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.14207v1)
- **Published**: 2021-06-27 11:37:59+00:00
- **Updated**: 2021-06-27 11:37:59+00:00
- **Authors**: Amith Khandakar, Muhammad E. H. Chowdhury, Mamun Bin Ibne Reaz, Sawal Hamid Md Ali, Md Anwarul Hasan, Serkan Kiranyaz, Tawsifur Rahman, Rashad Alfkey, Ahmad Ashrif A. Bakar, Rayaz A. Malik
- **Comment**: 23 pages, 8 Figures
- **Journal**: None
- **Summary**: Diabetes foot ulceration (DFU) and amputation are a cause of significant morbidity. The prevention of DFU may be achieved by the identification of patients at risk of DFU and the institution of preventative measures through education and offloading. Several studies have reported that thermogram images may help to detect an increase in plantar temperature prior to DFU. However, the distribution of plantar temperature may be heterogeneous, making it difficult to quantify and utilize to predict outcomes. We have compared a machine learning-based scoring technique with feature selection and optimization techniques and learning classifiers to several state-of-the-art Convolutional Neural Networks (CNNs) on foot thermogram images and propose a robust solution to identify the diabetic foot. A comparatively shallow CNN model, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram image-based classification and the AdaBoost Classifier used 10 features and achieved an F1 score of 97 %. A comparison of the inference time for the best-performing networks confirmed that the proposed algorithm can be deployed as a smartphone application to allow the user to monitor the progression of the DFU in a home setting.



### Representation Based Regression for Object Distance Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.14208v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14208v1)
- **Published**: 2021-06-27 11:45:54+00:00
- **Updated**: 2021-06-27 11:45:54+00:00
- **Authors**: Mete Ahishali, Mehmet Yamac, Serkan Kiranyaz, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we propose a novel approach to predict the distances of the detected objects in an observed scene. The proposed approach modifies the recently proposed Convolutional Support Estimator Networks (CSENs). CSENs are designed to compute a direct mapping for the Support Estimation (SE) task in a representation-based classification problem. We further propose and demonstrate that representation-based methods (sparse or collaborative representation) can be used in well-designed regression problems. To the best of our knowledge, this is the first representation-based method proposed for performing a regression task by utilizing the modified CSENs; and hence, we name this novel approach as Representation-based Regression (RbR). The initial version of CSENs has a proxy mapping stage (i.e., a coarse estimation for the support set) that is required for the input. In this study, we improve the CSEN model by proposing Compressive Learning CSEN (CL-CSEN) that has the ability to jointly optimize the so-called proxy mapping stage along with convolutional layers. The experimental evaluations using the KITTI 3D Object Detection distance estimation dataset show that the proposed method can achieve a significantly improved distance estimation performance over all competing methods. Finally, the software implementations of the methods are publicly shared at https://github.com/meteahishali/CSENDistance.



### Multi-Modal Transformer for Accelerated MR Imaging
- **Arxiv ID**: http://arxiv.org/abs/2106.14248v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14248v3)
- **Published**: 2021-06-27 15:01:30+00:00
- **Updated**: 2022-05-11 13:03:03+00:00
- **Authors**: Chun-Mei Feng, Yunlu Yan, Geng Chen, Yong Xu, Ling Shao, Huazhu Fu
- **Comment**: https://github.com/chunmeifeng/MTrans
- **Journal**: None
- **Summary**: Accelerated multi-modal magnetic resonance (MR) imaging is a new and effective solution for fast MR imaging, providing superior performance in restoring the target modality from its undersampled counterpart with guidance from an auxiliary modality. However, existing works simply combine the auxiliary modality as prior information, lacking in-depth investigations on the potential mechanisms for fusing different modalities. Further, they usually rely on the convolutional neural networks (CNNs), which is limited by the intrinsic locality in capturing the long-distance dependency. To this end, we propose a multi-modal transformer (MTrans), which is capable of transferring multi-scale features from the target modality to the auxiliary modality, for accelerated MR imaging. To capture deep multi-modal information, our MTrans utilizes an improved multi-head attention mechanism, named cross attention module, which absorbs features from the auxiliary modality that contribute to the target modality. Our framework provides three appealing benefits: (i) Our MTrans use an improved transformers for multi-modal MR imaging, affording more global information compared with existing CNN-based methods. (ii) A new cross attention module is proposed to exploit the useful information in each modality at different scales. The small patch in the target modality aims to keep more fine details, the large patch in the auxiliary modality aims to obtain high-level context features from the larger region and supplement the target modality effectively. (iii) We evaluate MTrans with various accelerated multi-modal MR imaging tasks, e.g., MR image reconstruction and super-resolution, where MTrans outperforms state-of-the-art methods on fastMRI and real-world clinical datasets.



### Using deep learning to detect patients at risk for prostate cancer despite benign biopsies
- **Arxiv ID**: http://arxiv.org/abs/2106.14256v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2106.14256v3)
- **Published**: 2021-06-27 15:21:33+00:00
- **Updated**: 2022-04-19 02:40:02+00:00
- **Authors**: Bojing Liu, Yinxi Wang, Philippe Weitz, Johan Lindberg, Johan Hartman, Lars Egevad, Henrik Gr√∂nberg, Martin Eklund, Mattias Rantalainen
- **Comment**: 13 pages, 3 figures
- **Journal**: None
- **Summary**: Background: Transrectal ultrasound guided systematic biopsies of the prostate is a routine procedure to establish a prostate cancer diagnosis. However, the 10-12 prostate core biopsies only sample a relatively small volume of the prostate, and tumour lesions in regions between biopsy cores can be missed, leading to a well-known low sensitivity to detect clinically relevant cancer. As a proof-of-principle, we developed and validated a deep convolutional neural network model to distinguish between morphological patterns in benign prostate biopsy whole slide images from men with and without established cancer. Methods: This study included 14,354 hematoxylin and eosin stained whole slide images from benign prostate biopsies from 1,508 men in two groups: men without an established prostate cancer (PCa) diagnosis and men with at least one core biopsy diagnosed with PCa. 80% of the participants were assigned as training data and used for model optimization (1,211 men), and the remaining 20% (297 men) as a held-out test set used to evaluate model performance. An ensemble of 10 deep convolutional neural network models was optimized for classification of biopsies from men with and without established cancer. Hyperparameter optimization and model selection was performed by cross-validation in the training data . Results: Area under the receiver operating characteristic curve (ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy level and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a specificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion: The developed model has the ability to detect men with risk of missed PCa due to under-sampling of the prostate. The proposed model has the potential to reduce the number of false negative cases in routine systematic prostate biopsies and to indicate men who could benefit from MRI-guided re-biopsy.



### SDOF-Tracker: Fast and Accurate Multiple Human Tracking by Skipped-Detection and Optical-Flow
- **Arxiv ID**: http://arxiv.org/abs/2106.14259v3
- **DOI**: 10.1587/transinf.2022EDP7022
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14259v3)
- **Published**: 2021-06-27 15:35:35+00:00
- **Updated**: 2022-04-30 13:03:47+00:00
- **Authors**: Hitoshi Nishimura, Satoshi Komorita, Yasutomo Kawanishi, Hiroshi Murase
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple human tracking is a fundamental problem for scene understanding. Although both accuracy and speed are required in real-world applications, recent tracking methods based on deep learning have focused on accuracy and require substantial running time. This study aims to improve running speed by performing human detection at a certain frame interval because it accounts for most of the running time. The question is how to maintain accuracy while skipping human detection. In this paper, we propose a method that complements the detection results with optical flow, based on the fact that someone's appearance does not change much between adjacent frames. To maintain the tracking accuracy, we introduce robust interest point selection within human regions and a tracking termination metric calculated by the distribution of the interest points. On the MOT20 dataset in the MOTChallenge, the proposed SDOF-Tracker achieved the best performance in terms of the total running speed while maintaining the MOTA metric. Our code is available at https://github.com/hitottiez/sdof-tracker.



### Deep Learning for Technical Document Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.14269v5
- **DOI**: 10.1109/TEM.2022.3152216
- **Categories**: **cs.LG**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2106.14269v5)
- **Published**: 2021-06-27 16:12:47+00:00
- **Updated**: 2022-02-19 12:49:52+00:00
- **Authors**: Shuo Jiang, Jie Hu, Christopher L. Magee, Jianxi Luo
- **Comment**: 16 pages, 8 figures, 9 tables
- **Journal**: IEEE Transactions on Engineering Management. Published Online.
  2022
- **Summary**: In large technology companies, the requirements for managing and organizing technical documents created by engineers and managers have increased dramatically in recent years, which has led to a higher demand for more scalable, accurate, and automated document classification. Prior studies have only focused on processing text for classification, whereas technical documents often contain multimodal information. To leverage multimodal information for document classification to improve the model performance, this paper presents a novel multimodal deep learning architecture, TechDoc, which utilizes three types of information, including natural language texts and descriptive images within documents and the associations among the documents. The architecture synthesizes the convolutional neural network, recurrent neural network, and graph neural network through an integrated training process. We applied the architecture to a large multimodal technical document database and trained the model for classifying documents based on the hierarchical International Patent Classification system. Our results show that TechDoc presents a greater classification accuracy than the unimodal methods and other state-of-the-art benchmarks. The trained model can potentially be scaled to millions of real-world multimodal technical documents, which is useful for data and knowledge management in large technology companies and organizations.



### Learning Mesh Representations via Binary Space Partitioning Tree Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.14274v2
- **DOI**: 10.1109/TPAMI.2021.3093440
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.14274v2)
- **Published**: 2021-06-27 16:37:54+00:00
- **Updated**: 2021-07-02 00:24:22+00:00
- **Authors**: Zhiqin Chen, Andrea Tagliasacchi, Hao Zhang
- **Comment**: Accepted to TPAMI. This is the extended journal version of BSP-Net
  (arXiv:1911.06971) from CVPR 2020
- **Journal**: None
- **Summary**: Polygonal meshes are ubiquitous, but have only played a relatively minor role in the deep learning revolution. State-of-the-art neural generative models for 3D shapes learn implicit functions and generate meshes via expensive iso-surfacing. We overcome these challenges by employing a classical spatial data structure from computer graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core operation of BSP involves recursive subdivision of 3D space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition without supervision. The network is trained to reconstruct a shape using a set of convexes obtained from a BSP-tree built over a set of planes, where the planes and convexes are both defined by learned network weights. BSP-Net directly outputs polygonal meshes from the inferred convexes. The generated meshes are watertight, compact (i.e., low-poly), and well suited to represent sharp geometry. We show that the reconstruction quality by BSP-Net is competitive with those from state-of-the-art methods while using much fewer primitives. We also explore variations to BSP-Net including using a more generic decoder for reconstruction, more general primitives than planes, as well as training a generative model with variational auto-encoders. Code is available at https://github.com/czq142857/BSP-NET-original.



### Learning without Forgetting for 3D Point Cloud Objects
- **Arxiv ID**: http://arxiv.org/abs/2106.14275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14275v1)
- **Published**: 2021-06-27 16:39:39+00:00
- **Updated**: 2021-06-27 16:39:39+00:00
- **Authors**: Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, Shafin Rahman
- **Comment**: Accepted in International Work-Conference on Artificial and Natural
  Neural Networks, (IWANN) 2021
- **Journal**: None
- **Summary**: When we fine-tune a well-trained deep learning model for a new set of classes, the network learns new concepts but gradually forgets the knowledge of old training. In some real-life applications, we may be interested in learning new classes without forgetting the capability of previous experience. Such learning without forgetting problem is often investigated using 2D image recognition tasks. In this paper, considering the growth of depth camera technology, we address the same problem for the 3D point cloud object data. This problem becomes more challenging in the 3D domain than 2D because of the unavailability of large datasets and powerful pretrained backbone models. We investigate knowledge distillation techniques on 3D data to reduce catastrophic forgetting of the previous training. Moreover, we improve the distillation process by using semantic word vectors of object classes. We observe that exploring the interrelation of old and new knowledge during training helps to learn new concepts without forgetting old ones. Experimenting on three 3D point cloud recognition backbones (PointNet, DGCNN, and PointConv) and synthetic (ModelNet40, ModelNet10) and real scanned (ScanObjectNN) datasets, we establish new baseline results on learning without forgetting for 3D data. This research will instigate many future works in this area.



### Darker than Black-Box: Face Reconstruction from Similarity Queries
- **Arxiv ID**: http://arxiv.org/abs/2106.14290v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14290v2)
- **Published**: 2021-06-27 17:25:46+00:00
- **Updated**: 2021-07-02 14:35:37+00:00
- **Authors**: Anton Razzhigaev, Klim Kireev, Igor Udovichenko, Aleksandr Petiushko
- **Comment**: None
- **Journal**: None
- **Summary**: Several methods for inversion of face recognition models were recently presented, attempting to reconstruct a face from deep templates. Although some of these approaches work in a black-box setup using only face embeddings, usually, on the end-user side, only similarity scores are provided. Therefore, these algorithms are inapplicable in such scenarios. We propose a novel approach that allows reconstructing the face querying only similarity scores of the black-box model. While our algorithm operates in a more general setup, experiments show that it is query efficient and outperforms the existing methods.



### Knee Osteoarthritis Severity Prediction using an Attentive Multi-Scale Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2106.14292v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14292v1)
- **Published**: 2021-06-27 17:29:46+00:00
- **Updated**: 2021-06-27 17:29:46+00:00
- **Authors**: Rohit Kumar Jain, Prasen Kumar Sharma, Sibaji Gaj, Arijit Sur, Palash Ghosh
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Knee Osteoarthritis (OA) is a destructive joint disease identified by joint stiffness, pain, and functional disability concerning millions of lives across the globe. It is generally assessed by evaluating physical symptoms, medical history, and other joint screening tests like radiographs, Magnetic Resonance Imaging (MRI), and Computed Tomography (CT) scans. Unfortunately, the conventional methods are very subjective, which forms a barrier in detecting the disease progression at an early stage. This paper presents a deep learning-based framework, namely OsteoHRNet, that automatically assesses the Knee OA severity in terms of Kellgren and Lawrence (KL) grade classification from X-rays. As a primary novelty, the proposed approach is built upon one of the most recent deep models, called the High-Resolution Network (HRNet), to capture the multi-scale features of knee X-rays. In addition, we have also incorporated an attention mechanism to filter out the counterproductive features and boost the performance further. Our proposed model has achieved the best multiclass accuracy of 71.74% and MAE of 0.311 on the baseline cohort of the OAI dataset, which is a remarkable gain over the existing best-published works. We have also employed the Gradient-based Class Activation Maps (Grad-CAMs) visualization to justify the proposed network learning.



### Crowdsourcing Evaluation of Saliency-based XAI Methods
- **Arxiv ID**: http://arxiv.org/abs/2107.00456v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, 68T01, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2107.00456v2)
- **Published**: 2021-06-27 17:37:53+00:00
- **Updated**: 2021-08-30 14:10:05+00:00
- **Authors**: Xiaotian Lu, Arseny Tolmachev, Tatsuya Yamamoto, Koh Takeuchi, Seiji Okajima, Tomoyoshi Takebayashi, Koji Maruhashi, Hisashi Kashima
- **Comment**: 16 pages, 7 figures, 2 tables, Accepted for ECML-PKDD 2021
- **Journal**: None
- **Summary**: Understanding the reasons behind the predictions made by deep neural networks is critical for gaining human trust in many important applications, which is reflected in the increasing demand for explainability in AI (XAI) in recent years. Saliency-based feature attribution methods, which highlight important parts of images that contribute to decisions by classifiers, are often used as XAI methods, especially in the field of computer vision. In order to compare various saliency-based XAI methods quantitatively, several approaches for automated evaluation schemes have been proposed; however, there is no guarantee that such automated evaluation metrics correctly evaluate explainability, and a high rating by an automated evaluation scheme does not necessarily mean a high explainability for humans. In this study, instead of the automated evaluation, we propose a new human-based evaluation scheme using crowdsourcing to evaluate XAI methods. Our method is inspired by a human computation game, "Peek-a-boom", and can efficiently compare different XAI methods by exploiting the power of crowds. We evaluate the saliency maps of various XAI methods on two datasets with automated and crowd-based evaluation schemes. Our experiments show that the result of our crowd-based evaluation scheme is different from those of automated evaluation schemes. In addition, we regard the crowd-based evaluation results as ground truths and provide a quantitative performance measure to compare different automated evaluation schemes. We also discuss the impact of crowd workers on the results and show that the varying ability of crowd workers does not significantly impact the results.



### 3D Reconstruction through Fusion of Cross-View Images
- **Arxiv ID**: http://arxiv.org/abs/2106.14306v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14306v1)
- **Published**: 2021-06-27 18:31:08+00:00
- **Updated**: 2021-06-27 18:31:08+00:00
- **Authors**: Rongjun Qin, Shuang Song, Xiao Ling, Mostafa Elhashash
- **Comment**: None
- **Journal**: None
- **Summary**: 3D recovery from multi-stereo and stereo images, as an important application of the image-based perspective geometry, serves many applications in computer vision, remote sensing and Geomatics. In this chapter, the authors utilize the imaging geometry and present approaches that perform 3D reconstruction from cross-view images that are drastically different in their viewpoints. We introduce our framework that takes ground-view images and satellite images for full 3D recovery, which includes necessary methods in satellite and ground-based point cloud generation from images, 3D data co-registration, fusion and mesh generation. We demonstrate our proposed framework on a dataset consisting of twelve satellite images and 150k video frames acquired through a vehicle-mounted Go-pro camera and demonstrate the reconstruction results. We have also compared our results with results generated from an intuitive processing pipeline that involves typical geo-registration and meshing methods.



### Geometric Processing for Image-based 3D Object Modeling
- **Arxiv ID**: http://arxiv.org/abs/2106.14307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14307v1)
- **Published**: 2021-06-27 18:33:30+00:00
- **Updated**: 2021-06-27 18:33:30+00:00
- **Authors**: Rongjun Qin, Xu Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Image-based 3D object modeling refers to the process of converting raw optical images to 3D digital representations of the objects. Very often, such models are desired to be dimensionally true, semantically labeled with photorealistic appearance (reality-based modeling). Laser scanning was deemed as the standard (and direct) way to obtaining highly accurate 3D measurements of objects, while one would have to abide the high acquisition cost and its unavailability on some of the platforms. Nowadays the image-based methods backboned by the recently developed advanced dense image matching algorithms and geo-referencing paradigms, are becoming the dominant approaches, due to its high flexibility, availability and low cost. The largely automated geometric processing of images in a 3D object reconstruction workflow, from ordered/unordered raw imagery to textured meshes, is becoming a critical part of the reality-based 3D modeling. This article summarizes the overall geometric processing workflow, with focuses on introducing the state-of-the-art methods of three major components of geometric processing: 1) geo-referencing; 2) Image dense matching 3) texture mapping. Finally, we will draw conclusions and share our outlooks of the topics discussed in this article.



### Change Detection for Geodatabase Updating
- **Arxiv ID**: http://arxiv.org/abs/2106.14309v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14309v1)
- **Published**: 2021-06-27 18:35:10+00:00
- **Updated**: 2021-06-27 18:35:10+00:00
- **Authors**: Rongjun Qin
- **Comment**: None
- **Journal**: None
- **Summary**: The geodatabase (vectorized data) nowadays becomes a rather standard digital city infrastructure; however, updating geodatabase efficiently and economically remains a fundamental and practical issue in the geospatial industry. The cost of building a geodatabase is extremely high and labor intensive, and very often the maps we use have several months and even years of latency. One solution is to develop more automated methods for (vectorized) geospatial data generation, which has been proven a difficult task in the past decades. An alternative solution is to first detect the differences between the new data and the existing geospatial data, and then only update the area identified as changes. The second approach is becoming more favored due to its high practicality and flexibility. A highly relevant technique is change detection. This article aims to provide an overview the state-of-the-art change detection methods in the field of Remote Sensing and Geomatics to support the task of updating geodatabases. Data used for change detection are highly disparate, we therefore structure our review intuitively based on the dimension of the data, being 1) change detection with 2D data; 2) change detection with 3D data. Conclusions will be drawn based on the reviewed efforts in the field, and we will share our outlooks of the topic of updating geodatabases.



### Learning stochastic object models from medical imaging measurements by use of advanced ambient generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/2106.14324v2
- **DOI**: 10.1117/1.JMI.9.1.015503
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.14324v2)
- **Published**: 2021-06-27 21:46:23+00:00
- **Updated**: 2022-02-27 18:44:58+00:00
- **Authors**: Weimin Zhou, Sayantan Bhadra, Frank J. Brooks, Hua Li, Mark A. Anastasio
- **Comment**: Journal of Medical Imaging
- **Journal**: J. Med. Imag. 9(1), 015503 (2022)
- **Summary**: Purpose: To objectively assess new medical imaging technologies via computer-simulations, it is important to account for the variability in the ensemble of objects to be imaged. This source of variability can be described by stochastic object models (SOMs). It is generally desirable to establish SOMs from experimental imaging measurements acquired by use of a well-characterized imaging system, but this task has remained challenging. Approach: A generative adversarial network (GAN)-based method that employs AmbientGANs with modern progressive or multiresolution training approaches is proposed. AmbientGANs established using the proposed training procedure are systematically validated in a controlled way using computer-simulated magnetic resonance imaging (MRI) data corresponding to a stylized imaging system. Emulated single-coil experimental MRI data are also employed to demonstrate the methods under less stylized conditions. Results: The proposed AmbientGAN method can generate clean images when the imaging measurements are contaminated by measurement noise. When the imaging measurement data are incomplete, the proposed AmbientGAN can reliably learn the distribution of the measurement components of the objects. Conclusions: Both visual examinations and quantitative analyses, including task-specific validations using the Hotelling observer, demonstrated that the proposed AmbientGAN method holds promise to establish realistic SOMs from imaging measurements.



### Blind Non-Uniform Motion Deblurring using Atrous Spatial Pyramid Deformable Convolution and Deblurring-Reblurring Consistency
- **Arxiv ID**: http://arxiv.org/abs/2106.14336v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14336v2)
- **Published**: 2021-06-27 23:14:52+00:00
- **Updated**: 2022-04-21 23:28:07+00:00
- **Authors**: Dong Huo, Abbas Masoumzadeh, Yee-Hong Yang
- **Comment**: CVPRW 2022
- **Journal**: None
- **Summary**: Many deep learning based methods are designed to remove non-uniform (spatially variant) motion blur caused by object motion and camera shake without knowing the blur kernel. Some methods directly output the latent sharp image in one stage, while others utilize a multi-stage strategy (\eg multi-scale, multi-patch, or multi-temporal) to gradually restore the sharp image. However, these methods have the following two main issues: 1) The computational cost of multi-stage is high; 2) The same convolution kernel is applied in different regions, which is not an ideal choice for non-uniform blur. Hence, non-uniform motion deblurring is still a challenging and open problem. In this paper, we propose a new architecture which consists of multiple Atrous Spatial Pyramid Deformable Convolution (ASPDC) modules to deblur an image end-to-end with more flexibility. Multiple ASPDC modules implicitly learn the pixel-specific motion with different dilation rates in the same layer to handle movements of different magnitude. To improve the training, we also propose a reblurring network to map the deblurred output back to the blurred input, which constrains the solution space. Our experimental results show that the proposed method outperforms state-of-the-art methods on the benchmark datasets. The code is available at https://github.com/Dong-Huo/ASPDC.




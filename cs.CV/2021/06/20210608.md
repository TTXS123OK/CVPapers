# Arxiv Papers in cs.CV on 2021-06-08
### Manifold Topology Divergence: a Framework for Comparing Data Manifolds
- **Arxiv ID**: http://arxiv.org/abs/2106.04024v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.AT, math.MG, 55N31, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2106.04024v2)
- **Published**: 2021-06-08 00:30:43+00:00
- **Updated**: 2021-10-28 23:05:35+00:00
- **Authors**: Serguei Barannikov, Ilya Trofimov, Grigorii Sotnikov, Ekaterina Trimbach, Alexander Korotin, Alexander Filippov, Evgeny Burnaev
- **Comment**: None
- **Journal**: 35th Conference on Neural Information Processing Systems (NeurIPS
  2021)
- **Summary**: We develop a framework for comparing data manifolds, aimed, in particular, towards the evaluation of deep generative models. We describe a novel tool, Cross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional space, tracks multiscale topology spacial discrepancies between manifolds on which the distributions are concentrated. Based on the Cross-Barcode, we introduce the Manifold Topology Divergence score (MTop-Divergence) and apply it to assess the performance of deep generative models in various domains: images, 3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN, CIFAR10, FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate that the MTop-Divergence accurately detects various degrees of mode-dropping, intra-mode collapse, mode invention, and image disturbance. Our algorithm scales well (essentially linearly) with the increase of the dimension of the ambient high-dimensional space. It is one of the first TDA-based practical methodologies that can be applied universally to datasets of different sizes and dimensions, including the ones on which the most recent GANs in the visual domain are trained. The proposed method is domain agnostic and does not rely on pre-trained networks.



### SpaceMeshLab: Spatial Context Memoization and Meshgrid Atrous Convolution Consensus for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.04025v1
- **DOI**: 10.1109/ICIP42928.2021.9506531
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04025v1)
- **Published**: 2021-06-08 00:38:02+00:00
- **Updated**: 2021-06-08 00:38:02+00:00
- **Authors**: Taehun Kim, Jinseong Kim, Daijin Kim
- **Comment**: 5 pages, 3 figures, 4 tables. To appear in the proceedings of the
  28th IEEE International Conference on Image Processing (IEEE - ICIP),
  September 19-22, 2021, Anchorage, Alaska, USA
- **Journal**: None
- **Summary**: Semantic segmentation networks adopt transfer learning from image classification networks which occurs a shortage of spatial context information. For this reason, we propose Spatial Context Memoization (SpaM), a bypassing branch for spatial context by retaining the input dimension and constantly communicating its spatial context and rich semantic information mutually with the backbone network. Multi-scale context information for semantic segmentation is crucial for dealing with diverse sizes and shapes of target objects in the given scene. Conventional multi-scale context scheme adopts multiple effective receptive fields by multiple dilation rates or pooling operations, but often suffer from misalignment problem with respect to the target pixel. To this end, we propose Meshgrid Atrous Convolution Consensus (MetroCon^2) which brings multi-scale scheme into fine-grained multi-scale object context using convolutions with meshgrid-like scattered dilation rates. SpaceMeshLab (ResNet-101 + SpaM + MetroCon^2) achieves 82.0% mIoU in Cityscapes test and 53.5% mIoU on Pascal-Context validation set.



### Subject-Independent Brain-Computer Interface for Decoding High-Level Visual Imagery Tasks
- **Arxiv ID**: http://arxiv.org/abs/2106.04026v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2106.04026v2)
- **Published**: 2021-06-08 00:39:31+00:00
- **Updated**: 2021-08-16 06:57:57+00:00
- **Authors**: Dae-Hyeok Lee, Dong-Kyun Han, Sung-Jin Kim, Ji-Hoon Jeong, Seong-Whan Lee
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: Brain-computer interface (BCI) is used for communication between humans and devices by recognizing status and intention of humans. Communication between humans and a drone using electroencephalogram (EEG) signals is one of the most challenging issues in the BCI domain. In particular, the control of drone swarms (the direction and formation) has more advantages compared to the control of a drone. The visual imagery (VI) paradigm is that subjects visually imagine specific objects or scenes. Reduction of the variability among EEG signals of subjects is essential for practical BCI-based systems. In this study, we proposed the subepoch-wise feature encoder (SEFE) to improve the performances in the subject-independent tasks by using the VI dataset. This study is the first attempt to demonstrate the possibility of generalization among subjects in the VI-based BCI. We used the leave-one-subject-out cross-validation for evaluating the performances. We obtained higher performances when including our proposed module than excluding our proposed module. The DeepConvNet with SEFE showed the highest performance of 0.72 among six different decoding models. Hence, we demonstrated the feasibility of decoding the VI dataset in the subject-independent task with robust performances by using our proposed module.



### Graph-MLP: Node Classification without Message Passing in Graph
- **Arxiv ID**: http://arxiv.org/abs/2106.04051v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2106.04051v1)
- **Published**: 2021-06-08 02:07:21+00:00
- **Updated**: 2021-06-08 02:07:21+00:00
- **Authors**: Yang Hu, Haoxuan You, Zhecan Wang, Zhicheng Wang, Erjin Zhou, Yue Gao
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing with non-Euclidean structural data. Both spatial-based and spectral-based GNNs are relying on adjacency matrix to guide message passing among neighbors during feature aggregation. Recent works have mainly focused on powerful message passing modules, however, in this paper, we show that none of the message passing modules is necessary. Instead, we propose a pure multilayer-perceptron-based framework, Graph-MLP with the supervision signal leveraging graph structure, which is sufficient for learning discriminative node representation. In model-level, Graph-MLP only includes multi-layer perceptrons, activation function, and layer normalization. In the loss level, we design a neighboring contrastive (NContrast) loss to bridge the gap between GNNs and MLPs by utilizing the adjacency information implicitly. This design allows our model to be lighter and more robust when facing large-scale graph data and corrupted adjacency information. Extensive experiments prove that even without adjacency information in testing phase, our framework can still reach comparable and even superior performance against the state-of-the-art models in the graph node classification task.



### Discriminative Triad Matching and Reconstruction for Weakly Referring Expression Grounding
- **Arxiv ID**: http://arxiv.org/abs/2106.04053v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.04053v1)
- **Published**: 2021-06-08 02:15:11+00:00
- **Updated**: 2021-06-08 02:15:11+00:00
- **Authors**: Mingjie Sun, Jimin Xiao, Eng Gee Lim, Si Liu, John Y. Goulermas
- **Comment**: TPAMI
- **Journal**: None
- **Summary**: In this paper, we are tackling the weakly-supervised referring expression grounding task, for the localization of a referent object in an image according to a query sentence, where the mapping between image regions and queries are not available during the training stage. In traditional methods, an object region that best matches the referring expression is picked out, and then the query sentence is reconstructed from the selected region, where the reconstruction difference serves as the loss for back-propagation. The existing methods, however, conduct both the matching and the reconstruction approximately as they ignore the fact that the matching correctness is unknown. To overcome this limitation, a discriminative triad is designed here as the basis to the solution, through which a query can be converted into one or multiple discriminative triads in a very scalable way. Based on the discriminative triad, we further propose the triad-level matching and reconstruction modules which are lightweight yet effective for the weakly-supervised training, making it three times lighter and faster than the previous state-of-the-art methods. One important merit of our work is its superior performance despite the simple and neat design. Specifically, the proposed method achieves a new state-of-the-art accuracy when evaluated on RefCOCO (39.21%), RefCOCO+ (39.18%) and RefCOCOg (43.24%) datasets, that is 4.17%, 4.08% and 7.8% higher than the previous one, respectively.



### Affinity Attention Graph Neural Network for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.04054v1
- **DOI**: 10.1109/TPAMI.2021.3083269
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04054v1)
- **Published**: 2021-06-08 02:19:21+00:00
- **Updated**: 2021-06-08 02:19:21+00:00
- **Authors**: Bingfeng Zhang, Jimin Xiao, Jianbo Jiao, Yunchao Wei, Yao Zhao
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TAPMI 2021)
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation is receiving great attention due to its low human annotation cost. In this paper, we aim to tackle bounding box supervised semantic segmentation, i.e., training accurate semantic segmentation models using bounding box annotations as supervision. To this end, we propose Affinity Attention Graph Neural Network ($A^2$GNN). Following previous practices, we first generate pseudo semantic-aware seeds, which are then formed into semantic graphs based on our newly proposed affinity Convolutional Neural Network (CNN). Then the built graphs are input to our $A^2$GNN, in which an affinity attention layer is designed to acquire the short- and long- distance information from soft graph edges to accurately propagate semantic labels from the confident seeds to the unlabeled pixels. However, to guarantee the precision of the seeds, we only adopt a limited number of confident pixel seed labels for $A^2$GNN, which may lead to insufficient supervision for training. To alleviate this issue, we further introduce a new loss function and a consistency-checking mechanism to leverage the bounding box constraint, so that more reliable guidance can be included for the model optimization. Experiments show that our approach achieves new state-of-the-art performances on Pascal VOC 2012 datasets (val: 76.5\%, test: 75.2\%). More importantly, our approach can be readily applied to bounding box supervised instance segmentation task or other weakly supervised semantic segmentation tasks, with state-of-the-art or comparable performance among almot all weakly supervised tasks on PASCAL VOC or COCO dataset. Our source code will be available at https://github.com/zbf1991/A2GNN.



### Semantically Adversarial Scenario Generation with Explicit Knowledge Guidance
- **Arxiv ID**: http://arxiv.org/abs/2106.04066v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.04066v6)
- **Published**: 2021-06-08 02:51:33+00:00
- **Updated**: 2023-07-20 00:24:58+00:00
- **Authors**: Wenhao Ding, Haohong Lin, Bo Li, Ding Zhao
- **Comment**: 20 pages, 13 figures
- **Journal**: None
- **Summary**: Generating adversarial scenarios, which have the potential to fail autonomous driving systems, provides an effective way to improve robustness. Extending purely data-driven generative models, recent specialized models satisfy additional controllable requirements such as embedding a traffic sign in a driving scene by manipulating patterns implicitly in the neuron level. In this paper, we introduce a method to incorporate domain knowledge explicitly in the generation process to achieve the Semantically Adversarial Generation (SAG). To be consistent with the composition of driving scenes, we first categorize the knowledge into two types, the property of objects and the relationship among objects. We then propose a tree-structured variational auto-encoder (T-VAE) to learn hierarchical scene representation. By imposing semantic rules on the properties of nodes and edges in the tree structure, explicit knowledge integration enables controllable generation. We construct a synthetic example to illustrate the controllability and explainability of our method in a succinct setting. We further extend to realistic environments for autonomous vehicles: our method efficiently identifies adversarial driving scenes against different state-of-the-art 3D point cloud segmentation models and satisfies the traffic rules specified as the explicit knowledge.



### LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution Homography Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.04067v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04067v2)
- **Published**: 2021-06-08 02:51:45+00:00
- **Updated**: 2021-06-13 04:55:23+00:00
- **Authors**: Ruizhi Shao, Gaochang Wu, Yuemei Zhou, Ying Fu, Yebin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-resolution image alignment is a key problem in multiscale gigapixel photography, which requires to estimate homography matrix using images with large resolution gap. Existing deep homography methods concatenate the input images or features, neglecting the explicit formulation of correspondences between them, which leads to degraded accuracy in cross-resolution challenges. In this paper, we consider the cross-resolution homography estimation as a multimodal problem, and propose a local transformer network embedded within a multiscale structure to explicitly learn correspondences between the multimodal inputs, namely, input images with different resolutions. The proposed local transformer adopts a local attention map specifically for each position in the feature. By combining the local transformer with the multiscale structure, the network is able to capture long-short range correspondences efficiently and accurately. Experiments on both the MS-COCO dataset and the real-captured cross-resolution dataset show that the proposed network outperforms existing state-of-the-art feature-based and deep-learning-based homography estimation methods, and is able to accurately align images under $10\times$ resolution gap.



### Salvage of Supervision in Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.04073v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04073v2)
- **Published**: 2021-06-08 03:13:24+00:00
- **Updated**: 2022-05-09 07:43:31+00:00
- **Authors**: Lin Sui, Chen-Lin Zhang, Jianxin Wu
- **Comment**: accepted by CVPR 2022
- **Journal**: None
- **Summary**: Weakly supervised object detection~(WSOD) has recently attracted much attention. However, the lack of bounding-box supervision makes its accuracy much lower than fully supervised object detection (FSOD), and currently modern FSOD techniques cannot be applied to WSOD. To bridge the performance and technical gaps between WSOD and FSOD, this paper proposes a new framework, Salvage of Supervision (SoS), with the key idea being to harness every potentially useful supervisory signal in WSOD: the weak image-level labels, the pseudo-labels, and the power of semi-supervised object detection. This paper proposes new approaches to utilize these weak and noisy signals effectively, and shows that each type of supervisory signal brings in notable improvements, outperforms existing WSOD methods (which mainly use only the weak labels) by large margins. The proposed SoS-WSOD method also has the ability to freely use modern FSOD techniques. SoS-WSOD achieves 64.4 $m\text{AP}_{50}$ on VOC2007, 61.9 $m\text{AP}_{50}$ on VOC2012 and 16.6 $m\text{AP}_{50:95}$ on MS-COCO, and also has fast inference speed. Ablations and visualization further verify the effectiveness of SoS.



### Variational AutoEncoder for Reference based Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2106.04090v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04090v1)
- **Published**: 2021-06-08 04:12:38+00:00
- **Updated**: 2021-06-08 04:12:38+00:00
- **Authors**: Zhi-Song Liu, Wan-Chi Siu, Li-Wen Wang
- **Comment**: 10 pages, 6 figures
- **Journal**: 2021 IEEE Conference on Computer Vision and Pattern Recognition
  Workshop
- **Summary**: In this paper, we propose a novel reference based image super-resolution approach via Variational AutoEncoder (RefVAE). Existing state-of-the-art methods mainly focus on single image super-resolution which cannot perform well on large upsampling factors, e.g., 8$\times$. We propose a reference based image super-resolution, for which any arbitrary image can act as a reference for super-resolution. Even using random map or low-resolution image itself, the proposed RefVAE can transfer the knowledge from the reference to the super-resolved images. Depending upon different references, the proposed method can generate different versions of super-resolved images from a hidden super-resolution space. Besides using different datasets for some standard evaluations with PSNR and SSIM, we also took part in the NTIRE2021 SR Space challenge and have provided results of the randomness evaluation of our approach. Compared to other state-of-the-art methods, our approach achieves higher diverse scores.



### Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer
- **Arxiv ID**: http://arxiv.org/abs/2106.04095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04095v1)
- **Published**: 2021-06-08 04:29:07+00:00
- **Updated**: 2021-06-08 04:29:07+00:00
- **Authors**: Yulin Li, Jianfeng He, Tianzhu Zhang, Xiang Liu, Yongdong Zhang, Feng Wu
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Occluded person re-identification (Re-ID) is a challenging task as persons are frequently occluded by various obstacles or other persons, especially in the crowd scenario. To address these issues, we propose a novel end-to-end Part-Aware Transformer (PAT) for occluded person Re-ID through diverse part discovery via a transformer encoderdecoder architecture, including a pixel context based transformer encoder and a part prototype based transformer decoder. The proposed PAT model enjoys several merits. First, to the best of our knowledge, this is the first work to exploit the transformer encoder-decoder architecture for occluded person Re-ID in a unified deep model. Second, to learn part prototypes well with only identity labels, we design two effective mechanisms including part diversity and part discriminability. Consequently, we can achieve diverse part discovery for occluded person Re-ID in a weakly supervised manner. Extensive experimental results on six challenging benchmarks for three tasks (occluded, partial and holistic Re-ID) demonstrate that our proposed PAT performs favorably against stat-of-the-art methods.



### Design of Low-Artifact Interpolation Kernels by Means of Computer Algebra
- **Arxiv ID**: http://arxiv.org/abs/2106.04104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04104v1)
- **Published**: 2021-06-08 05:06:51+00:00
- **Updated**: 2021-06-08 05:06:51+00:00
- **Authors**: Peter Karpov
- **Comment**: 22 pages, 6 figures
- **Journal**: None
- **Summary**: We present a number of new piecewise-polynomial kernels for image interpolation. The kernels are constructed by optimizing a measure of interpolation quality based on the magnitude of anisotropic artifacts. The kernel design process is performed symbolically using Mathematica computer algebra system. Experimental evaluation involving 14 image quality assessment methods demonstrates that our results compare favorably with the existing linear interpolators.



### Fully Transformer Networks for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.04108v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04108v3)
- **Published**: 2021-06-08 05:15:28+00:00
- **Updated**: 2021-12-28 05:50:43+00:00
- **Authors**: Sitong Wu, Tianyi Wu, Fangjian Lin, Shengwei Tian, Guodong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers have shown impressive performance in various natural language processing and computer vision tasks, due to the capability of modeling long-range dependencies. Recent progress has demonstrated that combining such Transformers with CNN-based semantic image segmentation models is very promising. However, it is not well studied yet on how well a pure Transformer based approach can achieve for image segmentation. In this work, we explore a novel framework for semantic image segmentation, which is encoder-decoder based Fully Transformer Networks (FTN). Specifically, we first propose a Pyramid Group Transformer (PGT) as the encoder for progressively learning hierarchical features, meanwhile reducing the computation complexity of the standard Visual Transformer (ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse semantic-level and spatial-level information from multiple levels of the PGT encoder for semantic image segmentation. Surprisingly, this simple baseline can achieve better results on multiple challenging semantic segmentation and face parsing benchmarks, including PASCAL Context, ADE20K, COCOStuff, and CelebAMask-HQ. The source code will be released on https://github.com/BR-IDL/PaddleViT.



### Harnessing Unrecognizable Faces for Improving Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.04112v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04112v2)
- **Published**: 2021-06-08 05:25:03+00:00
- **Updated**: 2021-09-14 19:43:53+00:00
- **Authors**: Siqi Deng, Yuanjun Xiong, Meng Wang, Wei Xia, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: The common implementation of face recognition systems as a cascade of a detection stage and a recognition or verification stage can cause problems beyond failures of the detector. When the detector succeeds, it can detect faces that cannot be recognized, no matter how capable the recognition system. Recognizability, a latent variable, should therefore be factored into the design and implementation of face recognition systems. We propose a measure of recognizability of a face image that leverages a key empirical observation: an embedding of face images, implemented by a deep neural network trained using mostly recognizable identities, induces a partition of the hypersphere whereby unrecognizable identities cluster together. This occurs regardless of the phenomenon that causes a face to be unrecognizable, it be optical or motion blur, partial occlusion, spatial quantization, poor illumination. Therefore, we use the distance from such an "unrecognizable identity" as a measure of recognizability, and incorporate it in the design of the over-all system. We show that accounting for recognizability reduces error rate of single-image face recognition by 58% at FAR=1e-5 on the IJB-C Covariate Verification benchmark, and reduces verification error rate by 24% at FAR=1e-5 in set-based recognition on the IJB-C benchmark.



### Multi-dataset Pretraining: A Unified Model for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.04121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.04121v1)
- **Published**: 2021-06-08 06:13:11+00:00
- **Updated**: 2021-06-08 06:13:11+00:00
- **Authors**: Bowen Shi, Xiaopeng Zhang, Haohang Xu, Wenrui Dai, Junni Zou, Hongkai Xiong, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Collecting annotated data for semantic segmentation is time-consuming and hard to scale up. In this paper, we for the first time propose a unified framework, termed as Multi-Dataset Pretraining, to take full advantage of the fragmented annotations of different datasets. The highlight is that the annotations from different domains can be efficiently reused and consistently boost performance for each specific domain. This is achieved by first pretraining the network via the proposed pixel-to-prototype contrastive loss over multiple datasets regardless of their taxonomy labels, and followed by fine-tuning the pretrained model over specific dataset as usual. In order to better model the relationship among images and classes from different datasets, we extend the pixel level embeddings via cross dataset mixing and propose a pixel-to-class sparse coding strategy that explicitly models the pixel-class similarity over the manifold embedding space. In this way, we are able to increase intra-class compactness and inter-class separability, as well as considering inter-class similarity across different datasets for better transferability. Experiments conducted on several benchmarks demonstrate its superior performance. Notably, MDP consistently outperforms the pretrained models over ImageNet by a considerable margin, while only using less than 10% samples for pretraining.



### Left Ventricle Contouring in Cardiac Images Based on Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.04127v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.04127v1)
- **Published**: 2021-06-08 06:30:32+00:00
- **Updated**: 2021-06-08 06:30:32+00:00
- **Authors**: Sixing Yin, Yameng Han, Shufang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation is one of the important tasks of computer-aided diagnosis in medical image analysis. Since most medical images have the characteristics of blurred boundaries and uneven intensity distribution, through existing segmentation methods, the discontinuity within the target area and the discontinuity of the target boundary are likely to lead to rough or even erroneous boundary delineation. In this paper, we propose a new iterative refined interactive segmentation method for medical images based on agent reinforcement learning, which focuses on the problem of target segmentation boundaries. We model the dynamic process of drawing the target contour in a certain order as a Markov Decision Process (MDP) based on a deep reinforcement learning method. In the dynamic process of continuous interaction between the agent and the image, the agent tracks the boundary point by point in order within a limited length range until the contour of the target is completely drawn. In this process, the agent can quickly improve the segmentation performance by exploring an interactive policy in the image. The method we proposed is simple and effective. At the same time, we evaluate our method on the cardiac MRI scan data set. Experimental results show that our method has a better segmentation effect on the left ventricle in a small number of medical image data sets, especially in terms of segmentation boundaries, this method is better than existing methods. Based on our proposed method, the dynamic generation process of the predicted contour trajectory of the left ventricle will be displayed online at https://github.com/H1997ym/LV-contour-trajectory.



### Conversational Fashion Image Retrieval via Multiturn Natural Language Feedback
- **Arxiv ID**: http://arxiv.org/abs/2106.04128v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2106.04128v1)
- **Published**: 2021-06-08 06:34:25+00:00
- **Updated**: 2021-06-08 06:34:25+00:00
- **Authors**: Yifei Yuan, Wai Lam
- **Comment**: Accepted by SIGIR 2021
- **Journal**: None
- **Summary**: We study the task of conversational fashion image retrieval via multiturn natural language feedback. Most previous studies are based on single-turn settings. Existing models on multiturn conversational fashion image retrieval have limitations, such as employing traditional models, and leading to ineffective performance. We propose a novel framework that can effectively handle conversational fashion image retrieval with multiturn natural language feedback texts. One characteristic of the framework is that it searches for candidate images based on exploitation of the encoded reference image and feedback text information together with the conversation history. Furthermore, the image fashion attribute information is leveraged via a mutual attention strategy. Since there is no existing fashion dataset suitable for the multiturn setting of our task, we derive a large-scale multiturn fashion dataset via additional manual annotation efforts on an existing single-turn dataset. The experiments show that our proposed model significantly outperforms existing state-of-the-art methods.



### EnMcGAN: Adversarial Ensemble Learning for 3D Complete Renal Structures Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.04130v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04130v1)
- **Published**: 2021-06-08 06:40:42+00:00
- **Updated**: 2021-06-08 06:40:42+00:00
- **Authors**: Yuting He, Rongjun Ge, Xiaoming Qi, Guanyu Yang, Yang Chen, Youyong Kong, Huazhong Shu, Jean-Louis Coatrieux, Shuo Li
- **Comment**: None
- **Journal**: Information Processing in Medical Imaging (IPMI) 2021
- **Summary**: 3D complete renal structures(CRS) segmentation targets on segmenting the kidneys, tumors, renal arteries and veins in one inference. Once successful, it will provide preoperative plans and intraoperative guidance for laparoscopic partial nephrectomy(LPN), playing a key role in the renal cancer treatment. However, no success has been reported in 3D CRS segmentation due to the complex shapes of renal structures, low contrast and large anatomical variation. In this study, we utilize the adversarial ensemble learning and propose Ensemble Multi-condition GAN(EnMcGAN) for 3D CRS segmentation for the first time. Its contribution is three-fold. 1)Inspired by windowing, we propose the multi-windowing committee which divides CTA image into multiple narrow windows with different window centers and widths enhancing the contrast for salient boundaries and soft tissues. And then, it builds an ensemble segmentation model on these narrow windows to fuse the segmentation superiorities and improve whole segmentation quality. 2)We propose the multi-condition GAN which equips the segmentation model with multiple discriminators to encourage the segmented structures meeting their real shape conditions, thus improving the shape feature extraction ability. 3)We propose the adversarial weighted ensemble module which uses the trained discriminators to evaluate the quality of segmented structures, and normalizes these evaluation scores for the ensemble weights directed at the input image, thus enhancing the ensemble results. 122 patients are enrolled in this study and the mean Dice coefficient of the renal structures achieves 84.6%. Extensive experiments with promising results on renal structures reveal powerful segmentation accuracy and great clinical significance in renal cancer treatment.



### Image Deformation Estimation via Multi-Objective Optimization
- **Arxiv ID**: http://arxiv.org/abs/2106.04139v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04139v2)
- **Published**: 2021-06-08 06:52:12+00:00
- **Updated**: 2022-06-09 06:43:36+00:00
- **Authors**: Takumi Nakane, Haoran Xie, Chao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The free-form deformation model can represent a wide range of non-rigid deformations by manipulating a control point lattice over the image. However, due to a large number of parameters, it is challenging to fit the free-form deformation model directly to the deformed image for deformation estimation because of the complexity of the fitness landscape. In this paper, we cast the registration task as a multi-objective optimization problem (MOP) according to the fact that regions affected by each control point overlap with each other. Specifically, by partitioning the template image into several regions and measuring the similarity of each region independently, multiple objectives are built and deformation estimation can thus be realized by solving the MOP with off-the-shelf multi-objective evolutionary algorithms (MOEAs). In addition, a coarse-to-fine strategy is realized by image pyramid combined with control point mesh subdivision. Specifically, the optimized candidate solutions of the current image level are inherited by the next level, which increases the ability to deal with large deformation. Also, a post-processing procedure is proposed to generate a single output utilizing the Pareto optimal solutions. Comparative experiments on both synthetic and real-world images show the effectiveness and usefulness of our deformation estimation method.



### Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.04144v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04144v6)
- **Published**: 2021-06-08 07:07:45+00:00
- **Updated**: 2021-10-26 14:20:35+00:00
- **Authors**: Gabriel Tjio, Ping Liu, Joey Tianyi Zhou, Rick Siow Mong Goh
- **Comment**: Accepted in WACV 2022
- **Journal**: None
- **Summary**: Convolutional neural networks typically perform poorly when the test (target domain) and training (source domain) data have significantly different distributions. While this problem can be mitigated by using the target domain data to align the source and target domain feature representations, the target domain data may be unavailable due to privacy concerns. Consequently, there is a need for methods that generalize well despite restricted access to target domain data during training. In this work, we propose an adversarial semantic hallucination approach (ASH), which combines a class-conditioned hallucination module and a semantic segmentation module. Since the segmentation performance varies across different classes, we design a semantic-conditioned style hallucination module to generate affine transformation parameters from semantic information in the segmentation probability maps of the source domain image. Unlike previous adaptation approaches, which treat all classes equally, ASH considers the class-wise differences. The segmentation module and the hallucination module compete adversarially, with the hallucination module generating increasingly "difficult" stylized images to challenge the segmentation module. In response, the segmentation module improves as it is trained with generated samples at an appropriate class-wise difficulty level. Our results on the Cityscapes and Mapillary benchmark datasets show that our method is competitive with state of the art work. Code is made available at https://github.com/gabriel-tjio/ASH.



### Few-Shot Action Localization without Knowing Boundaries
- **Arxiv ID**: http://arxiv.org/abs/2106.04150v2
- **DOI**: 10.1145/3460426.3463643
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04150v2)
- **Published**: 2021-06-08 07:32:43+00:00
- **Updated**: 2021-09-23 07:05:38+00:00
- **Authors**: Ting-Ting Xie, Christos Tzelepis, Fan Fu, Ioannis Patras
- **Comment**: ICMR21 Camera ready; link to code:
  https://github.com/June01/WFSAL-icmr21
- **Journal**: None
- **Summary**: Learning to localize actions in long, cluttered, and untrimmed videos is a hard task, that in the literature has typically been addressed assuming the availability of large amounts of annotated training samples for each class -- either in a fully-supervised setting, where action boundaries are known, or in a weakly-supervised setting, where only class labels are known for each video. In this paper, we go a step further and show that it is possible to learn to localize actions in untrimmed videos when a) only one/few trimmed examples of the target action are available at test time, and b) when a large collection of videos with only class label annotation (some trimmed and some weakly annotated untrimmed ones) are available for training; with no overlap between the classes used during training and testing. To do so, we propose a network that learns to estimate Temporal Similarity Matrices (TSMs) that model a fine-grained similarity pattern between pairs of videos (trimmed or untrimmed), and uses them to generate Temporal Class Activation Maps (TCAMs) for seen or unseen classes. The TCAMs serve as temporal attention mechanisms to extract video-level representations of untrimmed videos, and to temporally localize actions at test time. To the best of our knowledge, we are the first to propose a weakly-supervised, one/few-shot action localization network that can be trained in an end-to-end fashion. Experimental results on THUMOS14 and ActivityNet1.2 datasets, show that our method achieves performance comparable or better to state-of-the-art fully-supervised, few-shot learning methods.



### Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2106.04151v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.04151v1)
- **Published**: 2021-06-08 07:35:40+00:00
- **Updated**: 2021-06-08 07:35:40+00:00
- **Authors**: Zhekai Du, Jingjing Li, Hongzu Su, Lei Zhu, Ke Lu
- **Comment**: Accepted to CVPR 2021, Codes are avaliable at
  https://github.com/lijin118/CGDM
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) aims to generalize the knowledge learned from a well-labeled source domain to an unlabeled target domain. Recently, adversarial domain adaptation with two distinct classifiers (bi-classifier) has been introduced into UDA which is effective to align distributions between different domains. Previous bi-classifier adversarial learning methods only focus on the similarity between the outputs of two distinct classifiers. However, the similarity of the outputs cannot guarantee the accuracy of target samples, i.e., target samples may match to wrong categories even if the discrepancy between two classifiers is small. To challenge this issue, in this paper, we propose a cross-domain gradient discrepancy minimization (CGDM) method which explicitly minimizes the discrepancy of gradients generated by source samples and target samples. Specifically, the gradient gives a cue for the semantic information of target samples so it can be used as a good supervision to improve the accuracy of target samples. In order to compute the gradient signal of target samples, we further obtain target pseudo labels through a clustering-based self-supervised learning. Extensive experiments on three widely used UDA datasets show that our method surpasses many previous state-of-the-arts. Codes are available at https://github.com/lijin118/CGDM.



### On Improving Adversarial Transferability of Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.04169v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.04169v3)
- **Published**: 2021-06-08 08:20:38+00:00
- **Updated**: 2022-03-03 12:08:18+00:00
- **Authors**: Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Fahad Shahbaz Khan, Fatih Porikli
- **Comment**: ICLR'22 (Spotlight), the first two authors contributed equally. Code:
  https://t.ly/hBbW
- **Journal**: None
- **Summary**: Vision transformers (ViTs) process input images as sequences of patches via self-attention; a radically different architecture than convolutional neural networks (CNNs). This makes it interesting to study the adversarial feature space of ViT models and their transferability. In particular, we observe that adversarial patterns found via conventional adversarial attacks show very \emph{low} black-box transferability even for large ViT models. We show that this phenomenon is only due to the sub-optimal attack procedures that do not leverage the true representation potential of ViTs. A deep ViT is composed of multiple blocks, with a consistent architecture comprising of self-attention and feed-forward layers, where each block is capable of independently producing a class token. Formulating an attack using only the last class token (conventional approach) does not directly leverage the discriminative information stored in the earlier tokens, leading to poor adversarial transferability of ViTs. Using the compositional nature of ViT models, we enhance transferability of existing attacks by introducing two novel strategies specific to the architecture of ViT models. (i) Self-Ensemble: We propose a method to find multiple discriminative pathways by dissecting a single ViT model into an ensemble of networks. This allows explicitly utilizing class-specific information at each ViT block. (ii) Token Refinement: We then propose to refine the tokens to further enhance the discriminative capacity at each block of ViT. Our token refinement systematically combines the class tokens with structural information preserved within the patch tokens.



### Highly accurate digital traffic recording as a basis for future mobility research: Methods and concepts of the research project HDV-Mess
- **Arxiv ID**: http://arxiv.org/abs/2106.04175v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04175v1)
- **Published**: 2021-06-08 08:28:46+00:00
- **Updated**: 2021-06-08 08:28:46+00:00
- **Authors**: Laurent Kloeker, Fabian Thomsen, Lutz Eckstein, Philip Trettner, Tim Elsner, Julius Nehring-Wirxel, Kersten Schuster, Leif Kobbelt, Michael Hoesch
- **Comment**: None
- **Journal**: None
- **Summary**: The research project HDV-Mess aims at a currently missing, but very crucial component for addressing important challenges in the field of connected and automated driving on public roads. The goal is to record traffic events at various relevant locations with high accuracy and to collect real traffic data as a basis for the development and validation of current and future sensor technologies as well as automated driving functions. For this purpose, it is necessary to develop a concept for a mobile modular system of measuring stations for highly accurate traffic data acquisition, which enables a temporary installation of a sensor and communication infrastructure at different locations. Within this paper, we first discuss the project goals before we present our traffic detection concept using mobile modular intelligent transport systems stations (ITS-Ss). We then explain the approaches for data processing of sensor raw data to refined trajectories, data communication, and data validation.



### White Paper Assistance: A Step Forward Beyond the Shortcut Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.04178v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04178v2)
- **Published**: 2021-06-08 08:35:44+00:00
- **Updated**: 2022-05-23 08:09:41+00:00
- **Authors**: Xuan Cheng, Tianshu Xie, Xiaomin Wang, Jiali Deng, Minghui Liu, Ming Liu
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: The promising performances of CNNs often overshadow the need to examine whether they are doing in the way we are actually interested. We show through experiments that even over-parameterized models would still solve a dataset by recklessly leveraging spurious correlations, or so-called 'shortcuts'. To combat with this unintended propensity, we borrow the idea of printer test page and propose a novel approach called White Paper Assistance. Our proposed method involves the white paper to detect the extent to which the model has preference for certain characterized patterns and alleviates it by forcing the model to make a random guess on the white paper. We show the consistent accuracy improvements that are manifest in various architectures, datasets and combinations with other techniques. Experiments have also demonstrated the versatility of our approach on fine-grained recognition, imbalanced classification and robustness to corruptions.



### Image2Point: 3D Point-Cloud Understanding with 2D Image Pretrained Models
- **Arxiv ID**: http://arxiv.org/abs/2106.04180v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.04180v3)
- **Published**: 2021-06-08 08:42:55+00:00
- **Updated**: 2022-04-23 20:15:14+00:00
- **Authors**: Chenfeng Xu, Shijia Yang, Tomer Galanti, Bichen Wu, Xiangyu Yue, Bohan Zhai, Wei Zhan, Peter Vajda, Kurt Keutzer, Masayoshi Tomizuka
- **Comment**: The code is avaliable at:
  \url{https://github.com/chenfengxu714/image2point}
- **Journal**: None
- **Summary**: 3D point-clouds and 2D images are different visual representations of the physical world. While human vision can understand both representations, computer vision models designed for 2D image and 3D point-cloud understanding are quite different. Our paper explores the potential of transferring 2D model architectures and weights to understand 3D point-clouds, by empirically investigating the feasibility of the transfer, the benefits of the transfer, and shedding light on why the transfer works. We discover that we can indeed use the same architecture and pretrained weights of a neural net model to understand both images and point-clouds. Specifically, we transfer the image-pretrained model to a point-cloud model by copying or inflating the weights. We find that finetuning the transformed image-pretrained models (FIP) with minimal efforts -- only on input, output, and normalization layers -- can achieve competitive performance on 3D point-cloud classification, beating a wide range of point-cloud models that adopt task-specific architectures and use a variety of tricks. When finetuning the whole model, the performance improves even further. Meanwhile, FIP improves data efficiency, reaching up to 10.0 top-1 accuracy percent on few-shot classification. It also speeds up the training of point-cloud models by up to 11.1x for a target accuracy (e.g., 90 % accuracy). Lastly, we provide an explanation of the image to point-cloud transfer from the aspect of neural collapse. The code is available at: \url{https://github.com/chenfengxu714/image2point}.



### LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization
- **Arxiv ID**: http://arxiv.org/abs/2106.04185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04185v1)
- **Published**: 2021-06-08 08:56:40+00:00
- **Updated**: 2021-06-08 08:56:40+00:00
- **Authors**: Avisek Lahiri, Vivek Kwatra, Christian Frueh, John Lewis, Chris Bregler
- **Comment**: Accepted to IEEE CVPR 2021. Brief demo video available at:
  https://www.youtube.com/watch?v=L1StbX9OznY
- **Journal**: None
- **Summary**: In this paper, we present a video-based learning framework for animating personalized 3D talking faces from audio. We introduce two training-time data normalizations that significantly improve data sample efficiency. First, we isolate and represent faces in a normalized space that decouples 3D geometry, head pose, and texture. This decomposes the prediction problem into regressions over the 3D face shape and the corresponding 2D texture atlas. Second, we leverage facial symmetry and approximate albedo constancy of skin to isolate and remove spatio-temporal lighting variations. Together, these normalizations allow simple networks to generate high fidelity lip-sync videos under novel ambient illumination while training with just a single speaker-specific video. Further, to stabilize temporal dynamics, we introduce an auto-regressive approach that conditions the model on its previous visual state. Human ratings and objective metrics demonstrate that our method outperforms contemporary state-of-the-art audio-driven video reenactment benchmarks in terms of realism, lip-sync and visual quality scores. We illustrate several applications enabled by our framework.



### Learning by Distillation: A Self-Supervised Learning Framework for Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.04195v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.04195v1)
- **Published**: 2021-06-08 09:13:34+00:00
- **Updated**: 2021-06-08 09:13:34+00:00
- **Authors**: Pengpeng Liu, Michael R. Lyu, Irwin King, Jia Xu
- **Comment**: TPAMI 2021
- **Journal**: None
- **Summary**: We present DistillFlow, a knowledge distillation approach to learning optical flow. DistillFlow trains multiple teacher models and a student model, where challenging transformations are applied to the input of the student model to generate hallucinated occlusions as well as less confident predictions. Then, a self-supervised learning framework is constructed: confident predictions from teacher models are served as annotations to guide the student model to learn optical flow for those less confident predictions. The self-supervised learning framework enables us to effectively learn optical flow from unlabeled data, not only for non-occluded pixels, but also for occluded pixels. DistillFlow achieves state-of-the-art unsupervised learning performance on both KITTI and Sintel datasets. Our self-supervised pre-trained model also provides an excellent initialization for supervised fine-tuning, suggesting an alternate training paradigm in contrast to current supervised learning methods that highly rely on pre-training on synthetic data. At the time of writing, our fine-tuned models ranked 1st among all monocular methods on the KITTI 2015 benchmark, and outperform all published methods on the Sintel Final benchmark. More importantly, we demonstrate the generalization capability of DistillFlow in three aspects: framework generalization, correspondence generalization and cross-dataset generalization.



### Grapevine Winter Pruning Automation: On Potential Pruning Points Detection through 2D Plant Modeling using Grapevine Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.04208v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.04208v1)
- **Published**: 2021-06-08 09:36:54+00:00
- **Updated**: 2021-06-08 09:36:54+00:00
- **Authors**: Miguel Fernandes, Antonello Scaldaferri, Giuseppe Fiameni, Tao Teng, Matteo Gatti, Stefano Poni, Claudio Semini, Darwin Caldwell, Fei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Grapevine winter pruning is a complex task, that requires skilled workers to execute it correctly. The complexity of this task is also the reason why it is time consuming. Considering that this operation takes about 80-120 hours/ha to be completed, and therefore is even more crucial in large-size vineyards, an automated system can help to speed up the process. To this end, this paper presents a novel multidisciplinary approach that tackles this challenging task by performing object segmentation on grapevine images, used to create a representative model of the grapevine plants. Second, a set of potential pruning points is generated from this plant representation. We will describe (a) a methodology for data acquisition and annotation, (b) a neural network fine-tuning for grapevine segmentation, (c) an image processing based method for creating the representative model of grapevines, starting from the inferred segmentation and (d) potential pruning points detection and localization, based on the plant model which is a simplification of the grapevine structure. With this approach, we are able to identify a significant set of potential pruning points on the canes, that can be used, with further selection, to derive the final set of the real pruning points.



### On the use of automatically generated synthetic image datasets for benchmarking face recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.04215v1
- **DOI**: 10.1109/IJCB52358.2021.9484363
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04215v1)
- **Published**: 2021-06-08 09:54:02+00:00
- **Updated**: 2021-06-08 09:54:02+00:00
- **Authors**: Laurent Colbois, Tiago de Freitas Pereira, Sbastien Marcel
- **Comment**: 11 pages, Accepted for publication in the 2021 International Joint
  Conference on Biometrics (IJCB 2021)
- **Journal**: None
- **Summary**: The availability of large-scale face datasets has been key in the progress of face recognition. However, due to licensing issues or copyright infringement, some datasets are not available anymore (e.g. MS-Celeb-1M). Recent advances in Generative Adversarial Networks (GANs), to synthesize realistic face images, provide a pathway to replace real datasets by synthetic datasets, both to train and benchmark face recognition (FR) systems. The work presented in this paper provides a study on benchmarking FR systems using a synthetic dataset. First, we introduce the proposed methodology to generate a synthetic dataset, without the need for human intervention, by exploiting the latent structure of a StyleGAN2 model with multiple controlled factors of variation. Then, we confirm that (i) the generated synthetic identities are not data subjects from the GAN's training dataset, which is verified on a synthetic dataset with 10K+ identities; (ii) benchmarking results on the synthetic dataset are a good substitution, often providing error rates and system ranking similar to the benchmarking on the real dataset.



### On the role of feedback in visual processing: a predictive coding perspective
- **Arxiv ID**: http://arxiv.org/abs/2106.04225v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2106.04225v1)
- **Published**: 2021-06-08 10:07:23+00:00
- **Updated**: 2021-06-08 10:07:23+00:00
- **Authors**: Andrea Alamia, Milad Mozafari, Bhavin Choksi, Rufin VanRullen
- **Comment**: 'Andrea Alamia' and 'Milad Mozafari' contributed equally to this work
- **Journal**: None
- **Summary**: Brain-inspired machine learning is gaining increasing consideration, particularly in computer vision. Several studies investigated the inclusion of top-down feedback connections in convolutional networks; however, it remains unclear how and when these connections are functionally helpful. Here we address this question in the context of object recognition under noisy conditions. We consider deep convolutional networks (CNNs) as models of feed-forward visual processing and implement Predictive Coding (PC) dynamics through feedback connections (predictive feedback) trained for reconstruction or classification of clean images. To directly assess the computational role of predictive feedback in various experimental situations, we optimize and interpret the hyper-parameters controlling the network's recurrent dynamics. That is, we let the optimization process determine whether top-down connections and predictive coding dynamics are functionally beneficial. Across different model depths and architectures (3-layer CNN, ResNet18, and EfficientNetB0) and against various types of noise (CIFAR100-C), we find that the network increasingly relies on top-down predictions as the noise level increases; in deeper networks, this effect is most prominent at lower layers. In addition, the accuracy of the network implementing PC dynamics significantly increases over time-steps, compared to its equivalent forward network. All in all, our results provide novel insights relevant to Neuroscience by confirming the computational role of feedback connections in sensory systems, and to Machine Learning by revealing how these can improve the robustness of current vision models.



### Provably Robust Detection of Out-of-distribution Data (almost) for free
- **Arxiv ID**: http://arxiv.org/abs/2106.04260v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04260v2)
- **Published**: 2021-06-08 11:40:49+00:00
- **Updated**: 2022-10-18 11:40:06+00:00
- **Authors**: Alexander Meinke, Julian Bitterwolf, Matthias Hein
- **Comment**: None
- **Journal**: None
- **Summary**: The application of machine learning in safety-critical systems requires a reliable assessment of uncertainty. However, deep neural networks are known to produce highly overconfident predictions on out-of-distribution (OOD) data. Even if trained to be non-confident on OOD data, one can still adversarially manipulate OOD data so that the classifier again assigns high confidence to the manipulated samples. We show that two previously published defenses can be broken by better adapted attacks, highlighting the importance of robustness guarantees around OOD data. Since the existing method for this task is hard to train and significantly limits accuracy, we construct a classifier that can simultaneously achieve provably adversarially robust OOD detection and high clean accuracy. Moreover, by slightly modifying the classifier's architecture our method provably avoids the asymptotic overconfidence problem of standard neural networks. We provide code for all our experiments.



### On the Connection between Local Attention and Dynamic Depth-wise Convolution
- **Arxiv ID**: http://arxiv.org/abs/2106.04263v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04263v5)
- **Published**: 2021-06-08 11:47:44+00:00
- **Updated**: 2022-08-04 09:27:15+00:00
- **Authors**: Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiaying Liu, Jingdong Wang
- **Comment**: ICLR 2022 Spotlight
- **Journal**: None
- **Summary**: Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as weight computation. Sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. Weight sharing: the connection weights for one position are shared across channels or within each group of channels. Dynamic weight: the connection weights are dynamically predicted according to each image instance. We point out that local attention resembles depth-wise convolution and its dynamic version in sparse connectivity. The main difference lies in weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions. We empirically observe that the models based on depth-wise convolution and the dynamic variant with lower computation complexity perform on-par with or sometimes slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation. These observations suggest that Local Vision Transformer takes advantage of two regularization forms and dynamic weight to increase the network capacity. Code is available at https://github.com/Atten4Vis/DemystifyLocalViT.



### HPRNet: Hierarchical Point Regression for Whole-Body Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.04269v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04269v2)
- **Published**: 2021-06-08 11:56:38+00:00
- **Updated**: 2021-08-23 09:47:22+00:00
- **Authors**: Nermin Samet, Emre Akbas
- **Comment**: accepted for publication at IMAVIS
- **Journal**: None
- **Summary**: In this paper, we present a new bottom-up one-stage method for whole-body pose estimation, which we call "hierarchical point regression," or HPRNet for short. In standard body pose estimation, the locations of $\sim 17$ major joints on the human body are estimated. Differently, in whole-body pose estimation, the locations of fine-grained keypoints (68 on face, 21 on each hand and 3 on each foot) are estimated as well, which creates a scale variance problem that needs to be addressed. To handle the scale variance among different body parts, we build a hierarchical point representation of body parts and jointly regress them. The relative locations of fine-grained keypoints in each part (e.g. face) are regressed in reference to the center of that part, whose location itself is estimated relative to the person center. In addition, unlike the existing two-stage methods, our method predicts whole-body pose in a constant time independent of the number of people in an image. On the COCO WholeBody dataset, HPRNet significantly outperforms all previous bottom-up methods on the keypoint detection of all whole-body parts (i.e. body, foot, face and hand); it also achieves state-of-the-art results on face (75.4 AP) and hand (50.4 AP) keypoint detection. Code and models are available at \url{https://github.com/nerminsamet/HPRNet}.



### A Synchronized Reprojection-based Model for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.04274v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04274v3)
- **Published**: 2021-06-08 12:11:56+00:00
- **Updated**: 2022-04-13 05:03:18+00:00
- **Authors**: Yicheng Deng, Cheng Sun, Yongqi Sun, Jiahui Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: 3D human pose estimation is still a challenging problem despite the large amount of work that has been done in this field. Generally, most methods directly use neural networks and ignore certain constraints (e.g., reprojection constraints and joint angle and bone length constraints). This paper proposes a weakly supervised GAN-based model for 3D human pose estimation that considers 3D information along with 2D information simultaneously, in which a reprojection network is employed to learn the mapping of the distribution from 3D poses to 2D poses. In particular, we train the reprojection network and the generative adversarial network synchronously. Furthermore, inspired by the typical kinematic chain space (KCS) matrix, we propose a weighted KCS matrix, which is added into the discriminator's input to impose joint angle and bone length constraints. The experimental results on Human3.6M show that our method outperforms state-of-the-art methods by approximately 24.7\%.



### Generative adversarial network with object detector discriminator for enhanced defect detection on ultrasonic B-scans
- **Arxiv ID**: http://arxiv.org/abs/2106.04281v1
- **DOI**: 10.1016/j.neucom.2021.06.094
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04281v1)
- **Published**: 2021-06-08 12:21:21+00:00
- **Updated**: 2021-06-08 12:21:21+00:00
- **Authors**: Luka Posilovi, Duje Medak, Marko Subasic, Marko Budimir, Sven Loncaric
- **Comment**: None
- **Journal**: None
- **Summary**: Non-destructive testing is a set of techniques for defect detection in materials. While the set of imaging techniques are manifold, ultrasonic imaging is the one used the most. The analysis is mainly performed by human inspectors manually analyzing recorded images. The low number of defects in real ultrasonic inspections and legal issues considering data from such inspections make it difficult to obtain proper results from automatic ultrasonic image (B-scan) analysis. In this paper, we present a novel deep learning Generative Adversarial Network model for generating ultrasonic B-scans with defects in distinct locations. Furthermore, we show that generated B-scans can be used for synthetic data augmentation, and can improve the performance of deep convolutional neural object detection networks. Our novel method is demonstrated on a dataset of almost 4000 B-scans with more than 6000 annotated defects. Defect detection performance when training on real data yielded average precision of 71%. By training only on generated data the results increased to 72.1%, and by mixing generated and real data we achieve 75.7% average precision. We believe that synthetic data generation can generalize to other challenges with limited datasets and could be used for training human personnel.



### NWT: Towards natural audio-to-video generation with representation learning
- **Arxiv ID**: http://arxiv.org/abs/2106.04283v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.AI, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2106.04283v1)
- **Published**: 2021-06-08 12:22:29+00:00
- **Updated**: 2021-06-08 12:22:29+00:00
- **Authors**: Rayhane Mama, Marc S. Tyndel, Hashiam Kadhim, Cole Clifford, Ragavan Thurairatnam
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we introduce NWT, an expressive speech-to-video model. Unlike approaches that use domain-specific intermediate representations such as pose keypoints, NWT learns its own latent representations, with minimal assumptions about the audio and video content. To this end, we propose a novel discrete variational autoencoder with adversarial loss, dVAE-Adv, which learns a new discrete latent representation we call Memcodes. Memcodes are straightforward to implement, require no additional loss terms, are stable to train compared with other approaches, and show evidence of interpretability. To predict on the Memcode space, we use an autoregressive encoder-decoder model conditioned on audio. Additionally, our model can control latent attributes in the generated video that are not annotated in the data. We train NWT on clips from HBO's Last Week Tonight with John Oliver. NWT consistently scores above other approaches in Mean Opinion Score (MOS) on tests of overall video naturalness, facial naturalness and expressiveness, and lipsync quality. This work sets a strong baseline for generalized audio-to-video synthesis. Samples are available at https://next-week-tonight.github.io/NWT/.



### Contrastive Representation Learning for Hand Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.04324v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04324v2)
- **Published**: 2021-06-08 13:31:58+00:00
- **Updated**: 2021-07-02 07:06:44+00:00
- **Authors**: Christian Zimmermann, Max Argus, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents improvements in monocular hand shape estimation by building on top of recent advances in unsupervised learning. We extend momentum contrastive learning and contribute a structured collection of hand images, well suited for visual representation learning, which we call HanCo. We find that the representation learned by established contrastive learning methods can be improved significantly by exploiting advanced background removal techniques and multi-view information. These allow us to generate more diverse instance pairs than those obtained by augmentations commonly used in exemplar based approaches. Our method leads to a more suitable representation for the hand shape estimation task and shows a 4.7% reduction in mesh error and a 3.6% improvement in F-score compared to an ImageNet pretrained baseline. We make our benchmark dataset publicly available, to encourage further research into this direction.



### Progressive Spatio-Temporal Bilinear Network with Monte Carlo Dropout for Landmark-based Facial Expression Recognition with Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.04332v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CC, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2106.04332v1)
- **Published**: 2021-06-08 13:40:30+00:00
- **Updated**: 2021-06-08 13:40:30+00:00
- **Authors**: Negar Heidari, Alexandros Iosifidis
- **Comment**: 6 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: Deep neural networks have been widely used for feature learning in facial expression recognition systems. However, small datasets and large intra-class variability can lead to overfitting. In this paper, we propose a method which learns an optimized compact network topology for real-time facial expression recognition utilizing localized facial landmark features. Our method employs a spatio-temporal bilinear layer as backbone to capture the motion of facial landmarks during the execution of a facial expression effectively. Besides, it takes advantage of Monte Carlo Dropout to capture the model's uncertainty which is of great importance to analyze and treat uncertain cases. The performance of our method is evaluated on three widely used datasets and it is comparable to that of video-based state-of-the-art methods while it has much less complexity.



### Segmentation and ABCD rule extraction for skin tumors classification
- **Arxiv ID**: http://arxiv.org/abs/2106.04372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04372v1)
- **Published**: 2021-06-08 14:07:59+00:00
- **Updated**: 2021-06-08 14:07:59+00:00
- **Authors**: Mahammed Messadi, Hocine Cherifi, Abdelhafid Bessaid
- **Comment**: None
- **Journal**: Journal of Convergence for Information Technology, 2014
- **Summary**: During the last years, computer vision-based diagnosis systems have been widely used in several hospitals and dermatology clinics, aiming at the early detection of malignant melanoma tumor, which is among the most frequent types of skin cancer. In this work, we present an automated diagnosis system based on the ABCD rule used in clinical diagnosis in order to discriminate benign from malignant skin lesions. First, to reduce the influence of small structures, a preprocessing step based on morphological and fast marching schemes is used. In the second step, an unsupervised approach for lesion segmentation is proposed. Iterative thresholding is applied to initialize level set automatically. As the detection of an automated border is an important step for the correctness of subsequent phases in the computerized melanoma recognition systems, we compare its accuracy with growcut and mean shift algorithms, and discuss how these results may influence in the following steps: the feature extraction and the final lesion classification. Relying on visual diagnosis four features: Asymmetry (A), Border (B), Color (C) and Diversity (D) are computed and used to construct a classification module based on artificial neural network for the recognition of malignant melanoma. This framework has been tested on a dermoscopic database [16] of 320 images. The classification results show an increasing true detection rate and a decreasing false positive rate.



### CSRNet: Cascaded Selective Resolution Network for Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.04400v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04400v2)
- **Published**: 2021-06-08 14:22:09+00:00
- **Updated**: 2022-04-19 02:50:17+00:00
- **Authors**: Jingjing Xiong, Lai-Man Po, Wing-Yin Yu, Chang Zhou, Pengfei Xian, Weifeng Ou
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time semantic segmentation has received considerable attention due to growing demands in many practical applications, such as autonomous vehicles, robotics, etc. Existing real-time segmentation approaches often utilize feature fusion to improve segmentation accuracy. However, they fail to fully consider the feature information at different resolutions and the receptive fields of the networks are relatively limited, thereby compromising the performance. To tackle this problem, we propose a light Cascaded Selective Resolution Network (CSRNet) to improve the performance of real-time segmentation through multiple context information embedding and enhanced feature aggregation. The proposed network builds a three-stage segmentation system, which integrates feature information from low resolution to high resolution and achieves feature refinement progressively. CSRNet contains two critical modules: the Shorted Pyramid Fusion Module (SPFM) and the Selective Resolution Module (SRM). The SPFM is a computationally efficient module to incorporate the global context information and significantly enlarge the receptive field at each stage. The SRM is designed to fuse multi-resolution feature maps with various receptive fields, which assigns soft channel attentions across the feature maps and helps to remedy the problem caused by multi-scale objects. Comprehensive experiments on two well-known datasets demonstrate that the proposed CSRNet effectively improves the performance for real-time segmentation.



### SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.04403v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.04403v2)
- **Published**: 2021-06-08 14:28:13+00:00
- **Updated**: 2021-06-09 05:39:51+00:00
- **Authors**: Ioannis Kazakos, Carles Ventura, Miriam Bellver, Carina Silberer, Xavier Giro-i-Nieto
- **Comment**: Accepted as poster at the NAACL 2021 Visually Grounded Interaction
  and Language (ViGIL) Workshop. 4 pages. Project website:
  https://imatge-upc.github.io/synthref/
- **Journal**: None
- **Summary**: Recent advances in deep learning have brought significant progress in visual grounding tasks such as language-guided video object segmentation. However, collecting large datasets for these tasks is expensive in terms of annotation time, which represents a bottleneck. To this end, we propose a novel method, namely SynthRef, for generating synthetic referring expressions for target objects in an image (or video frame), and we also present and disseminate the first large-scale dataset with synthetic referring expressions for video object segmentation. Our experiments demonstrate that by training with our synthetic referring expressions one can improve the ability of a model to generalize across different datasets, without any additional annotation cost. Moreover, our formulation allows its application to any object detection or segmentation dataset.



### On the relation between statistical learning and perceptual distances
- **Arxiv ID**: http://arxiv.org/abs/2106.04427v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2106.04427v4)
- **Published**: 2021-06-08 14:56:56+00:00
- **Updated**: 2022-03-16 13:22:05+00:00
- **Authors**: Alexander Hepburn, Valero Laparra, Raul Santos-Rodriguez, Johannes Ball, Jess Malo
- **Comment**: None
- **Journal**: None
- **Summary**: It has been demonstrated many times that the behavior of the human visual system is connected to the statistics of natural images. Since machine learning relies on the statistics of training data as well, the above connection has interesting implications when using perceptual distances (which mimic the behavior of the human visual system) as a loss function. In this paper, we aim to unravel the non-trivial relationships between the probability distribution of the data, perceptual distances, and unsupervised machine learning. To this end, we show that perceptual sensitivity is correlated with the probability of an image in its close neighborhood. We also explore the relation between distances induced by autoencoders and the probability distribution of the training data, as well as how these induced distances are correlated with human perception. Finally, we find perceptual distances do not always lead to noticeable gains in performance over Euclidean distance in common image processing tasks, except when data is scarce and the perceptual distance provides regularization. We propose this may be due to a \emph{double-counting} effect of the image statistics, once in the perceptual distance and once in the training procedure.



### Artificial Intelligence in Minimally Invasive Interventional Treatment
- **Arxiv ID**: http://arxiv.org/abs/2106.15306v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV, I.2.1; I.2.10; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2106.15306v1)
- **Published**: 2021-06-08 14:57:25+00:00
- **Updated**: 2021-06-08 14:57:25+00:00
- **Authors**: Daniel Ruijters
- **Comment**: None
- **Journal**: None
- **Summary**: Minimally invasive image guided treatment procedures often employ advanced image processing algorithms. The recent developments of artificial intelligence algorithms harbor potential to further enhance this domain. In this article we explore several application areas within the minimally invasive treatment space and discuss the deployment of artificial intelligence within these areas.



### SDGMNet: Statistic-based Dynamic Gradient Modulation for Local Descriptor Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.04434v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.04434v2)
- **Published**: 2021-06-08 15:10:31+00:00
- **Updated**: 2021-06-09 12:45:28+00:00
- **Authors**: Jiayi Ma, Yuxin Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Modifications on triplet loss that rescale the back-propagated gradients of special pairs have made significant progress on local descriptor learning. However, current gradient modulation strategies are mainly static so that they would suffer from changes of training phases or datasets. In this paper, we propose a dynamic gradient modulation, named SDGMNet, to improve triplet loss for local descriptor learning. The core of our method is formulating modulation functions with statistical characteristics which are estimated dynamically. Firstly, we perform deep analysis on back propagation of general triplet-based loss and introduce included angle for distance measure. On this basis, auto-focus modulation is employed to moderate the impact of statistically uncommon individual pairs in stochastic gradient descent optimization; probabilistic margin cuts off the gradients of proportional Siamese pairs that are believed to reach the optimum; power adjustment balances the total weights of negative pairs and positive pairs. Extensive experiments demonstrate that our novel descriptor surpasses previous state-of-the-arts on standard benchmarks including patch verification, matching and retrieval tasks.



### A multi-centre polyp detection and segmentation dataset for generalisability assessment
- **Arxiv ID**: http://arxiv.org/abs/2106.04463v3
- **DOI**: 10.1038/s41597-023-01981-y
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.04463v3)
- **Published**: 2021-06-08 15:48:17+00:00
- **Updated**: 2023-05-19 09:10:09+00:00
- **Authors**: Sharib Ali, Debesh Jha, Noha Ghatwary, Stefano Realdon, Renato Cannizzaro, Osama E. Salem, Dominique Lamarque, Christian Daul, Michael A. Riegler, Kim V. Anonsen, Andreas Petlund, Pl Halvorsen, Jens Rittscher, Thomas de Lange, James E. East
- **Comment**: 19 pages
- **Journal**: Sci Data 10, 75 (2023)
- **Summary**: Polyps in the colon are widely known cancer precursors identified by colonoscopy. Whilst most polyps are benign, the polyp's number, size and surface structure are linked to the risk of colon cancer. Several methods have been developed to automate polyp detection and segmentation. However, the main issue is that they are not tested rigorously on a large multicentre purpose-built dataset, one reason being the lack of a comprehensive public dataset. As a result, the developed methods may not generalise to different population datasets. To this extent, we have curated a dataset from six unique centres incorporating more than 300 patients. The dataset includes both single frame and sequence data with 3762 annotated polyp labels with precise delineation of polyp boundaries verified by six senior gastroenterologists. To our knowledge, this is the most comprehensive detection and pixel-level segmentation dataset (referred to as \textit{PolypGen}) curated by a team of computational scientists and expert gastroenterologists. The paper provides insight into data construction and annotation strategies, quality assurance, and technical validation. Our dataset can be downloaded from \url{ https://doi.org/10.7303/syn26376615}.



### Interpreting Deep Learning based Cerebral Palsy Prediction with Channel Attention
- **Arxiv ID**: http://arxiv.org/abs/2106.04471v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04471v1)
- **Published**: 2021-06-08 15:57:17+00:00
- **Updated**: 2021-06-08 15:57:17+00:00
- **Authors**: Manli Zhu, Qianhui Men, Edmond S. L. Ho, Howard Leung, Hubert P. H. Shum
- **Comment**: None
- **Journal**: None
- **Summary**: Early prediction of cerebral palsy is essential as it leads to early treatment and monitoring. Deep learning has shown promising results in biomedical engineering thanks to its capacity of modelling complicated data with its non-linear architecture. However, due to their complex structure, deep learning models are generally not interpretable by humans, making it difficult for clinicians to rely on the findings. In this paper, we propose a channel attention module for deep learning models to predict cerebral palsy from infants' body movements, which highlights the key features (i.e. body joints) the model identifies as important, thereby indicating why certain diagnostic results are found. To highlight the capacity of the deep network in modelling input features, we utilize raw joint positions instead of hand-crafted features. We validate our system with a real-world infant movement dataset. Our proposed channel attention module enables the visualization of the vital joints to this disease that the network considers. Our system achieves 91.67% accuracy, suppressing other state-of-the-art deep learning methods.



### MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras
- **Arxiv ID**: http://arxiv.org/abs/2106.04477v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04477v2)
- **Published**: 2021-06-08 16:03:50+00:00
- **Updated**: 2022-02-07 10:17:12+00:00
- **Authors**: Xuelin Chen, Weiyu Li, Daniel Cohen-Or, Niloy J. Mitra, Baoquan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing novel views of dynamic humans from stationary monocular cameras is a specialized but desirable setup. This is particularly attractive as it does not require static scenes, controlled environments, or specialized capture hardware. In contrast to techniques that exploit multi-view observations, the problem of modeling a dynamic scene from a single view is significantly more under-constrained and ill-posed. In this paper, we introduce Neural Motion Consensus Flow (MoCo-Flow), a representation that models dynamic humans in stationary monocular cameras using a 4D continuous time-variant function. We learn the proposed representation by optimizing for a dynamic scene that minimizes the total rendering error, over all the observed images. At the heart of our work lies a carefully designed optimization scheme, which includes a dedicated initialization step and is constrained by a motion consensus regularization on the estimated motion flow. We extensively evaluate MoCo-Flow on several datasets that contain human motions of varying complexity, and compare, both qualitatively and quantitatively, to several baselines and ablated variations of our methods, showing the efficacy and merits of the proposed approach. Pretrained model, code, and data will be released for research purposes upon paper acceptance.



### Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions
- **Arxiv ID**: http://arxiv.org/abs/2106.04484v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.04484v2)
- **Published**: 2021-06-08 16:09:47+00:00
- **Updated**: 2021-09-17 14:53:59+00:00
- **Authors**: Daniel Rosenberg, Itai Gat, Amir Feder, Roi Reichart
- **Comment**: ACL 2021. Our code and data are available at
  https://danrosenberg.github.io/rad-measure/
- **Journal**: None
- **Summary**: Deep learning algorithms have shown promising results in visual question answering (VQA) tasks, but a more careful look reveals that they often do not understand the rich signal they are being fed with. To understand and better measure the generalization capabilities of VQA systems, we look at their robustness to counterfactually augmented data. Our proposed augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes. Using these augmentations, we propose a new robustness measure, Robustness to Augmented Data (RAD), which measures the consistency of model predictions between original and augmented examples. Through extensive experimentation, we show that RAD, unlike classical accuracy measures, can quantify when state-of-the-art systems are not robust to counterfactuals. We find substantial failure cases which reveal that current VQA systems are still brittle. Finally, we connect between robustness and generalization, demonstrating the predictive power of RAD for performance on unseen augmentations.



### Low-Rank Subspaces in GANs
- **Arxiv ID**: http://arxiv.org/abs/2106.04488v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04488v2)
- **Published**: 2021-06-08 16:16:32+00:00
- **Updated**: 2021-11-30 18:50:56+00:00
- **Authors**: Jiapeng Zhu, Ruili Feng, Yujun Shen, Deli Zhao, Zhengjun Zha, Jingren Zhou, Qifeng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The latent space of a Generative Adversarial Network (GAN) has been shown to encode rich semantics within some subspaces. To identify these subspaces, researchers typically analyze the statistical information from a collection of synthesized data, and the identified subspaces tend to control image attributes globally (i.e., manipulating an attribute causes the change of an entire image). By contrast, this work introduces low-rank subspaces that enable more precise control of GAN generation. Concretely, given an arbitrary image and a region of interest (e.g., eyes of face images), we manage to relate the latent space to the image region with the Jacobian matrix and then use low-rank factorization to discover steerable latent subspaces. There are three distinguishable strengths of our approach that can be aptly called LowRankGAN. First, compared to analytic algorithms in prior work, our low-rank factorization of Jacobians is able to find the low-dimensional representation of attribute manifold, making image editing more precise and controllable. Second, low-rank factorization naturally yields a null space of attributes such that moving the latent code within it only affects the outer region of interest. Therefore, local image editing can be simply achieved by projecting an attribute vector into the null space without relying on a spatial mask as existing methods do. Third, our method can robustly work with a local region from one image for analysis yet well generalize to other images, making it much easy to use in practice. Extensive experiments on state-of-the-art GAN models (including StyleGAN2 and BigGAN) trained on various datasets demonstrate the effectiveness of our LowRankGAN.



### Recurrent Inference Machines as inverse problem solvers for MR relaxometry
- **Arxiv ID**: http://arxiv.org/abs/2106.07379v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07379v1)
- **Published**: 2021-06-08 16:50:49+00:00
- **Updated**: 2021-06-08 16:50:49+00:00
- **Authors**: E. R. Sabidussi, S. Klein, M. W. A. Caan, S. Bazrafkan, A. J. den Dekker, J. Sijbers, W. J. Niessen, D. H. J. Poot
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: In this paper, we propose the use of Recurrent Inference Machines (RIMs) to perform T1 and T2 mapping. The RIM is a neural network framework that learns an iterative inference process based on the signal model, similar to conventional statistical methods for quantitative MRI (QMRI), such as the Maximum Likelihood Estimator (MLE). This framework combines the advantages of both data-driven and model-based methods, and, we hypothesize, is a promising tool for QMRI. Previously, RIMs were used to solve linear inverse reconstruction problems. Here, we show that they can also be used to optimize non-linear problems and estimate relaxometry maps with high precision and accuracy. The developed RIM framework is evaluated in terms of accuracy and precision and compared to an MLE method and an implementation of the ResNet. The results show that the RIM improves the quality of estimates compared to the other techniques in Monte Carlo experiments with simulated data, test-retest analysis of a system phantom, and in-vivo scans. Additionally, inference with the RIM is 150 times faster than the MLE, and robustness to (slight) variations of scanning parameters is demonstrated. Hence, the RIM is a promising and flexible method for QMRI. Coupled with an open-source training data generation tool, it presents a compelling alternative to previous methods.



### MVT: Mask Vision Transformer for Facial Expression Recognition in the wild
- **Arxiv ID**: http://arxiv.org/abs/2106.04520v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04520v2)
- **Published**: 2021-06-08 16:58:10+00:00
- **Updated**: 2021-07-10 13:03:06+00:00
- **Authors**: Hanting Li, Mingzhe Sui, Feng Zhao, Zhengjun Zha, Feng Wu
- **Comment**: 11 pages, 6 figures, 5 tables, conference
- **Journal**: None
- **Summary**: Facial Expression Recognition (FER) in the wild is an extremely challenging task in computer vision due to variant backgrounds, low-quality facial images, and the subjectiveness of annotators. These uncertainties make it difficult for neural networks to learn robust features on limited-scale datasets. Moreover, the networks can be easily distributed by the above factors and perform incorrect decisions. Recently, vision transformer (ViT) and data-efficient image transformers (DeiT) present their significant performance in traditional classification tasks. The self-attention mechanism makes transformers obtain a global receptive field in the first layer which dramatically enhances the feature extraction capability. In this work, we first propose a novel pure transformer-based mask vision transformer (MVT) for FER in the wild, which consists of two modules: a transformer-based mask generation network (MGN) to generate a mask that can filter out complex backgrounds and occlusion of face images, and a dynamic relabeling module to rectify incorrect labels in FER datasets in the wild. Extensive experimental results demonstrate that our MVT outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with 89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable result on AffectNet-8 with 61.40%.



### RobustNav: Towards Benchmarking Robustness in Embodied Navigation
- **Arxiv ID**: http://arxiv.org/abs/2106.04531v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.04531v1)
- **Published**: 2021-06-08 17:14:33+00:00
- **Updated**: 2021-06-08 17:14:33+00:00
- **Authors**: Prithvijit Chattopadhyay, Judy Hoffman, Roozbeh Mottaghi, Aniruddha Kembhavi
- **Comment**: 18 pages, 8 figures, Code: https://github.com/allenai/robustnav
- **Journal**: None
- **Summary**: As an attempt towards assessing the robustness of embodied navigation agents, we propose RobustNav, a framework to quantify the performance of embodied navigation agents when exposed to a wide variety of visual - affecting RGB inputs - and dynamics - affecting transition dynamics - corruptions. Most recent efforts in visual navigation have typically focused on generalizing to novel target environments with similar appearance and dynamics characteristics. With RobustNav, we find that some standard embodied navigation agents significantly underperform (or fail) in the presence of visual or dynamics corruptions. We systematically analyze the kind of idiosyncrasies that emerge in the behavior of such agents when operating under corruptions. Finally, for visual corruptions in RobustNav, we show that while standard techniques to improve robustness such as data-augmentation and self-supervised adaptation offer some zero-shot resistance and improvements in navigation performance, there is still a long way to go in terms of recovering lost performance relative to clean "non-corrupt" settings, warranting more research in this direction. Our code is available at https://github.com/allenai/robustnav



### Chasing Sparsity in Vision Transformers: An End-to-End Exploration
- **Arxiv ID**: http://arxiv.org/abs/2106.04533v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.04533v3)
- **Published**: 2021-06-08 17:18:00+00:00
- **Updated**: 2021-10-22 21:45:38+00:00
- **Authors**: Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, Zhangyang Wang
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Vision transformers (ViTs) have recently received explosive popularity, but their enormous model sizes and training costs remain daunting. Conventional post-training pruning often incurs higher training budgets. In contrast, this paper aims to trim down both the training memory overhead and the inference complexity, without sacrificing the achievable accuracy. We carry out the first-of-its-kind comprehensive exploration, on taking a unified approach of integrating sparsity in ViTs "from end to end". Specifically, instead of training full ViTs, we dynamically extract and train sparse subnetworks, while sticking to a fixed small parameter budget. Our approach jointly optimizes model parameters and explores connectivity throughout training, ending up with one sparse network as the final output. The approach is seamlessly extended from unstructured to structured sparsity, the latter by considering to guide the prune-and-grow of self-attention heads inside ViTs. We further co-explore data and architecture sparsity for additional efficiency gains by plugging in a novel learnable token selector to adaptively determine the currently most vital patches. Extensive results on ImageNet with diverse ViT backbones validate the effectiveness of our proposals which obtain significantly reduced computational cost and almost unimpaired generalization. Perhaps most surprisingly, we find that the proposed sparse (co-)training can sometimes improve the ViT accuracy rather than compromising it, making sparsity a tantalizing "free lunch". For example, our sparsified DeiT-Small at (5%, 50%) sparsity for (data, architecture), improves 0.28% top-1 accuracy, and meanwhile enjoys 49.32% FLOPs and 4.40% running time savings. Our codes are available at https://github.com/VITA-Group/SViTE.



### Object Based Attention Through Internal Gating
- **Arxiv ID**: http://arxiv.org/abs/2106.04540v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2106.04540v1)
- **Published**: 2021-06-08 17:20:50+00:00
- **Updated**: 2021-06-08 17:20:50+00:00
- **Authors**: Jordan Lei, Ari S. Benjamin, Konrad P. Kording
- **Comment**: None
- **Journal**: None
- **Summary**: Object-based attention is a key component of the visual system, relevant for perception, learning, and memory. Neurons tuned to features of attended objects tend to be more active than those associated with non-attended objects. There is a rich set of models of this phenomenon in computational neuroscience. However, there is currently a divide between models that successfully match physiological data but can only deal with extremely simple problems and models of attention used in computer vision. For example, attention in the brain is known to depend on top-down processing, whereas self-attention in deep learning does not. Here, we propose an artificial neural network model of object-based attention that captures the way in which attention is both top-down and recurrent. Our attention model works well both on simple test stimuli, such as those using images of handwritten digits, and on more complex stimuli, such as natural images drawn from the COCO dataset. We find that our model replicates a range of findings from neuroscience, including attention-invariant tuning, inhibition of return, and attention-mediated scaling of activity. Understanding object based attention is both computationally interesting and a key problem for computational neuroscience.



### DETReg: Unsupervised Pretraining with Region Priors for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.04550v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04550v5)
- **Published**: 2021-06-08 17:39:14+00:00
- **Updated**: 2023-07-20 02:00:22+00:00
- **Authors**: Amir Bar, Xin Wang, Vadim Kantorov, Colorado J Reed, Roei Herzig, Gal Chechik, Anna Rohrbach, Trevor Darrell, Amir Globerson
- **Comment**: Project page: https://www.amirbar.net/detreg/
- **Journal**: None
- **Summary**: Recent self-supervised pretraining methods for object detection largely focus on pretraining the backbone of the object detector, neglecting key parts of detection architecture. Instead, we introduce DETReg, a new self-supervised method that pretrains the entire object detection network, including the object localization and embedding components. During pretraining, DETReg predicts object localizations to match the localizations from an unsupervised region proposal generator and simultaneously aligns the corresponding feature embeddings with embeddings from a self-supervised image encoder. We implement DETReg using the DETR family of detectors and show that it improves over competitive baselines when finetuned on COCO, PASCAL VOC, and Airbus Ship benchmarks. In low-data regimes DETReg achieves improved performance, e.g., when training with only 1% of the labels and in the few-shot learning settings.



### Hierarchical Lovsz Embeddings for Proposal-free Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.04555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04555v1)
- **Published**: 2021-06-08 17:43:54+00:00
- **Updated**: 2021-06-08 17:43:54+00:00
- **Authors**: Tommi Kerola, Jie Li, Atsushi Kanehira, Yasunori Kudo, Alexis Vallet, Adrien Gaidon
- **Comment**: 13 pages, 9 figures, including supplementary material. To be
  published in CVPR 2021
- **Journal**: None
- **Summary**: Panoptic segmentation brings together two separate tasks: instance and semantic segmentation. Although they are related, unifying them faces an apparent paradox: how to learn simultaneously instance-specific and category-specific (i.e. instance-agnostic) representations jointly. Hence, state-of-the-art panoptic segmentation methods use complex models with a distinct stream for each task. In contrast, we propose Hierarchical Lov\'asz Embeddings, per pixel feature vectors that simultaneously encode instance- and category-level discriminative information. We use a hierarchical Lov\'asz hinge loss to learn a low-dimensional embedding space structured into a unified semantic and instance hierarchy without requiring separate network branches or object proposals. Besides modeling instances precisely in a proposal-free manner, our Hierarchical Lov\'asz Embeddings generalize to categories by using a simple Nearest-Class-Mean classifier, including for non-instance "stuff" classes where instance segmentation methods are not applicable. Our simple model achieves state-of-the-art results compared to existing proposal-free panoptic segmentation methods on Cityscapes, COCO, and Mapillary Vistas. Furthermore, our model demonstrates temporal stability between video frames.



### Scaling Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.04560v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.04560v2)
- **Published**: 2021-06-08 17:47:39+00:00
- **Updated**: 2022-06-20 09:13:51+00:00
- **Authors**: Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer
- **Comment**: Xiaohua, Alex, and Lucas contributed equally; CVPR 2022
- **Journal**: None
- **Summary**: Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.



### Data-Efficient Instance Generation from Instance Discrimination
- **Arxiv ID**: http://arxiv.org/abs/2106.04566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04566v1)
- **Published**: 2021-06-08 17:52:59+00:00
- **Updated**: 2021-06-08 17:52:59+00:00
- **Authors**: Ceyuan Yang, Yujun Shen, Yinghao Xu, Bolei Zhou
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have significantly advanced image synthesis, however, the synthesis quality drops significantly given a limited amount of training data. To improve the data efficiency of GAN training, prior work typically employs data augmentation to mitigate the overfitting of the discriminator yet still learn the discriminator with a bi-classification (i.e., real vs. fake) task. In this work, we propose a data-efficient Instance Generation (InsGen) method based on instance discrimination. Concretely, besides differentiating the real domain from the fake domain, the discriminator is required to distinguish every individual image, no matter it comes from the training set or from the generator. In this way, the discriminator can benefit from the infinite synthesized samples for training, alleviating the overfitting problem caused by insufficient training data. A noise perturbation strategy is further introduced to improve its discriminative power. Meanwhile, the learned instance discrimination capability from the discriminator is in turn exploited to encourage the generator for diverse generation. Extensive experiments demonstrate the effectiveness of our method on a variety of datasets and training settings. Noticeably, on the setting of 2K training images from the FFHQ dataset, we outperform the state-of-the-art approach with 23.5% FID improvement.



### Simulated Adversarial Testing of Face Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/2106.04569v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.04569v3)
- **Published**: 2021-06-08 17:58:10+00:00
- **Updated**: 2022-05-31 16:41:58+00:00
- **Authors**: Nataniel Ruiz, Adam Kortylewski, Weichao Qiu, Cihang Xie, Sarah Adel Bargal, Alan Yuille, Stan Sclaroff
- **Comment**: Published at IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2022
- **Journal**: None
- **Summary**: Most machine learning models are validated and tested on fixed datasets. This can give an incomplete picture of the capabilities and weaknesses of the model. Such weaknesses can be revealed at test time in the real world. The risks involved in such failures can be loss of profits, loss of time or even loss of life in certain critical applications. In order to alleviate this issue, simulators can be controlled in a fine-grained manner using interpretable parameters to explore the semantic image manifold. In this work, we propose a framework for learning how to test machine learning algorithms using simulators in an adversarial manner in order to find weaknesses in the model before deploying it in critical scenarios. We apply this method in a face recognition setup. We show that certain weaknesses of models trained on real data can be discovered using simulated samples. Using our proposed method, we can find adversarial synthetic faces that fool contemporary face recognition models. This demonstrates the fact that these models have weaknesses that are not measured by commonly used validation datasets. We hypothesize that this type of adversarial examples are not isolated, but usually lie in connected spaces in the latent space of the simulator. We present a method to find these adversarial regions as opposed to the typical adversarial points found in the adversarial example literature.



### BERT Learns to Teach: Knowledge Distillation with Meta Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.04570v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04570v3)
- **Published**: 2021-06-08 17:59:03+00:00
- **Updated**: 2022-04-01 22:29:40+00:00
- **Authors**: Wangchunshu Zhou, Canwen Xu, Julian McAuley
- **Comment**: ACL 2022 (main conference)
- **Journal**: None
- **Summary**: We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.



### Check It Again: Progressive Visual Question Answering via Visual Entailment
- **Arxiv ID**: http://arxiv.org/abs/2106.04605v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04605v1)
- **Published**: 2021-06-08 18:00:38+00:00
- **Updated**: 2021-06-08 18:00:38+00:00
- **Authors**: Qingyi Si, Zheng Lin, Mingyu Zheng, Peng Fu, Weiping Wang
- **Comment**: ACL-2021
- **Journal**: None
- **Summary**: While sophisticated Visual Question Answering models have achieved remarkable success, they tend to answer questions only according to superficial correlations between question and answer. Several recent approaches have been developed to address this language priors problem. However, most of them predict the correct answer according to one best output without checking the authenticity of answers. Besides, they only explore the interaction between image and question, ignoring the semantics of candidate answers. In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment. Specifically, we first select the candidate answers relevant to the question or the image, then we rerank the candidate answers by a visual entailment task, which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer. Experimental results show the effectiveness of our proposed framework, which establishes a new state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement.



### Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style
- **Arxiv ID**: http://arxiv.org/abs/2106.04619v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.04619v4)
- **Published**: 2021-06-08 18:18:09+00:00
- **Updated**: 2022-01-14 10:55:38+00:00
- **Authors**: Julius von Kgelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Schlkopf, Michel Besserve, Francesco Locatello
- **Comment**: NeurIPS 2021 final camera-ready revision (with minor corrections)
- **Journal**: None
- **Summary**: Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.



### Densely connected normalizing flows
- **Arxiv ID**: http://arxiv.org/abs/2106.04627v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04627v3)
- **Published**: 2021-06-08 18:24:41+00:00
- **Updated**: 2021-11-02 13:05:50+00:00
- **Authors**: Matej Grci, Ivan Grubii, Sinia egvi
- **Comment**: Accepted at NeurIPS2021
- **Journal**: None
- **Summary**: Normalizing flows are bijective mappings between inputs and latent representations with a fully factorized distribution. They are very attractive due to exact likelihood valuation and efficient sampling. However, their effective capacity is often insufficient since the bijectivity constraint limits the model width. We address this issue by incrementally padding intermediate representations with noise. We precondition the noise in accordance with previous invertible units, which we describe as cross-unit coupling. Our invertible glow-like modules increase the model expressivity by fusing a densely connected block with Nystrom self-attention. We refer to our architecture as DenseFlow since both cross-unit and intra-module couplings rely on dense connectivity. Experiments show significant improvements due to the proposed contributions and reveal state-of-the-art density estimation under moderate computing budgets.



### PAM: Understanding Product Images in Cross Product Category Attribute Extraction
- **Arxiv ID**: http://arxiv.org/abs/2106.04630v1
- **DOI**: 10.1145/3447548.3467164
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.04630v1)
- **Published**: 2021-06-08 18:30:17+00:00
- **Updated**: 2021-06-08 18:30:17+00:00
- **Authors**: Rongmei Lin, Xiang He, Jie Feng, Nasser Zalmout, Yan Liang, Li Xiong, Xin Luna Dong
- **Comment**: KDD 2021
- **Journal**: None
- **Summary**: Understanding product attributes plays an important role in improving online shopping experience for customers and serves as an integral part for constructing a product knowledge graph. Most existing methods focus on attribute extraction from text description or utilize visual information from product images such as shape and color. Compared to the inputs considered in prior works, a product image in fact contains more information, represented by a rich mixture of words and visual clues with a layout carefully designed to impress customers. This work proposes a more inclusive framework that fully utilizes these different modalities for attribute extraction. Inspired by recent works in visual question answering, we use a transformer based sequence to sequence model to fuse representations of product text, Optical Character Recognition (OCR) tokens and visual objects detected in the product image. The framework is further extended with the capability to extract attribute value across multiple product categories with a single model, by training the decoder to predict both product category and attribute value and conditioning its output on product category. The model provides a unified attribute extraction solution desirable at an e-commerce platform that offers numerous product categories with a diverse body of product attributes. We evaluated the model on two product attributes, one with many possible values and one with a small set of possible values, over 14 product categories and found the model could achieve 15% gain on the Recall and 10% gain on the F1 score compared to existing methods using text-only features.



### VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2106.04632v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2106.04632v2)
- **Published**: 2021-06-08 18:34:21+00:00
- **Updated**: 2021-08-18 21:55:27+00:00
- **Authors**: Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, Tamara Lee Berg, Mohit Bansal, Jingjing Liu, Lijuan Wang, Zicheng Liu
- **Comment**: To appear in 35th Conference on Neural Information Processing Systems
  (NeurIPS 2021) Track on Datasets and Benchmarks
- **Journal**: None
- **Summary**: Most existing video-and-language (VidL) research focuses on a single dataset, or multiple datasets of a single task. In reality, a truly useful VidL system is expected to be easily generalizable to diverse tasks, domains, and datasets. To facilitate the evaluation of such systems, we introduce Video-And-Language Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets over 3 popular tasks: (i) text-to-video retrieval; (ii) video question answering; and (iii) video captioning. VALUE benchmark aims to cover a broad range of video genres, video lengths, data volumes, and task difficulty levels. Rather than focusing on single-channel videos with visual information only, VALUE promotes models that leverage information from both video frames and their associated subtitles, as well as models that share knowledge across multiple tasks. We evaluate various baseline methods with and without large-scale VidL pre-training, and systematically investigate the impact of video input channels, fusion methods, and different video representations. We also study the transferability between tasks, and conduct multi-task learning under different settings. The significant gap between our best model and human performance calls for future study for advanced VidL models. VALUE is available at https://value-benchmark.github.io/.



### TED-net: Convolution-free T2T Vision Transformer-based Encoder-decoder Dilation network for Low-dose CT Denoising
- **Arxiv ID**: http://arxiv.org/abs/2106.04650v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2106.04650v1)
- **Published**: 2021-06-08 19:26:55+00:00
- **Updated**: 2021-06-08 19:26:55+00:00
- **Authors**: Dayang Wang, Zhan Wu, Hengyong Yu
- **Comment**: 10 pages, manuscript for 12th International Workshop on Machine
  Learning in Medical Imaging
- **Journal**: None
- **Summary**: Low dose computed tomography is a mainstream for clinical applications. How-ever, compared to normal dose CT, in the low dose CT (LDCT) images, there are stronger noise and more artifacts which are obstacles for practical applications. In the last few years, convolution-based end-to-end deep learning methods have been widely used for LDCT image denoising. Recently, transformer has shown superior performance over convolution with more feature interactions. Yet its ap-plications in LDCT denoising have not been fully cultivated. Here, we propose a convolution-free T2T vision transformer-based Encoder-decoder Dilation net-work (TED-net) to enrich the family of LDCT denoising algorithms. The model is free of convolution blocks and consists of a symmetric encoder-decoder block with sole transformer. Our model is evaluated on the AAPM-Mayo clinic LDCT Grand Challenge dataset, and results show outperformance over the state-of-the-art denoising methods.



### Automatic 2D-3D Registration without Contrast Agent during Neurovascular Interventions
- **Arxiv ID**: http://arxiv.org/abs/2106.15308v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2106.15308v1)
- **Published**: 2021-06-08 20:16:04+00:00
- **Updated**: 2021-06-08 20:16:04+00:00
- **Authors**: Robert Homan, Ren van Rijsselt, Daniel Ruijters
- **Comment**: None
- **Journal**: None
- **Summary**: Fusing live fluoroscopy images with a 3D rotational reconstruction of the vasculature allows to navigate endovascular devices in minimally invasive neuro-vascular treatment, while reducing the usage of harmful iodine contrast medium. The alignment of the fluoroscopy images and the 3D reconstruction is initialized using the sensor information of the X-ray C-arm geometry. Patient motion is then corrected by an image-based registration algorithm, based on a gradient difference similarity measure using digital reconstructed radiographs of the 3D reconstruction. This algorithm does not require the vessels in the fluoroscopy image to be filled with iodine contrast agent, but rather relies on gradients in the image (bone structures, sinuses) as landmark features. This paper investigates the accuracy, robustness and computation time aspects of the image-based registration algorithm. Using phantom experiments 97% of the registration attempts passed the success criterion of a residual registration error of less than 1 mm translation and 3{\deg} rotation. The paper establishes a new method for validation of 2D-3D registration without requiring changes to the clinical workflow, such as attaching fiducial markers. As a consequence, this method can be retrospectively applied to pre-existing clinical data. For clinical data experiments, 87% of the registration attempts passed the criterion of a residual translational error of < 1 mm, and 84% possessed a rotational error of < 3{\deg}.



### Understanding top-down attention using task-oriented ablation design
- **Arxiv ID**: http://arxiv.org/abs/2106.11339v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11339v1)
- **Published**: 2021-06-08 21:01:47+00:00
- **Updated**: 2021-06-08 21:01:47+00:00
- **Authors**: Freddie Bickford Smith, Brett D Roads, Xiaoliang Luo, Bradley C Love
- **Comment**: None
- **Journal**: None
- **Summary**: Top-down attention allows neural networks, both artificial and biological, to focus on the information most relevant for a given task. This is known to enhance performance in visual perception. But it remains unclear how attention brings about its perceptual boost, especially when it comes to naturalistic settings like recognising an object in an everyday scene. What aspects of a visual task does attention help to deal with? We aim to answer this with a computational experiment based on a general framework called task-oriented ablation design. First we define a broad range of visual tasks and identify six factors that underlie task variability. Then on each task we compare the performance of two neural networks, one with top-down attention and one without. These comparisons reveal the task-dependence of attention's perceptual boost, giving a clearer idea of the role attention plays. Whereas many existing cognitive accounts link attention to stimulus-level variables, such as visual clutter and object scale, we find greater explanatory power in system-level variables that capture the interaction between the model, the distribution of training data and the task format. This finding suggests a shift in how attention is studied could be fruitful. We make publicly available our code and results, along with statistics relevant to ImageNet-based experiments beyond this one. Our contribution serves to support the development of more human-like vision models and the design of more informative machine-learning experiments.



### OODIn: An Optimised On-Device Inference Framework for Heterogeneous Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2106.04723v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04723v1)
- **Published**: 2021-06-08 22:38:18+00:00
- **Updated**: 2021-06-08 22:38:18+00:00
- **Authors**: Stylianos I. Venieris, Ioannis Panopoulos, Iakovos S. Venieris
- **Comment**: Accepted at the 7th IEEE International Conference on Smart Computing
  (SMARTCOMP), 2021
- **Journal**: None
- **Summary**: Radical progress in the field of deep learning (DL) has led to unprecedented accuracy in diverse inference tasks. As such, deploying DL models across mobile platforms is vital to enable the development and broad availability of the next-generation intelligent apps. Nevertheless, the wide and optimised deployment of DL models is currently hindered by the vast system heterogeneity of mobile devices, the varying computational cost of different DL models and the variability of performance needs across DL applications. This paper proposes OODIn, a framework for the optimised deployment of DL apps across heterogeneous mobile devices. OODIn comprises a novel DL-specific software architecture together with an analytical framework for modelling DL applications that: (1) counteract the variability in device resources and DL models by means of a highly parametrised multi-layer design; and (2) perform a principled optimisation of both model- and system-level parameters through a multi-objective formulation, designed for DL inference apps, in order to adapt the deployment to the user-specified performance requirements and device capabilities. Quantitative evaluation shows that the proposed framework consistently outperforms status-quo designs across heterogeneous devices and delivers up to 4.3x and 3.5x performance gain over highly optimised platform- and model-aware designs respectively, while effectively adapting execution to dynamic changes in resource availability.



### Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study of the 2019 Indian Election on WhatsApp
- **Arxiv ID**: http://arxiv.org/abs/2106.04726v2
- **DOI**: None
- **Categories**: **cs.SI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04726v2)
- **Published**: 2021-06-08 23:08:47+00:00
- **Updated**: 2021-07-23 17:49:31+00:00
- **Authors**: Ashkan Kazemi, Kiran Garimella, Gautam Kishore Shahi, Devin Gaffney, Scott A. Hale
- **Comment**: None
- **Journal**: None
- **Summary**: There is currently no easy way to fact-check content on WhatsApp and other end-to-end encrypted platforms at scale. In this paper, we analyze the usefulness of a crowd-sourced "tipline" through which users can submit content ("tips") that they want fact-checked. We compare the tips sent to a WhatsApp tipline run during the 2019 Indian national elections with the messages circulating in large, public groups on WhatsApp and other social media platforms during the same period. We find that tiplines are a very useful lens into WhatsApp conversations: a significant fraction of messages and images sent to the tipline match with the content being shared on public WhatsApp groups and other social media. Our analysis also shows that tiplines cover the most popular content well, and a majority of such content is often shared to the tipline before appearing in large, public WhatsApp groups. Overall, our findings suggest tiplines can be an effective source for discovering content to fact-check.



### AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2106.04732v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04732v2)
- **Published**: 2021-06-08 23:39:12+00:00
- **Updated**: 2022-03-15 16:42:38+00:00
- **Authors**: David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, Alex Kurakin
- **Comment**: Accepted to ICLR 2022
- **Journal**: None
- **Summary**: We extend semi-supervised learning to the problem of domain adaptation to learn significantly higher-accuracy models that train on one data distribution and test on a different one. With the goal of generality, we introduce AdaMatch, a method that unifies the tasks of unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA). In an extensive experimental study, we compare its behavior with respective state-of-the-art techniques from SSL, SSDA, and UDA on vision classification tasks. We find AdaMatch either matches or significantly exceeds the state-of-the-art in each case using the same hyper-parameters regardless of the dataset or task. For example, AdaMatch nearly doubles the accuracy compared to that of the prior state-of-the-art on the UDA task for DomainNet and even exceeds the accuracy of the prior state-of-the-art obtained with pre-training by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by providing AdaMatch with just one labeled example per class from the target domain (i.e., the SSDA setting), we increase the target accuracy by an additional 6.1%, and with 5 labeled examples, by 13.6%.




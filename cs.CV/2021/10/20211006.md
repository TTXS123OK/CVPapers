# Arxiv Papers in cs.CV on 2021-10-06
### A Hierarchical Variational Neural Uncertainty Model for Stochastic Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2110.03446v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03446v1)
- **Published**: 2021-10-06 00:25:22+00:00
- **Updated**: 2021-10-06 00:25:22+00:00
- **Authors**: Moitreya Chatterjee, Narendra Ahuja, Anoop Cherian
- **Comment**: Accepted at ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: Predicting the future frames of a video is a challenging task, in part due to the underlying stochastic real-world phenomena. Prior approaches to solve this task typically estimate a latent prior characterizing this stochasticity, however do not account for the predictive uncertainty of the (deep learning) model. Such approaches often derive the training signal from the mean-squared error (MSE) between the generated frame and the ground truth, which can lead to sub-optimal training, especially when the predictive uncertainty is high. Towards this end, we introduce Neural Uncertainty Quantifier (NUQ) - a stochastic quantification of the model's predictive uncertainty, and use it to weigh the MSE loss. We propose a hierarchical, variational framework to derive NUQ in a principled manner using a deep, Bayesian graphical model. Our experiments on four benchmark stochastic video prediction datasets show that our proposed framework trains more effectively compared to the state-of-the-art models (especially when the training sets are small), while demonstrating better video generation quality and diversity against several evaluation metrics.



### Influence-Balanced Loss for Imbalanced Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.02444v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02444v1)
- **Published**: 2021-10-06 01:12:40+00:00
- **Updated**: 2021-10-06 01:12:40+00:00
- **Authors**: Seulki Park, Jongin Lim, Younghan Jeon, Jin Young Choi
- **Comment**: Published in ICCV 2021
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2021, pp. 735-744
- **Summary**: In this paper, we propose a balancing training method to address problems in imbalanced data learning. To this end, we derive a new loss used in the balancing training phase that alleviates the influence of samples that cause an overfitted decision boundary. The proposed loss efficiently improves the performance of any type of imbalance learning methods. In experiments on multiple benchmark data sets, we demonstrate the validity of our method and reveal that the proposed loss outperforms the state-of-the-art cost-sensitive loss methods. Furthermore, since our loss is not restricted to a specific task, model, or training method, it can be easily used in combination with other recent re-sampling, meta-learning, and cost-sensitive learning methods for class-imbalance problems.



### Ripple Attention for Visual Perception with Sub-quadratic Complexity
- **Arxiv ID**: http://arxiv.org/abs/2110.02453v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02453v2)
- **Published**: 2021-10-06 02:00:38+00:00
- **Updated**: 2022-06-15 13:59:31+00:00
- **Authors**: Lin Zheng, Huijie Pan, Lingpeng Kong
- **Comment**: 19 pages, 2 figures, ICML 2022 camera ready
- **Journal**: None
- **Summary**: Transformer architectures are now central to sequence modeling tasks. At its heart is the attention mechanism, which enables effective modeling of long-term dependencies in a sequence. Recently, transformers have been successfully applied in the computer vision domain, where 2D images are first segmented into patches and then treated as 1D sequences. Such linearization, however, impairs the notion of spatial locality in images, which bears important visual clues. To bridge the gap, we propose ripple attention, a sub-quadratic attention mechanism for vision transformers. Built upon the recent kernel-based efficient attention mechanisms, we design a novel dynamic programming algorithm that weights contributions of different tokens to a query with respect to their relative spatial distances in the 2D space in linear observed time. Extensive experiments and analyses demonstrate the effectiveness of ripple attention on various visual tasks.



### Post-hoc Models for Performance Estimation of Machine Learning Inference
- **Arxiv ID**: http://arxiv.org/abs/2110.02459v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, ACM-class: I.4, I.4.8.e, I.1.2.e
- **Links**: [PDF](http://arxiv.org/pdf/2110.02459v1)
- **Published**: 2021-10-06 02:20:37+00:00
- **Updated**: 2021-10-06 02:20:37+00:00
- **Authors**: Xuechen Zhang, Samet Oymak, Jiasi Chen
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Estimating how well a machine learning model performs during inference is critical in a variety of scenarios (for example, to quantify uncertainty, or to choose from a library of available models). However, the standard accuracy estimate of softmax confidence is not versatile and cannot reliably predict different performance metrics (e.g., F1-score, recall) or the performance in different application scenarios or input domains. In this work, we systematically generalize performance estimation to a diverse set of metrics and scenarios and discuss generalized notions of uncertainty calibration. We propose the use of post-hoc models to accomplish this goal and investigate design parameters, including the model type, feature engineering, and performance metric, to achieve the best estimation quality. Emphasis is given to object detection problems and, unlike prior work, our approach enables the estimation of per-image metrics such as recall and F1-score. Through extensive experiments with computer vision models and datasets in three use cases -- mobile edge offloading, model selection, and dataset shift -- we find that proposed post-hoc models consistently outperform the standard calibrated confidence baselines. To the best of our knowledge, this is the first work to develop a unified framework to address different performance estimation problems for machine learning inference.



### TSN-CA: A Two-Stage Network with Channel Attention for Low-Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2110.02477v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2110.02477v1)
- **Published**: 2021-10-06 03:20:18+00:00
- **Updated**: 2021-10-06 03:20:18+00:00
- **Authors**: Xinxu Wei, Xianshi Zhang, Shisen Wang, Yanlin Huang, Yongjie Li
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Low-light image enhancement is a challenging low-level computer vision task because after we enhance the brightness of the image, we have to deal with amplified noise, color distortion, detail loss, blurred edges, shadow blocks and halo artifacts. In this paper, we propose a Two-Stage Network with Channel Attention (denoted as TSN-CA) to enhance the brightness of the low-light image and restore the enhanced images from various kinds of degradation. In the first stage, we enhance the brightness of the low-light image in HSV space and use the information of H and S channels to help the recovery of details in V channel. In the second stage, we integrate Channel Attention (CA) mechanism into the skip connection of U-Net in order to restore the brightness-enhanced image from severe kinds of degradation in RGB space. We train and evaluate the performance of our proposed model on the LOL real-world and synthetic datasets. In addition, we test our model on several other commonly used datasets without Ground-Truth. We conduct extensive experiments to demonstrate that our method achieves excellent effect on brightness enhancement as well as denoising, details preservation and halo artifacts elimination. Our method outperforms many other state-of-the-art methods qualitatively and quantitatively.



### Deep Slap Fingerprint Segmentation for Juveniles and Adults
- **Arxiv ID**: http://arxiv.org/abs/2110.04067v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.04067v2)
- **Published**: 2021-10-06 04:48:23+00:00
- **Updated**: 2022-05-03 21:29:17+00:00
- **Authors**: M. G. Sarwar Murshed, Robert Kline, Keivan Bahmani, Faraz Hussain, Stephanie Schuckers
- **Comment**: None
- **Journal**: In 2021 IEEE International Conference on Consumer Electronics-Asia
  (ICCE-Asia) (pp. 1-4). IEEE
- **Summary**: Many fingerprint recognition systems capture four fingerprints in one image. In such systems, the fingerprint processing pipeline must first segment each four-fingerprint slap into individual fingerprints. Note that most of the current fingerprint segmentation algorithms have been designed and evaluated using only adult fingerprint datasets. In this work, we have developed a human-annotated in-house dataset of 15790 slaps of which 9084 are adult samples and 6706 are samples drawn from children from ages 4 to 12. Subsequently, the dataset is used to evaluate the matching performance of the NFSEG, a slap fingerprint segmentation system developed by NIST, on slaps from adults and juvenile subjects. Our results reveal the lower performance of NFSEG on slaps from juvenile subjects. Finally, we utilized our novel dataset to develop the Mask-RCNN based Clarkson Fingerprint Segmentation (CFSEG). Our matching results using the Verifinger fingerprint matcher indicate that CFSEG outperforms NFSEG for both adults and juvenile slaps. The CFSEG model is publicly available at \url{https://github.com/keivanB/Clarkson_Finger_Segment}



### MToFNet: Object Anti-Spoofing with Mobile Time-of-Flight Data
- **Arxiv ID**: http://arxiv.org/abs/2110.04066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.04066v1)
- **Published**: 2021-10-06 05:24:33+00:00
- **Updated**: 2021-10-06 05:24:33+00:00
- **Authors**: Yonghyun Jeong, Doyeon Kim, Jaehyeon Lee, Minki Hong, Solbi Hwang, Jongwon Choi
- **Comment**: None
- **Journal**: None
- **Summary**: In online markets, sellers can maliciously recapture others' images on display screens to utilize as spoof images, which can be challenging to distinguish in human eyes. To prevent such harm, we propose an anti-spoofing method using the paired rgb images and depth maps provided by the mobile camera with a Time-of-Fight sensor. When images are recaptured on display screens, various patterns differing by the screens as known as the moir\'e patterns can be also captured in spoof images. These patterns lead the anti-spoofing model to be overfitted and unable to detect spoof images recaptured on unseen media. To avoid the issue, we build a novel representation model composed of two embedding models, which can be trained without considering the recaptured images. Also, we newly introduce mToF dataset, the largest and most diverse object anti-spoofing dataset, and the first to utilize ToF data. Experimental results confirm that our model achieves robust generalization even across unseen domains.



### Attack as the Best Defense: Nullifying Image-to-image Translation GANs via Limit-aware Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2110.02516v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02516v1)
- **Published**: 2021-10-06 05:46:31+00:00
- **Updated**: 2021-10-06 05:46:31+00:00
- **Authors**: Chin-Yuan Yeh, Hsi-Wen Chen, Hong-Han Shuai, De-Nian Yang, Ming-Syan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: With the successful creation of high-quality image-to-image (Img2Img) translation GANs comes the non-ethical applications of DeepFake and DeepNude. Such misuses of img2img techniques present a challenging problem for society. In this work, we tackle the problem by introducing the Limit-Aware Self-Guiding Gradient Sliding Attack (LaS-GSA). LaS-GSA follows the Nullifying Attack to cancel the img2img translation process under a black-box setting. In other words, by processing input images with the proposed LaS-GSA before publishing, any targeted img2img GANs can be nullified, preventing the model from maliciously manipulating the images. To improve efficiency, we introduce the limit-aware random gradient-free estimation and the gradient sliding mechanism to estimate the gradient that adheres to the adversarial limit, i.e., the pixel value limitations of the adversarial example. Theoretical justifications validate how the above techniques prevent inefficiency caused by the adversarial limit in both the direction and the step length. Furthermore, an effective self-guiding prior is extracted solely from the threat model and the target image to efficiently leverage the prior information and guide the gradient estimation process. Extensive experiments demonstrate that LaS-GSA requires fewer queries to nullify the image translation process with higher success rates than 4 state-of-the-art black-box methods.



### ActiveMatch: End-to-end Semi-supervised Active Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.02521v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02521v2)
- **Published**: 2021-10-06 06:07:40+00:00
- **Updated**: 2022-08-05 04:54:16+00:00
- **Authors**: Xinkai Yuan, Zilinghan Li, Gaoang Wang
- **Comment**: 5 pages, 2 figures, accepted by ICIP 2022
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) is an efficient framework that can train models with both labeled and unlabeled data, but may generate ambiguous and non-distinguishable representations when lacking adequate labeled samples. With human-in-the-loop, active learning can iteratively select informative unlabeled samples for labeling and training to improve the performance in the SSL framework. However, most existing active learning approaches rely on pre-trained features, which is not suitable for end-to-end learning. To deal with the drawbacks of SSL, in this paper, we propose a novel end-to-end representation learning method, namely ActiveMatch, which combines SSL with contrastive learning and active learning to fully leverage the limited labels. Starting from a small amount of labeled data with unsupervised contrastive learning as a warm-up, ActiveMatch then combines SSL and supervised contrastive learning, and actively selects the most representative samples for labeling during the training, resulting in better representations towards the classification. Compared with MixMatch and FixMatch with the same amount of labeled data, we show that ActiveMatch achieves the state-of-the-art performance, with 89.24% accuracy on CIFAR-10 with 100 collected labels, and 92.20% accuracy with 200 collected labels.



### Coarse-to-Fine Reasoning for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2110.02526v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02526v2)
- **Published**: 2021-10-06 06:29:52+00:00
- **Updated**: 2022-04-19 08:54:22+00:00
- **Authors**: Binh X. Nguyen, Tuong Do, Huy Tran, Erman Tjiputra, Quang D. Tran, Anh Nguyen
- **Comment**: Accepted in CVPR 2022 Workshops
- **Journal**: None
- **Summary**: Bridging the semantic gap between image and question is an important step to improve the accuracy of the Visual Question Answering (VQA) task. However, most of the existing VQA methods focus on attention mechanisms or visual relations for reasoning the answer, while the features at different semantic levels are not fully utilized. In this paper, we present a new reasoning framework to fill the gap between visual features and semantic clues in the VQA task. Our method first extracts the features and predicates from the image and question. We then propose a new reasoning framework to effectively jointly learn these features and predicates in a coarse-to-fine manner. The intensively experimental results on three large-scale VQA datasets show that our proposed approach achieves superior accuracy comparing with other state-of-the-art methods. Furthermore, our reasoning framework also provides an explainable way to understand the decision of the deep neural network when predicting the answer.



### On the Importance of Firth Bias Reduction in Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.02529v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02529v2)
- **Published**: 2021-10-06 06:32:37+00:00
- **Updated**: 2022-04-14 21:54:00+00:00
- **Authors**: Saba Ghaffari, Ehsan Saleh, David Forsyth, Yu-xiong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Learning accurate classifiers for novel categories from very few examples, known as few-shot image classification, is a challenging task in statistical machine learning and computer vision. The performance in few-shot classification suffers from the bias in the estimation of classifier parameters; however, an effective underlying bias reduction technique that could alleviate this issue in training few-shot classifiers has been overlooked. In this work, we demonstrate the effectiveness of Firth bias reduction in few-shot classification. Theoretically, Firth bias reduction removes the $O(N^{-1})$ first order term from the small-sample bias of the Maximum Likelihood Estimator. Here we show that the general Firth bias reduction technique simplifies to encouraging uniform class assignment probabilities for multinomial logistic classification, and almost has the same effect in cosine classifiers. We derive an easy-to-implement optimization objective for Firth penalized multinomial logistic and cosine classifiers, which is equivalent to penalizing the cross-entropy loss with a KL-divergence between the uniform label distribution and the predictions. Then, we empirically evaluate that it is consistently effective across the board for few-shot image classification, regardless of (1) the feature representations from different backbones, (2) the number of samples per class, and (3) the number of classes. Finally, we show the robustness of Firth bias reduction, in the case of imbalanced data distribution. Our implementation is available at https://github.com/ehsansaleh/firth_bias_reduction



### 3D-FCT: Simultaneous 3D Object Detection and Tracking Using Feature Correlation
- **Arxiv ID**: http://arxiv.org/abs/2110.02531v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2110.02531v1)
- **Published**: 2021-10-06 06:36:29+00:00
- **Updated**: 2021-10-06 06:36:29+00:00
- **Authors**: Naman Sharma, Hocksoon Lim
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection using LiDAR data remains a key task for applications like autonomous driving and robotics. Unlike in the case of 2D images, LiDAR data is almost always collected over a period of time. However, most work in this area has focused on performing detection independent of the temporal domain. In this paper we present 3D-FCT, a Siamese network architecture that utilizes temporal information to simultaneously perform the related tasks of 3D object detection and tracking. The network is trained to predict the movement of an object based on the correlation features of extracted keypoints across time. Calculating correlation across keypoints only allows for real-time object detection. We further extend the multi-task objective to include a tracking regression loss. Finally, we produce high accuracy detections by linking short-term object tracklets into long term tracks based on the predicted tracks. Our proposed method is evaluated on the KITTI tracking dataset where it is shown to provide an improvement of 5.57% mAP over a state-of-the-art approach.



### See Yourself in Others: Attending Multiple Tasks for Own Failure Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.02549v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.02549v2)
- **Published**: 2021-10-06 07:42:57+00:00
- **Updated**: 2022-02-28 15:07:35+00:00
- **Authors**: Boyang Sun, Jiaxu Xing, Hermann Blum, Roland Siegwart, Cesar Cadena
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous robots deal with unexpected scenarios in real environments. Given input images, various visual perception tasks can be performed, e.g., semantic segmentation, depth estimation and normal estimation. These different tasks provide rich information for the whole robotic perception system. All tasks have their own characteristics while sharing some latent correlations. However, some of the task predictions may suffer from the unreliability dealing with complex scenes and anomalies. We propose an attention-based failure detection approach by exploiting the correlations among multiple tasks. The proposed framework infers task failures by evaluating the individual prediction, across multiple visual perception tasks for different regions in an image. The formulation of the evaluations is based on an attention network supervised by multi-task uncertainty estimation and their corresponding prediction errors. Our proposed framework generates more accurate estimations of the prediction error for the different task's predictions.



### A Review of Computer Vision Technologies for Fish Tracking
- **Arxiv ID**: http://arxiv.org/abs/2110.02551v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02551v3)
- **Published**: 2021-10-06 07:46:35+00:00
- **Updated**: 2022-01-21 09:57:31+00:00
- **Authors**: Zhenbo Li, Weiran Li, Fei Li, Meng Yuan
- **Comment**: We are working on the structure of this paper, optimizing the
  concepts and classification of detectors and trackers which are prone to
  ambiguity in Section 3
- **Journal**: None
- **Summary**: Fish tracking based on computer vision is a complex and challenging task in fishery production and ecological studies. Most of the applications of fish tracking use classic filtering algorithms, which lack in accuracy and efficiency. To solve this issue, deep learning methods utilized deep neural networks to extract the features, which achieve a good performance in the fish tracking. Some one-stage detection algorithms have gradually been adopted in this area for the real-time applications. The transfer learning to fish target is the current development direction. At present, fish tracking technology is not enough to cover actual application requirements. According to the literature data collected by us, there has not been any extensive review about vision-based fish tracking in the community. In this paper, we introduced the development and application prospects of fish tracking technology in last ten years. Firstly, we introduced the open source datasets of fish, and summarized the preprocessing technologies of underwater images. Secondly, we analyzed the detection and tracking algorithms for fish, and sorted out some transferable frontier tracking model. Thirdly, we listed the actual applications, metrics and bottlenecks of the fish tracking such as occlusion and multi-scale. Finally, we give the discussion for fish tracking datasets, solutions of the bottlenecks, and improvements. We expect that our work can help the fish tracking models to achieve higher accuracy and robustness.



### MTCD: Cataract Detection via Near Infrared Eye Images
- **Arxiv ID**: http://arxiv.org/abs/2110.02564v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02564v1)
- **Published**: 2021-10-06 08:10:28+00:00
- **Updated**: 2021-10-06 08:10:28+00:00
- **Authors**: Pavani Tripathi, Yasmeena Akhter, Mahapara Khurshid, Aditya Lakra, Rohit Keshari, Mayank Vatsa, Richa Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Globally, cataract is a common eye disease and one of the leading causes of blindness and vision impairment. The traditional process of detecting cataracts involves eye examination using a slit-lamp microscope or ophthalmoscope by an ophthalmologist, who checks for clouding of the normally clear lens of the eye. The lack of resources and unavailability of a sufficient number of experts pose a burden to the healthcare system throughout the world, and researchers are exploring the use of AI solutions for assisting the experts. Inspired by the progress in iris recognition, in this research, we present a novel algorithm for cataract detection using near-infrared eye images. The NIR cameras, which are popularly used in iris recognition, are of relatively low cost and easy to operate compared to ophthalmoscope setup for data capture. However, such NIR images have not been explored for cataract detection. We present deep learning-based eye segmentation and multitask network classification networks for cataract detection using NIR images as input. The proposed segmentation algorithm efficiently and effectively detects non-ideal eye boundaries and is cost-effective, and the classification network yields very high classification performance on the cataract dataset.



### A Few-shot Learning Graph Multi-Trajectory Evolution Network for Forecasting Multimodal Baby Connectivity Development from a Baseline Timepoint
- **Arxiv ID**: http://arxiv.org/abs/2110.03535v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03535v1)
- **Published**: 2021-10-06 08:26:57+00:00
- **Updated**: 2021-10-06 08:26:57+00:00
- **Authors**: Alaa Bessadok, Ahmed Nebli, Mohamed Ali Mahjoub, Gang Li, Weili Lin, Dinggang Shen, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: Charting the baby connectome evolution trajectory during the first year after birth plays a vital role in understanding dynamic connectivity development of baby brains. Such analysis requires acquisition of longitudinal connectomic datasets. However, both neonatal and postnatal scans are rarely acquired due to various difficulties. A small body of works has focused on predicting baby brain evolution trajectory from a neonatal brain connectome derived from a single modality. Although promising, large training datasets are essential to boost model learning and to generalize to a multi-trajectory prediction from different modalities (i.e., functional and morphological connectomes). Here, we unprecedentedly explore the question: Can we design a few-shot learning-based framework for predicting brain graph trajectories across different modalities? To this aim, we propose a Graph Multi-Trajectory Evolution Network (GmTE-Net), which adopts a teacher-student paradigm where the teacher network learns on pure neonatal brain graphs and the student network learns on simulated brain graphs given a set of different timepoints. To the best of our knowledge, this is the first teacher-student architecture tailored for brain graph multi-trajectory growth prediction that is based on few-shot learning and generalized to graph neural networks (GNNs). To boost the performance of the student network, we introduce a local topology-aware distillation loss that forces the predicted graph topology of the student network to be consistent with the teacher network. Experimental results demonstrate substantial performance gains over benchmark methods. Hence, our GmTE-Net can be leveraged to predict atypical brain connectivity trajectory evolution across various modalities. Our code is available at https: //github.com/basiralab/GmTE-Net.



### One Representative-Shot Learning Using a Population-Driven Template with Application to Brain Connectivity Classification and Evolution Prediction
- **Arxiv ID**: http://arxiv.org/abs/2110.11238v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11238v1)
- **Published**: 2021-10-06 08:36:00+00:00
- **Updated**: 2021-10-06 08:36:00+00:00
- **Authors**: Umut Guvercin, Mohammed Amine Gharsallaoui, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning presents a challenging paradigm for training discriminative models on a few training samples representing the target classes to discriminate. However, classification methods based on deep learning are ill-suited for such learning as they need large amounts of training data --let alone one-shot learning. Recently, graph neural networks (GNNs) have been introduced to the field of network neuroscience, where the brain connectivity is encoded in a graph. However, with scarce neuroimaging datasets particularly for rare diseases and low-resource clinical facilities, such data-devouring architectures might fail in learning the target task. In this paper, we take a very different approach in training GNNs, where we aim to learn with one sample and achieve the best performance --a formidable challenge to tackle. Specifically, we present the first one-shot paradigm where a GNN is trained on a single population-driven template --namely a connectional brain template (CBT). A CBT is a compact representation of a population of brain graphs capturing the unique connectivity patterns shared across individuals. It is analogous to brain image atlases for neuroimaging datasets. Using a one-representative CBT as a training sample, we alleviate the training load of GNN models while boosting their performance across a variety of classification and regression tasks. We demonstrate that our method significantly outperformed benchmark one-shot learning methods with downstream classification and time-dependent brain graph data forecasting tasks while competing with the train-on-all conventional training strategy. Our source code can be found at https://github.com/basiralab/one-representative-shot-learning.



### Efficient Multi-Modal Embeddings from Structured Data
- **Arxiv ID**: http://arxiv.org/abs/2110.02577v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.02577v1)
- **Published**: 2021-10-06 08:42:09+00:00
- **Updated**: 2021-10-06 08:42:09+00:00
- **Authors**: Anita L. Ver≈ë, Ann Copestake
- **Comment**: 5 pages, 5 pages of appendix, 7 figures
- **Journal**: None
- **Summary**: Multi-modal word semantics aims to enhance embeddings with perceptual input, assuming that human meaning representation is grounded in sensory experience. Most research focuses on evaluation involving direct visual input, however, visual grounding can contribute to linguistic applications as well. Another motivation for this paper is the growing need for more interpretable models and for evaluating model efficiency regarding size and performance. This work explores the impact of visual information for semantics when the evaluation involves no direct visual input, specifically semantic similarity and relatedness. We investigate a new embedding type in-between linguistic and visual modalities, based on the structured annotations of Visual Genome. We compare uni- and multi-modal models including structured, linguistic and image based representations. We measure the efficiency of each model with regard to data and model size, modality / data distribution and information gain. The analysis includes an interpretation of embedding structures. We found that this new embedding conveys complementary information for text based embeddings. It achieves comparable performance in an economic way, using orders of magnitude less resources than visual models.



### Decoupled Adaptation for Cross-Domain Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.02578v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02578v3)
- **Published**: 2021-10-06 08:43:59+00:00
- **Updated**: 2022-05-09 05:03:53+00:00
- **Authors**: Junguang Jiang, Baixu Chen, Jianmin Wang, Mingsheng Long
- **Comment**: None
- **Journal**: ICLR 2022
- **Summary**: Cross-domain object detection is more challenging than object classification since multiple objects exist in an image and the location of each object is unknown in the unlabeled target domain. As a result, when we adapt features of different objects to enhance the transferability of the detector, the features of the foreground and the background are easy to be confused, which may hurt the discriminability of the detector. Besides, previous methods focused on category adaptation but ignored another important part for object detection, i.e., the adaptation on bounding box regression. To this end, we propose D-adapt, namely Decoupled Adaptation, to decouple the adversarial adaptation and the training of the detector. Besides, we fill the blank of regression domain adaptation in object detection by introducing a bounding box adaptor. Experiments show that D-adapt achieves state-of-the-art results on four cross-domain object detection tasks and yields 17% and 21% relative improvement on benchmark datasets Clipart1k and Comic2k in particular.



### Test-time Batch Statistics Calibration for Covariate Shift
- **Arxiv ID**: http://arxiv.org/abs/2110.04065v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.04065v1)
- **Published**: 2021-10-06 08:45:03+00:00
- **Updated**: 2021-10-06 08:45:03+00:00
- **Authors**: Fuming You, Jingjing Li, Zhou Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have a clear degradation when applying to the unseen environment due to the covariate shift. Conventional approaches like domain adaptation requires the pre-collected target data for iterative training, which is impractical in real-world applications. In this paper, we propose to adapt the deep models to the novel environment during inference. An previous solution is test time normalization, which substitutes the source statistics in BN layers with the target batch statistics. However, we show that test time normalization may potentially deteriorate the discriminative structures due to the mismatch between target batch statistics and source parameters. To this end, we present a general formulation $\alpha$-BN to calibrate the batch statistics by mixing up the source and target statistics for both alleviating the domain shift and preserving the discriminative structures. Based on $\alpha$-BN, we further present a novel loss function to form a unified test time adaptation framework Core, which performs the pairwise class correlation online optimization. Extensive experiments show that our approaches achieve the state-of-the-art performance on total twelve datasets from three topics, including model robustness to corruptions, domain generalization on image classification and semantic segmentation. Particularly, our $\alpha$-BN improves 28.4\% to 43.9\% on GTA5 $\rightarrow$ Cityscapes without any training, even outperforms the latest source-free domain adaptation method.



### Deep Transfer Learning for Land Use and Land Cover Classification: A Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/2110.02580v3
- **DOI**: 10.3390/s21238083
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02580v3)
- **Published**: 2021-10-06 08:46:57+00:00
- **Updated**: 2021-11-25 05:00:07+00:00
- **Authors**: Raoof Naushad, Tarunpreet Kaur, Ebrahim Ghaderpour
- **Comment**: None
- **Journal**: Sensors 2021, 21(23), 8083
- **Summary**: Efficiently implementing remote sensing image classification with high spatial resolution imagery can provide a significant value in Land Use and Land Cover (LULC) classification. The new advances in remote sensing and deep learning technologies have facilitated the extraction of spatiotemporal information for LULC classification. Moreover, the diverse disciplines of science, including remote sensing, have utilised tremendous improvements in image classification by Convolutional Neural Networks (CNNs) with transfer learning. In this study, instead of training CNNs from scratch, the transfer learning is applied to fine-tune pre-trained networks Visual Geometry Group (VGG16) and Wide Residual Networks (WRNs), by replacing the final layer with additional layers, for LULC classification using the red-green-blue version of the EuroSAT dataset. Moreover, the performance and computational time are compared and optimised with techniques, such as early stopping, gradient clipping, adaptive learning rates, and data augmentation. The proposed approaches have addressed the limited-data problem, and very good accuracies are achieved. The results show that the proposed method based on the WRNs performs better than the previous best-stated results in terms of the computational efficiency and accuracy from 98.57% to 99.17%.



### FADNet++: Real-Time and Accurate Disparity Estimation with Configurable Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.02582v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02582v1)
- **Published**: 2021-10-06 08:50:33+00:00
- **Updated**: 2021-10-06 08:50:33+00:00
- **Authors**: Qiang Wang, Shaohuai Shi, Shizhen Zheng, Kaiyong Zhao, Xiaowen Chu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2003.10758
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have achieved great success in the area of computer vision. The disparity estimation problem tends to be addressed by DNNs which achieve much better prediction accuracy than traditional hand-crafted feature-based methods. However, the existing DNNs hardly serve both efficient computation and rich expression capability, which makes them difficult for deployment in real-time and high-quality applications, especially on mobile devices. To this end, we propose an efficient, accurate, and configurable deep network for disparity estimation named FADNet++. Leveraging several liberal network design and training techniques, FADNet++ can boost its accuracy with a fast model inference speed for real-time applications. Besides, it enables users to easily configure different sizes of models for balancing accuracy and inference efficiency. We conduct extensive experiments to demonstrate the effectiveness of FADNet++ on both synthetic and realistic datasets among six GPU devices varying from server to mobile platforms. Experimental results show that FADNet++ and its variants achieve state-of-the-art prediction accuracy, and run at a significant order of magnitude faster speed than existing 3D models. With the constraint of running at above 15 frames per second (FPS) on a mobile GPU, FADNet++ achieves a new state-of-the-art result for the SceneFlow dataset.



### Recurrent Brain Graph Mapper for Predicting Time-Dependent Brain Graph Evaluation Trajectory
- **Arxiv ID**: http://arxiv.org/abs/2110.11237v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11237v1)
- **Published**: 2021-10-06 09:25:55+00:00
- **Updated**: 2021-10-06 09:25:55+00:00
- **Authors**: Alpay Tekin, Ahmed Nebli, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: Several brain disorders can be detected by observing alterations in the brain's structural and functional connectivities. Neurological findings suggest that early diagnosis of brain disorders, such as mild cognitive impairment (MCI), can prevent and even reverse its development into Alzheimer's disease (AD). In this context, recent studies aimed to predict the evolution of brain connectivities over time by proposing machine learning models that work on brain images. However, such an approach is costly and time-consuming. Here, we propose to use brain connectivities as a more efficient alternative for time-dependent brain disorder diagnosis by regarding the brain as instead a large interconnected graph characterizing the interconnectivity scheme between several brain regions. We term our proposed method Recurrent Brain Graph Mapper (RBGM), a novel efficient edge-based recurrent graph neural network that predicts the time-dependent evaluation trajectory of a brain graph from a single baseline. Our RBGM contains a set of recurrent neural network-inspired mappers for each time point, where each mapper aims to project the ground-truth brain graph onto its next time point. We leverage the teacher forcing method to boost training and improve the evolved brain graph quality. To maintain the topological consistency between the predicted brain graphs and their corresponding ground-truth brain graphs at each time point, we further integrate a topological loss. We also use l1 loss to capture time-dependency and minimize the distance between the brain graph at consecutive time points for regularization. Benchmarks against several variants of RBGM and state-of-the-art methods prove that we can achieve the same accuracy in predicting brain graph evolution more efficiently, paving the way for novel graph neural network architecture and a highly efficient training scheme.



### Inter-Domain Alignment for Predicting High-Resolution Brain Networks Using Teacher-Student Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.03452v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2110.03452v1)
- **Published**: 2021-10-06 09:31:44+00:00
- **Updated**: 2021-10-06 09:31:44+00:00
- **Authors**: Basar Demir, Alaa Bessadok, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and automated super-resolution image synthesis is highly desired since it has the great potential to circumvent the need for acquiring high-cost medical scans and a time-consuming preprocessing pipeline of neuroimaging data. However, existing deep learning frameworks are solely designed to predict high-resolution (HR) image from a low-resolution (LR) one, which limits their generalization ability to brain graphs (i.e., connectomes). A small body of works has focused on superresolving brain graphs where the goal is to predict a HR graph from a single LR graph. Although promising, existing works mainly focus on superresolving graphs belonging to the same domain (e.g., functional), overlooking the domain fracture existing between multimodal brain data distributions (e.g., morphological and structural). To this aim, we propose a novel inter-domain adaptation framework namely, Learn to SuperResolve Brain Graphs with Knowledge Distillation Network (L2S-KDnet), which adopts a teacher-student paradigm to superresolve brain graphs. Our teacher network is a graph encoder-decoder that firstly learns the LR brain graph embeddings, and secondly learns how to align the resulting latent representations to the HR ground truth data distribution using an adversarial regularization. Ultimately, it decodes the HR graphs from the aligned embeddings. Next, our student network learns the knowledge of the aligned brain graphs as well as the topological structure of the predicted HR graphs transferred from the teacher. We further leverage the decoder of the teacher to optimize the student network. L2S-KDnet presents the first TS architecture tailored for brain graph super-resolution synthesis that is based on inter-domain alignment. Our experimental results demonstrate substantial performance gains over benchmark methods.



### Focus on the Common Good: Group Distributional Robustness Follows
- **Arxiv ID**: http://arxiv.org/abs/2110.02619v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.02619v2)
- **Published**: 2021-10-06 09:47:41+00:00
- **Updated**: 2022-04-20 08:22:33+00:00
- **Authors**: Vihari Piratla, Praneeth Netrapalli, Sunita Sarawagi
- **Comment**: Presented at ICLR 2022; Code can be found at:
  https://github.com/vihari/cgd
- **Journal**: None
- **Summary**: We consider the problem of training a classification model with group annotated training data. Recent work has established that, if there is distribution shift across different groups, models trained using the standard empirical risk minimization (ERM) objective suffer from poor performance on minority groups and that group distributionally robust optimization (Group-DRO) objective is a better alternative. The starting point of this paper is the observation that though Group-DRO performs better than ERM on minority groups for some benchmark datasets, there are several other datasets where it performs much worse than ERM. Inspired by ideas from the closely related problem of domain generalization, this paper proposes a new and simple algorithm that explicitly encourages learning of features that are shared across various groups. The key insight behind our proposed algorithm is that while Group-DRO focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by Group-DRO. Empirically, we show that our proposed algorithm matches or achieves better performance compared to strong contemporary baselines including ERM and Group-DRO on standard benchmarks on both minority groups and across all groups. Theoretically, we show that the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions.



### StairwayGraphNet for Inter- and Intra-modality Multi-resolution Brain Graph Alignment and Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.04279v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2110.04279v1)
- **Published**: 2021-10-06 09:49:38+00:00
- **Updated**: 2021-10-06 09:49:38+00:00
- **Authors**: Islem Mhiri, Mohamed Ali Mahjoub, Islem Rekik
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2107.06281
- **Journal**: None
- **Summary**: Synthesizing multimodality medical data provides complementary knowledge and helps doctors make precise clinical decisions. Although promising, existing multimodal brain graph synthesis frameworks have several limitations. First, they mainly tackle only one problem (intra- or inter-modality), limiting their generalizability to synthesizing inter- and intra-modality simultaneously. Second, while few techniques work on super-resolving low-resolution brain graphs within a single modality (i.e., intra), inter-modality graph super-resolution remains unexplored though this would avoid the need for costly data collection and processing. More importantly, both target and source domains might have different distributions, which causes a domain fracture between them. To fill these gaps, we propose a multi-resolution StairwayGraphNet (SG-Net) framework to jointly infer a target graph modality based on a given modality and super-resolve brain graphs in both inter and intra domains. Our SG-Net is grounded in three main contributions: (i) predicting a target graph from a source one based on a novel graph generative adversarial network in both inter (e.g., morphological-functional) and intra (e.g., functional-functional) domains, (ii) generating high-resolution brain graphs without resorting to the time consuming and expensive MRI processing steps, and (iii) enforcing the source distribution to match that of the ground truth graphs using an inter-modality aligner to relax the loss function to optimize. Moreover, we design a new Ground Truth-Preserving loss function to guide both generators in learning the topological structure of ground truth brain graphs more accurately. Our comprehensive experiments on predicting target brain graphs from source graphs using a multi-resolution stairway showed the outperformance of our method in comparison with its variants and state-of-the-art method.



### Is An Image Worth Five Sentences? A New Look into Semantics for Image-Text Matching
- **Arxiv ID**: http://arxiv.org/abs/2110.02623v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.02623v1)
- **Published**: 2021-10-06 09:54:28+00:00
- **Updated**: 2021-10-06 09:54:28+00:00
- **Authors**: Ali Furkan Biten, Andres Mafla, Lluis Gomez, Dimosthenis Karatzas
- **Comment**: Accepted WACV 2022
- **Journal**: None
- **Summary**: The task of image-text matching aims to map representations from different modalities into a common joint visual-textual embedding. However, the most widely used datasets for this task, MSCOCO and Flickr30K, are actually image captioning datasets that offer a very limited set of relationships between images and sentences in their ground-truth annotations. This limited ground truth information forces us to use evaluation metrics based on binary relevance: given a sentence query we consider only one image as relevant. However, many other relevant images or captions may be present in the dataset. In this work, we propose two metrics that evaluate the degree of semantic relevance of retrieved items, independently of their annotated binary relevance. Additionally, we incorporate a novel strategy that uses an image captioning metric, CIDEr, to define a Semantic Adaptive Margin (SAM) to be optimized in a standard triplet loss. By incorporating our formulation to existing models, a \emph{large} improvement is obtained in scenarios where available training data is limited. We also demonstrate that the performance on the annotated image-caption pairs is maintained while improving on other non-annotated relevant items when employing the full training set. Code with our metrics and adaptive margin formulation will be made public.



### CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation
- **Arxiv ID**: http://arxiv.org/abs/2110.02624v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T07, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2110.02624v2)
- **Published**: 2021-10-06 09:55:19+00:00
- **Updated**: 2022-04-28 18:50:35+00:00
- **Authors**: Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, Kamal Rahimi Malekshan
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Generating shapes using natural language can enable new ways of imagining and creating the things around us. While significant recent progress has been made in text-to-image generation, text-to-shape generation remains a challenging problem due to the unavailability of paired text and shape data at a large scale. We present a simple yet effective method for zero-shot text-to-shape generation that circumvents such data scarcity. Our proposed method, named CLIP-Forge, is based on a two-stage training process, which only depends on an unlabelled shape dataset and a pre-trained image-text network such as CLIP. Our method has the benefits of avoiding expensive inference time optimization, as well as the ability to generate multiple shapes for a given text. We not only demonstrate promising zero-shot generalization of the CLIP-Forge model qualitatively and quantitatively, but also provide extensive comparative evaluations to better understand its behavior.



### Recurrent Multigraph Integrator Network for Predicting the Evolution of Population-Driven Brain Connectivity Templates
- **Arxiv ID**: http://arxiv.org/abs/2110.03453v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2110.03453v1)
- **Published**: 2021-10-06 10:00:05+00:00
- **Updated**: 2021-10-06 10:00:05+00:00
- **Authors**: Oytun Demirbilek, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: Learning how to estimate a connectional brain template(CBT) from a population of brain multigraphs, where each graph (e.g., functional) quantifies a particular relationship between pairs of brain regions of interest (ROIs), allows to pin down the unique connectivity patterns shared across individuals. Specifically, a CBT is viewed as an integral representation of a set of highly heterogeneous graphs and ideally meeting the centeredness (i.e., minimum distance to all graphs in the population) and discriminativeness (i.e., distinguishes the healthy from the disordered population) criteria. So far, existing works have been limited to only integrating and fusing a population of brain multigraphs acquired at a single timepoint. In this paper, we unprecedentedly tackle the question: Given a baseline multigraph population, can we learn how to integrate and forecast its CBT representations at follow-up timepoints? Addressing such question is of paramount in predicting common alternations across healthy and disordered populations. To fill this gap, we propose Recurrent Multigraph Integrator Network (ReMI-Net), the first graph recurrent neural network which infers the baseline CBT of an input population t1 and predicts its longitudinal evolution over time (ti > t1). Our ReMI-Net is composed of recurrent neural blocks with graph convolutional layers using a cross-node message passing to first learn hidden-states embeddings of each CBT node (i.e., brain region of interest) and then predict its evolution at the consecutive timepoint. Moreover, we design a novel time-dependent loss to regularize the CBT evolution trajectory over time and further introduce a cyclic recursion and learnable normalization layer to generate well-centered CBTs from time-dependent hidden-state embeddings. Finally, we derive the CBT adjacency matrix from the learned hidden state graph representation.



### MovingFashion: a Benchmark for the Video-to-Shop Challenge
- **Arxiv ID**: http://arxiv.org/abs/2110.02627v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02627v4)
- **Published**: 2021-10-06 10:00:59+00:00
- **Updated**: 2021-10-14 11:45:30+00:00
- **Authors**: Marco Godi, Christian Joppi, Geri Skenderi, Marco Cristani
- **Comment**: Accepted at WACV 2022
- **Journal**: None
- **Summary**: Retrieving clothes which are worn in social media videos (Instagram, TikTok) is the latest frontier of e-fashion, referred to as "video-to-shop" in the computer vision literature. In this paper we present MovingFashion, the first publicly available dataset to cope with this challenge. MovingFashion is composed of 14855 social videos, each one of them associated to e-commerce "shop" images where the corresponding clothing items are clearly portrayed. In addition, we present a network for retrieving the shop images in this scenario, dubbed SEAM Match-RCNN. The model is trained by image-to-video domain adaptation, allowing to use video sequences where only their association with a shop image is given, eliminating the need of millions of annotated bounding boxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted sum of few frames (10) of a social video is enough to individuate the correct product within the first 5 retrieved items in a 14K+ shop element gallery with an accuracy of 80%. This provides the best performance on MovingFashion, comparing exhaustively against the related state-of-the-art approaches and alternative baselines.



### Learning Sparse Masks for Diffusion-based Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2110.02636v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02636v4)
- **Published**: 2021-10-06 10:20:59+00:00
- **Updated**: 2022-05-16 06:28:03+00:00
- **Authors**: Tobias Alt, Pascal Peter, Joachim Weickert
- **Comment**: To appear in A. J. Pinho, P. Georgieva, L. F. Teixeira, J. A.
  S\'anchez (Eds.): Pattern Recognition and Image Analysis. Lecture Notes in
  Computer Science, Springer, Cham, 2022
- **Journal**: None
- **Summary**: Diffusion-based inpainting is a powerful tool for the reconstruction of images from sparse data. Its quality strongly depends on the choice of known data. Optimising their spatial location -- the inpainting mask -- is challenging. A commonly used tool for this task are stochastic optimisation strategies. However, they are slow as they compute multiple inpainting results. We provide a remedy in terms of a learned mask generation model. By emulating the complete inpainting pipeline with two networks for mask generation and neural surrogate inpainting, we obtain a model for highly efficient adaptive mask generation. Experiments indicate that our model can achieve competitive quality with an acceleration by as much as four orders of magnitude. Our findings serve as a basis for making diffusion-based inpainting more attractive for applications such as image compression, where fast encoding is highly desirable.



### 2nd Place Solution to Google Landmark Recognition Competition 2021
- **Arxiv ID**: http://arxiv.org/abs/2110.02638v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02638v2)
- **Published**: 2021-10-06 10:28:38+00:00
- **Updated**: 2021-10-07 01:37:14+00:00
- **Authors**: Shubin Dai
- **Comment**: None
- **Journal**: None
- **Summary**: As Transformer-based architectures have recently shown encouraging progresses in computer vision. In this work, we present the solution to the Google Landmark Recognition 2021 Challenge held on Kaggle, which is an improvement on our last year's solution by changing three designs, including (1) Using Swin and CSWin as backbone for feature extraction, (2) Train on full GLDv2, and (3) Using full GLDv2 images as index image set for kNN search.   With these modifications, our solution significantly improves last year solution on this year competition. Our full pipeline, after ensembling Swin, CSWin, EfficientNet B7 models, scores 0.4907 on the private leaderboard which help us to get the 2nd place in the competition.



### A Weighted Generalized Coherence Approach for Sensing Matrix Design
- **Arxiv ID**: http://arxiv.org/abs/2110.02645v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, eess.IV, eess.SP, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2110.02645v1)
- **Published**: 2021-10-06 10:44:21+00:00
- **Updated**: 2021-10-06 10:44:21+00:00
- **Authors**: Ameya Anjarlekar, Ajit Rajwade
- **Comment**: 8 pages, 16 figures
- **Journal**: None
- **Summary**: As compared to using randomly generated sensing matrices, optimizing the sensing matrix w.r.t. a carefully designed criterion is known to lead to better quality signal recovery given a set of compressive measurements. In this paper, we propose generalizations of the well-known mutual coherence criterion for optimizing sensing matrices starting from random initial conditions. We term these generalizations as bi-coherence or tri-coherence and they are based on a criterion that discourages any one column of the sensing matrix from being close to a sparse linear combination of other columns. We also incorporate training data to further improve the sensing matrices through weighted coherence, weighted bi-coherence, or weighted tri-coherence criteria, which assign weights to sensing matrix columns as per their importance. An algorithm is also presented to solve the optimization problems. Finally, the effectiveness of the proposed algorithm is demonstrated through empirical results.



### Weak Novel Categories without Tears: A Survey on Weak-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.02651v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02651v3)
- **Published**: 2021-10-06 11:04:36+00:00
- **Updated**: 2022-10-02 11:14:32+00:00
- **Authors**: Li Niu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning is a data-hungry approach, which requires massive training data. However, it is time-consuming and labor-intensive to collect abundant fully-annotated training data for all categories. Assuming the existence of base categories with adequate fully-annotated training samples, different paradigms requiring fewer training samples or weaker annotations for novel categories have attracted growing research interest. Among them, zero-shot (resp., few-shot) learning explores using zero (resp., a few) training samples for novel categories, which lowers the quantity requirement for novel categories. Instead, weak-shot learning lowers the quality requirement for novel categories. Specifically, sufficient training samples are collected for novel categories but they only have weak annotations. In different tasks, weak annotations are presented in different forms (e.g., noisy labels for image classification, image labels for object detection, bounding boxes for segmentation), similar to the definitions in weakly supervised learning. Therefore, weak-shot learning can also be treated as weakly supervised learning with auxiliary fully supervised categories. In this paper, we discuss the existing weak-shot learning methodologies in different tasks and summarize the codes at https://github.com/bcmi/Awesome-Weak-Shot-Learning.



### Towards Robotic Knee Arthroscopy: Multi-Scale Network for Tissue-Tool Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.02657v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.02657v1)
- **Published**: 2021-10-06 11:20:01+00:00
- **Updated**: 2021-10-06 11:20:01+00:00
- **Authors**: Shahnewaz Ali, Prof. Ross Crawford, Dr. Frederic Maire, Assoc. Prof. Ajay K. Pandey
- **Comment**: None
- **Journal**: None
- **Summary**: Tissue awareness has a great demand to improve surgical accuracy in minimally invasive procedures. In arthroscopy, it is one of the challenging tasks due to surgical sites exhibit limited features and textures. Moreover, arthroscopic surgical video shows high intra-class variations. Arthroscopic videos are recorded with endoscope known as arthroscope which records tissue structures at proximity, therefore, frames contain minimal joint structure. As consequences, fully conventional network-based segmentation model suffers from long- and short- term dependency problems. In this study, we present a densely connected shape aware multi-scale segmentation model which captures multi-scale features and integrates shape features to achieve tissue-tool segmentations. The model has been evaluated with three distinct datasets. Moreover, with the publicly available polyp dataset our proposed model achieved 5.09 % accuracy improvement.



### A New Weakly Supervised Learning Approach for Real-time Iron Ore Feed Load Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.04063v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2110.04063v1)
- **Published**: 2021-10-06 11:24:47+00:00
- **Updated**: 2021-10-06 11:24:47+00:00
- **Authors**: Li Guo, Yonghong Peng, Rui Qin, Bingyu Liu
- **Comment**: 11 pages, 15 figures This paper has been submitted to the Journal of
  Minerals Engineering (https://www.journals.elsevier.com/minerals-engineering)
- **Journal**: None
- **Summary**: Iron ore feed load control is one of the most critical settings in a mineral grinding process, directly impacting the quality of final products. The setting of the feed load is mainly determined by the characteristics of the ore pellets. However, the characterisation of ore is challenging to acquire in many production environments, leading to poor feed load settings and inefficient production processes. This paper presents our work using deep learning models for direct ore feed load estimation from ore pellet images. To address the challenges caused by the large size of a full ore pellets image and the shortage of accurately annotated data, we treat the whole modelling process as a weakly supervised learning problem. A two-stage model training algorithm and two neural network architectures are proposed. The experiment results show competitive model performance, and the trained models can be used for real-time feed load estimation for grind process optimisation.



### S-Extension Patch: A simple and efficient way to extend an object detection model
- **Arxiv ID**: http://arxiv.org/abs/2110.02670v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02670v2)
- **Published**: 2021-10-06 11:44:19+00:00
- **Updated**: 2022-01-20 12:21:15+00:00
- **Authors**: Dishant Parikh
- **Comment**: Accepted and presented at ICDSMLA 2021. Proceedings to be published
  with Springer
- **Journal**: None
- **Summary**: While building convolutional network-based systems, the toll it takes to train the network is something that cannot be ignored. In cases where we need to append additional capabilities to the existing model, the attention immediately goes towards retraining techniques. In this paper, I show how to leverage knowledge about the dataset to append the class faster while maintaining the speed of inference as well as the accuracies; while reducing the amount of time and data required. The method can extend a class in the existing object detection model in 1/10th of the time compared to the other existing methods. S-Extension patch not only offers faster training but also speed and ease of adaptation, as it can be appended to any existing system, given it fulfills the similarity threshold condition.



### Long-tailed Distribution Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2110.02686v1
- **DOI**: 10.1145/3474085.3475479
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02686v1)
- **Published**: 2021-10-06 12:15:22+00:00
- **Updated**: 2021-10-06 12:15:22+00:00
- **Authors**: Zhiliang Peng, Wei Huang, Zonghao Guo, Xiaosong Zhang, Jianbin Jiao, Qixiang Ye
- **Comment**: Accepted in acm mm2021
- **Journal**: None
- **Summary**: Recognizing images with long-tailed distributions remains a challenging problem while there lacks an interpretable mechanism to solve this problem. In this study, we formulate Long-tailed recognition as Domain Adaption (LDA), by modeling the long-tailed distribution as an unbalanced domain and the general distribution as a balanced domain. Within the balanced domain, we propose to slack the generalization error bound, which is defined upon the empirical risks of unbalanced and balanced domains and the divergence between them. We propose to jointly optimize empirical risks of the unbalanced and balanced domains and approximate their domain divergence by intra-class and inter-class distances, with the aim to adapt models trained on the long-tailed distribution to general distributions in an interpretable way. Experiments on benchmark datasets for image recognition, object detection, and instance segmentation validate that our LDA approach, beyond its interpretability, achieves state-of-the-art performance. Code is available at https://github.com/pengzhiliang/LDA.



### Objects in Semantic Topology
- **Arxiv ID**: http://arxiv.org/abs/2110.02687v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.02687v2)
- **Published**: 2021-10-06 12:15:30+00:00
- **Updated**: 2022-02-16 07:07:23+00:00
- **Authors**: Shuo Yang, Peize Sun, Yi Jiang, Xiaobo Xia, Ruiheng Zhang, Zehuan Yuan, Changhu Wang, Ping Luo, Min Xu
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: A more realistic object detection paradigm, Open-World Object Detection, has arisen increasing research interests in the community recently. A qualified open-world object detector can not only identify objects of known categories, but also discover unknown objects, and incrementally learn to categorize them when their annotations progressively arrive. Previous works rely on independent modules to recognize unknown categories and perform incremental learning, respectively. In this paper, we provide a unified perspective: Semantic Topology. During the life-long learning of an open-world object detector, all object instances from the same category are assigned to their corresponding pre-defined node in the semantic topology, including the `unknown' category. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that semantic topology, either randomly-generated or derived from a well-trained language model, could outperform the current state-of-the-art open-world object detectors by a large margin, e.g., the absolute open-set error is reduced from 7832 to 2546, exhibiting the inherent superiority of semantic topology on open-world object detection.



### Reversible Attack based on Local Visual Adversarial Perturbation
- **Arxiv ID**: http://arxiv.org/abs/2110.02700v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02700v3)
- **Published**: 2021-10-06 12:37:58+00:00
- **Updated**: 2023-01-02 09:25:17+00:00
- **Authors**: Li Chen, Shaowei Zhu, Zhaoxia Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Adding perturbations to images can mislead classification models to produce incorrect results. Recently, researchers exploited adversarial perturbations to protect image privacy from retrieval by intelligent models. However, adding adversarial perturbations to images destroys the original data, making images useless in digital forensics and other fields. To prevent illegal or unauthorized access to sensitive image data such as human faces without impeding legitimate users, the use of reversible adversarial attack techniques is increasing. The original image can be recovered from its reversible adversarial examples. However, existing reversible adversarial attack methods are designed for traditional imperceptible adversarial perturbations and ignore the local visible adversarial perturbation. In this paper, we propose a new method for generating reversible adversarial examples based on local visible adversarial perturbation. The information needed for image recovery is embedded into the area beyond the adversarial patch by the reversible data hiding technique. To reduce image distortion, lossless compression and the B-R-G (bluered-green) embedding principle are adopted. Experiments on CIFAR-10 and ImageNet datasets show that the proposed method can restore the original images error-free while ensuring good attack performance.



### A Neural Anthropometer Learning from Body Dimensions Computed on Human 3D Meshes
- **Arxiv ID**: http://arxiv.org/abs/2110.04064v1
- **DOI**: 10.1109/SSCI50451.2021.9660069
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.04064v1)
- **Published**: 2021-10-06 12:56:05+00:00
- **Updated**: 2021-10-06 12:56:05+00:00
- **Authors**: Yansel Gonz√°lez Tejeda, Helmut A. Mayer
- **Comment**: Accepted to the IEEE Symposium Series on Computational Intelligence
  (IEEE SSCI 2021)
- **Journal**: 2021 IEEE Symposium Series on Computational Intelligence (SSCI)
- **Summary**: Human shape estimation has become increasingly important both theoretically and practically, for instance, in 3D mesh estimation, distance garment production and computational forensics, to mention just a few examples. As a further specialization, \emph{Human Body Dimensions Estimation} (HBDE) focuses on estimating human body measurements like shoulder width or chest circumference from images or 3D meshes usually using supervised learning approaches. The main obstacle in this context is the data scarcity problem, as collecting this ground truth requires expensive and difficult procedures. This obstacle can be overcome by obtaining realistic human measurements from 3D human meshes. However, a) there are no well established methods to calculate HBDs from 3D meshes and b) there are no benchmarks to fairly compare results on the HBDE task. Our contribution is twofold. On the one hand, we present a method to calculate right and left arm length, shoulder width, and inseam (crotch height) from 3D meshes with focus on potential medical, virtual try-on and distance tailoring applications. On the other hand, we use four additional body dimensions calculated using recently published methods to assemble a set of eight body dimensions which we use as a supervision signal to our Neural Anthropometer: a convolutional neural network capable of estimating these dimensions. To assess the estimation, we train the Neural Anthropometer with synthetic images of 3D meshes, from which we calculated the HBDs and observed that the network's overall mean estimate error is $20.89$ mm (relative error of 2.84\%). The results we present are fully reproducible and establish a fair baseline for research on the task of HBDE, therefore enabling the community with a valuable method.



### DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2110.02711v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02711v6)
- **Published**: 2021-10-06 12:59:39+00:00
- **Updated**: 2022-08-11 13:36:19+00:00
- **Authors**: Gwanghyun Kim, Taesung Kwon, Jong Chul Ye
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Recently, GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts. However, their applications to diverse real images are still difficult due to the limited GAN inversion capability. Specifically, these approaches often have difficulties in reconstructing images with novel poses, views, and highly variable contents compared to the training data, altering object identity, or producing unwanted image artifacts. To mitigate these problems and enable faithful manipulation of real images, we propose a novel method, dubbed DiffusionCLIP, that performs text-driven image manipulation using diffusion models. Based on full inversion capability and high-quality image generation power of recent diffusion models, our method performs zero-shot image manipulation successfully even between unseen domains and takes another step towards general application by manipulating images from a widely varying ImageNet dataset. Furthermore, we propose a novel noise combination method that allows straightforward multi-attribute manipulation. Extensive experiments and human evaluation confirmed robust and superior manipulation performance of our methods compared to the existing baselines. Code is available at https://github.com/gwang-kim/DiffusionCLIP.git.



### ParaDiS: Parallelly Distributable Slimmable Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.02724v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.02724v2)
- **Published**: 2021-10-06 13:17:08+00:00
- **Updated**: 2021-11-29 09:50:25+00:00
- **Authors**: Alexey Ozerov, Anne Lambert, Suresh Kirthi Kumaraswamy
- **Comment**: None
- **Journal**: None
- **Summary**: When several limited power devices are available, one of the most efficient ways to make profit of these resources, while reducing the processing latency and communication load, is to run in parallel several neural sub-networks and to fuse the result at the end of processing. However, such a combination of sub-networks must be trained specifically for each particular configuration of devices (characterized by number of devices and their capacities) which may vary over different model deployments and even within the same deployment. In this work we introduce parallelly distributable slimmable (ParaDiS) neural networks that are splittable in parallel among various device configurations without retraining. While inspired by slimmable networks allowing instant adaptation to resources on just one device, ParaDiS networks consist of several multi-device distributable configurations or switches that strongly share the parameters between them. We evaluate ParaDiS framework on MobileNet v1 and ResNet-50 architectures on ImageNet classification task and WDSR architecture for image super-resolution task. We show that ParaDiS switches achieve similar or better accuracy than the individual models, i.e., distributed models of the same structure trained individually. Moreover, we show that, as compared to universally slimmable networks that are not distributable, the accuracy of distributable ParaDiS switches either does not drop at all or drops by a maximum of 1 % only in the worst cases. Finally, once distributed over several devices, ParaDiS outperforms greatly slimmable models.



### Contrastive Learning for Unsupervised Radar Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.02744v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.02744v1)
- **Published**: 2021-10-06 13:34:09+00:00
- **Updated**: 2021-10-06 13:34:09+00:00
- **Authors**: Matthew Gadd, Daniele De Martini, Paul Newman
- **Comment**: accepted for publication at the IEEE International Conference on
  Advanced Robotics (ICAR) 2021. arXiv admin note: substantial text overlap
  with arXiv:2106.06703
- **Journal**: None
- **Summary**: We learn, in an unsupervised way, an embedding from sequences of radar images that is suitable for solving the place recognition problem with complex radar data. Our method is based on invariant instance feature learning but is tailored for the task of re-localisation by exploiting for data augmentation the temporal successivity of data as collected by a mobile platform moving through the scene smoothly. We experiment across two prominent urban radar datasets totalling over 400 km of driving and show that we achieve a new radar place recognition state-of-the-art. Specifically, the proposed system proves correct for 98.38% of the queries that it is presented with over a challenging re-localisation sequence, using only the single nearest neighbour in the learned metric space. We also find that our learned model shows better understanding of out-of-lane loop closures at arbitrary orientation than non-learned radar scan descriptors.



### SIRe-Networks: Convolutional Neural Networks Architectural Extension for Information Preservation via Skip/Residual Connections and Interlaced Auto-Encoders
- **Arxiv ID**: http://arxiv.org/abs/2110.02776v2
- **DOI**: 10.1016/j.neunet.2022.06.030
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.02776v2)
- **Published**: 2021-10-06 13:54:49+00:00
- **Updated**: 2022-10-26 11:20:50+00:00
- **Authors**: Danilo Avola, Luigi Cinque, Alessio Fagioli, Gian Luca Foresti
- **Comment**: None
- **Journal**: Neural Networks 153 (2022): 386-398
- **Summary**: Improving existing neural network architectures can involve several design choices such as manipulating the loss functions, employing a diverse learning strategy, exploiting gradient evolution at training time, optimizing the network hyper-parameters, or increasing the architecture depth. The latter approach is a straightforward solution, since it directly enhances the representation capabilities of a network; however, the increased depth generally incurs in the well-known vanishing gradient problem. In this paper, borrowing from different methods addressing this issue, we introduce an interlaced multi-task learning strategy, defined SIRe, to reduce the vanishing gradient in relation to the object classification task. The presented methodology directly improves a convolutional neural network (CNN) by preserving information from the input image through interlaced auto-encoders (AEs), and further refines the base network architecture by means of skip and residual connections. To validate the presented methodology, a simple CNN and various implementations of famous networks are extended via the SIRe strategy and extensively tested on five collections, i.e., MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and Caltech-256; where the SIRe-extended architectures achieve significantly increased performances across all models and datasets, thus confirming the presented approach effectiveness.



### Study on Transfer Learning Capabilities for Pneumonia Classification in Chest-X-Rays Image
- **Arxiv ID**: http://arxiv.org/abs/2110.02780v2
- **DOI**: 10.1016/j.cmpb.2022.106833
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.02780v2)
- **Published**: 2021-10-06 14:00:18+00:00
- **Updated**: 2022-04-23 06:58:52+00:00
- **Authors**: Danilo Avola, Andrea Bacciu, Luigi Cinque, Alessio Fagioli, Marco Raoul Marini, Riccardo Taiello
- **Comment**: None
- **Journal**: Computer Methods and Programs in Biomedicine, 2022
- **Summary**: Over the last year, the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) and its variants have highlighted the importance of screening tools with high diagnostic accuracy for new illnesses such as COVID-19. To that regard, deep learning approaches have proven as effective solutions for pneumonia classification, especially when considering chest-x-rays images. However, this lung infection can also be caused by other viral, bacterial or fungi pathogens. Consequently, efforts are being poured toward distinguishing the infection source to help clinicians to diagnose the correct disease origin. Following this tendency, this study further explores the effectiveness of established neural network architectures on the pneumonia classification task through the transfer learning paradigm. To present a comprehensive comparison, 12 well-known ImageNet pre-trained models were fine-tuned and used to discriminate among chest-x-rays of healthy people, and those showing pneumonia symptoms derived from either a viral (i.e., generic or SARS-CoV-2) or bacterial source. Furthermore, since a common public collection distinguishing between such categories is currently not available, two distinct datasets of chest-x-rays images, describing the aforementioned sources, were combined and employed to evaluate the various architectures. The experiments were performed using a total of 6330 images split between train, validation and test sets. For all models, common classification metrics were computed (e.g., precision, f1-score) and most architectures obtained significant performances, reaching, among the others, up to 84.46% average f1-score when discriminating the 4 identified classes. Moreover, confusion matrices and activation maps computed via the Grad-CAM algorithm were also reported to present an informed discussion on the networks classifications.



### 3rd Place Solution to Google Landmark Recognition Competition 2021
- **Arxiv ID**: http://arxiv.org/abs/2110.02794v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02794v2)
- **Published**: 2021-10-06 14:17:54+00:00
- **Updated**: 2021-10-07 14:43:15+00:00
- **Authors**: Cheng Xu, Weimin Wang, Shuai Liu, Yong Wang, Yuxiang Tang, Tianling Bian, Yanyu Yan, Qi She, Cheng Yang
- **Comment**: Corrected typos
- **Journal**: None
- **Summary**: In this paper, we show our solution to the Google Landmark Recognition 2021 Competition. Firstly, embeddings of images are extracted via various architectures (i.e. CNN-, Transformer- and hybrid-based), which are optimized by ArcFace loss. Then we apply an efficient pipeline to re-rank predictions by adjusting the retrieval score with classification logits and non-landmark distractors. Finally, the ensembled model scores 0.489 on the private leaderboard, achieving the 3rd place in the 2021 edition of the Google Landmark Recognition Competition.



### Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs
- **Arxiv ID**: http://arxiv.org/abs/2110.02797v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02797v2)
- **Published**: 2021-10-06 14:18:47+00:00
- **Updated**: 2021-10-11 14:28:50+00:00
- **Authors**: Philipp Benz, Soomin Ham, Chaoning Zhang, Adil Karjauv, In So Kweon
- **Comment**: Code:
  https://github.com/phibenz/robustness_comparison_vit_mlp-mixer_cnn
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have become the de facto gold standard in computer vision applications in the past years. Recently, however, new model architectures have been proposed challenging the status quo. The Vision Transformer (ViT) relies solely on attention modules, while the MLP-Mixer architecture substitutes the self-attention modules with Multi-Layer Perceptrons (MLPs). Despite their great success, CNNs have been widely known to be vulnerable to adversarial attacks, causing serious concerns for security-sensitive applications. Thus, it is critical for the community to know whether the newly proposed ViT and MLP-Mixer are also vulnerable to adversarial attacks. To this end, we empirically evaluate their adversarial robustness under several adversarial attack setups and benchmark them against the widely used CNNs. Overall, we find that the two architectures, especially ViT, are more robust than their CNN models. Using a toy example, we also provide empirical evidence that the lower adversarial robustness of CNNs can be partially attributed to their shift-invariant property. Our frequency analysis suggests that the most robust ViT architectures tend to rely more on low-frequency features compared with CNNs. Additionally, we have an intriguing finding that MLP-Mixer is extremely vulnerable to universal adversarial perturbations.



### Accelerated First Order Methods for Variational Imaging
- **Arxiv ID**: http://arxiv.org/abs/2110.02813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02813v1)
- **Published**: 2021-10-06 14:41:03+00:00
- **Updated**: 2021-10-06 14:41:03+00:00
- **Authors**: Joseph Bartlett, Jinming Duan
- **Comment**: None
- **Journal**: None
- **Summary**: In this thesis, we offer a thorough investigation of different regularisation terms used in variational imaging problems, together with detailed optimisation processes of these problems. We begin by studying smooth problems and partially non-smooth problems in the form of Tikhonov denoising and Total Variation (TV) denoising, respectively.   For Tikhonov denoising, we study an accelerated gradient method with adaptive restart, which shows a very rapid convergence rate. However, it is not straightforward to apply this fast algorithm to TV denoising, due to the non-smoothness of its built-in regularisation. To tackle this issue, we propose to utilise duality to convert such a non-smooth problem into a smooth one so that the accelerated gradient method with restart applies naturally.   However, we notice that both Tikhonov and TV regularisations have drawbacks, in the form of blurred image edges and staircase artefacts, respectively. To overcome these drawbacks, we propose a novel adaption to Total Generalised Variation (TGV) regularisation called Total Smooth Variation (TSV), which retains edges and meanwhile does not produce results which contain staircase artefacts. To optimise TSV effectively, we then propose the Accelerated Proximal Gradient Algorithm (APGA) which also utilises adaptive restart techniques. Compared to existing state-of-the-art regularisations (e.g. TV), TSV is shown to obtain more effective results on denoising problems as well as advanced imaging applications such as magnetic resonance imaging (MRI) reconstruction and optical flow. TSV removes the staircase artefacts observed when using TV regularisation, but has the added advantage over TGV that it can be efficiently optimised using gradient based methods with Nesterov acceleration and adaptive restart. Code is available at https://github.com/Jbartlett6/Accelerated-First-Order-Method-for-Variational-Imaging.



### QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.03861v3
- **DOI**: None
- **Categories**: **quant-ph**, cs.AI, cs.CL, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2110.03861v3)
- **Published**: 2021-10-06 14:44:51+00:00
- **Updated**: 2021-11-22 23:25:39+00:00
- **Authors**: Jun Qi, Chao-Han Huck Yang, Pin-Yu Chen
- **Comment**: Preprint. A Non-archival and preliminary venue was presented in
  NeurIPS 2021, Quantum Tensor Networks in Machine Learning Workshop
- **Journal**: Quantum Tensor Networks in Machine Learning Workshop, NeurIPS 2021
- **Summary**: The advent of noisy intermediate-scale quantum (NISQ) computers raises a crucial challenge to design quantum neural networks for fully quantum learning tasks. To bridge the gap, this work proposes an end-to-end learning framework named QTN-VQC, by introducing a trainable quantum tensor network (QTN) for quantum embedding on a variational quantum circuit (VQC). The architecture of QTN is composed of a parametric tensor-train network for feature extraction and a tensor product encoding for quantum embedding. We highlight the QTN for quantum embedding in terms of two perspectives: (1) we theoretically characterize QTN by analyzing its representation power of input features; (2) QTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the generation of quantum embedding to the output measurement. Our experiments on the MNIST dataset demonstrate the advantages of QTN for quantum embedding over other quantum embedding approaches.



### Semantic Prediction: Which One Should Come First, Recognition or Prediction?
- **Arxiv ID**: http://arxiv.org/abs/2110.02829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02829v1)
- **Published**: 2021-10-06 15:01:05+00:00
- **Updated**: 2021-10-06 15:01:05+00:00
- **Authors**: Hafez Farazi, Jan Nogga, and Sven Behnke
- **Comment**: None
- **Journal**: None
- **Summary**: The ultimate goal of video prediction is not forecasting future pixel-values given some previous frames. Rather, the end goal of video prediction is to discover valuable internal representations from the vast amount of available unlabeled video data in a self-supervised fashion for downstream tasks. One of the primary downstream tasks is interpreting the scene's semantic composition and using it for decision-making. For example, by predicting human movements, an observer can anticipate human activities and collaborate in a shared workspace. There are two main ways to achieve the same outcome, given a pre-trained video prediction and pre-trained semantic extraction model; one can first apply predictions and then extract semantics or first extract semantics and then predict. We investigate these configurations using the Local Frequency Domain Transformer Network (LFDTN) as the video prediction model and U-Net as the semantic extraction model on synthetic and real datasets.



### Shallow Features Guide Unsupervised Domain Adaptation for Semantic Segmentation at Class Boundaries
- **Arxiv ID**: http://arxiv.org/abs/2110.02833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02833v1)
- **Published**: 2021-10-06 15:05:48+00:00
- **Updated**: 2021-10-06 15:05:48+00:00
- **Authors**: Adriano Cardace, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano
- **Comment**: Accepted at WACV 2022
- **Journal**: None
- **Summary**: Although deep neural networks have achieved remarkable results for the task of semantic segmentation, they usually fail to generalize towards new domains, especially when performing synthetic-to-real adaptation. Such domain shift is particularly noticeable along class boundaries, invalidating one of the main goals of semantic segmentation that consists in obtaining sharp segmentation masks. In this work, we specifically address this core problem in the context of Unsupervised Domain Adaptation and present a novel low-level adaptation strategy that allows us to obtain sharp predictions. Moreover, inspired by recent self-training techniques, we introduce an effective data augmentation that alleviates the noise typically present at semantic boundaries when employing pseudo-labels for self-training. Our contributions can be easily integrated into other popular adaptation frameworks, and extensive experiments show that they effectively improve performance along class boundaries.



### WHO-Hand Hygiene Gesture Classification System
- **Arxiv ID**: http://arxiv.org/abs/2110.02842v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.02842v1)
- **Published**: 2021-10-06 15:15:10+00:00
- **Updated**: 2021-10-06 15:15:10+00:00
- **Authors**: Rashmi Bakshi
- **Comment**: arXiv admin note: text overlap with arXiv:2108.08127
- **Journal**: None
- **Summary**: The recent ongoing coronavirus pandemic highlights the importance of hand hygiene practices in our daily lives, with governments and worldwide health authorities promoting good hand hygiene practices. More than one million cases of hospital-acquired infections occur in Europe annually. Hand hygiene compliance may reduce the risk of transmission by reducing the number of infections as well as healthcare expenditures. In this paper, the World Health Organization, hand hygiene gestures are recorded and analyzed with the construction of an aluminum frame, placed at the laboratory sink. The hand hygiene gestures are recorded for thirty participants after conducting a training session about hand hygiene gestures demonstration. The video recordings are converted into image files and are organized into six different hand hygiene classes. The Resnet50 framework selection for the classification of multiclass hand hygiene stages. The model is trained with the first set of classes; Fingers Interlaced, P2PFingers Interlaced, and Rotational Rub for 25 epochs. An accuracy of 44 percent for the first set of experiments with a loss score greater than 1.5 in the validation set is achieved. The training steps for the second set of classes; Rub hands palm to palm, Fingers Interlocked, Thumb Rub are 50 epochs. An accuracy of 72 percent is achieved for the second set with a loss score of less than 0.8 for the validation set. In this work, a preliminary analysis of a robust hand hygiene dataset with transfer learning takes place. The future aim for deploying a hand hygiene prediction system for healthcare workers in real-time.



### Automatic Identification of the End-Diastolic and End-Systolic Cardiac Frames from Invasive Coronary Angiography Videos
- **Arxiv ID**: http://arxiv.org/abs/2110.02844v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.02844v1)
- **Published**: 2021-10-06 15:16:55+00:00
- **Updated**: 2021-10-06 15:16:55+00:00
- **Authors**: Yinghui Meng, Minghao Dong, Xumin Dai, Haipeng Tang, Chen Zhao, Jingfeng Jiang, Shun Xu, Ying Zhou, Fubao Zhu1, Zhihui Xu, Weihua Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic identification of proper image frames at the end-diastolic (ED) and end-systolic (ES) frames during the review of invasive coronary angiograms (ICA) is important to assess blood flow during a cardiac cycle, reconstruct the 3D arterial anatomy from bi-planar views, and generate the complementary fusion map with myocardial images. The current identification method primarily relies on visual interpretation, making it not only time-consuming but also less reproducible. In this paper, we propose a new method to automatically identify angiographic image frames associated with the ED and ES cardiac phases by using the trajectories of key vessel points (i.e. landmarks). More specifically, a detection algorithm is first used to detect the key points of coronary arteries, and then an optical flow method is employed to track the trajectories of the selected key points. The ED and ES frames are identified based on all these trajectories. Our method was tested with 62 ICA videos from two separate medical centers (22 and 9 patients in sites 1 and 2, respectively). Comparing consensus interpretations by two human expert readers, excellent agreement was achieved by the proposed algorithm: the agreement rates within a one-frame range were 92.99% and 92.73% for the automatic identification of the ED and ES image frames, respectively. In conclusion, the proposed automated method showed great potential for being an integral part of automated ICA image analysis.



### Seed Classification using Synthetic Image Datasets Generated from Low-Altitude UAV Imagery
- **Arxiv ID**: http://arxiv.org/abs/2110.02846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02846v1)
- **Published**: 2021-10-06 15:18:17+00:00
- **Updated**: 2021-10-06 15:18:17+00:00
- **Authors**: Venkat Margapuri, Niketa Penumajji, Mitchell Neilsen
- **Comment**: None
- **Journal**: None
- **Summary**: Plant breeding programs extensively monitor the evolution of seed kernels for seed certification, wherein lies the need to appropriately label the seed kernels by type and quality. However, the breeding environments are large where the monitoring of seed kernels can be challenging due to the minuscule size of seed kernels. The use of unmanned aerial vehicles aids in seed monitoring and labeling since they can capture images at low altitudes whilst being able to access even the remotest areas in the environment. A key bottleneck in the labeling of seeds using UAV imagery is drone altitude i.e. the classification accuracy decreases as the altitude increases due to lower image detail. Convolutional neural networks are a great tool for multi-class image classification when there is a training dataset that closely represents the different scenarios that the network might encounter during evaluation. The article addresses the challenge of training data creation using Domain Randomization wherein synthetic image datasets are generated from a meager sample of seeds captured by the bottom camera of an autonomously driven Parrot AR Drone 2.0. Besides, the article proposes a seed classification framework as a proof-of-concept using the convolutional neural networks of Microsoft's ResNet-100, Oxford's VGG-16, and VGG-19. To enhance the classification accuracy of the framework, an ensemble model is developed resulting in an overall accuracy of 94.6%.



### Fully Convolutional Cross-Scale-Flows for Image-based Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.02855v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02855v1)
- **Published**: 2021-10-06 15:35:13+00:00
- **Updated**: 2021-10-06 15:35:13+00:00
- **Authors**: Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, Bastian Wandt
- **Comment**: None
- **Journal**: None
- **Summary**: In industrial manufacturing processes, errors frequently occur at unpredictable times and in unknown manifestations. We tackle the problem of automatic defect detection without requiring any image samples of defective parts. Recent works model the distribution of defect-free image data, using either strong statistical priors or overly simplified data representations. In contrast, our approach handles fine-grained representations incorporating the global and local image context while flexibly estimating the density. To this end, we propose a novel fully convolutional cross-scale normalizing flow (CS-Flow) that jointly processes multiple feature maps of different scales. Using normalizing flows to assign meaningful likelihoods to input samples allows for efficient defect detection on image-level. Moreover, due to the preserved spatial arrangement the latent space of the normalizing flow is interpretable which enables to localize defective regions in the image. Our work sets a new state-of-the-art in image-level defect detection on the benchmark datasets Magnetic Tile Defects and MVTec AD showing a 100% AUROC on 4 out of 15 classes.



### Exploring the Common Principal Subspace of Deep Features in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.02863v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.02863v1)
- **Published**: 2021-10-06 15:48:32+00:00
- **Updated**: 2021-10-06 15:48:32+00:00
- **Authors**: Haoran Liu, Haoyi Xiong, Yaqing Wang, Haozhe An, Dongrui Wu, Dejing Dou
- **Comment**: Main Text with Appendix, accepted by Machine Learning
- **Journal**: None
- **Summary**: We find that different Deep Neural Networks (DNNs) trained with the same dataset share a common principal subspace in latent spaces, no matter in which architectures (e.g., Convolutional Neural Networks (CNNs), Multi-Layer Preceptors (MLPs) and Autoencoders (AEs)) the DNNs were built or even whether labels have been used in training (e.g., supervised, unsupervised, and self-supervised learning). Specifically, we design a new metric $\mathcal{P}$-vector to represent the principal subspace of deep features learned in a DNN, and propose to measure angles between the principal subspaces using $\mathcal{P}$-vectors. Small angles (with cosine close to $1.0$) have been found in the comparisons between any two DNNs trained with different algorithms/architectures. Furthermore, during the training procedure from random scratch, the angle decrease from a larger one ($70^\circ-80^\circ$ usually) to the small one, which coincides the progress of feature space learning from scratch to convergence. Then, we carry out case studies to measure the angle between the $\mathcal{P}$-vector and the principal subspace of training dataset, and connect such angle with generalization performance. Extensive experiments with practically-used Multi-Layer Perceptron (MLPs), AEs and CNNs for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.   Interpretability of Deep Learning, Feature Learning, and Subspaces of Deep Features



### Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.02865v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2110.02865v3)
- **Published**: 2021-10-06 15:51:38+00:00
- **Updated**: 2022-03-16 04:16:28+00:00
- **Authors**: Alan Jeffares, Qinghai Guo, Pontus Stenetorp, Timoleon Moraitis
- **Comment**: Spotlight paper at ICLR 2022
- **Journal**: International Conference on Learning Representations.
  https://openreview.net/forum?id=iMH1e5k7n3L (2022)
- **Summary**: Biological spiking neural networks (SNNs) can temporally encode information in their outputs, e.g. in the rank order in which neurons fire, whereas artificial neural networks (ANNs) conventionally do not. As a result, models of SNNs for neuromorphic computing are regarded as potentially more rapid and efficient than ANNs when dealing with temporal input. On the other hand, ANNs are simpler to train, and usually achieve superior performance. Here we show that temporal coding such as rank coding (RC) inspired by SNNs can also be applied to conventional ANNs such as LSTMs, and leads to computational savings and speedups. In our RC for ANNs, we apply backpropagation through time using the standard real-valued activations, but only from a strategically early time step of each sequential input example, decided by a threshold-crossing event. Learning then incorporates naturally also *when* to produce an output, without other changes to the model or the algorithm. Both the forward and the backward training pass can be significantly shortened by skipping the remaining input sequence after that first event. RC-training also significantly reduces time-to-insight during inference, with a minimal decrease in accuracy. The desired speed-accuracy trade-off is tunable by varying the threshold or a regularization parameter that rewards output entropy. We demonstrate these in two toy problems of sequence classification, and in a temporally-encoded MNIST dataset where our RC model achieves 99.19% accuracy after the first input time-step, outperforming the state of the art in temporal coding with SNNs, as well as in spoken-word classification of Google Speech Commands, outperforming non-RC-trained early inference with LSTMs.



### ClimateGAN: Raising Climate Change Awareness by Generating Images of Floods
- **Arxiv ID**: http://arxiv.org/abs/2110.02871v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2110.02871v1)
- **Published**: 2021-10-06 15:54:57+00:00
- **Updated**: 2021-10-06 15:54:57+00:00
- **Authors**: Victor Schmidt, Alexandra Sasha Luccioni, M√©lisande Teng, Tianyu Zhang, Alexia Reynaud, Sunand Raghupathi, Gautier Cosne, Adrien Juraver, Vahe Vardanyan, Alex Hernandez-Garcia, Yoshua Bengio
- **Comment**: None
- **Journal**: ICLR 2022
- **Summary**: Climate change is a major threat to humanity, and the actions required to prevent its catastrophic consequences include changes in both policy-making and individual behaviour. However, taking action requires understanding the effects of climate change, even though they may seem abstract and distant. Projecting the potential consequences of extreme climate events such as flooding in familiar places can help make the abstract impacts of climate change more concrete and encourage action. As part of a larger initiative to build a website that projects extreme climate events onto user-chosen photos, we present our solution to simulate photo-realistic floods on authentic images. To address this complex task in the absence of suitable training data, we propose ClimateGAN, a model that leverages both simulated and real data for unsupervised domain adaptation and conditional image generation. In this paper, we describe the details of our framework, thoroughly evaluate components of our architecture and demonstrate that our model is capable of robustly generating photo-realistic flooding.



### SDA-GAN: Unsupervised Image Translation Using Spectral Domain Attention-Guided Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2110.02873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02873v1)
- **Published**: 2021-10-06 15:59:43+00:00
- **Updated**: 2021-10-06 15:59:43+00:00
- **Authors**: Qizhou Wang, Maksim Makarenko
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: This work introduced a novel GAN architecture for unsupervised image translation on the task of face style transform. A spectral attention-based mechanism is embedded into the design along with spatial attention on the image contents. We proved that neural network has the potential of learning complex transformations such as Fourier transform, within considerable computational cost. The model is trained and tested in comparison to the baseline model, which only uses spatial attention. The performance improvement of our approach is significant especially when the source and target domain include different complexity (reduced FID to 49.18 from 142.84). In the translation process, a spectra filling effect was introduced due to the implementation of FFT and spectral attention. Another style transfer task and real-world object translation are also studied in this paper.



### Meta Internal Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.02900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02900v1)
- **Published**: 2021-10-06 16:27:38+00:00
- **Updated**: 2021-10-06 16:27:38+00:00
- **Authors**: Raphael Bensadoun, Shir Gur, Tomer Galanti, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Internal learning for single-image generation is a framework, where a generator is trained to produce novel images based on a single image. Since these models are trained on a single image, they are limited in their scale and application. To overcome these issues, we propose a meta-learning approach that enables training over a collection of images, in order to model the internal statistics of the sample image more effectively. In the presented meta-learning approach, a single-image GAN model is generated given an input image, via a convolutional feedforward hypernetwork $f$. This network is trained over a dataset of images, allowing for feature sharing among different models, and for interpolation in the space of generative models. The generated single-image model contains a hierarchy of multiple generators and discriminators. It is therefore required to train the meta-learner in an adversarial manner, which requires careful design choices that we justify by a theoretical analysis. Our results show that the models obtained are as suitable as single-image GANs for many common image applications, significantly reduce the training time per image without loss in performance, and introduce novel capabilities, such as interpolation and feedforward modeling of novel images.



### SAIC_Cambridge-HuPBA-FBK Submission to the EPIC-Kitchens-100 Action Recognition Challenge 2021
- **Arxiv ID**: http://arxiv.org/abs/2110.02902v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02902v1)
- **Published**: 2021-10-06 16:29:47+00:00
- **Updated**: 2021-10-06 16:29:47+00:00
- **Authors**: Swathikiran Sudhakaran, Adrian Bulat, Juan-Manuel Perez-Rua, Alex Falcon, Sergio Escalera, Oswald Lanz, Brais Martinez, Georgios Tzimiropoulos
- **Comment**: Ranked third in the EPIC-Kitchens-100 Action Recognition Challenge @
  CVPR 2021
- **Journal**: None
- **Summary**: This report presents the technical details of our submission to the EPIC-Kitchens-100 Action Recognition Challenge 2021. To participate in the challenge we deployed spatio-temporal feature extraction and aggregation models we have developed recently: GSF and XViT. GSF is an efficient spatio-temporal feature extracting module that can be plugged into 2D CNNs for video action recognition. XViT is a convolution free video feature extractor based on transformer architecture. We design an ensemble of GSF and XViT model families with different backbones and pretraining to generate the prediction scores. Our submission, visible on the public leaderboard, achieved a top-1 action recognition accuracy of 44.82%, using only RGB.



### Grasp-Oriented Fine-grained Cloth Segmentation without Real Supervision
- **Arxiv ID**: http://arxiv.org/abs/2110.02903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02903v1)
- **Published**: 2021-10-06 16:31:20+00:00
- **Updated**: 2021-10-06 16:31:20+00:00
- **Authors**: Ruijie Ren, Mohit Gurnani Rajesh, Jordi Sanchez-Riera, Fan Zhang, Yurun Tian, Antonio Agudo, Yiannis Demiris, Krystian Mikolajczyk, Francesc Moreno-Noguer
- **Comment**: 6 pages, 4 figures. Submitted to International Conference on Robotics
  and Automation (ICRA)
- **Journal**: None
- **Summary**: Automatically detecting graspable regions from a single depth image is a key ingredient in cloth manipulation. The large variability of cloth deformations has motivated most of the current approaches to focus on identifying specific grasping points rather than semantic parts, as the appearance and depth variations of local regions are smaller and easier to model than the larger ones. However, tasks like cloth folding or assisted dressing require recognising larger segments, such as semantic edges that carry more information than points. The first goal of this paper is therefore to tackle the problem of fine-grained region detection in deformed clothes using only a depth image. As a proof of concept, we implement an approach for T-shirts, and define up to 6 semantic regions of varying extent, including edges on the neckline, sleeve cuffs, and hem, plus top and bottom grasping points. We introduce a U-net based network to segment and label these parts. The second contribution of our work is concerned with the level of supervision that we require to train the proposed network. While most approaches learn to detect grasping points by combining real and synthetic annotations, in this work we defy the limitations of the synthetic data, and propose a multilayered domain adaptation (DA) strategy that does not use real annotations at all. We thoroughly evaluate our approach on real depth images of a T-shirt annotated with fine-grained labels. We show that training our network solely with synthetic data and the proposed DA yields results competitive with models trained on real data.



### HIRE-SNN: Harnessing the Inherent Robustness of Energy-Efficient Deep Spiking Neural Networks by Training with Crafted Input Noise
- **Arxiv ID**: http://arxiv.org/abs/2110.11417v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.11417v1)
- **Published**: 2021-10-06 16:48:48+00:00
- **Updated**: 2021-10-06 16:48:48+00:00
- **Authors**: Souvik Kundu, Massoud Pedram, Peter A. Beerel
- **Comment**: 10 pages, 11 figures, 7 tables, International Conference on Computer
  Vision
- **Journal**: None
- **Summary**: Low-latency deep spiking neural networks (SNNs) have become a promising alternative to conventional artificial neural networks (ANNs) because of their potential for increased energy efficiency on event-driven neuromorphic hardware. Neural networks, including SNNs, however, are subject to various adversarial attacks and must be trained to remain resilient against such attacks for many applications. Nevertheless, due to prohibitively high training costs associated with SNNs, analysis, and optimization of deep SNNs under various adversarial attacks have been largely overlooked. In this paper, we first present a detailed analysis of the inherent robustness of low-latency SNNs against popular gradient-based attacks, namely fast gradient sign method (FGSM) and projected gradient descent (PGD). Motivated by this analysis, to harness the model robustness against these attacks we present an SNN training algorithm that uses crafted input noise and incurs no additional training time. To evaluate the merits of our algorithm, we conducted extensive experiments with variants of VGG and ResNet on both CIFAR-10 and CIFAR-100 datasets. Compared to standard trained direct input SNNs, our trained models yield improved classification accuracy of up to 13.7% and 10.1% on FGSM and PGD attack-generated images, respectively, with negligible loss in clean image accuracy. Our models also outperform inherently robust SNNs trained on rate-coded inputs with improved or similar classification performance on attack-generated images while having up to 25x and 4.6x lower latency and computation energy, respectively.



### Shifting Capsule Networks from the Cloud to the Deep Edge
- **Arxiv ID**: http://arxiv.org/abs/2110.02911v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.5
- **Links**: [PDF](http://arxiv.org/pdf/2110.02911v2)
- **Published**: 2021-10-06 16:52:01+00:00
- **Updated**: 2022-06-15 10:41:49+00:00
- **Authors**: Miguel Costa, Diogo Costa, Tiago Gomes, Sandro Pinto
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule networks (CapsNets) are an emerging trend in image processing. In contrast to a convolutional neural network, CapsNets are not vulnerable to object deformation, as the relative spatial information of the objects is preserved across the network. However, their complexity is mainly related to the capsule structure and the dynamic routing mechanism, which makes it almost unreasonable to deploy a CapsNet, in its original form, in a resource-constrained device powered by a small microcontroller (MCU). In an era where intelligence is rapidly shifting from the cloud to the edge, this high complexity imposes serious challenges to the adoption of CapsNets at the very edge. To tackle this issue, we present an API for the execution of quantized CapsNets in Arm Cortex-M and RISC-V MCUs. Our software kernels extend the Arm CMSIS-NN and RISC-V PULP-NN to support capsule operations with 8-bit integers as operands. Along with it, we propose a framework to perform post-training quantization of a CapsNet. Results show a reduction in memory footprint of almost 75%, with accuracy loss ranging from 0.07% to 0.18%. In terms of throughput, our Arm Cortex-M API enables the execution of primary capsule and capsule layers with medium-sized kernels in just 119.94 and 90.60 milliseconds (ms), respectively (STM32H755ZIT6U, Cortex-M7 @ 480 MHz). For the GAP-8 SoC (RISC-V RV32IMCXpulp @ 170 MHz), the latency drops to 7.02 and 38.03 ms, respectively.



### Boosting RANSAC via Dual Principal Component Pursuit
- **Arxiv ID**: http://arxiv.org/abs/2110.02918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02918v1)
- **Published**: 2021-10-06 17:04:45+00:00
- **Updated**: 2021-10-06 17:04:45+00:00
- **Authors**: Yunchen Yang, Xinyue Zhang, Tianjiao Ding, Daniel P. Robinson, Rene Vidal, Manolis C. Tsakiris
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we revisit the problem of local optimization in RANSAC. Once a so-far-the-best model has been found, we refine it via Dual Principal Component Pursuit (DPCP), a robust subspace learning method with strong theoretical support and efficient algorithms. The proposed DPCP-RANSAC has far fewer parameters than existing methods and is scalable. Experiments on estimating two-view homographies, fundamental and essential matrices, and three-view homographic tensors using large-scale datasets show that our approach consistently has higher accuracy than state-of-the-art alternatives.



### Adversarial Attacks on Spiking Convolutional Neural Networks for Event-based Vision
- **Arxiv ID**: http://arxiv.org/abs/2110.02929v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02929v3)
- **Published**: 2021-10-06 17:20:05+00:00
- **Updated**: 2022-12-05 12:49:10+00:00
- **Authors**: Julian B√ºchel, Gregor Lenz, Yalun Hu, Sadique Sheik, Martino Sorbaro
- **Comment**: 9 pages plus Supplementary Material. Accepted in Frontiers in
  Neuroscience -- Neuromorphic Engineering
- **Journal**: None
- **Summary**: Event-based dynamic vision sensors provide very sparse output in the form of spikes, which makes them suitable for low-power applications. Convolutional spiking neural networks model such event-based data and develop their full energy-saving potential when deployed on asynchronous neuromorphic hardware. Event-based vision being a nascent field, the sensitivity of spiking neural networks to potentially malicious adversarial attacks has received little attention so far. We show how white-box adversarial attack algorithms can be adapted to the discrete and sparse nature of event-based visual data, and demonstrate smaller perturbation magnitudes at higher success rates than the current state-of-the-art algorithms. For the first time, we also verify the effectiveness of these perturbations directly on neuromorphic hardware. Finally, we discuss the properties of the resulting perturbations, the effect of adversarial training as a defense strategy, and future directions.



### On Cropped versus Uncropped Training Sets in Tabular Structure Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.02933v2
- **DOI**: 10.1016/j.neucom.2022.09.094
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02933v2)
- **Published**: 2021-10-06 17:28:38+00:00
- **Updated**: 2021-10-07 03:22:42+00:00
- **Authors**: Yakup Akkaya, Murat Simsek, Burak Kantarci, Shahzad Khan
- **Comment**: None
- **Journal**: Neurocomputing, Volume 513, 2022, Pages 114-126
- **Summary**: Automated document processing for tabular information extraction is highly desired in many organizations, from industry to government. Prior works have addressed this problem under table detection and table structure detection tasks. Proposed solutions leveraging deep learning approaches have been giving promising results in these tasks. However, the impact of dataset structures on table structure detection has not been investigated. In this study, we provide a comparison of table structure detection performance with cropped and uncropped datasets. The cropped set consists of only table images that are cropped from documents assuming tables are detected perfectly. The uncropped set consists of regular document images. Experiments show that deep learning models can improve the detection performance by up to 9% in average precision and average recall on the cropped versions. Furthermore, the impact of cropped images is negligible under the Intersection over Union (IoU) values of 50%-70% when compared to the uncropped versions. However, beyond 70% IoU thresholds, cropped datasets provide significantly higher detection performance.



### Topologically Consistent Multi-View Face Inference Using Volumetric Sampling
- **Arxiv ID**: http://arxiv.org/abs/2110.02948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02948v1)
- **Published**: 2021-10-06 17:55:08+00:00
- **Updated**: 2021-10-06 17:55:08+00:00
- **Authors**: Tianye Li, Shichen Liu, Timo Bolkart, Jiayi Liu, Hao Li, Yajie Zhao
- **Comment**: International Conference on Computer Vision (ICCV)
- **Journal**: None
- **Summary**: High-fidelity face digitization solutions often combine multi-view stereo (MVS) techniques for 3D reconstruction and a non-rigid registration step to establish dense correspondence across identities and expressions. A common problem is the need for manual clean-up after the MVS step, as 3D scans are typically affected by noise and outliers and contain hairy surface regions that need to be cleaned up by artists. Furthermore, mesh registration tends to fail for extreme facial expressions. Most learning-based methods use an underlying 3D morphable model (3DMM) to ensure robustness, but this limits the output accuracy for extreme facial expressions. In addition, the global bottleneck of regression architectures cannot produce meshes that tightly fit the ground truth surfaces. We propose ToFu, Topologically consistent Face from multi-view, a geometry inference framework that can produce topologically consistent meshes across facial identities and expressions using a volumetric representation instead of an explicit underlying 3DMM. Our novel progressive mesh generation network embeds the topological structure of the face in a feature volume, sampled from geometry-aware local features. A coarse-to-fine architecture facilitates dense and accurate facial mesh predictions in a consistent mesh topology. ToFu further captures displacement maps for pore-level geometric details and facilitates high-quality rendering in the form of albedo and specular reflectance maps. These high-quality assets are readily usable by production studios for avatar creation, animation and physically-based skin rendering. We demonstrate state-of-the-art geometric and correspondence accuracy, while only taking 0.385 seconds to compute a mesh with 10K vertices, which is three orders of magnitude faster than traditional techniques. The code and the model are available for research purposes at https://tianyeli.github.io/tofu.



### Video Autoencoder: self-supervised disentanglement of static 3D structure and motion
- **Arxiv ID**: http://arxiv.org/abs/2110.02951v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02951v1)
- **Published**: 2021-10-06 17:57:42+00:00
- **Updated**: 2021-10-06 17:57:42+00:00
- **Authors**: Zihang Lai, Sifei Liu, Alexei A. Efros, Xiaolong Wang
- **Comment**: Accepted to ICCV 2021. Project page:
  https://zlai0.github.io/VideoAutoencoder
- **Journal**: None
- **Summary**: A video autoencoder is proposed for learning disentan- gled representations of 3D structure and camera pose from videos in a self-supervised manner. Relying on temporal continuity in videos, our work assumes that the 3D scene structure in nearby video frames remains static. Given a sequence of video frames as input, the video autoencoder extracts a disentangled representation of the scene includ- ing: (i) a temporally-consistent deep voxel feature to represent the 3D structure and (ii) a 3D trajectory of camera pose for each frame. These two representations will then be re-entangled for rendering the input video frames. This video autoencoder can be trained directly using a pixel reconstruction loss, without any ground truth 3D or camera pose annotations. The disentangled representation can be applied to a range of tasks, including novel view synthesis, camera pose estimation, and video generation by motion following. We evaluate our method on several large- scale natural video datasets, and show generalization results on out-of-domain images.



### Tensor-to-Image: Image-to-Image Translation with Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2110.08037v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.08037v1)
- **Published**: 2021-10-06 17:57:45+00:00
- **Updated**: 2021-10-06 17:57:45+00:00
- **Authors**: Yiƒüit G√ºnd√º√ß
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers gain huge attention since they are first introduced and have a wide range of applications. Transformers start to take over all areas of deep learning and the Vision transformers paper also proved that they can be used for computer vision tasks. In this paper, we utilized a vision transformer-based custom-designed model, tensor-to-image, for the image to image translation. With the help of self-attention, our model was able to generalize and apply to different problems without a single modification.



### Learning Canonical Embedding for Non-rigid Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/2110.02994v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02994v1)
- **Published**: 2021-10-06 18:09:13+00:00
- **Updated**: 2021-10-06 18:09:13+00:00
- **Authors**: Abhishek Sharma, Maks Ovsjanikov
- **Comment**: Under Review
- **Journal**: None
- **Summary**: This paper provides a novel framework that learns canonical embeddings for non-rigid shape matching. In contrast to prior work in this direction, our framework is trained end-to-end and thus avoids instabilities and constraints associated with the commonly-used Laplace-Beltrami basis or sequential optimization schemes. On multiple datasets, we demonstrate that learning self symmetry maps with a deep functional map projects 3D shapes into a low dimensional canonical embedding that facilitates non-rigid shape correspondence via a simple nearest neighbor search. Our framework outperforms multiple recent learning based methods on FAUST and SHREC benchmarks while being computationally cheaper, data-efficient, and robust.



### Multi-Scale Convolutional Neural Network for Automated AMD Classification using Retinal OCT Images
- **Arxiv ID**: http://arxiv.org/abs/2110.03002v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03002v2)
- **Published**: 2021-10-06 18:20:58+00:00
- **Updated**: 2022-02-04 06:29:43+00:00
- **Authors**: Saman Sotoudeh-Paima, Ata Jodeiri, Fedra Hajizadeh, Hamid Soltanian-Zadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries, especially in people over 60 years of age. The workload of specialists and the healthcare system in this field has increased in recent years mainly due to the prevalence of population aging worldwide and the chronic nature of AMD. Recent developments in deep learning have provided a unique opportunity to develop fully automated diagnosis frameworks. Considering the presence of AMD-related retinal pathologies in varying sizes in OCT images, our objective was to propose a multi-scale convolutional neural network (CNN) capable of distinguishing pathologies using receptive fields with various sizes. The multi-scale CNN was designed based on the feature pyramid network (FPN) structure and was used to diagnose normal and two common clinical characteristics of dry and wet AMD, namely drusen and choroidal neovascularization (CNV). The proposed method was evaluated on a national dataset gathered at Noor Eye Hospital (NEH) and the UCSD public dataset. Experimental results show the superior performance of our proposed multi-scale structure over several well-known OCT classification frameworks. This feature combination strategy has proved to be effective on all tested backbone models, with improvements ranging from 0.4% to 3.3%. In addition, gradual learning has proven to improve performance in two consecutive stages. In the first stage, the performance was boosted from 87.2%+-2.5% to 92.0%+-1.6% using pre-trained ImageNet weights. In the second stage, another performance boost from 92.0%+-1.6% to 93.4%+-1.4% was observed due to fine-tuning the previous model on the UCSD dataset. Lastly, generating heatmaps provided additional proof for the effectiveness of our multi-scale structure, enabling the detection of retinal pathologies appearing in different sizes.



### Unsupervised Selective Labeling for More Effective Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.03006v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03006v4)
- **Published**: 2021-10-06 18:25:50+00:00
- **Updated**: 2023-08-23 16:47:25+00:00
- **Authors**: Xudong Wang, Long Lian, Stella X. Yu
- **Comment**: Accepted by ECCV 2022; Fixed a few typos
- **Journal**: None
- **Summary**: Given an unlabeled dataset and an annotation budget, we study how to selectively label a fixed number of instances so that semi-supervised learning (SSL) on such a partially labeled dataset is most effective. We focus on selecting the right data to label, in addition to usual SSL's propagating labels from labeled data to the rest unlabeled data. This instance selection task is challenging, as without any labeled data we do not know what the objective of learning should be. Intuitively, no matter what the downstream task is, instances to be labeled must be representative and diverse: The former would facilitate label propagation to unlabeled data, whereas the latter would ensure coverage of the entire dataset. We capture this idea by selecting cluster prototypes, either in a pretrained feature space, or along with feature optimization, both without labels. Our unsupervised selective labeling consistently improves SSL methods over state-of-the-art active learning given labeled data, by 8 to 25 times in label efficiency. For example, it boosts FixMatch by 10% (14%) in accuracy on CIFAR-10 (ImageNet-1K) with 0.08% (0.2%) labeled data, demonstrating that small computation spent on selecting what data to label brings significant gain especially under a low annotation budget. Our work sets a new standard for practical and efficient SSL.



### DeepBBS: Deep Best Buddies for Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2110.03016v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03016v2)
- **Published**: 2021-10-06 19:00:07+00:00
- **Updated**: 2021-10-16 09:22:07+00:00
- **Authors**: Itan Hezroni, Amnon Drory, Raja Giryes, Shai Avidan
- **Comment**: Accepted to 3DV 2021
- **Journal**: None
- **Summary**: Recently, several deep learning approaches have been proposed for point cloud registration. These methods train a network to generate a representation that helps finding matching points in two 3D point clouds. Finding good matches allows them to calculate the transformation between the point clouds accurately. Two challenges of these techniques are dealing with occlusions and generalizing to objects of classes unseen during training. This work proposes DeepBBS, a novel method for learning a representation that takes into account the best buddy distance between points during training. Best Buddies (i.e., mutual nearest neighbors) are pairs of points nearest to each other. The Best Buddies criterion is a strong indication for correct matches that, in turn, leads to accurate registration. Our experiments show improved performance compared to previous methods. In particular, our learned representation leads to an accurate registration for partial shapes and in unseen categories.



### Dynamically Decoding Source Domain Knowledge for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2110.03027v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03027v2)
- **Published**: 2021-10-06 19:21:24+00:00
- **Updated**: 2021-12-05 06:23:27+00:00
- **Authors**: Cuicui Kang, Karthik Nandakumar
- **Comment**: None
- **Journal**: None
- **Summary**: Optimizing the performance of classifiers on samples from unseen domains remains a challenging problem. While most existing studies on domain generalization focus on learning domain-invariant feature representations, multi-expert frameworks have been proposed as a possible solution and have demonstrated promising performance. However, current multi-expert learning frameworks fail to fully exploit source domain knowledge during inference, resulting in sub-optimal performance. In this work, we propose to adapt Transformers for the purpose of dynamically decoding source domain knowledge for domain generalization. Specifically, we build one domain-specific local expert per source domain and one domain-agnostic feature branch as query. A Transformer encoder encodes all domain-specific features as source domain knowledge in memory. In the Transformer decoder, the domain-agnostic query interacts with the memory in the cross-attention module, and domains that are similar to the input will contribute more to the attention output. Thus, source domain knowledge gets dynamically decoded for inference of the current input from unseen domain. This mechanism enables the proposed method to generalize well to unseen domains. The proposed method has been evaluated on three benchmarks in the domain generalization field and shown to have the best performance compared to state-of-the-art methods.



### FOD-A: A Dataset for Foreign Object Debris in Airports
- **Arxiv ID**: http://arxiv.org/abs/2110.03072v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03072v2)
- **Published**: 2021-10-06 21:11:50+00:00
- **Updated**: 2022-01-26 20:38:41+00:00
- **Authors**: Travis Munyer, Pei-Chi Huang, Chenyu Huang, Xin Zhong
- **Comment**: This paper has been accepted for publication by 20th IEEE
  International Conference on Machine Learning and Applications. The copyright
  is with the IEEE
- **Journal**: None
- **Summary**: Foreign Object Debris (FOD) detection has attracted increased attention in the area of machine learning and computer vision. However, a robust and publicly available image dataset for FOD has not been initialized. To this end, this paper introduces an image dataset of FOD, named FOD in Airports (FOD-A). FOD-A object categories have been selected based on guidance from prior documentation and related research by the Federal Aviation Administration (FAA). In addition to the primary annotations of bounding boxes for object detection, FOD-A provides labeled environmental conditions. As such, each annotation instance is further categorized into three light level categories (bright, dim, and dark) and two weather categories (dry and wet). Currently, FOD-A has released 31 object categories and over 30,000 annotation instances. This paper presents the creation methodology, discusses the publicly available dataset extension process, and demonstrates the practicality of FOD-A with widely used machine learning models for object detection.



### Large-Scale Topological Radar Localization Using Learned Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2110.03081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03081v1)
- **Published**: 2021-10-06 21:57:23+00:00
- **Updated**: 2021-10-06 21:57:23+00:00
- **Authors**: Jacek Komorowski, Monika Wysoczanska, Tomasz Trzcinski
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a method for large-scale topological localization based on radar scan images using learned descriptors. We present a simple yet efficient deep network architecture to compute a rotationally invariant discriminative global descriptor from a radar scan image. The performance and generalization ability of the proposed method is experimentally evaluated on two large scale driving datasets: MulRan and Oxford Radar RobotCar. Additionally, we present a comparative evaluation of radar-based and LiDAR-based localization using learned global descriptors. Our code and trained models are publicly available on the project website.



### Construction Site Safety Monitoring and Excavator Activity Analysis System
- **Arxiv ID**: http://arxiv.org/abs/2110.03083v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.03083v3)
- **Published**: 2021-10-06 22:11:35+00:00
- **Updated**: 2022-07-24 23:01:51+00:00
- **Authors**: Sibo Zhang, Liangjun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: With the recent advancements in deep learning and computer vision, the AI-powered construction machine such as autonomous excavator has made significant progress. Safety is the most important section in modern construction, where construction machines are more and more automated. In this paper, we propose a vision-based excavator perception, activity analysis, and safety monitoring system. Our perception system could detect multi-class construction machines and humans in real-time while estimating the poses and actions of the excavator. Then, we present a novel safety monitoring and excavator activity analysis system based on the perception result. To evaluate the performance of our method, we collect a dataset using the Autonomous Excavator System (AES) including multi-class of objects in different lighting conditions with human annotations. We also evaluate our method on a benchmark construction dataset. The results showed our YOLO v5 multi-class objects detection model improved inference speed by 8 times (YOLO v5 x-large) to 34 times (YOLO v5 small) compared with Faster R-CNN/ YOLO v3 model. Furthermore, the accuracy of YOLO v5 models is improved by 2.7% (YOLO v5 x-large) while model size is reduced by 63.9% (YOLO v5 x-large) to 93.9% (YOLO v5 small). The experimental results show that the proposed action recognition approach outperforms the state-of-the-art approaches on top-1 accuracy by about 5.18%. The proposed real-time safety monitoring system is not only designed for our Autonomous Excavator System (AES) in solid waste scenes, it can also be applied to general construction scenarios.



### Player Tracking and Identification in Ice Hockey
- **Arxiv ID**: http://arxiv.org/abs/2110.03090v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03090v2)
- **Published**: 2021-10-06 22:37:08+00:00
- **Updated**: 2021-12-02 22:56:12+00:00
- **Authors**: Kanav Vats, Pascale Walters, Mehrnaz Fani, David A. Clausi, John Zelek
- **Comment**: Submitted to Expert Systems with Applications
- **Journal**: None
- **Summary**: Tracking and identifying players is a fundamental step in computer vision-based ice hockey analytics. The data generated by tracking is used in many other downstream tasks, such as game event detection and game strategy analysis. Player tracking and identification is a challenging problem since the motion of players in hockey is fast-paced and non-linear when compared to pedestrians. There is also significant camera panning and zooming in hockey broadcast video. Identifying players in ice hockey is challenging since the players of the same team look almost identical, with the jersey number the only discriminating factor between players. In this paper, an automated system to track and identify players in broadcast NHL hockey videos is introduced. The system is composed of three components (1) Player tracking, (2) Team identification and (3) Player identification. Due to the absence of publicly available datasets, the datasets used to train the three components are annotated manually. Player tracking is performed with the help of a state of the art tracking algorithm obtaining a Multi-Object Tracking Accuracy (MOTA) score of 94.5%. For team identification, the away-team jerseys are grouped into a single class and home-team jerseys are grouped in classes according to their jersey color. A convolutional neural network is then trained on the team identification dataset. The team identification network gets an accuracy of 97% on the test set. A novel player identification model is introduced that utilizes a temporal one-dimensional convolutional network to identify players from player bounding box sequences. The player identification model further takes advantage of the available NHL game roster data to obtain a player identification accuracy of 83%.



### Improving Fractal Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2110.03091v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03091v2)
- **Published**: 2021-10-06 22:39:51+00:00
- **Updated**: 2021-12-17 17:32:10+00:00
- **Authors**: Connor Anderson, Ryan Farrell
- **Comment**: Accepted to WACV 2022. 15 pages, 15 figures. Added note about error,
  removed erroneous result
- **Journal**: None
- **Summary**: The deep neural networks used in modern computer vision systems require enormous image datasets to train them. These carefully-curated datasets typically have a million or more images, across a thousand or more distinct categories. The process of creating and curating such a dataset is a monumental undertaking, demanding extensive effort and labelling expense and necessitating careful navigation of technical and social issues such as label accuracy, copyright ownership, and content bias.   What if we had a way to harness the power of large image datasets but with few or none of the major issues and concerns currently faced? This paper extends the recent work of Kataoka et. al. (2020), proposing an improved pre-training dataset based on dynamically-generated fractal images. Challenging issues with large-scale image datasets become points of elegance for fractal pre-training: perfect label accuracy at zero cost; no need to store/transmit large image archives; no privacy/demographic bias/concerns of inappropriate content, as no humans are pictured; limitless supply and diversity of images; and the images are free/open-source. Perhaps surprisingly, avoiding these difficulties imposes only a small penalty in performance. Leveraging a newly-proposed pre-training task -- multi-instance prediction -- our experiments demonstrate that fine-tuning a network pre-trained using fractals attains 92.7-98.1% of the accuracy of an ImageNet pre-trained network.



### Improving Pneumonia Localization via Cross-Attention on Medical Images and Reports
- **Arxiv ID**: http://arxiv.org/abs/2110.03094v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03094v1)
- **Published**: 2021-10-06 22:47:48+00:00
- **Updated**: 2021-10-06 22:47:48+00:00
- **Authors**: Riddhish Bhalodia, Ali Hatamizadeh, Leo Tam, Ziyue Xu, Xiaosong Wang, Evrim Turkbey, Daguang Xu
- **Comment**: Published at MICCAI 2021
- **Journal**: None
- **Summary**: Localization and characterization of diseases like pneumonia are primary steps in a clinical pipeline, facilitating detailed clinical diagnosis and subsequent treatment planning. Additionally, such location annotated datasets can provide a pathway for deep learning models to be used for downstream tasks. However, acquiring quality annotations is expensive on human resources and usually requires domain expertise. On the other hand, medical reports contain a plethora of information both about pneumonia characteristics and its location. In this paper, we propose a novel weakly-supervised attention-driven deep learning model that leverages encoded information in medical reports during training to facilitate better localization. Our model also performs classification of attributes that are associated to pneumonia and extracted from medical reports for supervision. Both the classification and localization are trained in conjunction and once trained, the model can be utilized for both the localization and characterization of pneumonia using only the input image. In this paper, we explore and analyze the model using chest X-ray datasets and demonstrate qualitatively and quantitatively that the introduction of textual information improves pneumonia localization. We showcase quantitative results on two datasets, MIMIC-CXR and Chest X-ray-8, and we also showcase severity characterization on the COVID-19 dataset.



### Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective
- **Arxiv ID**: http://arxiv.org/abs/2110.03095v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.03095v2)
- **Published**: 2021-10-06 22:51:26+00:00
- **Updated**: 2022-02-10 16:27:51+00:00
- **Authors**: Luca Scimeca, Seong Joon Oh, Sanghyuk Chun, Michael Poli, Sangdoo Yun
- **Comment**: To be published in "The International Conference on Learning
  Representations" (ICLR 2022)(Accepted) First two authors have contributed
  equally
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) often rely on easy-to-learn discriminatory features, or cues, that are not necessarily essential to the problem at hand. For example, ducks in an image may be recognized based on their typical background scenery, such as lakes or streams. This phenomenon, also known as shortcut learning, is emerging as a key limitation of the current generation of machine learning models. In this work, we introduce a set of experiments to deepen our understanding of shortcut learning and its implications. We design a training setup with several shortcut cues, named WCST-ML, where each cue is equally conducive to the visual recognition problem at hand. Even under equal opportunities, we observe that (1) certain cues are preferred to others, (2) solutions biased to the easy-to-learn cues tend to converge to relatively flat minima on the loss surface, and (3) the solutions focusing on those preferred cues are far more abundant in the parameter space. We explain the abundance of certain cues via their Kolmogorov (descriptional) complexity: solutions corresponding to Kolmogorov-simple cues are abundant in the parameter space and are thus preferred by DNNs. Our studies are based on the synthetic dataset DSprites and the face dataset UTKFace. In our WCST-ML, we observe that the inborn bias of models leans toward simple cues, such as color and ethnicity. Our findings emphasize the importance of active human intervention to remove the inborn model biases that may cause negative societal impacts.



### SPEED+: Next-Generation Dataset for Spacecraft Pose Estimation across Domain Gap
- **Arxiv ID**: http://arxiv.org/abs/2110.03101v2
- **DOI**: 10.1109/AERO53065.2022.9843439
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03101v2)
- **Published**: 2021-10-06 23:22:24+00:00
- **Updated**: 2021-12-09 22:17:12+00:00
- **Authors**: Tae Ha Park, Marcus M√§rtens, Gurvan Lecuyer, Dario Izzo, Simone D'Amico
- **Comment**: None
- **Journal**: 2022 IEEE Aerospace Conference (AERO), 2022
- **Summary**: Autonomous vision-based spaceborne navigation is an enabling technology for future on-orbit servicing and space logistics missions. While computer vision in general has benefited from Machine Learning (ML), training and validating spaceborne ML models are extremely challenging due to the impracticality of acquiring a large-scale labeled dataset of images of the intended target in the space environment. Existing datasets, such as Spacecraft PosE Estimation Dataset (SPEED), have so far mostly relied on synthetic images for both training and validation, which are easy to mass-produce but fail to resemble the visual features and illumination variability inherent to the target spaceborne images. In order to bridge the gap between the current practices and the intended applications in future space missions, this paper introduces SPEED+: the next generation spacecraft pose estimation dataset with specific emphasis on domain gap. In addition to 60,000 synthetic images for training, SPEED+ includes 9,531 hardware-in-the-loop images of a spacecraft mockup model captured from the Testbed for Rendezvous and Optical Navigation (TRON) facility. TRON is a first-of-a-kind robotic testbed capable of capturing an arbitrary number of target images with accurate and maximally diverse pose labels and high-fidelity spaceborne illumination conditions. SPEED+ is used in the second international Satellite Pose Estimation Challenge co-hosted by SLAB and the Advanced Concepts Team of the European Space Agency to evaluate and compare the robustness of spaceborne ML models trained on synthetic images.



### MetaCOG: Learning a Metacognition to Recover What Objects Are Actually There
- **Arxiv ID**: http://arxiv.org/abs/2110.03105v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03105v3)
- **Published**: 2021-10-06 23:37:21+00:00
- **Updated**: 2023-08-29 18:15:10+00:00
- **Authors**: Marlene Berke, Zhangir Azerbayev, Mario Belledonne, Zenna Tavares, Julian Jara-Ettinger
- **Comment**: 12 pages, 4 figures
- **Journal**: None
- **Summary**: Humans not only form representations about the world based on what we see, but also learn meta-cognitive representations about how our own vision works. This enables us to recognize when our vision is unreliable (e.g., when we realize that we are experiencing a visual illusion) and enables us to question what we see. Inspired by this human capacity, we present MetaCOG: a model that increases the robustness of object detectors by learning representations of their reliability, and does so without feedback. Specifically, MetaCOG is a hierarchical probabilistic model that expresses a joint distribution over the objects in a 3D scene and the outputs produced by a detector. When paired with an off-the-shelf object detector, MetaCOG takes detections as input and infers the detector's tendencies to miss objects of certain categories and to hallucinate objects that are not actually present, all without access to ground-truth object labels. When paired with three modern neural object detectors, MetaCOG learns useful and accurate meta-cognitive representations, resulting in improved performance on the detection task. Additionally, we show that MetaCOG is robust to varying levels of error in the detections. Our results are a proof-of-concept for a novel approach to the problem of correcting a faulty vision system's errors. The model code, datasets, results, and demos are available: https://osf.io/8b9qt/?view_only=8c1b1c412c6b4e1697e3c7859be2fce6




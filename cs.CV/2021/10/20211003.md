# Arxiv Papers in cs.CV on 2021-10-03
### Disarranged Zone Learning (DZL): An unsupervised and dynamic automatic stenosis recognition methodology based on coronary angiography
- **Arxiv ID**: http://arxiv.org/abs/2110.00896v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00896v1)
- **Published**: 2021-10-03 00:27:58+00:00
- **Updated**: 2021-10-03 00:27:58+00:00
- **Authors**: Yanan Dai, Pengxiong Zhu, Bangde Xue, Yun Ling, Xibao Shi, Liang Geng, Qi Zhang, Jun Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We proposed a novel unsupervised methodology named Disarranged Zone Learning (DZL) to automatically recognize stenosis in coronary angiography. The methodology firstly disarranges the frames in a video, secondly it generates an effective zone and lastly trains an encoder-decoder GRU model to learn the capability to recover disarranged frames. The breakthrough of our study is to discover and validate the Sequence Intensity (Recover Difficulty) is a measure of Coronary Artery Stenosis Status. Hence, the prediction accuracy of DZL is used as an approximator of coronary stenosis indicator. DZL is an unsupervised methodology and no label engineering effort is needed, the sub GRU model in DZL works as a self-supervised approach. So DZL could theoretically utilize infinitely huge amounts of coronary angiographies to learn and improve performance without laborious data labeling. There is no data preprocessing precondition to run DZL as it dynamically utilizes the whole video, hence it is easy to be implemented and generalized to overcome the data heterogeneity of coronary angiography. The overall average precision score achieves 0.93, AUC achieves 0.8 for this pure methodology. The highest segmented average precision score is 0.98 and the highest segmented AUC is 0.87 for coronary occlusion indicator. Finally, we developed a software demo to implement DZL methodology.



### Anti-aliasing Deep Image Classifiers using Novel Depth Adaptive Blurring and Activation Function
- **Arxiv ID**: http://arxiv.org/abs/2110.00899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00899v1)
- **Published**: 2021-10-03 01:00:52+00:00
- **Updated**: 2021-10-03 01:00:52+00:00
- **Authors**: Md Tahmid Hossain, Shyh Wei Teng, Ferdous Sohel, Guojun Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional networks are vulnerable to image translation or shift, partly due to common down-sampling layers, e.g., max-pooling and strided convolution. These operations violate the Nyquist sampling rate and cause aliasing. The textbook solution is low-pass filtering (blurring) before down-sampling, which can benefit deep networks as well. Even so, non-linearity units, such as ReLU, often re-introduce the problem, suggesting that blurring alone may not suffice. In this work, first, we analyse deep features with Fourier transform and show that Depth Adaptive Blurring is more effective, as opposed to monotonic blurring. To this end, we outline how this can replace existing down-sampling methods. Second, we introduce a novel activation function -- with a built-in low pass filter, to keep the problem from reappearing. From experiments, we observe generalisation on other forms of transformations and corruptions as well, e.g., rotation, scale, and noise. We evaluate our method under three challenging settings: (1) a variety of image translations; (2) adversarial attacks -- both $\ell_{p}$ bounded and unbounded; and (3) data corruptions and perturbations. In each setting, our method achieves state-of-the-art results and improves clean accuracy on various benchmark datasets.



### GROWN: GRow Only When Necessary for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.00908v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00908v1)
- **Published**: 2021-10-03 02:31:04+00:00
- **Updated**: 2021-10-03 02:31:04+00:00
- **Authors**: Li Yang, Sen Lin, Junshan Zhang, Deliang Fan
- **Comment**: 8 pages. arXiv admin note: text overlap with arXiv:2007.07617 by
  other authors
- **Journal**: None
- **Summary**: Catastrophic forgetting is a notorious issue in deep learning, referring to the fact that Deep Neural Networks (DNN) could forget the knowledge about earlier tasks when learning new tasks. To address this issue, continual learning has been developed to learn new tasks sequentially and perform knowledge transfer from the old tasks to the new ones without forgetting. While recent structure-based learning methods show the capability of alleviating the forgetting problem, these methods start from a redundant full-size network and require a complex learning process to gradually grow-and-prune or search the network structure for each task, which is inefficient. To address this problem and enable efficient network expansion for new tasks, we first develop a learnable sparse growth method eliminating the additional pruning/searching step in previous structure-based methods. Building on this learnable sparse growth method, we then propose GROWN, a novel end-to-end continual learning framework to dynamically grow the model only when necessary. Different from all previous structure-based methods, GROWN starts from a small seed network, instead of a full-sized one. We validate GROWN on multiple datasets against state-of-the-art methods, which shows superior performance in both accuracy and model size. For example, we achieve 1.0\% accuracy gain on average compared to the current SOTA results on CIFAR-100 Superclass 20 tasks setting.



### Attention module improves both performance and interpretability of 4D fMRI decoding neural network
- **Arxiv ID**: http://arxiv.org/abs/2110.00920v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00920v1)
- **Published**: 2021-10-03 04:15:42+00:00
- **Updated**: 2021-10-03 04:15:42+00:00
- **Authors**: Zhoufan Jiang, Yanming Wang, ChenWei Shi, Yueyang Wu, Rongjie Hu, Shishuo Chen, Sheng Hu, Xiaoxiao Wang, Bensheng Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Decoding brain cognitive states from neuroimaging signals is an important topic in neuroscience. In recent years, deep neural networks (DNNs) have been recruited for multiple brain state decoding and achieved good performance. However, the open question of how to interpret the DNN black box remains unanswered. Capitalizing on advances in machine learning, we integrated attention modules into brain decoders to facilitate an in-depth interpretation of DNN channels. A 4D convolution operation was also included to extract temporo-spatial interaction within the fMRI signal. The experiments showed that the proposed model obtains a very high accuracy (97.4%) and outperforms previous researches on the 7 different task benchmarks from the Human Connectome Project (HCP) dataset. The visualization analysis further illustrated the hierarchical emergence of task-specific masks with depth. Finally, the model was retrained to regress individual traits within the HCP and to classify viewing images from the BOLD5000 dataset, respectively. Transfer learning also achieves good performance. A further visualization analysis shows that, after transfer learning, low-level attention masks remained similar to the source domain, whereas high-level attention masks changed adaptively. In conclusion, the proposed 4D model with attention module performed well and facilitated interpretation of DNNs, which is helpful for subsequent research.



### Bounding Box Tightness Prior for Weakly Supervised Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.00934v1
- **DOI**: 10.1007/978-3-030-87196-3_49
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.00934v1)
- **Published**: 2021-10-03 06:19:20+00:00
- **Updated**: 2021-10-03 06:19:20+00:00
- **Authors**: Juan Wang, Bin Xia
- **Comment**: MICCAI 2021
- **Journal**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention (MICCAI), pp. 526-536, 2021
- **Summary**: This paper presents a weakly supervised image segmentation method that adopts tight bounding box annotations. It proposes generalized multiple instance learning (MIL) and smooth maximum approximation to integrate the bounding box tightness prior into the deep neural network in an end-to-end manner. In generalized MIL, positive bags are defined by parallel crossing lines with a set of different angles, and negative bags are defined as individual pixels outside of any bounding boxes. Two variants of smooth maximum approximation, i.e., $\alpha$-softmax function and $\alpha$-quasimax function, are exploited to conquer the numeral instability introduced by maximum function of bag prediction. The proposed approach was evaluated on two pubic medical datasets using Dice coefficient. The results demonstrate that it outperforms the state-of-the-art methods. The codes are available at \url{https://github.com/wangjuan313/wsis-boundingbox}.



### Anatomical Landmarks Localization for 3D Foot Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2110.00937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.00937v1)
- **Published**: 2021-10-03 06:24:40+00:00
- **Updated**: 2021-10-03 06:24:40+00:00
- **Authors**: Sheldon Fung, Xuequan Lu, Mantas Mykolaitis, Gediminas Kostkevicius, Domantas Ozerenskis
- **Comment**: submitted for review
- **Journal**: None
- **Summary**: 3D anatomical landmarks play an important role in health research. Their automated prediction/localization thus becomes a vital task. In this paper, we introduce a deformation method for 3D anatomical landmarks prediction. It utilizes a source model with anatomical landmarks which are annotated by clinicians, and deforms this model non-rigidly to match the target model. Two constraints are introduced in the optimization, which are responsible for alignment and smoothness, respectively. Experiments are performed on our dataset and the results demonstrate the robustness of our method, and show that it yields better performance than the state-of-the-art techniques in most cases.



### Artificial Intelligence For Breast Cancer Detection: Trends & Directions
- **Arxiv ID**: http://arxiv.org/abs/2110.00942v1
- **DOI**: 10.1016/j.compbiomed.2022.105221
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.00942v1)
- **Published**: 2021-10-03 07:22:21+00:00
- **Updated**: 2021-10-03 07:22:21+00:00
- **Authors**: Shahid Munir Shah, Rizwan Ahmed Khan, Sheeraz Arif, Unaiza Sajid
- **Comment**: None
- **Journal**: Computers in Biology and Medicine 2022
- **Summary**: In the last decade, researchers working in the domain of computer vision and Artificial Intelligence (AI) have beefed up their efforts to come up with the automated framework that not only detects but also identifies stage of breast cancer. The reason for this surge in research activities in this direction are mainly due to advent of robust AI algorithms (deep learning), availability of hardware that can train those robust and complex AI algorithms and accessibility of large enough dataset required for training AI algorithms. Different imaging modalities that have been exploited by researchers to automate the task of breast cancer detection are mammograms, ultrasound, magnetic resonance imaging, histopathological images or any combination of them. This article analyzes these imaging modalities and presents their strengths, limitations and enlists resources from where their datasets can be accessed for research purpose. This article then summarizes AI and computer vision based state-of-the-art methods proposed in the last decade, to detect breast cancer using various imaging modalities. Generally, in this article we have focused on to review frameworks that have reported results using mammograms as it is most widely used breast imaging modality that serves as first test that medical practitioners usually prescribe for the detection of breast cancer. Second reason of focusing on mammogram imaging modalities is the availability of its labeled datasets. Datasets availability is one of the most important aspect for the development of AI based frameworks as such algorithms are data hungry and generally quality of dataset affects performance of AI based algorithms. In a nutshell, this research article will act as a primary resource for the research community working in the field of automated breast imaging analysis.



### CDRNet: Accurate Cup-to-Disc Ratio Measurement with Tight Bounding Box Supervision in Fundus Photography Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.00943v2
- **DOI**: 10.1007/s11042-022-14183-2
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.00943v2)
- **Published**: 2021-10-03 07:23:40+00:00
- **Updated**: 2022-11-09 02:35:51+00:00
- **Authors**: Juan Wang, Bin Xia
- **Comment**: 18 pages, 7 figures, 8 tables, Multimedia Tools and Applications
- **Journal**: None
- **Summary**: The cup-to-disc ratio (CDR) is one of the most significant indicator for glaucoma diagnosis. Different from the use of costly fully supervised learning formulation with pixel-wise annotations in the literature, this study investigates the feasibility of accurate CDR measurement in fundus images using only tight bounding box supervision. For this purpose, we develop a two-task network named as CDRNet for accurate CDR measurement, one for weakly supervised image segmentation, and the other for bounding-box regression. The weakly supervised image segmentation task is implemented based on generalized multiple instance learning formulation and smooth maximum approximation, and the bounding-box regression task outputs class-specific bounding box prediction in a single scale at the original image resolution. To get accurate bounding box prediction, a class-specific bounding-box normalizer and an expected intersection-over-union are proposed. In the experiments, the proposed approach was evaluated by a testing set with 1200 images using CDR error and $F_1$ score for CDR measurement and dice coefficient for image segmentation. A grader study was conducted to compare the performance of the proposed approach with those of individual graders. The experimental results indicate that the proposed approach outperforms the state-of-the-art performance obtained from the fully supervised image segmentation (FSIS) approach using pixel-wise annotation for CDR measurement. Its performance is also better than those of individual graders. In addition, the proposed approach gets performance close to the state-of-the-art obtained from FSIS and the performance of individual graders for optic cup and disc segmentation. The codes are available at \url{https://github.com/wangjuan313/CDRNet}.



### Interactive Segmentation for COVID-19 Infection Quantification on Longitudinal CT scans
- **Arxiv ID**: http://arxiv.org/abs/2110.00948v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00948v2)
- **Published**: 2021-10-03 08:06:38+00:00
- **Updated**: 2023-06-01 08:55:54+00:00
- **Authors**: Michelle Xiao-Lin Foo, Seong Tae Kim, Magdalini Paschali, Leili Goli, Egon Burian, Marcus Makowski, Rickmer Braren, Nassir Navab, Thomas Wendler
- **Comment**: 10 pages, 11 figures, 4 tables
- **Journal**: None
- **Summary**: Consistent segmentation of COVID-19 patient's CT scans across multiple time points is essential to assess disease progression and response to therapy accurately. Existing automatic and interactive segmentation models for medical images only use data from a single time point (static). However, valuable segmentation information from previous time points is often not used to aid the segmentation of a patient's follow-up scans. Also, fully automatic segmentation techniques frequently produce results that would need further editing for clinical use. In this work, we propose a new single network model for interactive segmentation that fully utilizes all available past information to refine the segmentation of follow-up scans. In the first segmentation round, our model takes 3D volumes of medical images from two-time points (target and reference) as concatenated slices with the additional reference time point segmentation as a guide to segment the target scan. In subsequent segmentation refinement rounds, user feedback in the form of scribbles that correct the segmentation and the target's previous segmentation results are additionally fed into the model. This ensures that the segmentation information from previous refinement rounds is retained. Experimental results on our in-house multiclass longitudinal COVID-19 dataset show that the proposed model outperforms its static version and can assist in localizing COVID-19 infections in patient's follow-up scans.



### An Unsupervised Video Game Playstyle Metric via State Discretization
- **Arxiv ID**: http://arxiv.org/abs/2110.00950v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.00950v1)
- **Published**: 2021-10-03 08:30:51+00:00
- **Updated**: 2021-10-03 08:30:51+00:00
- **Authors**: Chiu-Chou Lin, Wei-Chen Chiu, I-Chen Wu
- **Comment**: This version was also published on UAI 2021
- **Journal**: 37th Conference on Uncertainty in Artificial Intelligence (UAI),
  2021
- **Summary**: On playing video games, different players usually have their own playstyles. Recently, there have been great improvements for the video game AIs on the playing strength. However, past researches for analyzing the behaviors of players still used heuristic rules or the behavior features with the game-environment support, thus being exhausted for the developers to define the features of discriminating various playstyles. In this paper, we propose the first metric for video game playstyles directly from the game observations and actions, without any prior specification on the playstyle in the target game. Our proposed method is built upon a novel scheme of learning discrete representations that can map game observations into latent discrete states, such that playstyles can be exhibited from these discrete states. Namely, we measure the playstyle distance based on game observations aligned to the same states. We demonstrate high playstyle accuracy of our metric in experiments on some video game platforms, including TORCS, RGSK, and seven Atari games, and for different agents including rule-based AI bots, learning-based AI bots, and human players.



### Graph Representation Learning for Spatial Image Steganalysis
- **Arxiv ID**: http://arxiv.org/abs/2110.00957v3
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00957v3)
- **Published**: 2021-10-03 09:09:08+00:00
- **Updated**: 2022-02-15 08:14:43+00:00
- **Authors**: Qiyun Liu, Hanzhou Wu
- **Comment**: https://scholar.google.com/citations?user=IdiF7M0AAAAJ&hl=en
- **Journal**: IEEE International Workshop on Multimedia Signal Processing (2022)
- **Summary**: In this paper, we introduce a graph representation learning architecture for spatial image steganalysis, which is motivated by the assumption that steganographic modifications unavoidably distort the statistical characteristics of the hidden graph features derived from cover images. In the detailed architecture, we translate each image to a graph, where nodes represent the patches of the image and edges indicate the local relationships between the patches. Each node is associated with a feature vector determined from the corresponding patch by a shallow convolutional neural network (CNN) structure. By feeding the graph to an attention network, the discriminative features can be learned for efficient steganalysis. Experiments indicate that the reported architecture achieves a competitive performance compared to the benchmark CNN model, which has shown the potential of graph learning for steganalysis.



### Fingerprint Matching using the Onion Peeling Approach and Turning Function
- **Arxiv ID**: http://arxiv.org/abs/2110.00958v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, 68-XX, F.0; A.0
- **Links**: [PDF](http://arxiv.org/pdf/2110.00958v1)
- **Published**: 2021-10-03 09:10:44+00:00
- **Updated**: 2021-10-03 09:10:44+00:00
- **Authors**: Nazanin Padkan, B. Sadeghi Bigham, Mohammad Reza Faraji
- **Comment**: None
- **Journal**: Gene Expression Patterns 47 (2023): 119299
- **Summary**: Fingerprint, as one of the most popular and robust biometric traits, can be used in automatic identification and verification systems to identify individuals. Fingerprint matching is a vital and challenging issue in fingerprint recognition systems. Most fingerprint matching algorithms are minutiae-based. The minutiae in fingerprints can be determined by their discontinuity. Ridge ending and ridge bifurcation are two frequently used minutiae in most fingerprint-based matching algorithms.   This paper presents a new minutiae-based fingerprint matching using the onion peeling approach. In the proposed method, fingerprints are aligned to find the matched minutiae points. Then, the nested convex polygons of matched minutiae points are constructed and the comparison between peer-to-peer polygons is performed by the turning function distance. Simplicity, accuracy, and low time complexity of the Onion peeling approach are three important factors that make it a standard method for fingerprint matching purposes. The performance of the proposed algorithm is evaluated on the database $FVC2002$. The results show that fingerprints of the same fingers have higher scores than different fingers. Since the fingerprints that the difference between the number of their layers is more than $2$ and the minutiae matching score lower than 0.15 are ignored, the better results are obtained.



### Translating Images into Maps
- **Arxiv ID**: http://arxiv.org/abs/2110.00966v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00966v2)
- **Published**: 2021-10-03 09:52:46+00:00
- **Updated**: 2022-03-30 17:27:23+00:00
- **Authors**: Avishkar Saha, Oscar Mendez Maldonado, Chris Russell, Richard Bowden
- **Comment**: Accepted to ICRA 2022
- **Journal**: None
- **Summary**: We approach instantaneous mapping, converting images to a top-down view of the world, as a translation problem. We show how a novel form of transformer network can be used to map from images and video directly to an overhead map or bird's-eye-view (BEV) of the world, in a single end-to-end network. We assume a 1-1 correspondence between a vertical scanline in the image, and rays passing through the camera location in an overhead map. This lets us formulate map generation from an image as a set of sequence-to-sequence translations. Posing the problem as translation allows the network to use the context of the image when interpreting the role of each pixel. This constrained formulation, based upon a strong physical grounding of the problem, leads to a restricted transformer network that is convolutional in the horizontal direction only. The structure allows us to make efficient use of data when training, and obtains state-of-the-art results for instantaneous mapping of three large-scale datasets, including a 15% and 30% relative gain against existing best performing methods on the nuScenes and Argoverse datasets, respectively. We make our code available on https://github.com/avishkarsaha/translating-images-into-maps.



### Semantic-Guided Zero-Shot Learning for Low-Light Image/Video Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2110.00970v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00970v4)
- **Published**: 2021-10-03 10:07:36+00:00
- **Updated**: 2021-11-25 13:40:06+00:00
- **Authors**: Shen Zheng, Gaurav Gupta
- **Comment**: Accepted by WACV 2022
- **Journal**: None
- **Summary**: Low-light images challenge both human perceptions and computer vision algorithms. It is crucial to make algorithms robust to enlighten low-light images for computational photography and computer vision applications such as real-time detection and segmentation. This paper proposes a semantic-guided zero-shot low-light enhancement network (SGZ) which is trained in the absence of paired images, unpaired datasets, and segmentation annotation. Firstly, we design an enhancement factor extraction network using depthwise separable convolution for an efficient estimate of the pixel-wise light deficiency of an low-light image. Secondly, we propose a recurrent image enhancement network to progressively enhance the low-light image with affordable model size. Finally, we introduce an unsupervised semantic segmentation network for preserving the semantic information during intensive enhancement. Extensive experiments on benchmark datasets and a low-light video demonstrate that our model outperforms the previous state-of-the-art. We further discuss the benefits of the proposed method for low-light detection and segmentation. Code is available at https://github.com/ShenZheng2000/Semantic-Guided-Low-Light-Image-Enhancement



### A Robust Scheme for 3D Point Cloud Copy Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.00972v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.00972v1)
- **Published**: 2021-10-03 10:10:07+00:00
- **Updated**: 2021-10-03 10:10:07+00:00
- **Authors**: Jiaqi Yang, Xuequan Lu, Wenzhi Chen
- **Comment**: submitted for review
- **Journal**: None
- **Summary**: Most existing 3D geometry copy detection research focused on 3D watermarking, which first embeds ``watermarks'' and then detects the added watermarks. However, this kind of methods is non-straightforward and may be less robust to attacks such as cropping and noise. In this paper, we focus on a fundamental and practical research problem: judging whether a point cloud is plagiarized or copied to another point cloud in the presence of several manipulations (e.g., similarity transformation, smoothing). We propose a novel method to address this critical problem. Our key idea is first to align the two point clouds and then calculate their similarity distance. We design three different measures to compute the similarity. We also introduce two strategies to speed up our method. Comprehensive experiments and comparisons demonstrate the effectiveness and robustness of our method in estimating the similarity of two given 3D point clouds.



### Landslide Detection in Real-Time Social Media Image Streams
- **Arxiv ID**: http://arxiv.org/abs/2110.04080v1
- **DOI**: 10.1007/s00521-023-08648-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04080v1)
- **Published**: 2021-10-03 10:52:19+00:00
- **Updated**: 2021-10-03 10:52:19+00:00
- **Authors**: Ferda Ofli, Muhammad Imran, Umair Qazi, Julien Roch, Catherine Pennington, Vanessa J. Banks, Remy Bossu
- **Comment**: Neural Comput & Applic (2023)
- **Journal**: None
- **Summary**: Lack of global data inventories obstructs scientific modeling of and response to landslide hazards which are oftentimes deadly and costly. To remedy this limitation, new approaches suggest solutions based on citizen science that requires active participation. However, as a non-traditional data source, social media has been increasingly used in many disaster response and management studies in recent years. Inspired by this trend, we propose to capitalize on social media data to mine landslide-related information automatically with the help of artificial intelligence (AI) techniques. Specifically, we develop a state-of-the-art computer vision model to detect landslides in social media image streams in real time. To that end, we create a large landslide image dataset labeled by experts and conduct extensive model training experiments. The experimental results indicate that the proposed model can be deployed in an online fashion to support global landslide susceptibility maps and emergency response.



### Adaptive Unfolding Total Variation Network for Low-Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2110.00984v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00984v4)
- **Published**: 2021-10-03 11:22:17+00:00
- **Updated**: 2021-10-07 00:54:18+00:00
- **Authors**: Chuanjun Zheng, Daming Shi, Wentian Shi
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Real-world low-light images suffer from two main degradations, namely, inevitable noise and poor visibility. Since the noise exhibits different levels, its estimation has been implemented in recent works when enhancing low-light images from raw Bayer space. When it comes to sRGB color space, the noise estimation becomes more complicated due to the effect of the image processing pipeline. Nevertheless, most existing enhancing algorithms in sRGB space only focus on the low visibility problem or suppress the noise under a hypothetical noise level, leading them impractical due to the lack of robustness. To address this issue,we propose an adaptive unfolding total variation network (UTVNet), which approximates the noise level from the real sRGB low-light image by learning the balancing parameter in the model-based denoising method with total variation regularization. Meanwhile, we learn the noise level map by unrolling the corresponding minimization process for providing the inferences of smoothness and fidelity constraints. Guided by the noise level map, our UTVNet can recover finer details and is more capable to suppress noise in real captured low-light scenes. Extensive experiments on real-world low-light images clearly demonstrate the superior performance of UTVNet over state-of-the-art methods.



### Keypoint Communities
- **Arxiv ID**: http://arxiv.org/abs/2110.00988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00988v1)
- **Published**: 2021-10-03 11:50:34+00:00
- **Updated**: 2021-10-03 11:50:34+00:00
- **Authors**: Duncan Zauss, Sven Kreiss, Alexandre Alahi
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: We present a fast bottom-up method that jointly detects over 100 keypoints on humans or objects, also referred to as human/object pose estimation. We model all keypoints belonging to a human or an object -- the pose -- as a graph and leverage insights from community detection to quantify the independence of keypoints. We use a graph centrality measure to assign training weights to different parts of a pose. Our proposed measure quantifies how tightly a keypoint is connected to its neighborhood. Our experiments show that our method outperforms all previous methods for human pose estimation with fine-grained keypoint annotations on the face, the hands and the feet with a total of 133 keypoints. We also show that our method generalizes to car poses.



### Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2110.00990v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00990v2)
- **Published**: 2021-10-03 11:59:37+00:00
- **Updated**: 2022-11-23 15:01:11+00:00
- **Authors**: Akash Sengupta, Ignas Budvytis, Roberto Cipolla
- **Comment**: ICCV 2021 (Edited to reduce file size)
- **Journal**: None
- **Summary**: This paper addresses the problem of 3D human body shape and pose estimation from an RGB image. This is often an ill-posed problem, since multiple plausible 3D bodies may match the visual evidence present in the input - particularly when the subject is occluded. Thus, it is desirable to estimate a distribution over 3D body shape and pose conditioned on the input image instead of a single 3D reconstruction. We train a deep neural network to estimate a hierarchical matrix-Fisher distribution over relative 3D joint rotation matrices (i.e. body pose), which exploits the human body's kinematic tree structure, as well as a Gaussian distribution over SMPL body shape parameters. To further ensure that the predicted shape and pose distributions match the visual evidence in the input image, we implement a differentiable rejection sampler to impose a reprojection loss between ground-truth 2D joint coordinates and samples from the predicted distributions, projected onto the image plane. We show that our method is competitive with the state-of-the-art in terms of 3D shape and pose metrics on the SSP-3D and 3DPW datasets, while also yielding a structured probability distribution over 3D body shape and pose, with which we can meaningfully quantify prediction uncertainty and sample multiple plausible 3D reconstructions to explain a given input image. Code is available at https://github.com/akashsengupta1997/HierarchicalProbabilistic3DHuman .



### Precise Object Placement with Pose Distance Estimations for Different Objects and Grippers
- **Arxiv ID**: http://arxiv.org/abs/2110.00992v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00992v1)
- **Published**: 2021-10-03 12:18:59+00:00
- **Updated**: 2021-10-03 12:18:59+00:00
- **Authors**: Kilian Kleeberger, Jonathan Schnitzler, Muhammad Usman Khalid, Richard Bormann, Werner Kraus, Marco F. Huber
- **Comment**: Accepted at 2021 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2021)
- **Journal**: None
- **Summary**: This paper introduces a novel approach for the grasping and precise placement of various known rigid objects using multiple grippers within highly cluttered scenes. Using a single depth image of the scene, our method estimates multiple 6D object poses together with an object class, a pose distance for object pose estimation, and a pose distance from a target pose for object placement for each automatically obtained grasp pose with a single forward pass of a neural network. By incorporating model knowledge into the system, our approach has higher success rates for grasping than state-of-the-art model-free approaches. Furthermore, our method chooses grasps that result in significantly more precise object placements than prior model-based work.



### Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2110.01013v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.01013v2)
- **Published**: 2021-10-03 14:31:46+00:00
- **Updated**: 2023-06-25 02:23:05+00:00
- **Authors**: Long Chen, Yuhang Zheng, Yulei Niu, Hanwang Zhang, Jun Xiao
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence, TPAMI
  2023. (Extension of CVPR'20 work). arXiv admin note: text overlap with
  arXiv:2003.06576
- **Journal**: None
- **Summary**: Today's VQA models still tend to capture superficial linguistic correlations in the training set and fail to generalize to the test set with different QA distributions. To reduce these language biases, recent VQA works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on diagnostic benchmarks for out-of-distribution testing. However, due to complex model design, these ensemble-based methods are unable to equip themselves with two indispensable characteristics of an ideal VQA model: 1) Visual-explainable: The model should rely on the right visual regions when making decisions. 2) Question-sensitive: The model should be sensitive to the linguistic variations in questions. To this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing and Training (CSST) strategy. After training with CSST, VQA models are forced to focus on all critical objects and words, which significantly improves both visual-explainable and question-sensitive abilities. Specifically, CSST is composed of two parts: Counterfactual Samples Synthesizing (CSS) and Counterfactual Samples Training (CST). CSS generates counterfactual samples by carefully masking critical objects in images or words in questions and assigning pseudo ground-truth answers. CST not only trains the VQA models with both complementary samples to predict respective ground-truth answers, but also urges the VQA models to further distinguish the original samples and superficially similar counterfactual ones. To facilitate the CST training, we propose two variants of supervised contrastive loss for VQA, and design an effective positive and negative sample selection mechanism based on CSS. Extensive experiments have shown the effectiveness of CSST. Particularly, by building on top of model LMH+SAR, we achieve record-breaking performance on all OOD benchmarks.



### EAR-U-Net: EfficientNet and attention-based residual U-Net for automatic liver segmentation in CT
- **Arxiv ID**: http://arxiv.org/abs/2110.01014v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01014v1)
- **Published**: 2021-10-03 14:43:18+00:00
- **Updated**: 2021-10-03 14:43:18+00:00
- **Authors**: Jinke Wang, Xiangyang Zhang, Peiqing Lv, Lubiao Zhou, Haiying Wang
- **Comment**: 26 pages
- **Journal**: None
- **Summary**: Purpose: This paper proposes a new network framework called EAR-U-Net, which leverages EfficientNetB4, attention gate, and residual learning techniques to achieve automatic and accurate liver segmentation. Methods: The proposed method is based on the U-Net framework. First, we use EfficientNetB4 as the encoder to extract more feature information during the encoding stage. Then, an attention gate is introduced in the skip connection to eliminate irrelevant regions and highlight features of a specific segmentation task. Finally, to alleviate the problem of gradient vanishment, we replace the traditional convolution of the decoder with a residual block to improve the segmentation accuracy. Results: We verified the proposed method on the LiTS17 and SLiver07 datasets and compared it with classical networks such as FCN, U-Net, Attention U-Net, and Attention Res-U-Net. In the Sliver07 evaluation, the proposed method achieved the best segmentation performance on all five standard metrics. Meanwhile, in the LiTS17 assessment, the best performance is obtained except for a slight inferior on RVD. Moreover, we also participated in the MICCIA-LiTS17 challenge, and the Dice per case score was 0.952. Conclusion: The proposed method's qualitative and quantitative results demonstrated its applicability in liver segmentation and proved its good prospect in computer-assisted liver segmentation.



### Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction
- **Arxiv ID**: http://arxiv.org/abs/2110.01015v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.01015v1)
- **Published**: 2021-10-03 14:46:08+00:00
- **Updated**: 2021-10-03 14:46:08+00:00
- **Authors**: Rishubh Parihar, Gaurav Ramola, Ranajit Saha, Ravi Kini, Aniket Rege, Sudha Velusamy
- **Comment**: 10 pages, 5 figures, 4 tables, ICCV Workshops 2021 - SRVU
- **Journal**: None
- **Summary**: Ever-increasing smartphone-generated video content demands intelligent techniques to edit and enhance videos on power-constrained devices. Most of the best performing algorithms for video understanding tasks like action recognition, localization, etc., rely heavily on rich spatio-temporal representations to make accurate predictions. For effective learning of the spatio-temporal representation, it is crucial to understand the underlying object motion patterns present in the video. In this paper, we propose a novel approach for understanding object motions via motion type classification. The proposed motion type classifier predicts a motion type for the video based on the trajectories of the objects present. Our classifier assigns a motion type for the given video from the following five primitive motion classes: linear, projectile, oscillatory, local and random. We demonstrate that the representations learned from the motion type classification generalizes well for the challenging downstream task of video retrieval. Further, we proposed a recommendation system for video playback style based on the motion type classifier predictions.



### Classification of Viral Pneumonia X-ray Images with the Aucmedi Framework
- **Arxiv ID**: http://arxiv.org/abs/2110.01017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01017v1)
- **Published**: 2021-10-03 14:57:43+00:00
- **Updated**: 2021-10-03 14:57:43+00:00
- **Authors**: Pia Schneider, Dominik MÃ¼ller, Frank Kramer
- **Comment**: 7 pages, open manuscript, student submission
- **Journal**: None
- **Summary**: In this work we use the AUCMEDI-Framework to train a deep neural network to classify chest X-ray images as either normal or viral pneumonia. Stratified k-fold cross-validation with k=3 is used to generate the validation-set and 15% of the data are set aside for the evaluation of the models of the different folds and ensembles each. A random-forest ensemble as well as a Soft-Majority-Vote ensemble are built from the predictions of the different folds. Evaluation metrics (Classification-Report, macro f1-scores, Confusion-Matrices, ROC-Curves) of the individual folds and the ensembles show that the classifier works well. Finally Grad-CAM and LIME explainable artificial intelligence (XAI) algorithms are applied to visualize the image features that are most important for the prediction. For Grad-CAM the heatmaps of the three folds are furthermore averaged for all images in order to calculate a mean XAI-heatmap. As the heatmaps of the different folds for most images differ only slightly this averaging procedure works well. However, only medical professionals can evaluate the quality of the features marked by the XAI. A comparison of the evaluation metrics with metrics of standard procedures such as PCR would also be important. Further limitations are discussed.



### DARDet: A Dense Anchor-free Rotated Object Detector in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2110.01025v1
- **DOI**: 10.1109/LGRS.2021.3122924
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01025v1)
- **Published**: 2021-10-03 15:28:14+00:00
- **Updated**: 2021-10-03 15:28:14+00:00
- **Authors**: Feng Zhang, Xueying Wang, Shilin Zhou, Yingqian Wang
- **Comment**: Code is available at https://github.com/zf020114/DARDet
- **Journal**: None
- **Summary**: Rotated object detection in aerial images has received increasing attention for a wide range of applications. However, it is also a challenging task due to the huge variations of scale, rotation, aspect ratio, and densely arranged targets. Most existing methods heavily rely on a large number of pre-defined anchors with different scales, angles, and aspect ratios, and are optimized with a distance loss. Therefore, these methods are sensitive to anchor hyper-parameters and easily suffer from performance degradation caused by boundary discontinuity. To handle this problem, in this paper, we propose a dense anchor-free rotated object detector (DARDet) for rotated object detection in aerial images. Our DARDet directly predicts five parameters of rotated boxes at each foreground pixel of feature maps. We design a new alignment convolution module to extracts aligned features and introduce a PIoU loss for precise and stable regression. Our method achieves state-of-the-art performance on three commonly used aerial objects datasets (i.e., DOTA, HRSC2016, and UCAS-AOD) while keeping high efficiency. Code is available at https://github.com/zf020114/DARDet.



### Universal Face Restoration With Memorized Modulation
- **Arxiv ID**: http://arxiv.org/abs/2110.01033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01033v1)
- **Published**: 2021-10-03 15:55:07+00:00
- **Updated**: 2021-10-03 15:55:07+00:00
- **Authors**: Jia Li, Huaibo Huang, Xiaofei Jia, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: Blind face restoration (BFR) is a challenging problem because of the uncertainty of the degradation patterns. This paper proposes a Restoration with Memorized Modulation (RMM) framework for universal BFR in diverse degraded scenes and heterogeneous domains. We apply random noise as well as unsupervised wavelet memory to adaptively modulate the face-enhancement generator, considering attentional denormalization in both layer and instance levels. Specifically, in the training stage, the low-level spatial feature embedding, the wavelet memory embedding obtained by wavelet transformation of the high-resolution image, as well as the disentangled high-level noise embeddings are integrated, with the guidance of attentional maps generated from layer normalization, instance normalization, and the original feature map. These three embeddings are respectively associated with the spatial content, high-frequency texture details, and a learnable universal prior against other blind image degradation patterns. We store the spatial feature of the low-resolution image and the corresponding wavelet style code as key and value in the memory unit, respectively. In the test stage, the wavelet memory value whose corresponding spatial key is the most matching with that of the inferred image is retrieved to modulate the generator. Moreover, the universal prior learned from the random noise has been memorized by training the modulation network. Experimental results show the superiority of the proposed method compared with the state-of-the-art methods, and a good generalization in the wild.



### RAP-Net: Region Attention Predictive Network for Precipitation Nowcasting
- **Arxiv ID**: http://arxiv.org/abs/2110.01035v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2110.01035v1)
- **Published**: 2021-10-03 15:55:18+00:00
- **Updated**: 2021-10-03 15:55:18+00:00
- **Authors**: Chuyao Luo, ZhengZhang, Rui Ye, Xutao Li, Yunming Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Natural disasters caused by heavy rainfall often cost huge loss of life and property. To avoid it, the task of precipitation nowcasting is imminent. To solve the problem, increasingly deep learning methods are proposed to forecast future radar echo images and then the predicted maps have converted the distribution of rainfall. The prevailing spatiotemporal sequence prediction methods apply ConvRNN structure which combines the Convolution and Recurrent neural network. Although improvements based on ConvRNN achieve remarkable success, these methods ignore capturing both local and global spatial features simultaneously, which degrades the nowcasting in the region of heavy rainfall. To address this issue, we proposed the Region Attention Block (RAB) and embed it into ConvRNN to enhance the forecast in the area with strong rainfall. Besides, the ConvRNN models are hard to memory longer history representations with limited parameters. Considering it, we propose Recall Attention Mechanism (RAM) to improve the prediction. By preserving longer temporal information, RAM contributes to the forecasting, especially in the middle rainfall intensity. The experiments show that the proposed model Region Attention Predictive Network (RAP-Net) has outperformed the state-of-art method.



### Marginally calibrated response distributions for end-to-end learning in autonomous driving
- **Arxiv ID**: http://arxiv.org/abs/2110.01050v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.01050v1)
- **Published**: 2021-10-03 17:19:12+00:00
- **Updated**: 2021-10-03 17:19:12+00:00
- **Authors**: Clara Hoffmann, Nadja Klein
- **Comment**: 17 pages, 9 figures
- **Journal**: None
- **Summary**: End-to-end learners for autonomous driving are deep neural networks that predict the instantaneous steering angle directly from images of the ahead-lying street. These learners must provide reliable uncertainty estimates for their predictions in order to meet safety requirements and initiate a switch to manual control in areas of high uncertainty. Yet end-to-end learners typically only deliver point predictions, since distributional predictions are associated with large increases in training time or additional computational resources during prediction. To address this shortcoming we investigate efficient and scalable approximate inference for the implicit copula neural linear model of Klein, Nott and Smith (2021) in order to quantify uncertainty for the predictions of end-to-end learners. The result are densities for the steering angle that are marginally calibrated, i.e.~the average of the estimated densities equals the empirical distribution of steering angles. To ensure the scalability to large $n$ regimes, we develop efficient estimation based on variational inference as a fast alternative to computationally intensive, exact inference via Hamiltonian Monte Carlo. We demonstrate the accuracy and speed of the variational approach in comparison to Hamiltonian Monte Carlo on two end-to-end learners trained for highway driving using the comma2k19 data set. The implicit copula neural linear model delivers accurate calibration, high-quality prediction intervals and allows to identify overconfident learners. Our approach also contributes to the explainability of black-box end-to-end learners, since predictive densities can be used to understand which steering actions the end-to-end learner sees as valid.



### Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control
- **Arxiv ID**: http://arxiv.org/abs/2110.01052v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ME, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.01052v5)
- **Published**: 2021-10-03 17:42:03+00:00
- **Updated**: 2022-09-29 23:51:57+00:00
- **Authors**: Anastasios N. Angelopoulos, Stephen Bates, Emmanuel J. CandÃ¨s, Michael I. Jordan, Lihua Lei
- **Comment**: Code available at https://github.com/aangelopoulos/ltt
- **Journal**: None
- **Summary**: We introduce a framework for calibrating machine learning models so that their predictions satisfy explicit, finite-sample statistical guarantees. Our calibration algorithms work with any underlying model and (unknown) data-generating distribution and do not require model refitting. The framework addresses, among other examples, false discovery rate control in multi-label classification, intersection-over-union control in instance segmentation, and the simultaneous control of the type-1 error of outlier detection and confidence set coverage in classification or regression. Our main insight is to reframe the risk-control problem as multiple hypothesis testing, enabling techniques and mathematical arguments different from those in the previous literature. We use the framework to provide new calibration methods for several core machine learning tasks, with detailed worked examples in computer vision and tabular medical data.



### A New Approach for Image Authentication Framework for Media Forensics Purpose
- **Arxiv ID**: http://arxiv.org/abs/2110.01065v1
- **DOI**: 10.4172/2324-9307.1000208
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01065v1)
- **Published**: 2021-10-03 18:31:37+00:00
- **Updated**: 2021-10-03 18:31:37+00:00
- **Authors**: Ahmad M Nagm, Khaled Y Youssef, Mohammad I Youssef
- **Comment**: 11 pages, 19 figures and one table
- **Journal**: Journal of Computer Engineering & Information Technology, 2017,
  6:6
- **Summary**: With the increasing widely spread digital media become using in most fields such as medical care, Oceanography, Exploration processing, security purpose, military fields and astronomy, evidence in criminals and more vital fields and then digital Images become have different appreciation values according to what is important of carried information by digital images. Due to the easy manipulation property of digital images (by proper computer software) makes us doubtful when are juries using digital images as forensic evidence in courts, especially, if the digital images are main evidence to demonstrate the relationship between suspects and the criminals. Obviously, here demonstrate importance of data Originality Protection methods to detect unauthorized process like modification or duplication and then enhancement protection of evidence to guarantee rights of incriminatory. In this paper, we shall introduce a novel digital forensic security framework for digital image authentication and originality identification techniques and related methodologies, algorithms and protocols that are applied on camera captured images. The approach depends on implanting secret code into RGB images that should indicate any unauthorized modification on the image under investigation. The secret code generation depends mainly on two main parameter types, namely the image characteristics and capturing device identifier. In this paper, the architecture framework will be analyzed, explained and discussed together with the associated protocols, algorithms and methodologies. Also, the secret code deduction and insertion techniques will be analyzed and discussed, in addition to the image benchmarking and quality testing techniques.




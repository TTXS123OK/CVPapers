# Arxiv Papers in cs.CV on 2021-10-29
### Adaptive Hierarchical Similarity Metric Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2111.00006v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.00006v1)
- **Published**: 2021-10-29 02:12:18+00:00
- **Updated**: 2021-10-29 02:12:18+00:00
- **Authors**: Jiexi Yan, Lei Luo, Cheng Deng, Heng Huang
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Deep Metric Learning (DML) plays a critical role in various machine learning tasks. However, most existing deep metric learning methods with binary similarity are sensitive to noisy labels, which are widely present in real-world data. Since these noisy labels often cause severe performance degradation, it is crucial to enhance the robustness and generalization ability of DML. In this paper, we propose an Adaptive Hierarchical Similarity Metric Learning method. It considers two noise-insensitive information, \textit{i.e.}, class-wise divergence and sample-wise consistency. Specifically, class-wise divergence can effectively excavate richer similarity information beyond binary in modeling by taking advantage of Hyperbolic metric learning, while sample-wise consistency can further improve the generalization ability of the model using contrastive augmentation. More importantly, we design an adaptive strategy to integrate this information in a unified view. It is noteworthy that the new method can be extended to any pair-based metric loss. Extensive experimental results on benchmark datasets demonstrate that our method achieves state-of-the-art performance compared with current deep metric learning approaches.



### Unsupervised Foreground Extraction via Deep Region Competition
- **Arxiv ID**: http://arxiv.org/abs/2110.15497v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.15497v3)
- **Published**: 2021-10-29 02:32:44+00:00
- **Updated**: 2021-12-25 14:18:17+00:00
- **Authors**: Peiyu Yu, Sirui Xie, Xiaojian Ma, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: We present Deep Region Competition (DRC), an algorithm designed to extract foreground objects from images in a fully unsupervised manner. Foreground extraction can be viewed as a special case of generic image segmentation that focuses on identifying and disentangling objects from the background. In this work, we rethink the foreground extraction by reconciling energy-based prior with generative image modeling in the form of Mixture of Experts (MoE), where we further introduce the learned pixel re-assignment as the essential inductive bias to capture the regularities of background regions. With this modeling, the foreground-background partition can be naturally found through Expectation-Maximization (EM). We show that the proposed method effectively exploits the interaction between the mixture components during the partitioning process, which closely connects to region competition, a seminal approach for generic image segmentation. Experiments demonstrate that DRC exhibits more competitive performances on complex real-world data and challenging multi-object scenes compared with prior methods. Moreover, we show empirically that DRC can potentially generalize to novel foreground objects even from categories unseen during training.



### UDIS: Unsupervised Discovery of Bias in Deep Visual Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/2110.15499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15499v1)
- **Published**: 2021-10-29 02:36:37+00:00
- **Updated**: 2021-10-29 02:36:37+00:00
- **Authors**: Arvindkumar Krishnakumar, Viraj Prabhu, Sruthi Sudhakar, Judy Hoffman
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have been shown to learn spurious correlations from data that sometimes lead to systematic failures for certain subpopulations. Prior work has typically diagnosed this by crowdsourcing annotations for various protected attributes and measuring performance, which is both expensive to acquire and difficult to scale. In this work, we propose UDIS, an unsupervised algorithm for surfacing and analyzing such failure modes. UDIS identifies subpopulations via hierarchical clustering of dataset embeddings and surfaces systematic failure modes by visualizing low performing clusters along with their gradient-weighted class-activation maps. We show the effectiveness of UDIS in identifying failure modes in models trained for image classification on the CelebA and MSCOCO datasets.



### Domain Agnostic Few-Shot Learning For Document Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2111.00007v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.00007v1)
- **Published**: 2021-10-29 03:19:31+00:00
- **Updated**: 2021-10-29 03:19:31+00:00
- **Authors**: Jaya Krishna Mandivarapu, Eric bunch, Glenn fung
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning aims to generalize to novel classes with only a few samples with class labels. Research in few-shot learning has borrowed techniques from transfer learning, metric learning, meta-learning, and Bayesian methods. These methods also aim to train models from limited training samples, and while encouraging performance has been achieved, they often fail to generalize to novel domains. Many of the existing meta-learning methods rely on training data for which the base classes are sampled from the same domain as the novel classes used for meta-testing. However, in many applications in the industry, such as document classification, collecting large samples of data for meta-learning is infeasible or impossible. While research in the field of the cross-domain few-shot learning exists, it is mostly limited to computer vision. To our knowledge, no work yet exists that examines the use of few-shot learning for classification of semi-structured documents (scans of paper documents) generated as part of a business workflow (forms, letters, bills, etc.). Here the domain shift is significant, going from natural images to the semi-structured documents of interest. In this work, we address the problem of few-shot document image classification under domain shift. We evaluate our work by extensive comparisons with existing methods. Experimental results demonstrate that the proposed method shows consistent improvements on the few-shot classification performance under domain shift.



### PEDENet: Image Anomaly Localization via Patch Embedding and Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.15525v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.15525v1)
- **Published**: 2021-10-29 03:52:56+00:00
- **Updated**: 2021-10-29 03:52:56+00:00
- **Authors**: Kaitai Zhang, Bin Wang, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: A neural network targeting at unsupervised image anomaly localization, called the PEDENet, is proposed in this work. PEDENet contains a patch embedding (PE) network, a density estimation (DE) network, and an auxiliary network called the location prediction (LP) network. The PE network takes local image patches as input and performs dimension reduction to get low-dimensional patch embeddings via a deep encoder structure. Being inspired by the Gaussian Mixture Model (GMM), the DE network takes those patch embeddings and then predicts the cluster membership of an embedded patch. The sum of membership probabilities is used as a loss term to guide the learning process. The LP network is a Multi-layer Perception (MLP), which takes embeddings from two neighboring patches as input and predicts their relative location. The performance of the proposed PEDENet is evaluated extensively and benchmarked with that of state-of-the-art methods.



### On Cross-Layer Alignment for Model Fusion of Heterogeneous Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.15538v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15538v3)
- **Published**: 2021-10-29 05:02:23+00:00
- **Updated**: 2023-02-20 04:41:24+00:00
- **Authors**: Dang Nguyen, Trang Nguyen, Khai Nguyen, Dinh Phung, Hung Bui, Nhat Ho
- **Comment**: Accepted to ICASSP 2023, 30 pages, 4 figures, 21 tables
- **Journal**: None
- **Summary**: Layer-wise model fusion via optimal transport, named OTFusion, applies soft neuron association for unifying different pre-trained networks to save computational resources. While enjoying its success, OTFusion requires the input networks to have the same number of layers. To address this issue, we propose a novel model fusion framework, named CLAFusion, to fuse neural networks with a different number of layers, which we refer to as heterogeneous neural networks, via cross-layer alignment. The cross-layer alignment problem, which is an unbalanced assignment problem, can be solved efficiently using dynamic programming. Based on the cross-layer alignment, our framework balances the number of layers of neural networks before applying layer-wise model fusion. Our experiments indicate that CLAFusion, with an extra finetuning process, improves the accuracy of residual networks on the CIFAR10, CIFAR100, and Tiny-ImageNet datasets. Furthermore, we explore its practical usage for model compression and knowledge distillation when applying to the teacher-student setting.



### Latent Cognizance: What Machine Really Learns
- **Arxiv ID**: http://arxiv.org/abs/2110.15548v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15548v1)
- **Published**: 2021-10-29 05:26:38+00:00
- **Updated**: 2021-10-29 05:26:38+00:00
- **Authors**: Pisit Nakjai, Jiradej Ponsawat, Tatpong Katanyukul
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Despite overwhelming achievements in recognition accuracy, extending an open-set capability -- ability to identify when the question is out of scope -- remains greatly challenging in a scalable machine learning inference. A recent research has discovered Latent Cognizance (LC) -- an insight on a recognition mechanism based on a new probabilistic interpretation, Bayesian theorem, and an analysis of an internal structure of a commonly-used recognition inference structure. The new interpretation emphasizes a latent assumption of an overlooked probabilistic condition on a learned inference model. Viability of LC has been shown on a task of sign language recognition, but its potential and implication can reach far beyond a specific domain and can move object recognition toward a scalable open-set recognition. However, LC new probabilistic interpretation has not been directly investigated. This article investigates the new interpretation under a traceable context. Our findings support the rationale on which LC is based and reveal a hidden mechanism underlying the learning classification inference. The ramification of these findings could lead to a simple yet effective solution to an open-set recognition.



### AI-Powered Semantic Segmentation and Fluid Volume Calculation of Lung CT images in Covid-19 Patients
- **Arxiv ID**: http://arxiv.org/abs/2110.15558v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T10 (Primary)
- **Links**: [PDF](http://arxiv.org/pdf/2110.15558v1)
- **Published**: 2021-10-29 05:50:57+00:00
- **Updated**: 2021-10-29 05:50:57+00:00
- **Authors**: Sabeerali K. P, Saleena T. S, Dr. Muhamed Ilyas P, Dr. Neha Mohan
- **Comment**: https://www.uietkuk.ac.in/etbs2021/wp-content/uploads/2021/02/Special-Session-Proposal-ETBS-2021.doc
- **Journal**: None
- **Summary**: COVID-19 pandemic is a deadly disease spreading very fast. People with the confronted immune system are susceptible to many health conditions. A highly significant condition is pneumonia, which is found to be the cause of death in the majority of patients. The main purpose of this study is to find the volume of GGO and consolidation of a covid-19 patient so that the physicians can prioritize the patients. Here we used transfer learning techniques for segmentation of lung CTs with the latest libraries and techniques which reduces training time and increases the accuracy of the AI Model. This system is trained with DeepLabV3+ network architecture and model Resnet50 with Imagenet weights. We used different augmentation techniques like Gaussian Noise, Horizontal shift, color variation, etc to get to the result. Intersection over Union(IoU) is used as the performance metrics. The IoU of lung masks is predicted as 99.78% and that of infected masks is as 89.01%. Our work effectively measures the volume of infected region by calculating the volume of infected and lung mask region of the patients.



### Exposing Deepfake with Pixel-wise AR and PPG Correlation from Faint Signals
- **Arxiv ID**: http://arxiv.org/abs/2110.15561v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15561v1)
- **Published**: 2021-10-29 06:05:52+00:00
- **Updated**: 2021-10-29 06:05:52+00:00
- **Authors**: Maoyu Mao, Jun Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfake poses a serious threat to the reliability of judicial evidence and intellectual property protection. In spite of an urgent need for Deepfake identification, existing pixel-level detection methods are increasingly unable to resist the growing realism of fake videos and lack generalization. In this paper, we propose a scheme to expose Deepfake through faint signals hidden in face videos. This scheme extracts two types of minute information hidden between face pixels-photoplethysmography (PPG) features and auto-regressive (AR) features, which are used as the basis for forensics in the temporal and spatial domains, respectively. According to the principle of PPG, tracking the absorption of light by blood cells allows remote estimation of the temporal domains heart rate (HR) of face video, and irregular HR fluctuations can be seen as traces of tampering. On the other hand, AR coefficients are able to reflect the inter-pixel correlation, and can also reflect the traces of smoothing caused by up-sampling in the process of generating fake faces. Furthermore, the scheme combines asymmetric convolution block (ACBlock)-based improved densely connected networks (DenseNets) to achieve face video authenticity forensics. Its asymmetric convolutional structure enhances the robustness of network to the input feature image upside-down and left-right flipping, so that the sequence of feature stitching does not affect detection results. Simulation results show that our proposed scheme provides more accurate authenticity detection results on multiple deep forgery datasets and has better generalization compared to the benchmark strategy.



### Unsupervised PET Reconstruction from a Bayesian Perspective
- **Arxiv ID**: http://arxiv.org/abs/2110.15568v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15568v1)
- **Published**: 2021-10-29 06:32:21+00:00
- **Updated**: 2021-10-29 06:32:21+00:00
- **Authors**: Chenyu Shen, Wenjun Xia, Hongwei Ye, Mingzheng Hou, Hu Chen, Yan Liu, Jiliu Zhou, Yi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Positron emission tomography (PET) reconstruction has become an ill-posed inverse problem due to low-count projection data, and a robust algorithm is urgently required to improve imaging quality. Recently, the deep image prior (DIP) has drawn much attention and has been successfully applied in several image restoration tasks, such as denoising and inpainting, since it does not need any labels (reference image). However, overfitting is a vital defect of this framework. Hence, many methods have been proposed to mitigate this problem, and DeepRED is a typical representation that combines DIP and regularization by denoising (RED). In this article, we leverage DeepRED from a Bayesian perspective to reconstruct PET images from a single corrupted sinogram without any supervised or auxiliary information. In contrast to the conventional denoisers customarily used in RED, a DnCNN-like denoiser, which can add an adaptive constraint to DIP and facilitate the computation of derivation, is employed. Moreover, to further enhance the regularization, Gaussian noise is injected into the gradient updates, deriving a Markov chain Monte Carlo (MCMC) sampler. Experimental studies on brain and whole-body datasets demonstrate that our proposed method can achieve better performance in terms of qualitative and quantitative results compared to several classic and state-of-the-art methods.



### Novel View Synthesis from a Single Image via Unsupervised learning
- **Arxiv ID**: http://arxiv.org/abs/2110.15569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15569v1)
- **Published**: 2021-10-29 06:32:49+00:00
- **Updated**: 2021-10-29 06:32:49+00:00
- **Authors**: Bingzheng Liu, Jianjun Lei, Bo Peng, Chuanbo Yu, Wanqing Li, Nam Ling
- **Comment**: 9 pages, submitted to TCSVT
- **Journal**: None
- **Summary**: View synthesis aims to generate novel views from one or more given source views. Although existing methods have achieved promising performance, they usually require paired views of different poses to learn a pixel transformation. This paper proposes an unsupervised network to learn such a pixel transformation from a single source viewpoint. In particular, the network consists of a token transformation module (TTM) that facilities the transformation of the features extracted from a source viewpoint image into an intrinsic representation with respect to a pre-defined reference pose and a view generation module (VGM) that synthesizes an arbitrary view from the representation. The learned transformation allows us to synthesize a novel view from any single source viewpoint image of unknown pose. Experiments on the widely used view synthesis datasets have demonstrated that the proposed network is able to produce comparable results to the state-of-the-art methods despite the fact that learning is unsupervised and only a single source viewpoint image is required for generating a novel view. The code will be available soon.



### ST-ABN: Visual Explanation Taking into Account Spatio-temporal Information for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.15574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15574v1)
- **Published**: 2021-10-29 06:40:53+00:00
- **Updated**: 2021-10-29 06:40:53+00:00
- **Authors**: Masahiro Mitsuhara, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi
- **Comment**: 15 pages, 3 figures
- **Journal**: None
- **Summary**: It is difficult for people to interpret the decision-making in the inference process of deep neural networks. Visual explanation is one method for interpreting the decision-making of deep learning. It analyzes the decision-making of 2D CNNs by visualizing an attention map that highlights discriminative regions. Visual explanation for interpreting the decision-making process in video recognition is more difficult because it is necessary to consider not only spatial but also temporal information, which is different from the case of still images. In this paper, we propose a visual explanation method called spatio-temporal attention branch network (ST-ABN) for video recognition. It enables visual explanation for both spatial and temporal information. ST-ABN acquires the importance of spatial and temporal information during network inference and applies it to recognition processing to improve recognition performance and visual explainability. Experimental results with Something-Something datasets V1 \& V2 demonstrated that ST-ABN enables visual explanation that takes into account spatial and temporal information simultaneously and improves recognition performance.



### Whole Brain Segmentation with Full Volume Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2110.15601v1
- **DOI**: 10.1016/j.compmedimag.2021.101991
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15601v1)
- **Published**: 2021-10-29 08:00:14+00:00
- **Updated**: 2021-10-29 08:00:14+00:00
- **Authors**: Yeshu Li, Jonathan Cui, Yilun Sheng, Xiao Liang, Jingdong Wang, Eric I-Chao Chang, Yan Xu
- **Comment**: Accepted to CMIG
- **Journal**: Computerized Medical Imaging and Graphics, Volume 93, October
  2021, 101991
- **Summary**: Whole brain segmentation is an important neuroimaging task that segments the whole brain volume into anatomically labeled regions-of-interest. Convolutional neural networks have demonstrated good performance in this task. Existing solutions, usually segment the brain image by classifying the voxels, or labeling the slices or the sub-volumes separately. Their representation learning is based on parts of the whole volume whereas their labeling result is produced by aggregation of partial segmentation. Learning and inference with incomplete information could lead to sub-optimal final segmentation result. To address these issues, we propose to adopt a full volume framework, which feeds the full volume brain image into the segmentation network and directly outputs the segmentation result for the whole brain volume. The framework makes use of complete information in each volume and can be implemented easily. An effective instance in this framework is given subsequently. We adopt the $3$D high-resolution network (HRNet) for learning spatially fine-grained representations and the mixed precision training scheme for memory-efficient training. Extensive experiment results on a publicly available $3$D MRI brain dataset show that our proposed model advances the state-of-the-art methods in terms of segmentation performance. Source code is publicly available at https://github.com/microsoft/VoxHRNet.



### Improving Camouflaged Object Detection with the Uncertainty of Pseudo-edge Labels
- **Arxiv ID**: http://arxiv.org/abs/2110.15606v1
- **DOI**: 10.1145/3469877.3490587
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15606v1)
- **Published**: 2021-10-29 08:15:47+00:00
- **Updated**: 2021-10-29 08:15:47+00:00
- **Authors**: Nobukatsu Kajiura, Hong Liu, Shin'ichi Satoh
- **Comment**: Accepted to ACM Multimedia Asia 2021
- **Journal**: None
- **Summary**: This paper focuses on camouflaged object detection (COD), which is a task to detect objects hidden in the background. Most of the current COD models aim to highlight the target object directly while outputting ambiguous camouflaged boundaries. On the other hand, the performance of the models considering edge information is not yet satisfactory. To this end, we propose a new framework that makes full use of multiple visual cues, i.e., saliency as well as edges, to refine the predicted camouflaged map. This framework consists of three key components, i.e., a pseudo-edge generator, a pseudo-map generator, and an uncertainty-aware refinement module. In particular, the pseudo-edge generator estimates the boundary that outputs the pseudo-edge label, and the conventional COD method serves as the pseudo-map generator that outputs the pseudo-map label. Then, we propose an uncertainty-based module to reduce the uncertainty and noise of such two pseudo labels, which takes both pseudo labels as input and outputs an edge-accurate camouflaged map. Experiments on various COD datasets demonstrate the effectiveness of our method with superior performance to the existing state-of-the-art methods.



### BiC-Net: Learning Efficient Spatio-Temporal Relation for Text-Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2110.15609v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2110.15609v3)
- **Published**: 2021-10-29 08:23:40+00:00
- **Updated**: 2022-06-01 11:49:41+00:00
- **Authors**: Ning Han, Jingjing Chen, Chuhao Shi, Yawen Zeng, Guangyi Xiao, Hao Chen
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: The task of text-video retrieval aims to understand the correspondence between language and vision, has gained increasing attention in recent years. Previous studies either adopt off-the-shelf 2D/3D-CNN and then use average/max pooling to directly capture spatial features with aggregated temporal information as global video embeddings, or introduce graph-based models and expert knowledge to learn local spatial-temporal relations. However, the existing methods have two limitations: 1) The global video representations learn video temporal information in a simple average/max pooling manner and do not fully explore the temporal information between every two frames. 2) The graph-based local video representations are handcrafted, it depends heavily on expert knowledge and empirical feedback, which may not be able to effectively mine the higher-level fine-grained visual relations. These limitations result in their inability to distinguish videos with the same visual components but with different relations. To solve this problem, we propose a novel cross-modal retrieval framework, Bi-Branch Complementary Network (BiC-Net), which modifies transformer architecture to effectively bridge text-video modalities in a complementary manner via combining local spatial-temporal relation and global temporal information. Specifically, local video representations are encoded using multiple transformer blocks and additional residual blocks to learn spatio-temporal relation features, calling the module a Spatio-Temporal Residual transformer (SRT). Meanwhile, Global video representations are encoded using a multi-layer transformer block to learn global temporal features. Finally, we align the spatio-temporal relation and global temporal features with the text feature on two embedding spaces for cross-modal text-video retrieval.



### Unsupervised Person Re-Identification with Wireless Positioning under Weak Scene Labeling
- **Arxiv ID**: http://arxiv.org/abs/2110.15610v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15610v2)
- **Published**: 2021-10-29 08:25:44+00:00
- **Updated**: 2023-04-05 11:07:23+00:00
- **Authors**: Yiheng Liu, Wengang Zhou, Qiaokang Xie, Houqiang Li
- **Comment**: Accepted by TPAMI 2022
- **Journal**: None
- **Summary**: Existing unsupervised person re-identification methods only rely on visual clues to match pedestrians under different cameras. Since visual data is essentially susceptible to occlusion, blur, clothing changes, etc., a promising solution is to introduce heterogeneous data to make up for the defect of visual data. Some works based on full-scene labeling introduce wireless positioning to assist cross-domain person re-identification, but their GPS labeling of entire monitoring scenes is laborious. To this end, we propose to explore unsupervised person re-identification with both visual data and wireless positioning trajectories under weak scene labeling, in which we only need to know the locations of the cameras. Specifically, we propose a novel unsupervised multimodal training framework (UMTF), which models the complementarity of visual data and wireless information. Our UMTF contains a multimodal data association strategy (MMDA) and a multimodal graph neural network (MMGN). MMDA explores potential data associations in unlabeled multimodal data, while MMGN propagates multimodal messages in the video graph based on the adjacency matrix learned from histogram statistics of wireless data. Thanks to the robustness of the wireless data to visual noise and the collaboration of various modules, UMTF is capable of learning a model free of the human label on data. Extensive experimental results conducted on two challenging datasets, i.e., WP-ReID and DukeMTMC-VideoReID demonstrate the effectiveness of the proposed method.



### Attacking Video Recognition Models with Bullet-Screen Comments
- **Arxiv ID**: http://arxiv.org/abs/2110.15629v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15629v2)
- **Published**: 2021-10-29 08:55:50+00:00
- **Updated**: 2022-01-11 08:14:21+00:00
- **Authors**: Kai Chen, Zhipeng Wei, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research has demonstrated that Deep Neural Networks (DNNs) are vulnerable to adversarial patches which introduce perceptible but localized changes to the input. Nevertheless, existing approaches have focused on generating adversarial patches on images, their counterparts in videos have been less explored. Compared with images, attacking videos is much more challenging as it needs to consider not only spatial cues but also temporal cues. To close this gap, we introduce a novel adversarial attack in this paper, the bullet-screen comment (BSC) attack, which attacks video recognition models with BSCs. Specifically, adversarial BSCs are generated with a Reinforcement Learning (RL) framework, where the environment is set as the target model and the agent plays the role of selecting the position and transparency of each BSC. By continuously querying the target models and receiving feedback, the agent gradually adjusts its selection strategies in order to achieve a high fooling rate with non-overlapping BSCs. As BSCs can be regarded as a kind of meaningful patch, adding it to a clean video will not affect people' s understanding of the video content, nor will arouse people' s suspicion. We conduct extensive experiments to verify the effectiveness of the proposed method. On both UCF-101 and HMDB-51 datasets, our BSC attack method can achieve about 90\% fooling rate when attacking three mainstream video recognition models, while only occluding \textless 8\% areas in the video. Our code is available at https://github.com/kay-ck/BSC-attack.



### Multi-Task and Multi-Modal Learning for RGB Dynamic Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.15639v1
- **DOI**: 10.1109/JSEN.2021.3123443
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15639v1)
- **Published**: 2021-10-29 09:22:39+00:00
- **Updated**: 2021-10-29 09:22:39+00:00
- **Authors**: Dinghao Fan, Hengjie Lu, Shugong Xu, Shan Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Gesture recognition is getting more and more popular due to various application possibilities in human-machine interaction. Existing multi-modal gesture recognition systems take multi-modal data as input to improve accuracy, but such methods require more modality sensors, which will greatly limit their application scenarios. Therefore we propose an end-to-end multi-task learning framework in training 2D convolutional neural networks. The framework can use the depth modality to improve accuracy during training and save costs by using only RGB modality during inference. Our framework is trained to learn a representation for multi-task learning: gesture segmentation and gesture recognition. Depth modality contains the prior information for the location of the gesture. Therefore it can be used as the supervision for gesture segmentation. A plug-and-play module named Multi-Scale-Decoder is designed to realize gesture segmentation, which contains two sub-decoder. It is used in the lower stage and higher stage respectively, and can help the network pay attention to key target areas, ignore irrelevant information, and extract more discriminant features. Additionally, the MSD module and depth modality are only used in the training stage to improve gesture recognition performance. Only RGB modality and network without MSD are required during inference. Experimental results on three public gesture recognition datasets show that our proposed method provides superior performance compared with existing gesture recognition frameworks. Moreover, using the proposed plug-and-play MSD in other 2D CNN-based frameworks also get an excellent accuracy improvement.



### Gabor filter incorporated CNN for compression
- **Arxiv ID**: http://arxiv.org/abs/2110.15644v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15644v2)
- **Published**: 2021-10-29 09:34:39+00:00
- **Updated**: 2021-12-14 01:22:13+00:00
- **Authors**: Akihiro Imamura, Nana Arizumi
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are remarkably successful in many computer vision tasks. However, the high cost of inference is problematic for embedded and real-time systems, so there are many studies on compressing the networks. On the other hand, recent advances in self-attention models showed that convolution filters are preferable to self-attention in the earlier layers, which indicates that stronger inductive biases are better in the earlier layers. As shown in convolutional filters, strong biases can train specific filters and construct unnecessarily filters to zero. This is analogous to classical image processing tasks, where choosing the suitable filters makes a compact dictionary to represent features. We follow this idea and incorporate Gabor filters in the earlier layers of CNNs for compression. The parameters of Gabor filters are learned through backpropagation, so the features are restricted to Gabor filters. We show that the first layer of VGG-16 for CIFAR-10 has 192 kernels/features, but learning Gabor filters requires an average of 29.4 kernels. Also, using Gabor filters, an average of 83% and 94% of kernels in the first and the second layer, respectively, can be removed on the altered ResNet-20, where the first five layers are exchanged with two layers of larger kernels for CIFAR-10.



### Scale-Aware Dynamic Network for Continuous-Scale Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2110.15655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15655v1)
- **Published**: 2021-10-29 09:57:48+00:00
- **Updated**: 2021-10-29 09:57:48+00:00
- **Authors**: Hanlin Wu, Ning Ni, Libao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Single-image super-resolution (SR) with fixed and discrete scale factors has achieved great progress due to the development of deep learning technology. However, the continuous-scale SR, which aims to use a single model to process arbitrary (integer or non-integer) scale factors, is still a challenging task. The existing SR models generally adopt static convolution to extract features, and thus unable to effectively perceive the change of scale factor, resulting in limited generalization performance on multi-scale SR tasks. Moreover, the existing continuous-scale upsampling modules do not make full use of multi-scale features and face problems such as checkerboard artifacts in the SR results and high computational complexity. To address the above problems, we propose a scale-aware dynamic network (SADN) for continuous-scale SR. First, we propose a scale-aware dynamic convolutional (SAD-Conv) layer for the feature learning of multiple SR tasks with various scales. The SAD-Conv layer can adaptively adjust the attention weights of multiple convolution kernels based on the scale factor, which enhances the expressive power of the model with a negligible extra computational cost. Second, we devise a continuous-scale upsampling module (CSUM) with the multi-bilinear local implicit function (MBLIF) for any-scale upsampling. The CSUM constructs multiple feature spaces with gradually increasing scales to approximate the continuous feature representation of an image, and then the MBLIF makes full use of multi-scale features to map arbitrary coordinates to RGB values in high-resolution space. We evaluate our SADN using various benchmarks. The experimental results show that the CSUM can replace the previous fixed-scale upsampling layers and obtain a continuous-scale SR network while maintaining performance. Our SADN uses much fewer parameters and outperforms the state-of-the-art SR methods.



### 3D-OOCS: Learning Prostate Segmentation with Inductive Bias
- **Arxiv ID**: http://arxiv.org/abs/2110.15664v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15664v2)
- **Published**: 2021-10-29 10:14:56+00:00
- **Updated**: 2022-04-20 12:32:39+00:00
- **Authors**: Shrajan Bhandary, Zahra Babaiee, Dejan Kostyszyn, Tobias Fechter, Constantinos Zamboglou, Anca-Ligia Grosu, Radu Grosu
- **Comment**: 6 pages, 1 figure. Accepted in the proceedings of the AAAI 2022
  Workshop: Trustworthy AI for Healthcare
- **Journal**: None
- **Summary**: Despite the great success of convolutional neural networks (CNN) in 3D medical image segmentation tasks, the methods currently in use are still not robust enough to the different protocols utilized by different scanners, and to the variety of image properties or artefacts they produce. To this end, we introduce OOCS-enhanced networks, a novel architecture inspired by the innate nature of visual processing in the vertebrates. With different 3D U-Net variants as the base, we add two 3D residual components to the second encoder blocks: on and off center-surround (OOCS). They generalise the ganglion pathways in the retina to a 3D setting. The use of 2D-OOCS in any standard CNN network complements the feedforward framework with sharp edge-detection inductive biases. The use of 3D-OOCS also helps 3D U-Nets to scrutinise and delineate anatomical structures present in 3D images with increased accuracy.We compared the state-of-the-art 3D U-Nets with their 3D-OOCS extensions and showed the superior accuracy and robustness of the latter in automatic prostate segmentation from 3D Magnetic Resonance Images (MRIs). For a fair comparison, we trained and tested all the investigated 3D U-Nets with the same pipeline, including automatic hyperparameter optimisation and data augmentation.



### SVBRDF Recovery From a Single Image With Highlights using a Pretrained Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2111.00943v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2111.00943v1)
- **Published**: 2021-10-29 10:39:06+00:00
- **Updated**: 2021-10-29 10:39:06+00:00
- **Authors**: Tao Wen, Beibei Wang, Lei Zhang, Jie Guo, Nicolas Holzschuch
- **Comment**: None
- **Journal**: None
- **Summary**: Spatially-varying bi-directional reflectance distribution functions (SVBRDFs) are crucial for designers to incorporate new materials in virtual scenes, making them look more realistic. Reconstruction of SVBRDFs is a long-standing problem. Existing methods either rely on extensive acquisition system or require huge datasets which are nontrivial to acquire. We aim to recover SVBRDFs from a single image, without any datasets. A single image contains incomplete information about the SVBRDF, making the reconstruction task highly ill-posed. It is also difficult to separate between the changes in color that are caused by the material and those caused by the illumination, without the prior knowledge learned from the dataset. In this paper, we use an unsupervised generative adversarial neural network (GAN) to recover SVBRDFs maps with a single image as input. To better separate the effects due to illumination from the effects due to the material, we add the hypothesis that the material is stationary and introduce a new loss function based on Fourier coefficients to enforce this stationarity. For efficiency, we train the network in two stages: reusing a trained model to initialize the SVBRDFs and fine-tune it based on the input image. Our method generates high-quality SVBRDFs maps from a single input photograph, and provides more vivid rendering results compared to previous work. The two-stage training boosts runtime performance, making it 8 times faster than previous work.



### Multi-target tracking for video surveillance using deep affinity network: a brief review
- **Arxiv ID**: http://arxiv.org/abs/2110.15674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15674v1)
- **Published**: 2021-10-29 10:44:26+00:00
- **Updated**: 2021-10-29 10:44:26+00:00
- **Authors**: Sanam Nisar Mangi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models are known to function like the human brain. Due to their functional mechanism, they are frequently utilized to accomplish tasks that require human intelligence. Multi-target tracking (MTT) for video surveillance is one of the important and challenging tasks, which has attracted the researcher's attention due to its potential applications in various domains. Multi-target tracking tasks require locating the objects individually in each frame, which remains a huge challenge as there are immediate changes in appearances and extreme occlusions of objects. In addition to that, the Multitarget tracking framework requires multiple tasks to perform i.e. target detection, estimating trajectory, associations between frame, and re-identification. Various methods have been suggested, and some assumptions are made to constrain the problem in the context of a particular problem. In this paper, the state-of-the-art MTT models, which leverage from deep learning representational power are reviewed.



### A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.15678v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15678v3)
- **Published**: 2021-10-29 10:53:12+00:00
- **Updated**: 2021-12-08 19:40:38+00:00
- **Authors**: Xingang Pan, Xudong Xu, Chen Change Loy, Christian Theobalt, Bo Dai
- **Comment**: Accepted to NeurIPS2021. We proposed ShadeGAN, which could perform
  shape-accurate 3D-aware image synthesis by modeling shading in generative
  implicit models
- **Journal**: None
- **Summary**: The advancement of generative radiance fields has pushed the boundary of 3D-aware image synthesis. Motivated by the observation that a 3D object should look realistic from multiple viewpoints, these methods introduce a multi-view constraint as regularization to learn valid 3D radiance fields from 2D images. Despite the progress, they often fall short of capturing accurate 3D shapes due to the shape-color ambiguity, limiting their applicability in downstream tasks. In this work, we address this ambiguity by proposing a novel shading-guided generative implicit model that is able to learn a starkly improved shape representation. Our key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions. This multi-lighting constraint is realized by modeling illumination explicitly and performing shading with various lighting conditions. Gradients are derived by feeding the synthesized images to a discriminator. To compensate for the additional computational burden of calculating surface normals, we further devise an efficient volume rendering strategy via surface tracking, reducing the training and inference time by 24% and 48%, respectively. Our experiments on multiple datasets show that the proposed approach achieves photorealistic 3D-aware image synthesis while capturing accurate underlying 3D shapes. We demonstrate improved performance of our approach on 3D shape reconstruction against existing methods, and show its applicability on image relighting. Our code will be released at https://github.com/XingangPan/ShadeGAN.



### False Positive Detection and Prediction Quality Estimation for LiDAR Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.15681v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.15681v1)
- **Published**: 2021-10-29 11:00:30+00:00
- **Updated**: 2021-10-29 11:00:30+00:00
- **Authors**: Pascal Colling, Matthias Rottmann, Lutz Roese-Koerner, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel post-processing tool for semantic segmentation of LiDAR point cloud data, called LidarMetaSeg, which estimates the prediction quality segmentwise. For this purpose we compute dispersion measures based on network probability outputs as well as feature measures based on point cloud input features and aggregate them on segment level. These aggregated measures are used to train a meta classification model to predict whether a predicted segment is a false positive or not and a meta regression model to predict the segmentwise intersection over union. Both models can then be applied to semantic segmentation inferences without knowing the ground truth. In our experiments we use different LiDAR segmentation models and datasets and analyze the power of our method. We show that our results outperform other standard approaches.



### IRA: A shape matching approach for recognition and comparison of generic atomic patterns
- **Arxiv ID**: http://arxiv.org/abs/2111.00939v1
- **DOI**: 10.1021/acs.jcim.1c00567
- **Categories**: **physics.comp-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.00939v1)
- **Published**: 2021-10-29 11:43:30+00:00
- **Updated**: 2021-10-29 11:43:30+00:00
- **Authors**: Miha Gunde, Nicolas Salles, Anne HÃ©meryck, Layla Martin-Samos
- **Comment**: 18 pages, 19 figures
- **Journal**: None
- **Summary**: We propose a versatile, parameter-less approach for solving the shape matching problem, specifically in the context of atomic structures when atomic assignments are not known a priori. The algorithm Iteratively suggests Rotated atom-centered reference frames and Assignments (Iterative Rotations and Assignments, IRA). The frame for which a permutationally invariant set-set distance, namely the Hausdorff distance, returns minimal value is chosen as the solution of the matching problem. IRA is able to find rigid rotations, reflections, translations, and permutations between structures with different numbers of atoms, for any atomic arrangement and pattern, periodic or not. When distortions are present between the structures, optimal rotation and translation are found by further applying a standard Singular Value Decomposition-based method. To compute the atomic assignments under the one-to-one assignment constraint, we develop our own algorithm, Constrained Shortest Distance Assignments (CShDA). The overall approach is extensively tested on several structures, including distorted structural fragments. Efficiency of the proposed algorithm is shown as a benchmark comparison against two other shape matching algorithms. We discuss the use of our approach for the identification and comparison of structures and structural fragments through two examples: a replica exchange trajectory of a cyanine molecule, in which we show how our approach could aid the exploration of relevant collective coordinates for clustering the data; and an SiO$_2$ amorphous model, in which we compute distortion scores and compare them with a classical strain-based potential. The source code and benchmark data are available at \url{https://github.com/mammasmias/IterativeRotationsAssignments}.



### An Effective Image Restorer: Denoising and Luminance Adjustment for Low-photon-count Imaging
- **Arxiv ID**: http://arxiv.org/abs/2110.15715v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15715v2)
- **Published**: 2021-10-29 12:16:30+00:00
- **Updated**: 2021-11-02 01:56:56+00:00
- **Authors**: Shansi Zhang, Edmund Y. Lam
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging under photon-scarce situations introduces challenges to many applications as the captured images are with low signal-to-noise ratio and poor luminance. In this paper, we investigate the raw image restoration under low-photon-count conditions by simulating the imaging of quanta image sensor (QIS). We develop a lightweight framework, which consists of a multi-level pyramid denoising network (MPDNet) and a luminance adjustment (LA) module to achieve separate denoising and luminance enhancement. The main component of our framework is the multi-skip attention residual block (MARB), which integrates multi-scale feature fusion and attention mechanism for better feature representation. Our MPDNet adopts the idea of Laplacian pyramid to learn the small-scale noise map and larger-scale high-frequency details at different levels, and feature extractions are conducted on the multi-scale input images to encode richer contextual information. Our LA module enhances the luminance of the denoised image by estimating its illumination, which can better avoid color distortion. Extensive experimental results have demonstrated that our image restorer can achieve superior performance on the degraded images with various photon levels by suppressing noise and recovering luminance and color effectively.



### CVAD: A generic medical anomaly detector based on Cascade VAE
- **Arxiv ID**: http://arxiv.org/abs/2110.15811v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15811v2)
- **Published**: 2021-10-29 14:20:43+00:00
- **Updated**: 2022-01-26 22:05:13+00:00
- **Authors**: Xiaoyuan Guo, Judy Wawira Gichoya, Saptarshi Purkayastha, Imon Banerjee
- **Comment**: 6 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Detecting out-of-distribution (OOD) samples in medical imaging plays an important role for downstream medical diagnosis. However, existing OOD detectors are demonstrated on natural images composed of inter-classes and have difficulty generalizing to medical images. The key issue is the granularity of OOD data in the medical domain, where intra-class OOD samples are predominant. We focus on the generalizability of OOD detection for medical images and propose a self-supervised Cascade Variational autoencoder-based Anomaly Detector (CVAD). We use a variational autoencoders' cascade architecture, which combines latent representation at multiple scales, before being fed to a discriminator to distinguish the OOD data from the in-distribution (ID) data. Finally, both the reconstruction error and the OOD probability predicted by the binary discriminator are used to determine the anomalies. We compare the performance with the state-of-the-art deep learning models to demonstrate our model's efficacy on various open-access medical imaging datasets for both intra- and inter-class OOD. Further extensive results on datasets including common natural datasets show our model's effectiveness and generalizability. The code is available at https://github.com/XiaoyuanGuo/CVAD.



### C-MADA: Unsupervised Cross-Modality Adversarial Domain Adaptation framework for medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.15823v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15823v1)
- **Published**: 2021-10-29 14:34:33+00:00
- **Updated**: 2021-10-29 14:34:33+00:00
- **Authors**: Maria Baldeon-Calisto, Susana K. Lai-Yuen
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: Deep learning models have obtained state-of-the-art results for medical image analysis. However, when these models are tested on an unseen domain there is a significant performance degradation. In this work, we present an unsupervised Cross-Modality Adversarial Domain Adaptation (C-MADA) framework for medical image segmentation. C-MADA implements an image- and feature-level adaptation method in a sequential manner. First, images from the source domain are translated to the target domain through an un-paired image-to-image adversarial translation with cycle-consistency loss. Then, a U-Net network is trained with the mapped source domain images and target domain images in an adversarial manner to learn domain-invariant feature representations. Furthermore, to improve the networks segmentation performance, information about the shape, texture, and con-tour of the predicted segmentation is included during the adversarial train-ing. C-MADA is tested on the task of brain MRI segmentation, obtaining competitive results.



### Turning Traffic Monitoring Cameras into Intelligent Sensors for Traffic Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.00941v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.00941v1)
- **Published**: 2021-10-29 15:39:06+00:00
- **Updated**: 2021-10-29 15:39:06+00:00
- **Authors**: Zijian Hu, William H. K. Lam, S. C. Wong, Andy H. F. Chow, Wei Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate traffic state information plays a pivotal role in the Intelligent Transportation Systems (ITS), and it is an essential input to various smart mobility applications such as signal coordination and traffic flow prediction. The current practice to obtain the traffic state information is through specialized sensors such as loop detectors and speed cameras. In most metropolitan areas, traffic monitoring cameras have been installed to monitor the traffic conditions on arterial roads and expressways, and the collected videos or images are mainly used for visual inspection by traffic engineers. Unfortunately, the data collected from traffic monitoring cameras are affected by the 4L characteristics: Low frame rate, Low resolution, Lack of annotated data, and Located in complex road environments. Therefore, despite the great potentials of the traffic monitoring cameras, the 4L characteristics hinder them from providing useful traffic state information (e.g., speed, flow, density). This paper focuses on the traffic density estimation problem as it is widely applicable to various traffic surveillance systems. To the best of our knowledge, there is a lack of the holistic framework for addressing the 4L characteristics and extracting the traffic density information from traffic monitoring camera data. In view of this, this paper proposes a framework for estimating traffic density using uncalibrated traffic monitoring cameras with 4L characteristics. The proposed framework consists of two major components: camera calibration and vehicle detection. The camera calibration method estimates the actual length between pixels in the images and videos, and the vehicle counts are extracted from the deep-learning-based vehicle detection method. Combining the two components, high-granular traffic density can be estimated. To validate the proposed framework, two case studies were conducted in Hong Kong and Sacramento. The results show that the Mean Absolute Error (MAE) in camera calibration is less than 0.2 meters out of 6 meters, and the accuracy of vehicle detection under various conditions is approximately 90%. Overall, the MAE for the estimated density is 9.04 veh/km/lane in Hong Kong and 1.30 veh/km/lane in Sacramento. The research outcomes can be used to calibrate the speed-density fundamental diagrams, and the proposed framework can provide accurate and real-time traffic information without installing additional sensors.



### Application of 2-D Convolutional Neural Networks for Damage Detection in Steel Frame Structures
- **Arxiv ID**: http://arxiv.org/abs/2110.15895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.15895v1)
- **Published**: 2021-10-29 16:29:31+00:00
- **Updated**: 2021-10-29 16:29:31+00:00
- **Authors**: Shahin Ghazvineh, Gholamreza Nouri, Seyed Hossein Hosseini Lavassani, Vahidreza Gharehbaghi, Andy Nguyen
- **Comment**: 17 pages, 5 Figures, 3 Tables
- **Journal**: None
- **Summary**: In this paper, we present an application of 2-D convolutional neural networks (2-D CNNs) designed to perform both feature extraction and classification stages as a single organism to solve the highlighted problems. The method uses a network of lighted CNNs instead of deep and takes raw acceleration signals as input. Using lighted CNNs, in which every one of them is optimized for a specific element, increases the accuracy and makes the network faster to perform. Also, a new framework is proposed for decreasing the data required in the training phase. We verified our method on Qatar University Grandstand Simulator (QUGS) benchmark data provided by Structural Dynamics Team. The results showed improved accuracy over other methods, and running time was adequate for real-time applications.



### Learning Co-segmentation by Segment Swapping for Retrieval and Discovery
- **Arxiv ID**: http://arxiv.org/abs/2110.15904v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15904v2)
- **Published**: 2021-10-29 16:51:16+00:00
- **Updated**: 2022-03-27 22:15:06+00:00
- **Authors**: Xi Shen, Alexei A. Efros, Armand Joulin, Mathieu Aubry
- **Comment**: add results of unsupervised saliency detection
- **Journal**: None
- **Summary**: The goal of this work is to efficiently identify visually similar patterns in images, e.g. identifying an artwork detail copied between an engraving and an oil painting, or recognizing parts of a night-time photograph visible in its daytime counterpart. Lack of training data is a key challenge for this co-segmentation task. We present a simple yet surprisingly effective approach to overcome this difficulty: we generate synthetic training pairs by selecting segments in an image and copy-pasting them into another image. We then learn to predict the repeated region masks. We find that it is crucial to predict the correspondences as an auxiliary task and to use Poisson blending and style transfer on the training pairs to generalize on real data. We analyse results with two deep architectures relevant to our joint image analysis task: a transformer-based architecture and Sparse Nc-Net, a recent network designed to predict coarse correspondences using 4D convolutions. We show our approach provides clear improvements for artwork details retrieval on the Brueghel dataset and achieves competitive performance on two place recognition benchmarks, Tokyo247 and Pitts30K. We also demonstrate the potential of our approach for unsupervised image collection analysis by introducing a spectral graph clustering approach to object discovery and demonstrating it on the object discovery dataset of \cite{rubinstein2013unsupervised} and the Brueghel dataset. Our code and data are available at http://imagine.enpc.fr/~shenx/SegSwap/.



### On the use of uncertainty in classifying Aedes Albopictus mosquitoes
- **Arxiv ID**: http://arxiv.org/abs/2110.15912v1
- **DOI**: 10.1109/JSTSP.2021.3122886
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.15912v1)
- **Published**: 2021-10-29 16:58:25+00:00
- **Updated**: 2021-10-29 16:58:25+00:00
- **Authors**: Gereziher Adhane, Mohammad Mahdi Dehshibi, David Masip
- **Comment**: This article has been accepted for publication in a future issue of
  IEEE Journal of Selected Topics in Signal Processing
- **Journal**: None
- **Summary**: The re-emergence of mosquito-borne diseases (MBDs), which kill hundreds of thousands of people each year, has been attributed to increased human population, migration, and environmental changes. Convolutional neural networks (CNNs) have been used by several studies to recognise mosquitoes in images provided by projects such as Mosquito Alert to assist entomologists in identifying, monitoring, and managing MBD. Nonetheless, utilising CNNs to automatically label input samples could involve incorrect predictions, which may mislead future epidemiological studies. Furthermore, CNNs require large numbers of manually annotated data. In order to address the mentioned issues, this paper proposes using the Monte Carlo Dropout method to estimate the uncertainty scores in order to rank the classified samples to reduce the need for human supervision in recognising Aedes albopictus mosquitoes. The estimated uncertainty was also used in an active learning framework, where just a portion of the data from large training sets was manually labelled. The experimental results show that the proposed classification method with rejection outperforms the competing methods by improving overall performance and reducing entomologist annotation workload. We also provide explainable visualisations of the different regions that contribute to a set of samples' uncertainty assessment.



### Estimating and Maximizing Mutual Information for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2110.15946v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2110.15946v3)
- **Published**: 2021-10-29 17:49:56+00:00
- **Updated**: 2023-05-11 13:08:01+00:00
- **Authors**: Aman Shrivastava, Yanjun Qi, Vicente Ordonez
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose Mutual Information Maximization Knowledge Distillation (MIMKD). Our method uses a contrastive objective to simultaneously estimate and maximize a lower bound on the mutual information of local and global feature representations between a teacher and a student network. We demonstrate through extensive experiments that this can be used to improve the performance of low capacity models by transferring knowledge from more performant but computationally expensive models. This can be used to produce better models that can be run on devices with low computational resources. Our method is flexible, we can distill knowledge from teachers with arbitrary network architectures to arbitrary student networks. Our empirical results show that MIMKD outperforms competing approaches across a wide range of student-teacher pairs with different capacities, with different architectures, and when student networks are with extremely low capacity. We are able to obtain 74.55% accuracy on CIFAR100 with a ShufflenetV2 from a baseline accuracy of 69.8% by distilling knowledge from ResNet-50. On Imagenet we improve a ResNet-18 network from 68.88% to 70.32% accuracy (1.44%+) using a ResNet-34 teacher network.



### A deep convolutional neural network for classification of Aedes albopictus mosquitoes
- **Arxiv ID**: http://arxiv.org/abs/2110.15956v1
- **DOI**: 10.1109/ACCESS.2021.3079700
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.15956v1)
- **Published**: 2021-10-29 17:58:32+00:00
- **Updated**: 2021-10-29 17:58:32+00:00
- **Authors**: Gereziher Adhane, Mohammad Mahdi Dehshibi, David Masip
- **Comment**: None
- **Journal**: IEEE Access, vol. 9, pp. 72681-72690, 2021
- **Summary**: Monitoring the spread of disease-carrying mosquitoes is a first and necessary step to control severe diseases such as dengue, chikungunya, Zika or yellow fever. Previous citizen science projects have been able to obtain large image datasets with linked geo-tracking information. As the number of international collaborators grows, the manual annotation by expert entomologists of the large amount of data gathered by these users becomes too time demanding and unscalable, posing a strong need for automated classification of mosquito species from images. We introduce the application of two Deep Convolutional Neural Networks in a comparative study to automate this classification task. We use the transfer learning principle to train two state-of-the-art architectures on the data provided by the Mosquito Alert project, obtaining testing accuracy of 94%. In addition, we applied explainable models based on the Grad-CAM algorithm to visualise the most discriminant regions of the classified images, which coincide with the white band stripes located at the legs, abdomen, and thorax of mosquitoes of the Aedes albopictus species. The model allows us to further analyse the classification errors. Visual Grad-CAM models show that they are linked to poor acquisition conditions and strong image occlusions.



### Visual Keyword Spotting with Attention
- **Arxiv ID**: http://arxiv.org/abs/2110.15957v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2110.15957v1)
- **Published**: 2021-10-29 17:59:04+00:00
- **Updated**: 2021-10-29 17:59:04+00:00
- **Authors**: K R Prajwal, Liliane Momeni, Triantafyllos Afouras, Andrew Zisserman
- **Comment**: Appears in: British Machine Vision Conference 2021 (BMVC 2021)
- **Journal**: None
- **Summary**: In this paper, we consider the task of spotting spoken keywords in silent video sequences -- also known as visual keyword spotting. To this end, we investigate Transformer-based models that ingest two streams, a visual encoding of the video and a phonetic encoding of the keyword, and output the temporal location of the keyword if present. Our contributions are as follows: (1) We propose a novel architecture, the Transpotter, that uses full cross-modal attention between the visual and phonetic streams; (2) We show through extensive evaluations that our model outperforms the prior state-of-the-art visual keyword spotting and lip reading methods on the challenging LRW, LRS2, LRS3 datasets by a large margin; (3) We demonstrate the ability of our model to spot words under the extreme conditions of isolated mouthings in sign language videos.



### On-device Real-time Hand Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.00038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.00038v1)
- **Published**: 2021-10-29 18:33:25+00:00
- **Updated**: 2021-10-29 18:33:25+00:00
- **Authors**: George Sung, Kanstantsin Sokal, Esha Uboweja, Valentin Bazarevsky, Jonathan Baccash, Eduard Gabriel Bazavan, Chuo-Ling Chang, Matthias Grundmann
- **Comment**: 5 pages, 6 figures; ICCV Workshop on Computer Vision for Augmented
  and Virtual Reality, Montreal, Canada, 2021
- **Journal**: None
- **Summary**: We present an on-device real-time hand gesture recognition (HGR) system, which detects a set of predefined static gestures from a single RGB camera. The system consists of two parts: a hand skeleton tracker and a gesture classifier. We use MediaPipe Hands as the basis of the hand skeleton tracker, improve the keypoint accuracy, and add the estimation of 3D keypoints in a world metric space. We create two different gesture classifiers, one based on heuristics and the other using neural networks (NN).



### CvS: Classification via Segmentation For Small Datasets
- **Arxiv ID**: http://arxiv.org/abs/2111.00042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.00042v1)
- **Published**: 2021-10-29 18:41:15+00:00
- **Updated**: 2021-10-29 18:41:15+00:00
- **Authors**: Nooshin Mojab, Philip S. Yu, Joelle A. Hallak, Darvin Yi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have shown promising results in a wide range of computer vision applications across various domains. The success of deep learning methods relies heavily on the availability of a large amount of data. Deep neural networks are prone to overfitting when data is scarce. This problem becomes even more severe for neural network with classification head with access to only a few data points. However, acquiring large-scale datasets is very challenging, laborious, or even infeasible in some domains. Hence, developing classifiers that are able to perform well in small data regimes is crucial for applications with limited data. This paper presents CvS, a cost-effective classifier for small datasets that derives the classification labels from predicting the segmentation maps. We employ the label propagation method to achieve a fully segmented dataset with only a handful of manually segmented data. We evaluate the effectiveness of our framework on diverse problems showing that CvS is able to achieve much higher classification results compared to previous methods when given only a handful of examples.



### Generalized Data Weighting via Class-level Gradient Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2111.00056v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2111.00056v1)
- **Published**: 2021-10-29 19:30:01+00:00
- **Updated**: 2021-10-29 19:30:01+00:00
- **Authors**: Can Chen, Shuhao Zheng, Xi Chen, Erqun Dong, Xue Liu, Hao Liu, Dejing Dou
- **Comment**: 17 pages, 8 figures, accepted by NeurIPS 2021 for a poster session,
  camera-ready version, initial submission to arXiv
- **Journal**: None
- **Summary**: Label noise and class imbalance are two major issues coexisting in real-world datasets. To alleviate the two issues, state-of-the-art methods reweight each instance by leveraging a small amount of clean and unbiased data. Yet, these methods overlook class-level information within each instance, which can be further utilized to improve performance. To this end, in this paper, we propose Generalized Data Weighting (GDW) to simultaneously mitigate label noise and class imbalance by manipulating gradients at the class level. To be specific, GDW unrolls the loss gradient to class-level gradients by the chain rule and reweights the flow of each gradient separately. In this way, GDW achieves remarkable performance improvement on both issues. Aside from the performance gain, GDW efficiently obtains class-level weights without introducing any extra computational cost compared with instance weighting methods. Specifically, GDW performs a gradient descent step on class-level weights, which only relies on intermediate gradients. Extensive experiments in various settings verify the effectiveness of GDW. For example, GDW outperforms state-of-the-art methods by $2.56\%$ under the $60\%$ uniform noise setting in CIFAR10. Our code is available at https://github.com/GGchen1997/GDW-NIPS2021.



### Polyline Generative Navigable Space Segmentation for Autonomous Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2111.00063v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.00063v2)
- **Published**: 2021-10-29 19:50:48+00:00
- **Updated**: 2023-03-05 18:08:42+00:00
- **Authors**: Zheng Chen, Zhengming Ding, David Crandall, Lantao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting navigable space is a fundamental capability for mobile robots navigating in unknown or unmapped environments. In this work, we treat visual navigable space segmentation as a scene decomposition problem and propose Polyline Segmentation Variational autoencoder Network (PSV-Net), a representation learning-based framework for learning the navigable space segmentation in a self-supervised manner. Current segmentation techniques heavily rely on fully-supervised learning strategies which demand a large amount of pixel-level annotated images. In this work, we propose a framework leveraging a Variational AutoEncoder (VAE) and an AutoEncoder (AE) to learn a polyline representation that compactly outlines the desired navigable space boundary. Through extensive experiments, we validate that the proposed PSV-Net can learn the visual navigable space with no or few labels, producing an accuracy comparable to fully-supervised state-of-the-art methods that use all available labels. In addition, we show that integrating the proposed navigable space segmentation model with a visual planner can achieve efficient mapless navigation in real environments.



### DeepDoseNet: A Deep Learning model for 3D Dose Prediction in Radiation Therapy
- **Arxiv ID**: http://arxiv.org/abs/2111.00077v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.AI, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.00077v1)
- **Published**: 2021-10-29 20:44:14+00:00
- **Updated**: 2021-10-29 20:44:14+00:00
- **Authors**: Mumtaz Hussain Soomro, Victor Gabriel Leandro Alves, Hamidreza Nourzadeh, Jeffrey V. Siebers
- **Comment**: None
- **Journal**: None
- **Summary**: The DeepDoseNet 3D dose prediction model based on ResNet and Dilated DenseNet is proposed. The 340 head-and-neck datasets from the 2020 AAPM OpenKBP challenge were utilized, with 200 for training, 40 for validation, and 100 for testing. Structures include 56Gy, 63Gy, 70Gy PTVs, and brainstem, spinal cord, right parotid, left parotid, larynx, esophagus, and mandible OARs. Mean squared error (MSE) loss, mean absolute error (MAE) loss, and MAE plus dose-volume histogram (DVH) based loss functions were investigated. Each model's performance was compared using a 3D dose score, $\bar{S_{D}}$, (mean absolute difference between ground truth and predicted 3D dose distributions) and a DVH score, $\bar{S_{DVH}}$ (mean absolute difference between ground truth and predicted dose-volume metrics).Furthermore, DVH metrics Mean[Gy] and D0.1cc [Gy] for OARs and D99%, D95%, D1% for PTVs were computed. DeepDoseNet with the MAE plus DVH-based loss function had the best dose score performance of the OpenKBP entries. MAE+DVH model had the lowest prediction error (P<0.0001, Wilcoxon test) on validation and test datasets (validation: $\bar{S_{D}}$=2.3Gy, $\bar{S_{DVH}}$=1.9Gy; test: $\bar{S_{D}}$=2.0Gy, $\bar{S_{DVH}}$=1.6Gy) followed by the MAE model (validation: $\bar{S_{D}}$=3.6Gy, $\bar{S_{DVH}}$=2.4Gy; test: $\bar{S_{D}}$=3.5Gy, $\bar{S_{DVH}}$=2.3Gy). The MSE model had the highest prediction error (validation: $\bar{S_{D}}$=3.7Gy, $\bar{S_{DVH}}$=3.2Gy; test: $\bar{S_{D}}$=3.6Gy, $\bar{S_{DVH}}$=3.0Gy). No significant difference was found among models in terms of Mean [Gy], but the MAE+DVH model significantly outperformed the MAE and MSE models in terms of D0.1cc[Gy], particularly for mandible and parotids on both validation (P<0.01) and test (P<0.0001) datasets. MAE+DVH outperformed (P<0.0001) in terms of D99%, D95%, D1% for targets. MAE+DVH reduced $\bar{S_{D}}$ by ~60% and $\bar{S_{DVH}}$ by ~70%.



### Deep Deterministic Uncertainty for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.00079v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.00079v1)
- **Published**: 2021-10-29 20:45:58+00:00
- **Updated**: 2021-10-29 20:45:58+00:00
- **Authors**: Jishnu Mukhoti, Joost van Amersfoort, Philip H. S. Torr, Yarin Gal
- **Comment**: None
- **Journal**: None
- **Summary**: We extend Deep Deterministic Uncertainty (DDU), a method for uncertainty estimation using feature space densities, to semantic segmentation. DDU enables quantifying and disentangling epistemic and aleatoric uncertainty in a single forward pass through the model. We study the similarity of feature representations of pixels at different locations for the same class and conclude that it is feasible to apply DDU location independently, which leads to a significant reduction in memory consumption compared to pixel dependent DDU. Using the DeepLab-v3+ architecture on Pascal VOC 2012, we show that DDU improves upon MC Dropout and Deep Ensembles while being significantly faster to compute.



### Fetal MRI by robust deep generative prior reconstruction and diffeomorphic registration: application to gestational age prediction
- **Arxiv ID**: http://arxiv.org/abs/2111.00102v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph, 92C50, 94A08, I.2.1; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2111.00102v1)
- **Published**: 2021-10-29 22:09:52+00:00
- **Updated**: 2021-10-29 22:09:52+00:00
- **Authors**: Lucilio Cordero-Grande, Juan Enrique OrtuÃ±o-Fisac, Alena Uus, Maria Deprez, AndrÃ©s Santos, Joseph V. Hajnal, MarÃ­a JesÃºs Ledesma-Carbayo
- **Comment**: 23 pages, 15 figures, 1 table
- **Journal**: None
- **Summary**: Magnetic resonance imaging of whole fetal body and placenta is limited by different sources of motion affecting the womb. Usual scanning techniques employ single-shot multi-slice sequences where anatomical information in different slices may be subject to different deformations, contrast variations or artifacts. Volumetric reconstruction formulations have been proposed to correct for these factors, but they must accommodate a non-homogeneous and non-isotropic sampling, so regularization becomes necessary. Thus, in this paper we propose a deep generative prior for robust volumetric reconstructions integrated with a diffeomorphic volume to slice registration method. Experiments are performed to validate our contributions and compare with a state of the art method in a cohort of $72$ fetal datasets in the range of $20-36$ weeks gestational age. Results suggest improved image resolution and more accurate prediction of gestational age at scan when comparing to a state of the art reconstruction method. In addition, gestational age prediction results from our volumetric reconstructions compare favourably with existing brain-based approaches, with boosted accuracy when integrating information of organs other than the brain. Namely, a mean absolute error of $0.618$ weeks ($R^2=0.958$) is achieved when combining fetal brain and trunk information.



### FC2T2: The Fast Continuous Convolutional Taylor Transform with Applications in Vision and Graphics
- **Arxiv ID**: http://arxiv.org/abs/2111.00110v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.00110v2)
- **Published**: 2021-10-29 22:58:42+00:00
- **Updated**: 2021-11-10 19:42:31+00:00
- **Authors**: Henning Lange, J. Nathan Kutz
- **Comment**: None
- **Journal**: None
- **Summary**: Series expansions have been a cornerstone of applied mathematics and engineering for centuries. In this paper, we revisit the Taylor series expansion from a modern Machine Learning perspective. Specifically, we introduce the Fast Continuous Convolutional Taylor Transform (FC2T2), a variant of the Fast Multipole Method (FMM), that allows for the efficient approximation of low dimensional convolutional operators in continuous space. We build upon the FMM which is an approximate algorithm that reduces the computational complexity of N-body problems from O(NM) to O(N+M) and finds application in e.g. particle simulations. As an intermediary step, the FMM produces a series expansion for every cell on a grid and we introduce algorithms that act directly upon this representation. These algorithms analytically but approximately compute the quantities required for the forward and backward pass of the backpropagation algorithm and can therefore be employed as (implicit) layers in Neural Networks. Specifically, we introduce a root-implicit layer that outputs surface normals and object distances as well as an integral-implicit layer that outputs a rendering of a radiance field given a 3D pose. In the context of Machine Learning, $N$ and $M$ can be understood as the number of model parameters and model evaluations respectively which entails that, for applications that require repeated function evaluations which are prevalent in Computer Vision and Graphics, unlike regular Neural Networks, the techniques introduce in this paper scale gracefully with parameters. For some applications, this results in a 200x reduction in FLOPs compared to state-of-the-art approaches at a reasonable or non-existent loss in accuracy.



### Classification of jujube fruit based on several pricing factors using machine learning methods
- **Arxiv ID**: http://arxiv.org/abs/2111.00112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.00112v1)
- **Published**: 2021-10-29 23:10:21+00:00
- **Updated**: 2021-10-29 23:10:21+00:00
- **Authors**: Abdollah Zakeri, Ruhollah Hedayati, Mohammad Khedmati, Mehran Taghipour-Gorjikolaie
- **Comment**: None
- **Journal**: None
- **Summary**: Jujube is a fruit mainly cultivated in India, China and Iran and has many health benefits. It is sold both fresh and dried. There are several factors in jujube pricing such as weight, wrinkles and defections. Some jujube farmers sell their product all at once, without any proper sorting or classification, for an average price. Our studies and experiences show that their profit can increase significantly if their product is sold after the sorting process. There are some traditional sorting methods for dried jujube fruit but they are costly, time consuming and can be inaccurate due to human error. Nowadays, computer vision combined with machine learning methods, is used increasingly in food industry for sorting and classification purposes and solve many of the traditional sorting methods' problems. In this paper we are proposing a computer vision-based method for grading jujube fruits using machine learning techniques which will take most of the important pricing factors into account and can be used to increase the profit of farmers. In this method we first acquire several images from different samples and then extract their visual features such as color features, shape and size features, texture features, defection and wrinkle features and then we select the most useful features using feature selection algorithms like PCA and CFS. A feature vector is obtained for each sample and we use these vectors to train our classifiers to be able to specify the corresponding pre-defined group for each of the samples. We used different classifiers and training methods in order to obtain the best result and by using decision tree we could reach 98.8% accuracy of the classification.



### Visual Explanations for Convolutional Neural Networks via Latent Traversal of Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.00116v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV, I.5.4; I.5.1; I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2111.00116v2)
- **Published**: 2021-10-29 23:26:09+00:00
- **Updated**: 2021-11-02 00:42:41+00:00
- **Authors**: Amil Dravid, Aggelos K. Katsaggelos
- **Comment**: 2 pages, 2 figures, to appear as extended abstract at AAAI-22
- **Journal**: None
- **Summary**: Lack of explainability in artificial intelligence, specifically deep neural networks, remains a bottleneck for implementing models in practice. Popular techniques such as Gradient-weighted Class Activation Mapping (Grad-CAM) provide a coarse map of salient features in an image, which rarely tells the whole story of what a convolutional neural network (CNN) learned. Using COVID-19 chest X-rays, we present a method for interpreting what a CNN has learned by utilizing Generative Adversarial Networks (GANs). Our GAN framework disentangles lung structure from COVID-19 features. Using this GAN, we can visualize the transition of a pair of COVID negative lungs in a chest radiograph to a COVID positive pair by interpolating in the latent space of the GAN, which provides fine-grained visualization of how the CNN responds to varying features within the lungs.



### Longitudinal Analysis of Mask and No-Mask on Child Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.00121v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.00121v5)
- **Published**: 2021-10-29 23:40:20+00:00
- **Updated**: 2022-03-13 07:11:23+00:00
- **Authors**: Praveen Kumar Chandaliya, Zahid Akhtar, Neeta Nain
- **Comment**: 5 Pages, 3 Figure
- **Journal**: None
- **Summary**: Face is one of the most widely employed traits for person recognition, even in many large-scale applications. Despite technological advancements in face recognition systems, they still face obstacles caused by pose, expression, occlusion, and aging variations. Owing to the COVID-19 pandemic, contactless identity verification has become exceedingly vital. Recently, few studies have been conducted on the effect of face mask on adult face recognition systems (FRS). However, the impact of aging with face mask on child subject recognition has not been adequately explored. Thus, the main objective of this study is analyzing the child longitudinal impact together with face mask and other covariates on FRS. Specifically, we performed a comparative investigation of three top performing publicly available face matchers and a post-COVID-19 commercial-off-the-shelf (COTS) system under child cross-age verification and identification settings using our generated synthetic mask and no-mask samples. Furthermore, we investigated the longitudinal consequence of eyeglasses with mask and no-mask. The study exploited no-mask longitudinal child face dataset (i.e., extended Indian Child Longitudinal Face Dataset) that contains 26,258 face images of 7,473 subjects in the age group of [2, 18] over an average time span of 3.35 years. Due to the combined effects of face mask and face aging, the FaceNet, PFE, ArcFace, and COTS face verification system accuracies decrease approximately 25%, 22%, 18%, 12%, respectively.



### Predicting Atlantic Multidecadal Variability
- **Arxiv ID**: http://arxiv.org/abs/2111.00124v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.00124v1)
- **Published**: 2021-10-29 23:56:24+00:00
- **Updated**: 2021-10-29 23:56:24+00:00
- **Authors**: Glenn Liu, Peidong Wang, Matthew Beveridge, Young-Oh Kwon, Iddo Drori
- **Comment**: 7 pages, 3 figures
- **Journal**: Tackling Climate Change with Machine Learning workshop at NeurIPS
  2021
- **Summary**: Atlantic Multidecadal Variability (AMV) describes variations of North Atlantic sea surface temperature with a typical cycle of between 60 and 70 years. AMV strongly impacts local climate over North America and Europe, therefore prediction of AMV, especially the extreme values, is of great societal utility for understanding and responding to regional climate change. This work tests multiple machine learning models to improve the state of AMV prediction from maps of sea surface temperature, salinity, and sea level pressure in the North Atlantic region. We use data from the Community Earth System Model 1 Large Ensemble Project, a state-of-the-art climate model with 3,440 years of data. Our results demonstrate that all of the models we use outperform the traditional persistence forecast baseline. Predicting the AMV is important for identifying future extreme temperatures and precipitation, as well as hurricane activity, in Europe and North America up to 25 years in advance.




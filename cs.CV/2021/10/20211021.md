# Arxiv Papers in cs.CV on 2021-10-21
### Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2110.10832v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10832v4)
- **Published**: 2021-10-21 00:08:17+00:00
- **Updated**: 2022-10-10 20:20:12+00:00
- **Authors**: Devansh Arpit, Huan Wang, Yingbo Zhou, Caiming Xiong
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: In Domain Generalization (DG) settings, models trained independently on a given set of training domains have notoriously chaotic performance on distribution shifted test domains, and stochasticity in optimization (e.g. seed) plays a big role. This makes deep learning models unreliable in real world settings. We first show that this chaotic behavior exists even along the training optimization trajectory of a single model, and propose a simple model averaging protocol that both significantly boosts domain generalization and diminishes the impact of stochasticity by improving the rank correlation between the in-domain validation accuracy and out-domain test accuracy, which is crucial for reliable early stopping. Taking advantage of our observation, we show that instead of ensembling unaveraged models (that is typical in practice), ensembling moving average models (EoA) from independent runs further boosts performance. We theoretically explain the boost in performance of ensembling and model averaging by adapting the well known Bias-Variance trade-off to the domain generalization setting. On the DomainBed benchmark, when using a pre-trained ResNet-50, this ensemble of averages achieves an average of $68.0\%$, beating vanilla ERM (w/o averaging/ensembling) by $\sim 4\%$, and when using a pre-trained RegNetY-16GF, achieves an average of $76.6\%$, beating vanilla ERM by $6\%$. Our code is available at https://github.com/salesforce/ensemble-of-averages.



### High-resolution rainfall-runoff modeling using graph neural network
- **Arxiv ID**: http://arxiv.org/abs/2110.10833v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10833v1)
- **Published**: 2021-10-21 00:12:02+00:00
- **Updated**: 2021-10-21 00:12:02+00:00
- **Authors**: Zhongrun Xiang, Ibrahim Demir
- **Comment**: None
- **Journal**: None
- **Summary**: Time-series modeling has shown great promise in recent studies using the latest deep learning algorithms such as LSTM (Long Short-Term Memory). These studies primarily focused on watershed-scale rainfall-runoff modeling or streamflow forecasting, but the majority of them only considered a single watershed as a unit. Although this simplification is very effective, it does not take into account spatial information, which could result in significant errors in large watersheds. Several studies investigated the use of GNN (Graph Neural Networks) for data integration by decomposing a large watershed into multiple sub-watersheds, but each sub-watershed is still treated as a whole, and the geoinformation contained within the watershed is not fully utilized. In this paper, we propose the GNRRM (Graph Neural Rainfall-Runoff Model), a novel deep learning model that makes full use of spatial information from high-resolution precipitation data, including flow direction and geographic information. When compared to baseline models, GNRRM has less over-fitting and significantly improves model performance. Our findings support the importance of hydrological data in deep learning-based rainfall-runoff modeling, and we encourage researchers to include more domain knowledge in their models.



### Integrating Visuospatial, Linguistic and Commonsense Structure into Story Visualization
- **Arxiv ID**: http://arxiv.org/abs/2110.10834v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10834v1)
- **Published**: 2021-10-21 00:16:02+00:00
- **Updated**: 2021-10-21 00:16:02+00:00
- **Authors**: Adyasha Maharana, Mohit Bansal
- **Comment**: EMNLP 2021 (16 pages)
- **Journal**: None
- **Summary**: While much research has been done in text-to-image synthesis, little work has been done to explore the usage of linguistic structure of the input text. Such information is even more important for story visualization since its inputs have an explicit narrative structure that needs to be translated into an image sequence (or visual story). Prior work in this domain has shown that there is ample room for improvement in the generated image sequence in terms of visual quality, consistency and relevance. In this paper, we first explore the use of constituency parse trees using a Transformer-based recurrent architecture for encoding structured input. Second, we augment the structured input with commonsense information and study the impact of this external knowledge on the generation of visual story. Third, we also incorporate visual structure via bounding boxes and dense captioning to provide feedback about the characters/objects in generated images within a dual learning setup. We show that off-the-shelf dense-captioning models trained on Visual Genome can improve the spatial structure of images from a different target domain without needing fine-tuning. We train the model end-to-end using intra-story contrastive loss (between words and image sub-regions) and show significant improvements in several metrics (and human evaluation) for multiple datasets. Finally, we provide an analysis of the linguistic and visuo-spatial information. Code and data: https://github.com/adymaharana/VLCStoryGan.



### A Domain Gap Aware Generative Adversarial Network for Multi-domain Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2110.10837v1
- **DOI**: 10.1109/TIP.2021.3125266
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10837v1)
- **Published**: 2021-10-21 00:33:06+00:00
- **Updated**: 2021-10-21 00:33:06+00:00
- **Authors**: Wenju Xu, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent image-to-image translation models have shown great success in mapping local textures between two domains. Existing approaches rely on a cycle-consistency constraint that supervises the generators to learn an inverse mapping. However, learning the inverse mapping introduces extra trainable parameters and it is unable to learn the inverse mapping for some domains. As a result, they are ineffective in the scenarios where (i) multiple visual image domains are involved; (ii) both structure and texture transformations are required; and (iii) semantic consistency is preserved. To solve these challenges, the paper proposes a unified model to translate images across multiple domains with significant domain gaps. Unlike previous models that constrain the generators with the ubiquitous cycle-consistency constraint to achieve the content similarity, the proposed model employs a perceptual self-regularization constraint. With a single unified generator, the model can maintain consistency over the global shapes as well as the local texture information across multiple domains. Extensive qualitative and quantitative evaluations demonstrate the effectiveness and superior performance over state-of-the-art models. It is more effective in representing shape deformation in challenging mappings with significant dataset variation across multiple domains.



### SMOF: Squeezing More Out of Filters Yields Hardware-Friendly CNN Pruning
- **Arxiv ID**: http://arxiv.org/abs/2110.10842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10842v1)
- **Published**: 2021-10-21 00:58:20+00:00
- **Updated**: 2021-10-21 00:58:20+00:00
- **Authors**: Yanli Liu, Bochen Guan, Qinwen Xu, Weiyi Li, Shuxue Quan
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: For many years, the family of convolutional neural networks (CNNs) has been a workhorse in deep learning. Recently, many novel CNN structures have been designed to address increasingly challenging tasks. To make them work efficiently on edge devices, researchers have proposed various structured network pruning strategies to reduce their memory and computational cost. However, most of them only focus on reducing the number of filter channels per layer without considering the redundancy within individual filter channels. In this work, we explore pruning from another dimension, the kernel size. We develop a CNN pruning framework called SMOF, which Squeezes More Out of Filters by reducing both kernel size and the number of filter channels. Notably, SMOF is friendly to standard hardware devices without any customized low-level implementations, and the pruning effort by kernel size reduction does not suffer from the fixed-size width constraint in SIMD units of general-purpose processors. The pruned networks can be deployed effortlessly with significant running time reduction. We also support these claims via extensive experiments on various CNN structures and general-purpose processors for mobile devices.



### Automated Scoring System of HER2 in Pathological Images under the Microscope
- **Arxiv ID**: http://arxiv.org/abs/2110.12900v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.AI, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12900v2)
- **Published**: 2021-10-21 01:38:35+00:00
- **Updated**: 2022-03-06 17:55:30+00:00
- **Authors**: Zichen Zhang, Lang Wang, Shuhao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is the most common cancer among women worldwide. The human epidermal growth factor receptor 2 (HER2) with immunohistochemical (IHC) is widely used for pathological evaluation to provide the appropriate therapy for patients with breast cancer. However, the deficiency of pathologists and subjective and susceptible to inter-observer variation of visual diagnosis are the main challenges. Recently, with the rapid development of artificial intelligence (AI) in disease diagnosis, several automated HER2 scoring methods using traditional computer vision or machine learning methods indicate the improvement of the HER2 diagnostic accuracy, but the unreasonable interpretation in pathology, as well as the expensive and ethical issues for annotation, make these methods still have a long way to deploy in hospitals to ease pathologists' burden in real. In this paper, we propose a HER2 automated scoring system that strictly follows the HER2 scoring guidelines simulating the real workflow of HER2 scores diagnosis by pathologists. Unlike the previous work, our method considers the positive control of HER2 to make sure the assay performance for each slide, eliminating work for repeated comparison between the current field of view (FOV) and positive control FOV, especially for the borderline cases. Besides, for each selected FOV under the microscope, our system provides real-time HER2 scores analysis and visualizations of the membrane staining intensity and completeness corresponding with the cell classifications. Our rigorous workflow along with the flexible interactive adjustion in demand substantially assists pathologists to finish the HER2 diagnosis faster and improves the robustness and accuracy. The proposed system will be embedded in our Thorough Eye platform for deployment in hospitals.



### Class-Discriminative CNN Compression
- **Arxiv ID**: http://arxiv.org/abs/2110.10864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10864v1)
- **Published**: 2021-10-21 02:54:05+00:00
- **Updated**: 2021-10-21 02:54:05+00:00
- **Authors**: Yuchen Liu, David Wentzlaff, S. Y. Kung
- **Comment**: None
- **Journal**: None
- **Summary**: Compressing convolutional neural networks (CNNs) by pruning and distillation has received ever-increasing focus in the community. In particular, designing a class-discrimination based approach would be desired as it fits seamlessly with the CNNs training objective. In this paper, we propose class-discriminative compression (CDC), which injects class discrimination in both pruning and distillation to facilitate the CNNs training goal. We first study the effectiveness of a group of discriminant functions for channel pruning, where we include well-known single-variate binary-class statistics like Student's T-Test in our study via an intuitive generalization. We then propose a novel layer-adaptive hierarchical pruning approach, where we use a coarse class discrimination scheme for early layers and a fine one for later layers. This method naturally accords with the fact that CNNs process coarse semantics in the early layers and extract fine concepts at the later. Moreover, we leverage discriminant component analysis (DCA) to distill knowledge of intermediate representations in a subspace with rich discriminative information, which enhances hidden layers' linear separability and classification accuracy of the student. Combining pruning and distillation, CDC is evaluated on CIFAR and ILSVRC 2012, where we consistently outperform the state-of-the-art results.



### LC3Net: Ladder context correlation complementary network for salient object detection
- **Arxiv ID**: http://arxiv.org/abs/2110.10869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10869v1)
- **Published**: 2021-10-21 03:12:32+00:00
- **Updated**: 2021-10-21 03:12:32+00:00
- **Authors**: Xian Fang, Jinchao Zhu, Xiuli Shao, Hongpeng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, existing salient object detection methods based on convolutional neural networks commonly resort to constructing discriminative networks to aggregate high level and low level features. However, contextual information is always not fully and reasonably utilized, which usually causes either the absence of useful features or contamination of redundant features. To address these issues, we propose a novel ladder context correlation complementary network (LC3Net) in this paper, which is equipped with three crucial components. At the beginning, we propose a filterable convolution block (FCB) to assist the automatic collection of information on the diversity of initial features, and it is simple yet practical. Besides, we propose a dense cross module (DCM) to facilitate the intimate aggregation of different levels of features by validly integrating semantic information and detailed information of both adjacent and non-adjacent layers. Furthermore, we propose a bidirectional compression decoder (BCD) to help the progressive shrinkage of multi-scale features from coarse to fine by leveraging multiple pairs of alternating top-down and bottom-up feature interaction flows. Extensive experiments demonstrate the superiority of our method against 16 state-of-the-art methods.



### HENet: Forcing a Network to Think More for Font Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.10872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10872v1)
- **Published**: 2021-10-21 03:25:47+00:00
- **Updated**: 2021-10-21 03:25:47+00:00
- **Authors**: Jingchao Chen, Shiyi Mu, Shugong Xu, Youdong Ding
- **Comment**: 8 pages, 2021 3rd International Conference on Advanced Information
  Science and System (AISS 2021)
- **Journal**: None
- **Summary**: Although lots of progress were made in Text Recognition/OCR in recent years, the task of font recognition is remaining challenging. The main challenge lies in the subtle difference between these similar fonts, which is hard to distinguish. This paper proposes a novel font recognizer with a pluggable module solving the font recognition task. The pluggable module hides the most discriminative accessible features and forces the network to consider other complicated features to solve the hard examples of similar fonts, called HE Block. Compared with the available public font recognition systems, our proposed method does not require any interactions at the inference stage. Extensive experiments demonstrate that HENet achieves encouraging performance, including on character-level dataset Explor_all and word-level dataset AdobeVFR



### Controllable and Compositional Generation with Latent-Space Energy-Based Models
- **Arxiv ID**: http://arxiv.org/abs/2110.10873v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10873v2)
- **Published**: 2021-10-21 03:31:45+00:00
- **Updated**: 2021-12-04 01:03:20+00:00
- **Authors**: Weili Nie, Arash Vahdat, Anima Anandkumar
- **Comment**: 32 pages, NeurIPS 2021
- **Journal**: None
- **Summary**: Controllable generation is one of the key requirements for successful adoption of deep generative models in real-world applications, but it still remains as a great challenge. In particular, the compositional ability to generate novel concept combinations is out of reach for most current models. In this work, we use energy-based models (EBMs) to handle compositional generation over a set of attributes. To make them scalable to high-resolution image generation, we introduce an EBM in the latent space of a pre-trained generative model such as StyleGAN. We propose a novel EBM formulation representing the joint distribution of data and attributes together, and we show how sampling from it is formulated as solving an ordinary differential equation (ODE). Given a pre-trained generator, all we need for controllable generation is to train an attribute classifier. Sampling with ODEs is done efficiently in the latent space and is robust to hyperparameters. Thus, our method is simple, fast to train, and efficient to sample. Experimental results show that our method outperforms the state-of-the-art in both conditional sampling and sequential editing. In compositional generation, our method excels at zero-shot generation of unseen attribute combinations. Also, by composing energy functions with logical operators, this work is the first to achieve such compositionality in generating photo-realistic images of resolution 1024x1024. Code is available at https://github.com/NVlabs/LACE.



### Evolving Transferable Neural Pruning Functions
- **Arxiv ID**: http://arxiv.org/abs/2110.10876v2
- **DOI**: 10.1145/3512290.3528694
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10876v2)
- **Published**: 2021-10-21 03:40:51+00:00
- **Updated**: 2022-08-04 02:10:56+00:00
- **Authors**: Yuchen Liu, S. Y. Kung, David Wentzlaff
- **Comment**: Published at GECCO 2022
- **Journal**: Proceedings of the Genetic and Evolutionary Computation
  Conference, 2022 (385--394)
- **Summary**: Structural design of neural networks is crucial for the success of deep learning. While most prior works in evolutionary learning aim at directly searching the structure of a network, few attempts have been made on another promising track, channel pruning, which recently has made major headway in designing efficient deep learning models. In fact, prior pruning methods adopt human-made pruning functions to score a channel's importance for channel pruning, which requires domain knowledge and could be sub-optimal. To this end, we pioneer the use of genetic programming (GP) to discover strong pruning metrics automatically. Specifically, we craft a novel design space to express high-quality and transferable pruning functions, which ensures an end-to-end evolution process where no manual modification is needed on the evolved functions for their transferability after evolution. Unlike prior methods, our approach can provide both compact pruned networks for efficient inference and novel closed-form pruning metrics which are mathematically explainable and thus generalizable to different pruning tasks. While the evolution is conducted on small datasets, our functions shows promising results when applied to more challenging datasets, different from those used in the evolution process. For example, on ILSVRC-2012, an evolved function achieves state-of-the-art pruning results.



### Evaluation of Various Open-Set Medical Imaging Tasks with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.10888v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10888v1)
- **Published**: 2021-10-21 04:19:41+00:00
- **Updated**: 2021-10-21 04:19:41+00:00
- **Authors**: Zongyuan Ge, Xin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The current generation of deep neural networks has achieved close-to-human results on "closed-set" image recognition; that is, the classes being evaluated overlap with the training classes. Many recent methods attempt to address the importance of the unknown, which are termed "open-set" recognition algorithms, try to reject unknown classes as well as maintain high recognition accuracy on known classes. However, it is still unclear how different general domain-trained open-set methods from ImageNet would perform on a different but more specific domain, such as the medical domain. Without principled and formal evaluations to measure the effectiveness of those general open-set methods, artificial intelligence (AI)-based medical diagnostics would experience ineffective adoption and increased risks of bad decision making. In this paper, we conduct rigorous evaluations amongst state-of-the-art open-set methods, exploring different open-set scenarios from "similar-domain" to "different-domain" scenarios and comparing them on various general and medical domain datasets. We summarise the results and core ideas and explain how the models react to various degrees of openness and different distributions of open classes. We show the main difference between general domain-trained and medical domain-trained open-set models with our quantitative and qualitative analysis of the results. We also identify aspects of model robustness in real clinical workflow usage according to confidence calibration and the inference efficiency.



### Privacy-Aware Identity Cloning Detection based on Deep Forest
- **Arxiv ID**: http://arxiv.org/abs/2110.10897v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10897v1)
- **Published**: 2021-10-21 04:55:52+00:00
- **Updated**: 2021-10-21 04:55:52+00:00
- **Authors**: Ahmed Alharbi, Hai Dong, Xun Yi, Prabath Abeysekara
- **Comment**: The 19th International Conference on Service Oriented Computing
  (ICSOC 2021). arXiv admin note: text overlap with arXiv:2109.15179
- **Journal**: None
- **Summary**: We propose a novel method to detect identity cloning of social-sensor cloud service providers to prevent the detrimental outcomes caused by identity deception. This approach leverages non-privacy-sensitive user profile data gathered from social networks and a powerful deep learning model to perform cloned identity detection. We evaluated the proposed method against the state-of-the-art identity cloning detection techniques and the other popular identity deception detection models atop a real-world dataset. The results show that our method significantly outperforms these techniques/models in terms of Precision and F1-score.



### Deep Image Matting with Flexible Guidance Input
- **Arxiv ID**: http://arxiv.org/abs/2110.10898v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10898v1)
- **Published**: 2021-10-21 04:59:27+00:00
- **Updated**: 2021-10-21 04:59:27+00:00
- **Authors**: Hang Cheng, Shugong Xu, Xiufeng Jiang, Rongrong Wang
- **Comment**: Accepted to BMVC2021
- **Journal**: None
- **Summary**: Image matting is an important computer vision problem. Many existing matting methods require a hand-made trimap to provide auxiliary information, which is very expensive and limits the real world usage. Recently, some trimap-free methods have been proposed, which completely get rid of any user input. However, their performance lag far behind trimap-based methods due to the lack of guidance information. In this paper, we propose a matting method that use Flexible Guidance Input as user hint, which means our method can use trimap, scribblemap or clickmap as guidance information or even work without any guidance input. To achieve this, we propose Progressive Trimap Deformation(PTD) scheme that gradually shrink the area of the foreground and background of the trimap with the training step increases and finally become a scribblemap. To make our network robust to any user scribble and click, we randomly sample points on foreground and background and perform curve fitting. Moreover, we propose Semantic Fusion Module(SFM) which utilize the Feature Pyramid Enhancement Module(FPEM) and Joint Pyramid Upsampling(JPU) in matting task for the first time. The experiments show that our method can achieve state-of-the-art results comparing with existing trimap-based and trimap-free methods.



### LARNet: Latent Action Representation for Human Action Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.10899v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10899v2)
- **Published**: 2021-10-21 05:04:32+00:00
- **Updated**: 2021-10-27 00:28:10+00:00
- **Authors**: Naman Biyani, Aayush J Rana, Shruti Vyas, Yogesh S Rawat
- **Comment**: British Machine Vision Conference (BMVC) 2021
- **Journal**: None
- **Summary**: We present LARNet, a novel end-to-end approach for generating human action videos. A joint generative modeling of appearance and dynamics to synthesize a video is very challenging and therefore recent works in video synthesis have proposed to decompose these two factors. However, these methods require a driving video to model the video dynamics. In this work, we propose a generative approach instead, which explicitly learns action dynamics in latent space avoiding the need of a driving video during inference. The generated action dynamics is integrated with the appearance using a recurrent hierarchical structure which induces motion at different scales to focus on both coarse as well as fine level action details. In addition, we propose a novel mix-adversarial loss function which aims at improving the temporal coherency of synthesized videos. We evaluate the proposed approach on four real-world human action datasets demonstrating the effectiveness of the proposed approach in generating human actions. Code available at https://github.com/aayushjr/larnet.



### A Fast Location Algorithm for Very Sparse Point Clouds Based on Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.10901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10901v1)
- **Published**: 2021-10-21 05:17:48+00:00
- **Updated**: 2021-10-21 05:17:48+00:00
- **Authors**: Shiyu Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Limited by the performance factor, it is arduous to recognize target object and locate it in Augmented Reality (AR) scenes on low-end mobile devices, especially which using monocular cameras. In this paper, we proposed an algorithm which can quickly locate the target object through image object detection in the circumstances of having very sparse feature points. We introduce YOLOv3-Tiny to our algorithm as the object detection module to filter the possible points and using Principal Component Analysis (PCA) to determine the location. We conduct the experiment in a manually designed scene by holding a smartphone and the results represent high positioning speed and accuracy of our method.



### Single-Modal Entropy based Active Learning for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2110.10906v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10906v2)
- **Published**: 2021-10-21 05:38:45+00:00
- **Updated**: 2021-11-18 06:44:13+00:00
- **Authors**: Dong-Jin Kim, Jae Won Cho, Jinsoo Choi, Yunjae Jung, In So Kweon
- **Comment**: Accepted to BMVC 2021
- **Journal**: None
- **Summary**: Constructing a large-scale labeled dataset in the real world, especially for high-level tasks (eg, Visual Question Answering), can be expensive and time-consuming. In addition, with the ever-growing amounts of data and architecture complexity, Active Learning has become an important aspect of computer vision research. In this work, we address Active Learning in the multi-modal setting of Visual Question Answering (VQA). In light of the multi-modal inputs, image and question, we propose a novel method for effective sample acquisition through the use of ad hoc single-modal branches for each input to leverage its information. Our mutual information based sample acquisition strategy Single-Modal Entropic Measure (SMEM) in addition to our self-distillation technique enables the sample acquisitor to exploit all present modalities and find the most informative samples. Our novel idea is simple to implement, cost-efficient, and readily adaptable to other multi-modal tasks. We confirm our findings on various VQA datasets through state-of-the-art performance by comparing to existing Active Learning baselines.



### Exploiting Inter-pixel Correlations in Unsupervised Domain Adaptation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.10916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10916v1)
- **Published**: 2021-10-21 06:11:44+00:00
- **Updated**: 2021-10-21 06:11:44+00:00
- **Authors**: Inseop Chung, Jayeon Yoo, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: "Self-training" has become a dominant method for semantic segmentation via unsupervised domain adaptation (UDA). It creates a set of pseudo labels for the target domain to give explicit supervision. However, the pseudo labels are noisy, sparse and do not provide any information about inter-pixel correlations. We regard inter-pixel correlation quite important because semantic segmentation is a task of predicting highly structured pixel-level outputs. Therefore, in this paper, we propose a method of transferring the inter-pixel correlations from the source domain to the target domain via a self-attention module. The module takes the prediction of the segmentation network as an input and creates a self-attended prediction that correlates similar pixels. The module is trained only on the source domain to learn the domain-invariant inter-pixel correlations, then later, it is used to train the segmentation network on the target domain. The network learns not only from the pseudo labels but also by following the output of the self-attention module which provides additional knowledge about the inter-pixel correlations. Through extensive experiments, we show that our method significantly improves the performance on two standard UDA benchmarks and also can be combined with recent state-of-the-art method to achieve better performance.



### CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization
- **Arxiv ID**: http://arxiv.org/abs/2110.10921v2
- **DOI**: 10.1109/TNNLS.2023.3262952
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10921v2)
- **Published**: 2021-10-21 06:26:31+00:00
- **Updated**: 2023-03-30 05:05:23+00:00
- **Authors**: Wenzheng Hu, Zhengping Che, Ning Liu, Mingyang Li, Jian Tang, Changshui Zhang, Jianqiang Wang
- **Comment**: Paper accepted by IEEE Transactions on Neural Networks and Learning
  Systems (TNNLS)
- **Journal**: None
- **Summary**: Deep convolutional neural networks are shown to be overkill with high parametric and computational redundancy in many application scenarios, and an increasing number of works have explored model pruning to obtain lightweight and efficient networks. However, most existing pruning approaches are driven by empirical heuristic and rarely consider the joint impact of channels, leading to unguaranteed and suboptimal performance. In this paper, we propose a novel channel pruning method via Class-Aware Trace Ratio Optimization (CATRO) to reduce the computational burden and accelerate the model inference. Utilizing class information from a few samples, CATRO measures the joint impact of multiple channels by feature space discriminations and consolidates the layer-wise impact of preserved channels. By formulating channel pruning as a submodular set function maximization problem, CATRO solves it efficiently via a two-stage greedy iterative optimization procedure. More importantly, we present theoretical justifications on convergence of CATRO and performance of pruned networks. Experimental results demonstrate that CATRO achieves higher accuracy with similar computation cost or lower computation cost with similar accuracy than other state-of-the-art channel pruning algorithms. In addition, because of its class-aware property, CATRO is suitable to prune efficient networks adaptively for various classification subtasks, enhancing handy deployment and usage of deep networks in real-world applications.



### Autonomous Dimension Reduction by Flattening Deformation of Data Manifold under an Intrinsic Deforming Field
- **Arxiv ID**: http://arxiv.org/abs/2110.10938v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10938v1)
- **Published**: 2021-10-21 07:20:23+00:00
- **Updated**: 2021-10-21 07:20:23+00:00
- **Authors**: Xiaodong Zhuang
- **Comment**: 18 pages, 23 figures
- **Journal**: None
- **Summary**: A new dimension reduction (DR) method for data sets is proposed by autonomous deforming of data manifolds. The deformation is guided by the proposed deforming vector field, which is defined by two kinds of virtual interactions between data points. The flattening of data manifold is achieved as an emergent behavior under the elastic and repelling interactions between data points, meanwhile the topological structure of the manifold is preserved. To overcome the uneven sampling (or "short-cut edge") problem, the soft neighborhood is proposed, in which the neighbor degree is defined and adaptive interactions between neighbor points is implemented. The proposed method provides a novel geometric viewpoint on dimension reduction. Experimental results prove the effectiveness of the proposed method in dimension reduction, and implicit feature of data sets may also be revealed.



### Multimodal Learning using Optimal Transport for Sarcasm and Humor Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.10949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10949v1)
- **Published**: 2021-10-21 07:51:56+00:00
- **Updated**: 2021-10-21 07:51:56+00:00
- **Authors**: Shraman Pramanick, Aniket Roy, Vishal M. Patel
- **Comment**: Accepted to WACV 2022
- **Journal**: None
- **Summary**: Multimodal learning is an emerging yet challenging research area. In this paper, we deal with multimodal sarcasm and humor detection from conversational videos and image-text pairs. Being a fleeting action, which is reflected across the modalities, sarcasm detection is challenging since large datasets are not available for this task in the literature. Therefore, we primarily focus on resource-constrained training, where the number of training samples is limited. To this end, we propose a novel multimodal learning system, MuLOT (Multimodal Learning using Optimal Transport), which utilizes self-attention to exploit intra-modal correspondence and optimal transport for cross-modal correspondence. Finally, the modalities are combined with multimodal attention fusion to capture the inter-dependencies across modalities. We test our approach for multimodal sarcasm and humor detection on three benchmark datasets - MUStARD (video, audio, text), UR-FUNNY (video, audio, text), MST (image, text) and obtain 2.1%, 1.54%, and 2.34% accuracy improvements over state-of-the-art.



### MOS: A Low Latency and Lightweight Framework for Face Detection, Landmark Localization, and Head Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.10953v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.10953v3)
- **Published**: 2021-10-21 08:05:53+00:00
- **Updated**: 2021-11-01 06:28:45+00:00
- **Authors**: Yepeng Liu, Zaiwang Gu, Shenghua Gao, Dong Wang, Yusheng Zeng, Jun Cheng
- **Comment**: Accepted at BMVC 2021
- **Journal**: None
- **Summary**: With the emergence of service robots and surveillance cameras, dynamic face recognition (DFR) in wild has received much attention in recent years. Face detection and head pose estimation are two important steps for DFR. Very often, the pose is estimated after the face detection. However, such sequential computations lead to higher latency. In this paper, we propose a low latency and lightweight network for simultaneous face detection, landmark localization and head pose estimation. Inspired by the observation that it is more challenging to locate the facial landmarks for faces with large angles, a pose loss is proposed to constrain the learning. Moreover, we also propose an uncertainty multi-task loss to learn the weights of individual tasks automatically. Another challenge is that robots often use low computational units like ARM based computing core and we often need to use lightweight networks instead of the heavy ones, which lead to performance drop especially for small and hard faces. In this paper, we propose online feedback sampling to augment the training samples across different scales, which increases the diversity of training data automatically. Through validation in commonly used WIDER FACE, AFLW and AFLW2000 datasets, the results show that the proposed method achieves the state-of-the-art performance in low computational resources. The code and data will be available at https://github.com/lyp-deeplearning/MOS-Multi-Task-Face-Detect.



### Multi-label Classification with Partial Annotations using Class-aware Selective Loss
- **Arxiv ID**: http://arxiv.org/abs/2110.10955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10955v1)
- **Published**: 2021-10-21 08:10:55+00:00
- **Updated**: 2021-10-21 08:10:55+00:00
- **Authors**: Emanuel Ben-Baruch, Tal Ridnik, Itamar Friedman, Avi Ben-Cohen, Nadav Zamir, Asaf Noy, Lihi Zelnik-Manor
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale multi-label classification datasets are commonly, and perhaps inevitably, partially annotated. That is, only a small subset of labels are annotated per sample. Different methods for handling the missing labels induce different properties on the model and impact its accuracy. In this work, we analyze the partial labeling problem, then propose a solution based on two key ideas. First, un-annotated labels should be treated selectively according to two probability quantities: the class distribution in the overall dataset and the specific label likelihood for a given data sample. We propose to estimate the class distribution using a dedicated temporary model, and we show its improved efficiency over a naive estimation computed using the dataset's partial annotations. Second, during the training of the target model, we emphasize the contribution of annotated labels over originally un-annotated labels by using a dedicated asymmetric loss. With our novel approach, we achieve state-of-the-art results on OpenImages dataset (e.g. reaching 87.3 mAP on V6). In addition, experiments conducted on LVIS and simulated-COCO demonstrate the effectiveness of our approach. Code is available at https://github.com/Alibaba-MIIL/PartialLabelingCSL.



### Vis-TOP: Visual Transformer Overlay Processor
- **Arxiv ID**: http://arxiv.org/abs/2110.10957v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/2110.10957v1)
- **Published**: 2021-10-21 08:11:12+00:00
- **Updated**: 2021-10-21 08:11:12+00:00
- **Authors**: Wei Hu, Dian Xu, Zimeng Fan, Fang Liu, Yanxiang He
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: In recent years, Transformer has achieved good results in Natural Language Processing (NLP) and has also started to expand into Computer Vision (CV). Excellent models such as the Vision Transformer and Swin Transformer have emerged. At the same time, the platform for Transformer models was extended to embedded devices to meet some resource-sensitive application scenarios. However, due to the large number of parameters, the complex computational flow and the many different structural variants of Transformer models, there are a number of issues that need to be addressed in its hardware design. This is both an opportunity and a challenge. We propose Vis-TOP (Visual Transformer Overlay Processor), an overlay processor for various visual Transformer models. It differs from coarse-grained overlay processors such as CPU, GPU, NPE, and from fine-grained customized designs for a specific model. Vis-TOP summarizes the characteristics of all visual Transformer models and implements a three-layer and two-level transformation structure that allows the model to be switched or changed freely without changing the hardware architecture. At the same time, the corresponding instruction bundle and hardware architecture are designed in three-layer and two-level transformation structure. After quantization of Swin Transformer tiny model using 8-bit fixed points (fix_8), we implemented an overlay processor on the ZCU102. Compared to GPU, the TOP throughput is 1.5x higher. Compared to the existing Transformer accelerators, our throughput per DSP is between 2.2x and 11.7x higher than others. In a word, the approach in this paper meets the requirements of real-time AI in terms of both resource consumption and inference speed. Vis-TOP provides a cost-effective and power-effective solution based on reconfigurable devices for computer vision at the edge.



### 2020 CATARACTS Semantic Segmentation Challenge
- **Arxiv ID**: http://arxiv.org/abs/2110.10965v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10965v2)
- **Published**: 2021-10-21 08:26:23+00:00
- **Updated**: 2022-02-24 14:40:08+00:00
- **Authors**: Imanol Luengo, Maria Grammatikopoulou, Rahim Mohammadi, Chris Walsh, Chinedu Innocent Nwoye, Deepak Alapatt, Nicolas Padoy, Zhen-Liang Ni, Chen-Chen Fan, Gui-Bin Bian, Zeng-Guang Hou, Heonjin Ha, Jiacheng Wang, Haojie Wang, Dong Guo, Lu Wang, Guotai Wang, Mobarakol Islam, Bharat Giddwani, Ren Hongliang, Theodoros Pissas, Claudio Ravasio, Martin Huber, Jeremy Birch, Joan M. Nunez Do Rio, Lyndon da Cruz, Christos Bergeles, Hongyu Chen, Fucang Jia, Nikhil KumarTomar, Debesh Jha, Michael A. Riegler, Pal Halvorsen, Sophia Bano, Uddhav Vaghela, Jianyuan Hong, Haili Ye, Feihong Huang, Da-Han Wang, Danail Stoyanov
- **Comment**: None
- **Journal**: None
- **Summary**: Surgical scene segmentation is essential for anatomy and instrument localization which can be further used to assess tissue-instrument interactions during a surgical procedure. In 2017, the Challenge on Automatic Tool Annotation for cataRACT Surgery (CATARACTS) released 50 cataract surgery videos accompanied by instrument usage annotations. These annotations included frame-level instrument presence information. In 2020, we released pixel-wise semantic annotations for anatomy and instruments for 4670 images sampled from 25 videos of the CATARACTS training set. The 2020 CATARACTS Semantic Segmentation Challenge, which was a sub-challenge of the 2020 MICCAI Endoscopic Vision (EndoVis) Challenge, presented three sub-tasks to assess participating solutions on anatomical structure and instrument segmentation. Their performance was assessed on a hidden test set of 531 images from 10 videos of the CATARACTS test set.



### Weakly Supervised Training of Monocular 3D Object Detectors Using Wide Baseline Multi-view Traffic Camera Data
- **Arxiv ID**: http://arxiv.org/abs/2110.10966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10966v1)
- **Published**: 2021-10-21 08:26:48+00:00
- **Updated**: 2021-10-21 08:26:48+00:00
- **Authors**: Matthew Howe, Ian Reid, Jamie Mackenzie
- **Comment**: Paper accepted at The 32nd British Machine Vision Conference, BMVC
  2021
- **Journal**: None
- **Summary**: Accurate 7DoF prediction of vehicles at an intersection is an important task for assessing potential conflicts between road users. In principle, this could be achieved by a single camera system that is capable of detecting the pose of each vehicle but this would require a large, accurately labelled dataset from which to train the detector. Although large vehicle pose datasets exist (ostensibly developed for autonomous vehicles), we find training on these datasets inadequate. These datasets contain images from a ground level viewpoint, whereas an ideal view for intersection observation would be elevated higher above the road surface. We develop an alternative approach using a weakly supervised method of fine tuning 3D object detectors for traffic observation cameras; showing in the process that large existing autonomous vehicle datasets can be leveraged for pre-training. To fine-tune the monocular 3D object detector, our method utilises multiple 2D detections from overlapping, wide-baseline views and a loss that encodes the subjacent geometric consistency. Our method achieves vehicle 7DoF pose prediction accuracy on our dataset comparable to the top performing monocular 3D object detectors on autonomous vehicle datasets. We present our training methodology, multi-view reprojection loss, and dataset.



### Memory Efficient Adaptive Attention For Multiple Domain Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.10969v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2110.10969v1)
- **Published**: 2021-10-21 08:33:29+00:00
- **Updated**: 2021-10-21 08:33:29+00:00
- **Authors**: Himanshu Pradeep Aswani, Abhiraj Sunil Kanse, Shubhang Bhatnagar, Amit Sethi
- **Comment**: 13 pages, 3 figures, 4 graphs, 3 tables
- **Journal**: None
- **Summary**: Training CNNs from scratch on new domains typically demands large numbers of labeled images and computations, which is not suitable for low-power hardware. One way to reduce these requirements is to modularize the CNN architecture and freeze the weights of the heavier modules, that is, the lower layers after pre-training. Recent studies have proposed alternative modular architectures and schemes that lead to a reduction in the number of trainable parameters needed to match the accuracy of fully fine-tuned CNNs on new domains. Our work suggests that a further reduction in the number of trainable parameters by an order of magnitude is possible. Furthermore, we propose that new modularization techniques for multiple domain learning should also be compared on other realistic metrics, such as the number of interconnections needed between the fixed and trainable modules, the number of training samples needed, the order of computations required and the robustness to partial mislabeling of the training data. On all of these criteria, the proposed architecture demonstrates advantages over or matches the current state-of-the-art.



### Pixel-Level Face Image Quality Assessment for Explainable Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.11001v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11001v3)
- **Published**: 2021-10-21 09:12:17+00:00
- **Updated**: 2021-11-30 09:37:14+00:00
- **Authors**: Philipp Terh√∂rst, Marco Huber, Naser Damer, Florian Kirchbuchner, Kiran Raja, Arjan Kuijper
- **Comment**: None
- **Journal**: None
- **Summary**: An essential factor to achieve high performance in face recognition systems is the quality of its samples. Since these systems are involved in daily life there is a strong need of making face recognition processes understandable for humans. In this work, we introduce the concept of pixel-level face image quality that determines the utility of pixels in a face image for recognition. We propose a training-free approach to assess the pixel-level qualities of a face image given an arbitrary face recognition network. To achieve this, a model-specific quality value of the input image is estimated and used to build a sample-specific quality regression model. Based on this model, quality-based gradients are back-propagated and converted into pixel-level quality estimates. In the experiments, we qualitatively and quantitatively investigated the meaningfulness of our proposed pixel-level qualities based on real and artificial disturbances and by comparing the explanation maps on faces incompliant with the ICAO standards. In all scenarios, the results demonstrate that the proposed solution produces meaningful pixel-level qualities enhancing the interpretability of the complete face image quality. The code is publicly available



### Towards Reducing Aleatoric Uncertainty for Medical Imaging Tasks
- **Arxiv ID**: http://arxiv.org/abs/2110.11012v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11012v2)
- **Published**: 2021-10-21 09:31:00+00:00
- **Updated**: 2022-05-08 21:58:37+00:00
- **Authors**: Abhishek Singh Sambyal, Narayanan C. Krishnan, Deepti R. Bathula
- **Comment**: Accepted in IEEE International Symposium on Biomedical Imaging (ISBI)
  2022
- **Journal**: None
- **Summary**: In safety-critical applications like medical diagnosis, certainty associated with a model's prediction is just as important as its accuracy. Consequently, uncertainty estimation and reduction play a crucial role. Uncertainty in predictions can be attributed to noise or randomness in data (aleatoric) and incorrect model inferences (epistemic). While model uncertainty can be reduced with more data or bigger models, aleatoric uncertainty is more intricate. This work proposes a novel approach that interprets data uncertainty estimated from a self-supervised task as noise inherent to the data and utilizes it to reduce aleatoric uncertainty in another task related to the same dataset via data augmentation. The proposed method was evaluated on a benchmark medical imaging dataset with image reconstruction as the self-supervised task and segmentation as the image analysis task. Our findings demonstrate the effectiveness of the proposed approach in significantly reducing the aleatoric uncertainty in the image segmentation task while achieving better or on-par performance compared to the standard augmentation techniques.



### Spatial Location Constraint Prototype Loss for Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.11013v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11013v3)
- **Published**: 2021-10-21 09:34:01+00:00
- **Updated**: 2021-11-12 09:59:14+00:00
- **Authors**: Ziheng Xia, Ganggang Dong, Penghui Wang, Hongwei Liu
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: One of the challenges in pattern recognition is open set recognition. Compared with closed set recognition, open set recognition needs to reduce not only the empirical risk, but also the open space risk, and the reduction of these two risks corresponds to classifying the known classes and identifying the unknown classes respectively. How to reduce the open space risk is the key of open set recognition. This paper explores the origin of the open space risk by analyzing the distribution of known and unknown classes features. On this basis, the spatial location constraint prototype loss function is proposed to reduce the two risks simultaneously. Extensive experiments on multiple benchmark datasets and many visualization results indicate that our methods is superior to most existing approaches.



### Augmenting Knowledge Distillation With Peer-To-Peer Mutual Learning For Model Compression
- **Arxiv ID**: http://arxiv.org/abs/2110.11023v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11023v2)
- **Published**: 2021-10-21 09:59:31+00:00
- **Updated**: 2021-10-22 08:15:21+00:00
- **Authors**: Usma Niyaz, Deepti R. Bathula
- **Comment**: changed the format of paper
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is an effective model compression technique where a compact student network is taught to mimic the behavior of a complex and highly trained teacher network. In contrast, Mutual Learning (ML) provides an alternative strategy where multiple simple student networks benefit from sharing knowledge, even in the absence of a powerful but static teacher network. Motivated by these findings, we propose a single-teacher, multi-student framework that leverages both KD and ML to achieve better performance. Furthermore, an online distillation strategy is utilized to train the teacher and students simultaneously. To evaluate the performance of the proposed approach, extensive experiments were conducted using three different versions of teacher-student networks on benchmark biomedical classification (MSI vs. MSS) and object detection (Polyp Detection) tasks. Ensemble of student networks trained in the proposed manner achieved better results than the ensemble of students trained using KD or ML individually, establishing the benefit of augmenting knowledge transfer from teacher to students with peer-to-peer learning between students.



### RefRec: Pseudo-labels Refinement via Shape Reconstruction for Unsupervised 3D Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2110.11036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11036v1)
- **Published**: 2021-10-21 10:24:05+00:00
- **Updated**: 2021-10-21 10:24:05+00:00
- **Authors**: Adriano Cardace, Riccardo Spezialetti, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano
- **Comment**: 3DV 2021 (Oral) Code: https://github.com/CVLAB-Unibo/RefRec
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) for point cloud classification is an emerging research problem with relevant practical motivations. Reliance on multi-task learning to align features across domains has been the standard way to tackle it. In this paper, we take a different path and propose RefRec, the first approach to investigate pseudo-labels and self-training in UDA for point clouds. We present two main innovations to make self-training effective on 3D data: i) refinement of noisy pseudo-labels by matching shape descriptors that are learned by the unsupervised task of shape reconstruction on both domains; ii) a novel self-training protocol that learns domain-specific decision boundaries and reduces the negative impact of mislabelled target samples and in-domain intra-class variability. RefRec sets the new state of the art in both standard benchmarks used to test UDA for point cloud classification, showcasing the effectiveness of self-training for this important problem.



### Improving the Deployment of Recycling Classification through Efficient Hyper-Parameter Analysis
- **Arxiv ID**: http://arxiv.org/abs/2110.11043v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11043v2)
- **Published**: 2021-10-21 10:42:14+00:00
- **Updated**: 2021-10-22 14:40:04+00:00
- **Authors**: Mazin Abdulmahmood, Ryan Grammenos
- **Comment**: None
- **Journal**: None
- **Summary**: The paradigm of automated waste classification has recently seen a shift in the domain of interest from conventional image processing techniques to powerful computer vision algorithms known as convolutional neural networks (CNN). Historically, CNNs have demonstrated a strong dependency on powerful hardware for real-time classification, yet the need for deployment on weaker embedded devices is greater than ever. The work in this paper proposes a methodology for reconstructing and tuning conventional image classification models, using EfficientNets, to decrease their parameterisation with no trade-off in model accuracy and develops a pipeline through TensorRT for accelerating such models to run at real-time on an NVIDIA Jetson Nano embedded device. The train-deployment discrepancy, relating how poor data augmentation leads to a discrepancy in model accuracy between training and deployment, is often neglected in many papers and thus the work is extended by analysing and evaluating the impact real world perturbations had on model accuracy once deployed. The scope of the work concerns developing a more efficient variant of WasteNet, a collaborative recycling classification model. The newly developed model scores a test-set accuracy of 95.8% with a real world accuracy of 95%, a 14% increase over the original. Our acceleration pipeline boosted model throughput by 750% to 24 inferences per second on the Jetson Nano and real-time latency of the system was verified through servomotor latency analysis.



### K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways
- **Arxiv ID**: http://arxiv.org/abs/2110.11048v2
- **DOI**: 10.1109/CVPRW56347.2022.00491
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.11048v2)
- **Published**: 2021-10-21 10:46:50+00:00
- **Updated**: 2022-06-06 11:09:19+00:00
- **Authors**: Donghee Paek, Seung-Hyun Kong, Kevin Tirta Wijaya
- **Comment**: 20 pages, 20 figures, 11 tables
- **Journal**: 2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshop on Autonomous Driving (WAD)
- **Summary**: Lane detection is a critical function for autonomous driving. With the recent development of deep learning and the publication of camera lane datasets and benchmarks, camera lane detection networks (CLDNs) have been remarkably developed. Unfortunately, CLDNs rely on camera images which are often distorted near the vanishing line and prone to poor lighting condition. This is in contrast with Lidar lane detection networks (LLDNs), which can directly extract the lane lines on the bird's eye view (BEV) for motion planning and operate robustly under various lighting conditions. However, LLDNs have not been actively studied, mostly due to the absence of large public lidar lane datasets. In this paper, we introduce KAIST-Lane (K-Lane), the world's first and the largest public urban road and highway lane dataset for Lidar. K-Lane has more than 15K frames and contains annotations of up to six lanes under various road and traffic conditions, e.g., occluded roads of multiple occlusion levels, roads at day and night times, merging (converging and diverging) and curved lanes. We also provide baseline networks we term Lidar lane detection networks utilizing global feature correlator (LLDN-GFC). LLDN-GFC exploits the spatial characteristics of lane lines on the point cloud, which are sparse, thin, and stretched along the entire ground plane of the point cloud. From experimental results, LLDN-GFC achieves the state-of-the-art performance with an F1- score of 82.1%, on the K-Lane. Moreover, LLDN-GFC shows strong performance under various lighting conditions, which is unlike CLDNs, and also robust even in the case of severe occlusions, unlike LLDNs using the conventional CNN. The K-Lane, LLDN-GFC training code, pre-trained models, and complete development kits including evaluation, visualization and annotation tools are available at https://github.com/kaist-avelab/k-lane.



### Transfer beyond the Field of View: Dense Panoramic Semantic Segmentation via Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2110.11062v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11062v1)
- **Published**: 2021-10-21 11:22:05+00:00
- **Updated**: 2021-10-21 11:22:05+00:00
- **Authors**: Jiaming Zhang, Chaoxiang Ma, Kailun Yang, Alina Roitberg, Kunyu Peng, Rainer Stiefelhagen
- **Comment**: Accepted to IEEE Transactions on Intelligent Transportation Systems
  (IEEE T-ITS). Dataset and code will be made publicly available at
  https://github.com/chma1024/DensePASS. arXiv admin note: substantial text
  overlap with arXiv:2108.06383
- **Journal**: None
- **Summary**: Autonomous vehicles clearly benefit from the expanded Field of View (FoV) of 360-degree sensors, but modern semantic segmentation approaches rely heavily on annotated training data which is rarely available for panoramic images. We look at this problem from the perspective of domain adaptation and bring panoramic semantic segmentation to a setting, where labelled training data originates from a different distribution of conventional pinhole camera images. To achieve this, we formalize the task of unsupervised domain adaptation for panoramic semantic segmentation and collect DensePASS - a novel densely annotated dataset for panoramic segmentation under cross-domain conditions, specifically built to study the Pinhole-to-Panoramic domain shift and accompanied with pinhole camera training examples obtained from Cityscapes. DensePASS covers both, labelled- and unlabelled 360-degree images, with the labelled data comprising 19 classes which explicitly fit the categories available in the source (i.e. pinhole) domain. Since data-driven models are especially susceptible to changes in data distribution, we introduce P2PDA - a generic framework for Pinhole-to-Panoramic semantic segmentation which addresses the challenge of domain divergence with different variants of attention-augmented domain adaptation modules, enabling the transfer in output-, feature-, and feature confidence spaces. P2PDA intertwines uncertainty-aware adaptation using confidence values regulated on-the-fly through attention heads with discrepant predictions. Our framework facilitates context exchange when learning domain correspondences and dramatically improves the adaptation performance of accuracy- and efficiency-focused models. Comprehensive experiments verify that our framework clearly surpasses unsupervised domain adaptation- and specialized panoramic segmentation approaches.



### Robust Edge-Direct Visual Odometry based on CNN edge detection and Shi-Tomasi corner optimization
- **Arxiv ID**: http://arxiv.org/abs/2110.11064v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.11064v1)
- **Published**: 2021-10-21 11:22:34+00:00
- **Updated**: 2021-10-21 11:22:34+00:00
- **Authors**: Kengdong Lu, Jintao Cheng, Yubin Zhou, Juncan Deng, Rui Fan, Kaiqing Luo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a robust edge-direct visual odometry (VO) based on CNN edge detection and Shi-Tomasi corner optimization. Four layers of pyramids were extracted from the image in the proposed method to reduce the motion error between frames. This solution used CNN edge detection and Shi-Tomasi corner optimization to extract information from the image. Then, the pose estimation is performed using the Levenberg-Marquardt (LM) algorithm and updating the keyframes. Our method was compared with the dense direct method, the improved direct method of Canny edge detection, and ORB-SLAM2 system on the RGB-D TUM benchmark. The experimental results indicate that our method achieves better robustness and accuracy.



### Grafting Transformer on Automatically Designed Convolutional Neural Network for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.11084v3
- **DOI**: 10.1109/TGRS.2022.3180685
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11084v3)
- **Published**: 2021-10-21 11:51:51+00:00
- **Updated**: 2023-04-06 12:30:49+00:00
- **Authors**: Xizhe Xue, Haokui Zhang, Bei Fang, Zongwen Bai, Ying Li
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) classification has been a hot topic for decides, as hyperspectral images have rich spatial and spectral information and provide strong basis for distinguishing different land-cover objects. Benefiting from the development of deep learning technologies, deep learning based HSI classification methods have achieved promising performance. Recently, several neural architecture search (NAS) algorithms have been proposed for HSI classification, which further improve the accuracy of HSI classification to a new level. In this paper, NAS and Transformer are combined for handling HSI classification task for the first time. Compared with previous work, the proposed method has two main differences. First, we revisit the search spaces designed in previous HSI classification NAS methods and propose a novel hybrid search space, consisting of the space dominated cell and the spectrum dominated cell. Compared with search spaces proposed in previous works, the proposed hybrid search space is more aligned with the characteristic of HSI data, that is, HSIs have a relatively low spatial resolution and an extremely high spectral resolution. Second, to further improve the classification accuracy, we attempt to graft the emerging transformer module on the automatically designed convolutional neural network (CNN) to add global information to local region focused features learned by CNN. Experimental results on three public HSI datasets show that the proposed method achieves much better performance than comparison approaches, including manually designed network and NAS based HSI classification methods. Especially on the most recently captured dataset Houston University, overall accuracy is improved by nearly 6 percentage points. Code is available at: https://github.com/Cecilia-xue/HyT-NAS.



### Reinforcement Learning Based Optimal Camera Placement for Depth Observation of Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2110.11106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11106v1)
- **Published**: 2021-10-21 12:47:47+00:00
- **Updated**: 2021-10-21 12:47:47+00:00
- **Authors**: Yichuan Chen, Manabu Tsukada, Hiroshi Esaki
- **Comment**: Accepted to IEEE International Conference on Networking, Sensing and
  Control (ICNSC) 2021
- **Journal**: None
- **Summary**: Exploring the most task-friendly camera setting -- optimal camera placement (OCP) problem -- in tasks that use multiple cameras is of great importance. However, few existing OCP solutions specialize in depth observation of indoor scenes, and most versatile solutions work offline. To this problem, an OCP online solution to depth observation of indoor scenes based on reinforcement learning is proposed in this paper. The proposed solution comprises a simulation environment that implements scene observation and reward estimation using shadow maps and an agent network containing a soft actor-critic (SAC)-based reinforcement learning backbone and a feature extractor to extract features from the observed point cloud layer-by-layer. Comparative experiments with two state-of-the-art optimization-based offline methods are conducted. The experimental results indicate that the proposed system outperforms seven out of ten test scenes in obtaining lower depth observation error. The total error in all test scenes is also less than 90% of the baseline ones. Therefore, the proposed system is more competent for depth camera placement in scenarios where there is no prior knowledge of the scenes or where a lower depth observation error is the main objective.



### Extraction of Positional Player Data from Broadcast Soccer Videos
- **Arxiv ID**: http://arxiv.org/abs/2110.11107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11107v1)
- **Published**: 2021-10-21 12:49:56+00:00
- **Updated**: 2021-10-21 12:49:56+00:00
- **Authors**: Jonas Theiner, Wolfgang Gritz, Eric M√ºller-Budack, Robert Rein, Daniel Memmert, Ralph Ewerth
- **Comment**: Accepted for publication at WACV'22; Preprint
- **Journal**: None
- **Summary**: Computer-aided support and analysis are becoming increasingly important in the modern world of sports. The scouting of potential prospective players, performance as well as match analysis, and the monitoring of training programs rely more and more on data-driven technologies to ensure success. Therefore, many approaches require large amounts of data, which are, however, not easy to obtain in general. In this paper, we propose a pipeline for the fully-automated extraction of positional data from broadcast video recordings of soccer matches. In contrast to previous work, the system integrates all necessary sub-tasks like sports field registration, player detection, or team assignment that are crucial for player position estimation. The quality of the modules and the entire system is interdependent. A comprehensive experimental evaluation is presented for the individual modules as well as the entire pipeline to identify the influence of errors to subsequent modules and the overall result. In this context, we propose novel evaluation metrics to compare the output with ground-truth positional data.



### A Deep Insight into Measuring Face Image Utility with General and Face-specific Image Quality Metrics
- **Arxiv ID**: http://arxiv.org/abs/2110.11111v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11111v2)
- **Published**: 2021-10-21 12:56:38+00:00
- **Updated**: 2021-10-22 12:01:38+00:00
- **Authors**: Biying Fu, Cong Chen, Olaf Henniger, Naser Damer
- **Comment**: Accepted at the IEEE Winter Conference on Applications of Computer
  Vision, WACV 2022
- **Journal**: None
- **Summary**: Quality scores provide a measure to evaluate the utility of biometric samples for biometric recognition. Biometric recognition systems require high-quality samples to achieve optimal performance. This paper focuses on face images and the measurement of face image utility with general and face-specific image quality metrics. While face-specific metrics rely on features of aligned face images, general image quality metrics can be used on the global image and relate to human perceptions. In this paper, we analyze the gap between the general image quality metrics and the face image quality metrics. Our contribution lies in a thorough examination of how different the image quality assessment algorithms relate to the utility for the face recognition task. The results of image quality assessment algorithms are further compared with those of dedicated face image quality assessment algorithms. In total, 25 different quality metrics are evaluated on three face image databases, BioSecure, LFW, and VGGFace2 using three open-source face recognition solutions, SphereFace, ArcFace, and FaceNet. Our results reveal a clear correlation between learned image metrics to face image utility even without being specifically trained as a face utility measure. Individual handcrafted features lack general stability and perform significantly worse than general face-specific quality metrics. We additionally provide a visual insight into the image areas contributing to the quality score of a selected set of quality assessment methods.



### A Strong Baseline for Semi-Supervised Incremental Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.11128v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11128v2)
- **Published**: 2021-10-21 13:25:52+00:00
- **Updated**: 2021-11-04 02:08:12+00:00
- **Authors**: Linlan Zhao, Dashan Guo, Yunlu Xu, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Xiangzhong Fang
- **Comment**: Accepted by BMVC2021
- **Journal**: None
- **Summary**: Few-shot learning (FSL) aims to learn models that generalize to novel classes with limited training samples. Recent works advance FSL towards a scenario where unlabeled examples are also available and propose semi-supervised FSL methods. Another line of methods also cares about the performance of base classes in addition to the novel ones and thus establishes the incremental FSL scenario. In this paper, we generalize the above two under a more realistic yet complex setting, named by Semi-Supervised Incremental Few-Shot Learning (S2 I-FSL). To tackle the task, we propose a novel paradigm containing two parts: (1) a well-designed meta-training algorithm for mitigating ambiguity between base and novel classes caused by unreliable pseudo labels and (2) a model adaptation mechanism to learn discriminative features for novel classes while preserving base knowledge using few labeled and all the unlabeled data. Extensive experiments on standard FSL, semi-supervised FSL, incremental FSL, and the firstly built S2 I-FSL benchmarks demonstrate the effectiveness of our proposed method.



### Dual Encoding U-Net for Spatio-Temporal Domain Shift Frame Prediction
- **Arxiv ID**: http://arxiv.org/abs/2110.11140v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11140v1)
- **Published**: 2021-10-21 13:44:21+00:00
- **Updated**: 2021-10-21 13:44:21+00:00
- **Authors**: Jay Santokhi, Dylan Hillier, Yiming Yang, Joned Sarwar, Anna Jordan, Emil Hewage
- **Comment**: 8 pages, 4 figures, 5 tables
- **Journal**: None
- **Summary**: The landscape of city-wide mobility behaviour has altered significantly over the past 18 months. The ability to make accurate and reliable predictions on such behaviour has likewise changed drastically with COVID-19 measures impacting how populations across the world interact with the different facets of mobility. This raises the question: "How does one use an abundance of pre-covid mobility data to make predictions on future behaviour in a present/post-covid environment?" This paper seeks to address this question by introducing an approach for traffic frame prediction using a lightweight Dual-Encoding U-Net built using only 12 Convolutional layers that incorporates a novel approach to skip-connections between Convolutional LSTM layers. This approach combined with an intuitive handling of training data can model both a temporal and spatio-temporal domain shift (gitlab.com/alchera/alchera-traffic4cast-2021).



### HCV: Hierarchy-Consistency Verification for Incremental Implicitly-Refined Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.11148v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11148v2)
- **Published**: 2021-10-21 13:54:00+00:00
- **Updated**: 2021-10-22 13:51:02+00:00
- **Authors**: Kai Wang, Xialei Liu, Luis Herranz, Joost van de Weijer
- **Comment**: accepted in BMVC 2021
- **Journal**: None
- **Summary**: Human beings learn and accumulate hierarchical knowledge over their lifetime. This knowledge is associated with previous concepts for consolidation and hierarchical construction. However, current incremental learning methods lack the ability to build a concept hierarchy by associating new concepts to old ones. A more realistic setting tackling this problem is referred to as Incremental Implicitly-Refined Classification (IIRC), which simulates the recognition process from coarse-grained categories to fine-grained categories. To overcome forgetting in this benchmark, we propose Hierarchy-Consistency Verification (HCV) as an enhancement to existing continual learning methods. Our method incrementally discovers the hierarchical relations between classes. We then show how this knowledge can be exploited during both training and inference. Experiments on three setups of varying difficulty demonstrate that our HCV module improves performance of existing continual learning methods under this IIRC setting by a large margin. Code is available in https://github.com/wangkai930418/HCV_IIRC.



### Each Attribute Matters: Contrastive Attention for Sentence-based Image Editing
- **Arxiv ID**: http://arxiv.org/abs/2110.11159v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.11159v1)
- **Published**: 2021-10-21 14:06:20+00:00
- **Updated**: 2021-10-21 14:06:20+00:00
- **Authors**: Liuqing Zhao, Fan Lyu, Fuyuan Hu, Kaizhu Huang, Fenglei Xu, Linyan Li
- **Comment**: Accepted by BMVC 2021
- **Journal**: None
- **Summary**: Sentence-based Image Editing (SIE) aims to deploy natural language to edit an image. Offering potentials to reduce expensive manual editing, SIE has attracted much interest recently. However, existing methods can hardly produce accurate editing and even lead to failures in attribute editing when the query sentence is with multiple editable attributes. To cope with this problem, by focusing on enhancing the difference between attributes, this paper proposes a novel model called Contrastive Attention Generative Adversarial Network (CA-GAN), which is inspired from contrastive training. Specifically, we first design a novel contrastive attention module to enlarge the editing difference between random combinations of attributes which are formed during training. We then construct an attribute discriminator to ensure effective editing on each attribute. A series of experiments show that our method can generate very encouraging results in sentence-based image editing with multiple attributes on CUB and COCO dataset. Our code is available at https://github.com/Zlq2021/CA-GAN



### Self-Supervised Visual Representation Learning Using Lightweight Architectures
- **Arxiv ID**: http://arxiv.org/abs/2110.11160v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11160v1)
- **Published**: 2021-10-21 14:13:10+00:00
- **Updated**: 2021-10-21 14:13:10+00:00
- **Authors**: Prathamesh Sonawane, Sparsh Drolia, Saqib Shamsi, Bhargav Jain
- **Comment**: 8 pages, 4 figures, 1 table, submitted to Artificial Intelligence and
  Statistics 2022 (AISTATS 2022)
- **Journal**: None
- **Summary**: In self-supervised learning, a model is trained to solve a pretext task, using a data set whose annotations are created by a machine. The objective is to transfer the trained weights to perform a downstream task in the target domain. We critically examine the most notable pretext tasks to extract features from image data and further go on to conduct experiments on resource constrained networks, which aid faster experimentation and deployment. We study the performance of various self-supervised techniques keeping all other parameters uniform. We study the patterns that emerge by varying model type, size and amount of pre-training done for the backbone as well as establish a standard to compare against for future research. We also conduct comprehensive studies to understand the quality of representations learned by different architectures.



### SLURP: Side Learning Uncertainty for Regression Problems
- **Arxiv ID**: http://arxiv.org/abs/2110.11182v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11182v2)
- **Published**: 2021-10-21 14:50:42+00:00
- **Updated**: 2022-01-03 14:33:02+00:00
- **Authors**: Xuanlong Yu, Gianni Franchi, Emanuel Aldea
- **Comment**: Accepted at BMVC 2021
- **Journal**: None
- **Summary**: It has become critical for deep learning algorithms to quantify their output uncertainties to satisfy reliability constraints and provide accurate results. Uncertainty estimation for regression has received less attention than classification due to the more straightforward standardized output of the latter class of tasks and their high importance. However, regression problems are encountered in a wide range of applications in computer vision. We propose SLURP, a generic approach for regression uncertainty estimation via a side learner that exploits the output and the intermediate representations generated by the main task model. We test SLURP on two critical regression tasks in computer vision: monocular depth and optical flow estimation. In addition, we conduct exhaustive benchmarks comprising transfer to different datasets and the addition of aleatoric noise. The results show that our proposal is generic and readily applicable to various regression problems and has a low computational cost with respect to existing solutions.



### On Hard Episodes in Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.11190v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11190v1)
- **Published**: 2021-10-21 14:58:58+00:00
- **Updated**: 2021-10-21 14:58:58+00:00
- **Authors**: Samyadeep Basu, Amr Sharaf, Nicolo Fusi, Soheil Feizi
- **Comment**: None
- **Journal**: None
- **Summary**: Existing meta-learners primarily focus on improving the average task accuracy across multiple episodes. Different episodes, however, may vary in hardness and quality leading to a wide gap in the meta-learner's performance across episodes. Understanding this issue is particularly critical in industrial few-shot settings, where there is limited control over test episodes as they are typically uploaded by end-users. In this paper, we empirically analyse the behaviour of meta-learners on episodes of varying hardness across three standard benchmark datasets: CIFAR-FS, mini-ImageNet, and tiered-ImageNet. Surprisingly, we observe a wide gap in accuracy of around 50% between the hardest and easiest episodes across all the standard benchmarks and meta-learners. We additionally investigate various properties of hard episodes and highlight their connection to catastrophic forgetting during meta-training. To address the issue of sub-par performance on hard episodes, we investigate and benchmark different meta-training strategies based on adversarial training and curriculum learning. We find that adversarial training strategies are much more powerful than curriculum learning in improving the prediction performance on hard episodes.



### Generative Adversarial Graph Convolutional Networks for Human Action Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.11191v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11191v3)
- **Published**: 2021-10-21 15:01:32+00:00
- **Updated**: 2021-10-25 07:25:28+00:00
- **Authors**: Bruno Degardin, Jo√£o Neves, Vasco Lopes, Jo√£o Brito, Ehsan Yaghoubi, Hugo Proen√ßa
- **Comment**: Published as a conference paper at WACV 2022. Code and pretrained
  models available at https://github.com/DegardinBruno/Kinetic-GAN
- **Journal**: None
- **Summary**: Synthesising the spatial and temporal dynamics of the human body skeleton remains a challenging task, not only in terms of the quality of the generated shapes, but also of their diversity, particularly to synthesise realistic body movements of a specific action (action conditioning). In this paper, we propose Kinetic-GAN, a novel architecture that leverages the benefits of Generative Adversarial Networks and Graph Convolutional Networks to synthesise the kinetics of the human body. The proposed adversarial architecture can condition up to 120 different actions over local and global body movements while improving sample quality and diversity through latent space disentanglement and stochastic variations. Our experiments were carried out in three well-known datasets, where Kinetic-GAN notably surpasses the state-of-the-art methods in terms of distribution quality metrics while having the ability to synthesise more than one order of magnitude regarding the number of different actions. Our code and models are publicly available at https://github.com/DegardinBruno/Kinetic-GAN.



### Robustness through Data Augmentation Loss Consistency
- **Arxiv ID**: http://arxiv.org/abs/2110.11205v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11205v3)
- **Published**: 2021-10-21 15:30:40+00:00
- **Updated**: 2023-01-24 10:55:13+00:00
- **Authors**: Tianjian Huang, Shaunak Halbe, Chinnadhurai Sankar, Pooyan Amini, Satwik Kottur, Alborz Geramifard, Meisam Razaviyayn, Ahmad Beirami
- **Comment**: 40 pages
- **Journal**: None
- **Summary**: While deep learning through empirical risk minimization (ERM) has succeeded at achieving human-level performance at a variety of complex tasks, ERM is not robust to distribution shifts or adversarial attacks. Synthetic data augmentation followed by empirical risk minimization (DA-ERM) is a simple and widely used solution to improve robustness in ERM. In addition, consistency regularization can be applied to further improve the robustness of the model by forcing the representation of the original sample and the augmented one to be similar. However, existing consistency regularization methods are not applicable to covariant data augmentation, where the label in the augmented sample is dependent on the augmentation function. For example, dialog state covaries with named entity when we augment data with a new named entity. In this paper, we propose data augmented loss invariant regularization (DAIR), a simple form of consistency regularization that is applied directly at the loss level rather than intermediate features, making it widely applicable to both invariant and covariant data augmentation regardless of network architecture, problem setup, and task. We apply DAIR to real-world learning problems involving covariant data augmentation: robust neural task-oriented dialog state tracking and robust visual question answering. We also apply DAIR to tasks involving invariant data augmentation: robust regression, robust classification against adversarial attacks, and robust ImageNet classification under distribution shift. Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal computational cost and sets new state-of-the-art results in several benchmarks involving covariant data augmentation. Our code of all experiments is available at: https://github.com/optimization-for-data-driven-science/DAIR.git



### PlaneRecNet: Multi-Task Learning with Cross-Task Consistency for Piece-Wise Plane Detection and Reconstruction from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2110.11219v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11219v2)
- **Published**: 2021-10-21 15:54:03+00:00
- **Updated**: 2022-01-30 13:50:54+00:00
- **Authors**: Yaxu Xie, Fangwen Shu, Jason Rambach, Alain Pagani, Didier Stricker
- **Comment**: accepted to BMVC 2021, code opensource:
  https://github.com/EryiXie/PlaneRecNet
- **Journal**: None
- **Summary**: Piece-wise 3D planar reconstruction provides holistic scene understanding of man-made environments, especially for indoor scenarios. Most recent approaches focused on improving the segmentation and reconstruction results by introducing advanced network architectures but overlooked the dual characteristics of piece-wise planes as objects and geometric models. Different from other existing approaches, we start from enforcing cross-task consistency for our multi-task convolutional neural network, PlaneRecNet, which integrates a single-stage instance segmentation network for piece-wise planar segmentation and a depth decoder to reconstruct the scene from a single RGB image. To achieve this, we introduce several novel loss functions (geometric constraint) that jointly improve the accuracy of piece-wise planar segmentation and depth estimation. Meanwhile, a novel Plane Prior Attention module is used to guide depth estimation with the awareness of plane instances. Exhaustive experiments are conducted in this work to validate the effectiveness and efficiency of our method.



### Detection of Driver Drowsiness by Calculating the Speed of Eye Blinking
- **Arxiv ID**: http://arxiv.org/abs/2110.11223v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11223v1)
- **Published**: 2021-10-21 16:02:05+00:00
- **Updated**: 2021-10-21 16:02:05+00:00
- **Authors**: Muhammad Fawwaz Yusri, Patrick Mangat, Oliver Wasenm√ºller
- **Comment**: This paper has been accepted at the Upper-Rhine Artificial
  Intelligence Symposium 2021
- **Journal**: None
- **Summary**: Many road accidents are caused by drowsiness of the driver. While there are methods to detect closed eyes, it is a non-trivial task to detect the gradual process of a driver becoming drowsy. We consider a simple real-time detection system for drowsiness merely based on the eye blinking rate derived from the eye aspect ratio. For the eye detection we use HOG and a linear SVM. If the speed of the eye blinking drops below some empirically determined threshold, the system triggers an alarm, hence preventing the driver from falling into microsleep. In this paper, we extensively evaluate the minimal requirements for the proposed system. We find that this system works well if the face is directed to the camera, but it becomes less reliable once the head is tilted significantly. The results of our evaluations provide the foundation for further developments of our drowsiness detection system.



### On the properties of some low-parameter models for color reproduction in terms of spectrum transformations and coverage of a color triangle
- **Arxiv ID**: http://arxiv.org/abs/2110.11255v1
- **DOI**: 10.1364/JOSAA.447508
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.11255v1)
- **Published**: 2021-10-21 16:29:46+00:00
- **Updated**: 2021-10-21 16:29:46+00:00
- **Authors**: Alexey Kroshnin, Viacheslav Vasilev, Egor Ershov, Denis Shepelev, Dmitry Nikolaev, Mikhail Tchobanou
- **Comment**: 23 pages, 2 figures
- **Journal**: None
- **Summary**: One of the classical approaches to solving color reproduction problems, such as color adaptation or color space transform, is the use of low-parameter spectral models. The strength of this approach is the ability to choose a set of properties that the model should have, be it a large coverage area of a color triangle, an accurate description of the addition or multiplication of spectra, knowing only the tristimulus corresponding to them. The disadvantage is that some of the properties of the mentioned spectral models are confirmed only experimentally. This work is devoted to the theoretical substantiation of various properties of spectral models. In particular, we prove that the banded model is the only model that simultaneously possesses the properties of closure under addition and multiplication. We also show that the Gaussian model is the limiting case of the von Mises model and prove that the set of protomers of the von Mises model unambiguously covers the color triangle in both the case of convex and non-convex spectral locus.



### Multi-Category Mesh Reconstruction From Image Collections
- **Arxiv ID**: http://arxiv.org/abs/2110.11256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11256v1)
- **Published**: 2021-10-21 16:32:31+00:00
- **Updated**: 2021-10-21 16:32:31+00:00
- **Authors**: Alessandro Simoni, Stefano Pini, Roberto Vezzani, Rita Cucchiara
- **Comment**: Accepted at 3DV 2021
- **Journal**: None
- **Summary**: Recently, learning frameworks have shown the capability of inferring the accurate shape, pose, and texture of an object from a single RGB image. However, current methods are trained on image collections of a single category in order to exploit specific priors, and they often make use of category-specific 3D templates. In this paper, we present an alternative approach that infers the textured mesh of objects combining a series of deformable 3D models and a set of instance-specific deformation, pose, and texture. Differently from previous works, our method is trained with images of multiple object categories using only foreground masks and rough camera poses as supervision. Without specific 3D templates, the framework learns category-level models which are deformed to recover the 3D shape of the depicted object. The instance-specific deformations are predicted independently for each vertex of the learned 3D mesh, enabling the dynamic subdivision of the mesh during the training process. Experiments show that the proposed framework can distinguish between different object categories and learn category-specific shape priors in an unsupervised manner. Predicted shapes are smooth and can leverage from multiple steps of subdivision during the training process, obtaining comparable or state-of-the-art results on two public datasets. Models and code are publicly released.



### MSO: Multi-Feature Space Joint Optimization Network for RGB-Infrared Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2110.11264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11264v1)
- **Published**: 2021-10-21 16:45:23+00:00
- **Updated**: 2021-10-21 16:45:23+00:00
- **Authors**: Yajun Gao, Tengfei Liang, Yi Jin, Xiaoyan Gu, Wu Liu, Yidong Li, Congyan Lang
- **Comment**: None
- **Journal**: None
- **Summary**: The RGB-infrared cross-modality person re-identification (ReID) task aims to recognize the images of the same identity between the visible modality and the infrared modality. Existing methods mainly use a two-stream architecture to eliminate the discrepancy between the two modalities in the final common feature space, which ignore the single space of each modality in the shallow layers. To solve it, in this paper, we present a novel multi-feature space joint optimization (MSO) network, which can learn modality-sharable features in both the single-modality space and the common space. Firstly, based on the observation that edge information is modality-invariant, we propose an edge features enhancement module to enhance the modality-sharable features in each single-modality space. Specifically, we design a perceptual edge features (PEF) loss after the edge fusion strategy analysis. According to our knowledge, this is the first work that proposes explicit optimization in the single-modality feature space on cross-modality ReID task. Moreover, to increase the difference between cross-modality distance and class distance, we introduce a novel cross-modality contrastive-center (CMCC) loss into the modality-joint constraints in the common feature space. The PEF loss and CMCC loss jointly optimize the model in an end-to-end manner, which markedly improves the network's performance. Extensive experiments demonstrate that the proposed model significantly outperforms state-of-the-art methods on both the SYSU-MM01 and RegDB datasets.



### Self-Supervised Monocular Scene Decomposition and Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.11275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11275v1)
- **Published**: 2021-10-21 17:03:08+00:00
- **Updated**: 2021-10-21 17:03:08+00:00
- **Authors**: Sadra Safadoust, Fatma G√ºney
- **Comment**: 3DV 2021
- **Journal**: None
- **Summary**: Self-supervised monocular depth estimation approaches either ignore independently moving objects in the scene or need a separate segmentation step to identify them. We propose MonoDepthSeg to jointly estimate depth and segment moving objects from monocular video without using any ground-truth labels. We decompose the scene into a fixed number of components where each component corresponds to a region on the image with its own transformation matrix representing its motion. We estimate both the mask and the motion of each component efficiently with a shared encoder. We evaluate our method on three driving datasets and show that our model clearly improves depth estimation while decomposing the scene into separately moving components.



### Fusion of complementary 2D and 3D mesostructural datasets using generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/2110.11281v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11281v3)
- **Published**: 2021-10-21 17:07:57+00:00
- **Updated**: 2022-09-30 14:47:45+00:00
- **Authors**: Amir Dahari, Steve Kench, Isaac Squires, Samuel J. Cooper
- **Comment**: None
- **Journal**: None
- **Summary**: Modelling the impact of a material's mesostructure on device level performance typically requires access to 3D image data containing all the relevant information to define the geometry of the simulation domain. This image data must include sufficient contrast between phases to distinguish each material, be of high enough resolution to capture the key details, but also have a large enough field-of-view to be representative of the material in general. It is rarely possible to obtain data with all of these properties from a single imaging technique. In this paper, we present a method for combining information from pairs of distinct but complementary imaging techniques in order to accurately reconstruct the desired multi-phase, high resolution, representative, 3D images. Specifically, we use deep convolutional generative adversarial networks to implement super-resolution, style transfer and dimensionality expansion. To demonstrate the widespread applicability of this tool, two pairs of datasets are used to validate the quality of the volumes generated by fusing the information from paired imaging techniques. Three key mesostructural metrics are calculated in each case to show the accuracy of this method. Having confidence in the accuracy of our method, we then demonstrate its power by applying to a real data pair from a lithium ion battery electrode, where the required 3D high resolution image data is not available anywhere in the literature. We believe this approach is superior to previously reported statistical material reconstruction methods both in terms of its fidelity and ease of use. Furthermore, much of the data required to train this algorithm already exists in the literature, waiting to be combined. As such, our open-access code could precipitate a step change by generating the hard to obtain high quality image volumes necessary to simulate behaviour at the mesoscale.



### The Effect of Wearing a Face Mask on Face Image Quality
- **Arxiv ID**: http://arxiv.org/abs/2110.11283v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11283v4)
- **Published**: 2021-10-21 17:12:21+00:00
- **Updated**: 2021-11-02 12:14:33+00:00
- **Authors**: Biying Fu, Florian Kirchbuchner, Naser Damer
- **Comment**: Accepted at the 16th IEEE International Conference on Automatic Face
  and Gesture Recognition, FG 2021
- **Journal**: None
- **Summary**: Due to the COVID-19 situation, face masks have become a main part of our daily life. Wearing mouth-and-nose protection has been made a mandate in many public places, to prevent the spread of the COVID-19 virus. However, face masks affect the performance of face recognition, since a large area of the face is covered. The effect of wearing a face mask on the different components of the face recognition system in a collaborative environment is a problem that is still to be fully studied. This work studies, for the first time, the effect of wearing a face mask on face image quality by utilising state-of-the-art face image quality assessment methods of different natures. This aims at providing better understanding on the effect of face masks on the operation of face recognition as a whole system. In addition, we further studied the effect of simulated masks on face image utility in comparison to real face masks. We discuss the correlation between the mask effect on face image quality and that on the face verification performance by automatic systems and human experts, indicating a consistent trend between both factors. The evaluation is conducted on the database containing (1) no-masked faces, (2) real face masks, and (3) simulated face masks, by synthetically generating digital facial masks on no-masked faces. Finally, a visual interpretation of the face areas contributing to the quality score of a selected set of quality assessment methods is provided to give a deeper insight into the difference of network decisions in masked and non-masked faces, among other variations.



### Multi-Object Tracking and Segmentation with a Space-Time Memory Network
- **Arxiv ID**: http://arxiv.org/abs/2110.11284v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11284v2)
- **Published**: 2021-10-21 17:13:17+00:00
- **Updated**: 2023-05-16 01:16:56+00:00
- **Authors**: Mehdi Miah, Guillaume-Alexandre Bilodeau, Nicolas Saunier
- **Comment**: arXiv admin note: text overlap with arXiv:2107.07067 Accepted at CRV
  2023 (Conference on Robots and Vision)
- **Journal**: None
- **Summary**: We propose a method for multi-object tracking and segmentation based on a novel memory-based mechanism to associate tracklets. The proposed tracker, MeNToS, addresses particularly the long-term data association problem, when objects are not observable for long time intervals. Indeed, the recently introduced HOTA metric (High Order Tracking Accuracy), which has a better alignment than the formerly established MOTA (Multiple Object Tracking Accuracy) with the human visual assessment of tracking, has shown that improvements are still needed for data association, despite the recent improvement in object detection. In MeNToS, after creating tracklets using instance segmentation and optical flow, the proposed method relies on a space-time memory network originally developed for one-shot video object segmentation to improve the association of sequence of detections (tracklets) with temporal gaps. We evaluate our tracker on KITTIMOTS and MOTSChallenge and we show the benefit of our data association strategy with the HOTA metric. Additional ablation studies demonstrate that our approach using a space-time memory network gives better and more robust long-term association than those based on a re-identification network. Our project page is at \url{www.mehdimiah.com/mentos+}.



### An Empirical Study on GANs with Margin Cosine Loss and Relativistic Discriminator
- **Arxiv ID**: http://arxiv.org/abs/2110.11293v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11293v2)
- **Published**: 2021-10-21 17:25:47+00:00
- **Updated**: 2021-10-22 02:28:47+00:00
- **Authors**: Cuong V. Nguyen, Tien-Dung Cao, Tram Truong-Huu, Khanh N. Pham, Binh T. Nguyen
- **Comment**: 16 pages, 5 figures
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have emerged as useful generative models, which are capable of implicitly learning data distributions of arbitrarily complex dimensions. However, the training of GANs is empirically well-known for being highly unstable and sensitive. The loss functions of both the discriminator and generator concerning their parameters tend to oscillate wildly during training. Different loss functions have been proposed to stabilize the training and improve the quality of images generated. In this paper, we perform an empirical study on the impact of several loss functions on the performance of standard GAN models, Deep Convolutional Generative Adversarial Networks (DCGANs). We introduce a new improvement that employs a relativistic discriminator to replace the classical deterministic discriminator in DCGANs and implement a margin cosine loss function for both the generator and discriminator. This results in a novel loss function, namely Relativistic Margin Cosine Loss (RMCosGAN). We carry out extensive experiments with four datasets: CIFAR-$10$, MNIST, STL-$10$, and CAT. We compare RMCosGAN performance with existing loss functions based on two metrics: Frechet inception distance and inception score. The experimental results show that RMCosGAN outperforms the existing ones and significantly improves the quality of images generated.



### Video and Text Matching with Conditioned Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2110.11298v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2110.11298v1)
- **Published**: 2021-10-21 17:31:50+00:00
- **Updated**: 2021-10-21 17:31:50+00:00
- **Authors**: Ameen Ali, Idan Schwartz, Tamir Hazan, Lior Wolf
- **Comment**: None
- **Journal**: WACV 2022
- **Summary**: We present a method for matching a text sentence from a given corpus to a given video clip and vice versa. Traditionally video and text matching is done by learning a shared embedding space and the encoding of one modality is independent of the other. In this work, we encode the dataset data in a way that takes into account the query's relevant information. The power of the method is demonstrated to arise from pooling the interaction data between words and frames. Since the encoding of the video clip depends on the sentence compared to it, the representation needs to be recomputed for each potential match. To this end, we propose an efficient shallow neural network. Its training employs a hierarchical triplet loss that is extendable to paragraph/video matching. The method is simple, provides explainability, and achieves state-of-the-art results for both sentence-clip and video-text by a sizable margin across five different datasets: ActivityNet, DiDeMo, YouCook2, MSR-VTT, and LSMDC. We also show that our conditioned representation can be transferred to video-guided machine translation, where we improved the current results on VATEX. Source code is available at https://github.com/AmeenAli/VideoMatch.



### Center Loss Regularization for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.11314v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11314v1)
- **Published**: 2021-10-21 17:46:44+00:00
- **Updated**: 2021-10-21 17:46:44+00:00
- **Authors**: Kaustubh Olpadkar, Ekta Gavas
- **Comment**: 16 pages, 9 figures, Submitted to the ICLR 2022 conference
- **Journal**: None
- **Summary**: The ability to learn different tasks sequentially is essential to the development of artificial intelligence. In general, neural networks lack this capability, the major obstacle being catastrophic forgetting. It occurs when the incrementally available information from non-stationary data distributions is continually acquired, disrupting what the model has already learned. Our approach remembers old tasks by projecting the representations of new tasks close to that of old tasks while keeping the decision boundaries unchanged. We employ the center loss as a regularization penalty that enforces new tasks' features to have the same class centers as old tasks and makes the features highly discriminative. This, in turn, leads to the least forgetting of already learned information. This method is easy to implement, requires minimal computational and memory overhead, and allows the neural network to maintain high performance across many sequentially encountered tasks. We also demonstrate that using the center loss in conjunction with the memory replay outperforms other replay-based strategies. Along with standard MNIST variants for continual learning, we apply our method to continual domain adaptation scenarios with the Digits and PACS datasets. We demonstrate that our approach is scalable, effective, and gives competitive performance compared to state-of-the-art continual learning methods.



### CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP
- **Arxiv ID**: http://arxiv.org/abs/2110.11316v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11316v4)
- **Published**: 2021-10-21 17:50:48+00:00
- **Updated**: 2022-11-07 13:57:43+00:00
- **Authors**: Andreas F√ºrst, Elisabeth Rumetshofer, Johannes Lehner, Viet Tran, Fei Tang, Hubert Ramsauer, David Kreil, Michael Kopp, G√ºnter Klambauer, Angela Bitto-Nemling, Sepp Hochreiter
- **Comment**: Published at NeurIPS 2022; Blog: https://ml-jku.github.io/cloob;
  GitHub: https://github.com/ml-jku/cloob
- **Journal**: None
- **Summary**: CLIP yielded impressive results on zero-shot transfer learning tasks and is considered as a foundation model like BERT or GPT3. CLIP vision models that have a rich representation are pre-trained using the InfoNCE objective and natural language supervision before they are fine-tuned on particular tasks. Though CLIP excels at zero-shot transfer learning, it suffers from an explaining away problem, that is, it focuses on one or few features, while neglecting other relevant features. This problem is caused by insufficiently extracting the covariance structure in the original multi-modal data. We suggest to use modern Hopfield networks to tackle the problem of explaining away. Their retrieved embeddings have an enriched covariance structure derived from co-occurrences of features in the stored embeddings. However, modern Hopfield networks increase the saturation effect of the InfoNCE objective which hampers learning. We propose to use the InfoLOOB objective to mitigate this saturation effect. We introduce the novel "Contrastive Leave One Out Boost" (CLOOB), which uses modern Hopfield networks for covariance enrichment together with the InfoLOOB objective. In experiments we compare CLOOB to CLIP after pre-training on the Conceptual Captions and the YFCC dataset with respect to their zero-shot transfer learning performance on other datasets. CLOOB consistently outperforms CLIP at zero-shot transfer learning across all considered architectures and datasets.



### Deep Curriculum Learning in Task Space for Multi-Class Based Mammography Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2110.11320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11320v1)
- **Published**: 2021-10-21 17:52:25+00:00
- **Updated**: 2021-10-21 17:52:25+00:00
- **Authors**: Jun Luo, Dooman Arefan, Margarita Zuley, Jules Sumkin, Shandong Wu
- **Comment**: 4-page abstract. Full paper to appear at SPIE Medical Imaging 2022
- **Journal**: None
- **Summary**: Mammography is used as a standard screening procedure for the potential patients of breast cancer. Over the past decade, it has been shown that deep learning techniques have succeeded in reaching near-human performance in a number of tasks, and its application in mammography is one of the topics that medical researchers most concentrate on. In this work, we propose an end-to-end Curriculum Learning (CL) strategy in task space for classifying the three categories of Full-Field Digital Mammography (FFDM), namely Malignant, Negative, and False recall. Specifically, our method treats this three-class classification as a "harder" task in terms of CL, and create an "easier" sub-task of classifying False recall against the combined group of Negative and Malignant. We introduce a loss scheduler to dynamically weight the contribution of the losses from the two tasks throughout the entire training process. We conduct experiments on an FFDM datasets of 1,709 images using 5-fold cross validation. The results show that our curriculum learning strategy can boost the performance for classifying the three categories of FFDM compared to the baseline strategies for model training.



### StyleAlign: Analysis and Applications of Aligned StyleGAN Models
- **Arxiv ID**: http://arxiv.org/abs/2110.11323v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11323v2)
- **Published**: 2021-10-21 17:55:16+00:00
- **Updated**: 2022-05-05 17:59:06+00:00
- **Authors**: Zongze Wu, Yotam Nitzan, Eli Shechtman, Dani Lischinski
- **Comment**: 44 pages, 37 figures
- **Journal**: Proc. 10th International Conference on Learning Representations,
  ICLR 2022
- **Summary**: In this paper, we perform an in-depth study of the properties and applications of aligned generative models. We refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the first detailed exploration of model alignment, also focusing on StyleGAN. First, we empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, we find that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple fine-tuning and inversion.



### Learning 3D Semantic Segmentation with only 2D Image Supervision
- **Arxiv ID**: http://arxiv.org/abs/2110.11325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11325v1)
- **Published**: 2021-10-21 17:56:28+00:00
- **Updated**: 2021-10-21 17:56:28+00:00
- **Authors**: Kyle Genova, Xiaoqi Yin, Abhijit Kundu, Caroline Pantofaru, Forrester Cole, Avneesh Sud, Brian Brewington, Brian Shucker, Thomas Funkhouser
- **Comment**: Accepted to 3DV 2021 (Oral)
- **Journal**: None
- **Summary**: With the recent growth of urban mapping and autonomous driving efforts, there has been an explosion of raw 3D data collected from terrestrial platforms with lidar scanners and color cameras. However, due to high labeling costs, ground-truth 3D semantic segmentation annotations are limited in both quantity and geographic diversity, while also being difficult to transfer across sensors. In contrast, large image collections with ground-truth semantic segmentations are readily available for diverse sets of scenes. In this paper, we investigate how to use only those labeled 2D image collections to supervise training 3D semantic segmentation models. Our approach is to train a 3D model from pseudo-labels derived from 2D semantic image segmentations using multiview fusion. We address several novel issues with this approach, including how to select trusted pseudo-labels, how to sample 3D scenes with rare object categories, and how to decouple input features from 2D images from pseudo-labels during training. The proposed network architecture, 2D3DNet, achieves significantly better performance (+6.2-11.4 mIoU) than baselines during experiments on a new urban dataset with lidar and images captured in 20 cities across 5 continents.



### A Fine-Grained Analysis on Distribution Shift
- **Arxiv ID**: http://arxiv.org/abs/2110.11328v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11328v2)
- **Published**: 2021-10-21 17:57:08+00:00
- **Updated**: 2021-11-25 20:59:18+00:00
- **Authors**: Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre Alvise-Rebuffi, Ira Ktena, Krishnamurthy Dvijotham, Taylan Cemgil
- **Comment**: None
- **Journal**: None
- **Summary**: Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained analysis of various distribution shifts. We provide a holistic analysis of current state-of-the-art methods by evaluating 19 distinct methods grouped into five categories across both synthetic and real-world datasets. Overall, we train more than 85K models. Our experimental framework can be easily extended to include new methods, shifts, and datasets. We find, unlike previous work~\citep{Gulrajani20}, that progress has been made over a standard ERM baseline; in particular, pretraining and augmentations (learned or heuristic) offer large gains in many cases. However, the best methods are not consistent over different datasets and shifts.



### Generalized Out-of-Distribution Detection: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2110.11334v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11334v2)
- **Published**: 2021-10-21 17:59:41+00:00
- **Updated**: 2022-08-03 10:46:12+00:00
- **Authors**: Jingkang Yang, Kaiyang Zhou, Yixuan Li, Ziwei Liu
- **Comment**: Comments on our Overleaf manuscript are welcome:
  https://www.overleaf.com/read/hwphdzjfqxbs
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. We then review each of these five areas by summarizing their recent technical developments, with a special focus on OOD detection methodologies. We conclude this survey with open challenges and potential research directions.



### Convex Joint Graph Matching and Clustering via Semidefinite Relaxations
- **Arxiv ID**: http://arxiv.org/abs/2110.11335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11335v1)
- **Published**: 2021-10-21 17:59:52+00:00
- **Updated**: 2021-10-21 17:59:52+00:00
- **Authors**: Maximilian Krahn, Florian Bernard, Vladislav Golyanik
- **Comment**: 12 pages, 8 figures; source code available; project webpage:
  https://4dqv.mpi-inf.mpg.de/JointGMC/
- **Journal**: International Conference on 3D Vision (3DV) 2021
- **Summary**: This paper proposes a new algorithm for simultaneous graph matching and clustering. For the first time in the literature, these two problems are solved jointly and synergetically without relying on any training data, which brings advantages for identifying similar arbitrary objects in compound 3D scenes and matching them. For joint reasoning, we first rephrase graph matching as a rigid point set registration problem operating on spectral graph embeddings. Consequently, we utilise efficient convex semidefinite program relaxations for aligning points in Hilbert spaces and add coupling constraints to model the mutual dependency and exploit synergies between both tasks. We outperform state of the art in challenging cases with non-perfectly matching and noisy graphs, and we show successful applications on real compound scenes with multiple 3D elements. Our source code and data are publicly available.



### Decentralised Person Re-Identification with Selective Knowledge Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2110.11384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.11384v1)
- **Published**: 2021-10-21 18:09:53+00:00
- **Updated**: 2021-10-21 18:09:53+00:00
- **Authors**: Shitong Sun, Guile Wu, Shaogang Gong
- **Comment**: accepted at BMVC2021
- **Journal**: None
- **Summary**: Existing person re-identification (Re-ID) methods mostly follow a centralised learning paradigm which shares all training data to a collection for model learning. This paradigm is limited when data from different sources cannot be shared due to privacy concerns. To resolve this problem, two recent works have introduced decentralised (federated) Re-ID learning for constructing a globally generalised model (server)without any direct access to local training data nor shared data across different source domains (clients). However, these methods are poor on how to adapt the generalised model to maximise its performance on individual client domain Re-ID tasks having different Re-ID label spaces, due to a lack of understanding of data heterogeneity across domains. We call this poor 'model personalisation'. In this work, we present a new Selective Knowledge Aggregation approach to decentralised person Re-ID to optimise the trade-off between model personalisation and generalisation. Specifically, we incorporate attentive normalisation into the normalisation layers in a deep ReID model and propose to learn local normalisation layers specific to each domain, which are decoupled from the global model aggregation in federated Re-ID learning. This helps to preserve model personalisation knowledge on each local client domain and learn instance-specific information. Further, we introduce a dual local normalisation mechanism to learn generalised normalisation layers in each local model, which are then transmitted to the global model for central aggregation. This facilitates selective knowledge aggregation on the server to construct a global generalised model for out-of-the-box deployment on unseen novel domains. Extensive experiments on eight person Re-ID datasets show that the proposed approach to decentralised Re-ID significantly outperforms the state-of-the-art decentralised methods.



### DEX: Domain Embedding Expansion for Generalized Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2110.11391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11391v1)
- **Published**: 2021-10-21 18:21:22+00:00
- **Updated**: 2021-10-21 18:21:22+00:00
- **Authors**: Eugene P. W. Ang, Lin Shan, Alex C. Kot
- **Comment**: Accepted into BMVC 2021
- **Journal**: None
- **Summary**: In recent years, supervised Person Re-identification (Person ReID) approaches have demonstrated excellent performance. However, when these methods are applied to inputs from a different camera network, they typically suffer from significant performance degradation. Different from most domain adaptation (DA) approaches addressing this issue, we focus on developing a domain generalization (DG) Person ReID model that can be deployed without additional fine-tuning or adaptation. In this paper, we propose the Domain Embedding Expansion (DEX) module. DEX dynamically manipulates and augments deep features based on person and domain labels during training, significantly improving the generalization capability and robustness of Person ReID models to unseen domains. We also developed a light version of DEX (DEXLite), applying negative sampling techniques to scale to larger datasets and reduce memory usage for multi-branch networks. Our proposed DEX and DEXLite can be combined with many existing methods, Bag-of-Tricks (BagTricks), the Multi-Granularity Network (MGN), and Part-Based Convolutional Baseline (PCB), in a plug-and-play manner. With DEX and DEXLite, existing methods can gain significant improvements when tested on other unseen datasets, thereby demonstrating the general applicability of our method. Our solution outperforms the state-of-the-art DG Person ReID methods in all large-scale benchmarks as well as in most the small-scale benchmarks.



### PROVES: Establishing Image Provenance using Semantic Signatures
- **Arxiv ID**: http://arxiv.org/abs/2110.11411v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2110.11411v1)
- **Published**: 2021-10-21 18:30:09+00:00
- **Updated**: 2021-10-21 18:30:09+00:00
- **Authors**: Mingyang Xie, Manav Kulshrestha, Shaojie Wang, Jinghan Yang, Ayan Chakrabarti, Ning Zhang, Yevgeniy Vorobeychik
- **Comment**: None
- **Journal**: None
- **Summary**: Modern AI tools, such as generative adversarial networks, have transformed our ability to create and modify visual data with photorealistic results. However, one of the deleterious side-effects of these advances is the emergence of nefarious uses in manipulating information in visual data, such as through the use of deep fakes. We propose a novel architecture for preserving the provenance of semantic information in images to make them less susceptible to deep fake attacks. Our architecture includes semantic signing and verification steps. We apply this architecture to verifying two types of semantic information: individual identities (faces) and whether the photo was taken indoors or outdoors. Verification accounts for a collection of common image transformation, such as translation, scaling, cropping, and small rotations, and rejects adversarial transformations, such as adversarially perturbed or, in the case of face verification, swapped faces. Experiments demonstrate that in the case of provenance of faces in an image, our approach is robust to black-box adversarial transformations (which are rejected) as well as benign transformations (which are accepted), with few false negatives and false positives. Background verification, on the other hand, is susceptible to black-box adversarial examples, but becomes significantly more robust after adversarial training.



### Fast Graph Sampling for Short Video Summarization using Gershgorin Disc Alignment
- **Arxiv ID**: http://arxiv.org/abs/2110.11420v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP, 05C90, I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2110.11420v2)
- **Published**: 2021-10-21 18:43:00+00:00
- **Updated**: 2021-10-25 02:31:51+00:00
- **Authors**: Sadid Sahami, Gene Cheung, Chia-Wen Lin
- **Comment**: 5 pages, 2 figures - Remove affiliation from author list
- **Journal**: None
- **Summary**: We study the problem of efficiently summarizing a short video into several keyframes, leveraging recent progress in fast graph sampling. Specifically, we first construct a similarity path graph (SPG) $\mathcal{G}$, represented by graph Laplacian matrix $\mathbf{L}$, where the similarities between adjacent frames are encoded as positive edge weights. We show that maximizing the smallest eigenvalue $\lambda_{\min}(\mathbf{B})$ of a coefficient matrix $\mathbf{B} = \text{diag}(\mathbf{a}) + \mu \mathbf{L}$, where $\mathbf{a}$ is the binary keyframe selection vector, is equivalent to minimizing a worst-case signal reconstruction error. We prove that, after partitioning $\mathcal{G}$ into $Q$ sub-graphs $\{\mathcal{G}^q\}^Q_{q=1}$, the smallest Gershgorin circle theorem (GCT) lower bound of $Q$ corresponding coefficient matrices -- $\min_q \lambda^-_{\min}(\mathbf{B}^q)$ -- is a lower bound for $\lambda_{\min}(\mathbf{B})$. This inspires a fast graph sampling algorithm to iteratively partition $\mathcal{G}$ into $Q$ sub-graphs using $Q$ samples (keyframes), while maximizing $\lambda^-_{\min}(\mathbf{B}^q)$ for each sub-graph $\mathcal{G}^q$. Experimental results show that our algorithm achieves comparable video summarization performance as state-of-the-art methods, at a substantially reduced complexity.



### MUGL: Large Scale Multi Person Conditional Action Generation with Locomotion
- **Arxiv ID**: http://arxiv.org/abs/2110.11460v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.11460v1)
- **Published**: 2021-10-21 20:11:53+00:00
- **Updated**: 2021-10-21 20:11:53+00:00
- **Authors**: Shubh Maheshwari, Debtanu Gupta, Ravi Kiran Sarvadevabhatla
- **Comment**: Accepted at WACV 2022. Project page :
  https://skeleton.iiit.ac.in/mugl
- **Journal**: None
- **Summary**: We introduce MUGL, a novel deep neural model for large-scale, diverse generation of single and multi-person pose-based action sequences with locomotion. Our controllable approach enables variable-length generations customizable by action category, across more than 100 categories. To enable intra/inter-category diversity, we model the latent generative space using a Conditional Gaussian Mixture Variational Autoencoder. To enable realistic generation of actions involving locomotion, we decouple local pose and global trajectory components of the action sequence. We incorporate duration-aware feature representations to enable variable-length sequence generation. We use a hybrid pose sequence representation with 3D pose sequences sourced from videos and 3D Kinect-based sequences of NTU-RGBD-120. To enable principled comparison of generation quality, we employ suitably modified strong baselines during evaluation. Although smaller and simpler compared to baselines, MUGL provides better quality generations, paving the way for practical and controllable large-scale human action generation.



### AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation
- **Arxiv ID**: http://arxiv.org/abs/2110.11474v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11474v2)
- **Published**: 2021-10-21 20:43:42+00:00
- **Updated**: 2021-10-25 01:12:47+00:00
- **Authors**: Khoa Vo, Hyekang Joo, Kashu Yamazaki, Sang Truong, Kris Kitani, Minh-Triet Tran, Ngan Le
- **Comment**: Accepted in BMVC 2021 (Oral Session)
- **Journal**: None
- **Summary**: Humans typically perceive the establishment of an action in a video through the interaction between an actor and the surrounding environment. An action only starts when the main actor in the video begins to interact with the environment, while it ends when the main actor stops the interaction. Despite the great progress in temporal action proposal generation, most existing works ignore the aforementioned fact and leave their model learning to propose actions as a black-box. In this paper, we make an attempt to simulate that ability of a human by proposing Actor Environment Interaction (AEI) network to improve the video representation for temporal action proposals generation. AEI contains two modules, i.e., perception-based visual representation (PVR) and boundary-matching module (BMM). PVR represents each video snippet by taking human-human relations and humans-environment relations into consideration using the proposed adaptive attention mechanism. Then, the video representation is taken by BMM to generate action proposals. AEI is comprehensively evaluated in ActivityNet-1.3 and THUMOS-14 datasets, on temporal action proposal and detection tasks, with two boundary-matching architectures (i.e., CNN-based and GCN-based) and two classifiers (i.e., Unet and P-GCN). Our AEI robustly outperforms the state-of-the-art methods with remarkable performance and generalization for both temporal action proposal generation and temporal action detection.



### MixNorm: Test-Time Adaptation Through Online Normalization Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.11478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11478v1)
- **Published**: 2021-10-21 21:04:42+00:00
- **Updated**: 2021-10-21 21:04:42+00:00
- **Authors**: Xuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, Ser-Nam Lim
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple and effective way to estimate the batch-norm statistics during test time, to fast adapt a source model to target test samples. Known as Test-Time Adaptation, most prior works studying this task follow two assumptions in their evaluation where (1) test samples come together as a large batch, and (2) all from a single test distribution. However, in practice, these two assumptions may not stand, the reasons for which we propose two new evaluation settings where batch sizes are arbitrary and multiple distributions are considered. Unlike the previous methods that require a large batch of single distribution during test time to calculate stable batch-norm statistics, our method avoid any dependency on large online batches and is able to estimate accurate batch-norm statistics with a single sample. The proposed method significantly outperforms the State-Of-The-Art in the newly proposed settings in Test-Time Adaptation Task, and also demonstrates improvements in various other settings such as Source-Free Unsupervised Domain Adaptation and Zero-Shot Classification.



### SymbioLCD: Ensemble-Based Loop Closure Detection using CNN-Extracted Objects and Visual Bag-of-Words
- **Arxiv ID**: http://arxiv.org/abs/2110.11491v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.11491v1)
- **Published**: 2021-10-21 21:34:57+00:00
- **Updated**: 2021-10-21 21:34:57+00:00
- **Authors**: Jonathan J. Y. Kim, Martin Urschler, Patricia J. Riddle, J√∂rg S. Wicker
- **Comment**: 7 pages. Accepted at 2021 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: Loop closure detection is an essential tool of Simultaneous Localization and Mapping (SLAM) to minimize drift in its localization. Many state-of-the-art loop closure detection (LCD) algorithms use visual Bag-of-Words (vBoW), which is robust against partial occlusions in a scene but cannot perceive the semantics or spatial relationships between feature points. CNN object extraction can address those issues, by providing semantic labels and spatial relationships between objects in a scene. Previous work has mainly focused on replacing vBoW with CNN-derived features. In this paper, we propose SymbioLCD, a novel ensemble-based LCD that utilizes both CNN-extracted objects and vBoW features for LCD candidate prediction. When used in tandem, the added elements of object semantics and spatial-awareness create a more robust and symbiotic loop closure detection system. The proposed SymbioLCD uses scale-invariant spatial and semantic matching, Hausdorff distance with temporal constraints, and a Random Forest that utilizes combined information from both CNN-extracted objects and vBoW features for predicting accurate loop closure candidates. Evaluation of the proposed method shows it outperforms other Machine Learning (ML) algorithms - such as SVM, Decision Tree and Neural Network, and demonstrates that there is a strong symbiosis between CNN-extracted object information and vBoW features which assists accurate LCD candidate prediction. Furthermore, it is able to perceive loop closure candidates earlier than state-of-the-art SLAM algorithms, utilizing added spatial and semantic information from CNN-extracted objects.



### Sequential Voting with Relational Box Fields for Active Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.11524v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11524v4)
- **Published**: 2021-10-21 23:40:45+00:00
- **Updated**: 2022-06-02 00:54:27+00:00
- **Authors**: Qichen Fu, Xingyu Liu, Kris M. Kitani
- **Comment**: In CVPR 2022. Project:
  https://fuqichen1998.github.io/SequentialVotingDet/
- **Journal**: None
- **Summary**: A key component of understanding hand-object interactions is the ability to identify the active object -- the object that is being manipulated by the human hand. In order to accurately localize the active object, any method must reason using information encoded by each image pixel, such as whether it belongs to the hand, the object, or the background. To leverage each pixel as evidence to determine the bounding box of the active object, we propose a pixel-wise voting function. Our pixel-wise voting function takes an initial bounding box as input and produces an improved bounding box of the active object as output. The voting function is designed so that each pixel inside of the input bounding box votes for an improved bounding box, and the box with the majority vote is selected as the output. We call the collection of bounding boxes generated inside of the voting function, the Relational Box Field, as it characterizes a field of bounding boxes defined in relationship to the current bounding box. While our voting function is able to improve the bounding box of the active object, one round of voting is typically not enough to accurately localize the active object. Therefore, we repeatedly apply the voting function to sequentially improve the location of the bounding box. However, since it is known that repeatedly applying a one-step predictor (i.e., auto-regressive processing with our voting function) can cause a data distribution shift, we mitigate this issue using reinforcement learning (RL). We adopt standard RL to learn the voting function parameters and show that it provides a meaningful improvement over a standard supervised learning approach. We perform experiments on two large-scale datasets: 100DOH and MECCANO, improving AP50 performance by 8% and 30%, respectively, over the state of the art.



### Digital and Physical-World Attacks on Remote Pulse Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.11525v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11525v1)
- **Published**: 2021-10-21 23:41:27+00:00
- **Updated**: 2021-10-21 23:41:27+00:00
- **Authors**: Jeremy Speth, Nathan Vance, Patrick Flynn, Kevin W. Bowyer, Adam Czajka
- **Comment**: None
- **Journal**: None
- **Summary**: Remote photoplethysmography (rPPG) is a technique for estimating blood volume changes from reflected light without the need for a contact sensor. We present the first examples of presentation attacks in the digital and physical domains on rPPG from face video. Digital attacks are easily performed by adding imperceptible periodic noise to the input videos. Physical attacks are performed with illumination from visible spectrum LEDs placed in close proximity to the face, while still being difficult to perceive with the human eye. We also show that our attacks extend beyond medical applications, since the method can effectively generate a strong periodic pulse on 3D-printed face masks, which presents difficulties for pulse-based face presentation attack detection (PAD). The paper concludes with ideas for using this work to improve robustness of rPPG methods and pulse-based face PAD.



### Wide Neural Networks Forget Less Catastrophically
- **Arxiv ID**: http://arxiv.org/abs/2110.11526v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11526v3)
- **Published**: 2021-10-21 23:49:23+00:00
- **Updated**: 2022-07-14 07:33:29+00:00
- **Authors**: Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Huiyi Hu, Razvan Pascanu, Dilan Gorur, Mehrdad Farajtabar
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: A primary focus area in continual learning research is alleviating the "catastrophic forgetting" problem in neural networks by designing new algorithms that are more robust to the distribution shifts. While the recent progress in continual learning literature is encouraging, our understanding of what properties of neural networks contribute to catastrophic forgetting is still limited. To address this, instead of focusing on continual learning algorithms, in this work, we focus on the model itself and study the impact of "width" of the neural network architecture on catastrophic forgetting, and show that width has a surprisingly significant effect on forgetting. To explain this effect, we study the learning dynamics of the network from various perspectives such as gradient orthogonality, sparsity, and lazy training regime. We provide potential explanations that are consistent with the empirical results across different architectures and continual learning benchmarks.




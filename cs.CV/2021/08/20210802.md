# Arxiv Papers in cs.CV on 2021-08-02
### GraphFPN: Graph Feature Pyramid Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.00580v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00580v3)
- **Published**: 2021-08-02 01:19:38+00:00
- **Updated**: 2022-01-08 12:21:21+00:00
- **Authors**: Gangming Zhao, Weifeng Ge, Yizhou Yu
- **Comment**: accepted by ICCV 2021, codes are updated at
  https://github.com/GangmingZhao/GraphFPN-Graph-Feature-Pyramid-Network-for-Object-Detection
- **Journal**: None
- **Summary**: Feature pyramids have been proven powerful in image understanding tasks that require multi-scale features. State-of-the-art methods for multi-scale feature learning focus on performing feature interactions across space and scales using neural networks with a fixed topology. In this paper, we propose graph feature pyramid networks that are capable of adapting their topological structures to varying intrinsic image structures and supporting simultaneous feature interactions across all scales. We first define an image-specific superpixel hierarchy for each input image to represent its intrinsic image structures. The graph feature pyramid network inherits its structure from this superpixel hierarchy. Contextual and hierarchical layers are designed to achieve feature interactions within the same scale and across different scales. To make these layers more powerful, we introduce two types of local channel attention for graph neural networks by generalizing global channel attention for convolutional neural networks. The proposed graph feature pyramid network can enhance the multiscale features from a convolutional feature pyramid network. We evaluate our graph feature pyramid network in the object detection task by integrating it into the Faster R-CNN algorithm. The modified algorithm outperforms not only previous state-of-the-art feature pyramid-based methods with a clear margin but also other popular detection methods on both MS-COCO 2017 validation and test datasets.



### Congested Crowd Instance Localization with Dilated Convolutional Swin Transformer
- **Arxiv ID**: http://arxiv.org/abs/2108.00584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00584v1)
- **Published**: 2021-08-02 01:27:53+00:00
- **Updated**: 2021-08-02 01:27:53+00:00
- **Authors**: Junyu Gao, Maoguo Gong, Xuelong Li
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Crowd localization is a new computer vision task, evolved from crowd counting. Different from the latter, it provides more precise location information for each instance, not just counting numbers for the whole crowd scene, which brings greater challenges, especially in extremely congested crowd scenes. In this paper, we focus on how to achieve precise instance localization in high-density crowd scenes, and to alleviate the problem that the feature extraction ability of the traditional model is reduced due to the target occlusion, the image blur, etc. To this end, we propose a Dilated Convolutional Swin Transformer (DCST) for congested crowd scenes. Specifically, a window-based vision transformer is introduced into the crowd localization task, which effectively improves the capacity of representation learning. Then, the well-designed dilated convolutional module is inserted into some different stages of the transformer to enhance the large-range contextual information. Extensive experiments evidence the effectiveness of the proposed methods and achieve state-of-the-art performance on five popular datasets. Especially, the proposed model achieves F1-measure of 77.5\% and MAE of 84.2 in terms of localization and counting performance, respectively.



### Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR
- **Arxiv ID**: http://arxiv.org/abs/2108.00587v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.00587v1)
- **Published**: 2021-08-02 01:37:39+00:00
- **Updated**: 2021-08-02 01:37:39+00:00
- **Authors**: Khoi Nguyen, Yen Nguyen, Bao Le
- **Comment**: None
- **Journal**: None
- **Summary**: Recent breakthroughs in the field of semi-supervised learning have achieved results that match state-of-the-art traditional supervised learning methods. Most successful semi-supervised learning approaches in computer vision focus on leveraging huge amount of unlabeled data, learning the general representation via data augmentation and transformation, creating pseudo labels, implementing different loss functions, and eventually transferring this knowledge to more task-specific smaller models. In this paper, we aim to conduct our analyses on three different aspects of SimCLR, the current state-of-the-art semi-supervised learning framework for computer vision. First, we analyze properties of contrast learning on fine-tuning, as we understand that contrast learning is what makes this method so successful. Second, we research knowledge distillation through teacher-forcing paradigm. We observe that when the teacher and the student share the same base model, knowledge distillation will achieve better result. Finally, we study how transfer learning works and its relationship with the number of classes on different data sets. Our results indicate that transfer learning performs better when number of classes are smaller.



### GTNet:Guided Transformer Network for Detecting Human-Object Interactions
- **Arxiv ID**: http://arxiv.org/abs/2108.00596v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00596v5)
- **Published**: 2021-08-02 02:06:33+00:00
- **Updated**: 2023-04-12 20:29:49+00:00
- **Authors**: A S M Iftekhar, Satish Kumar, R. Austin McEver, Suya You, B. S. Manjunath
- **Comment**: accepted for presentation in Pattern Recognition and Tracking XXXIV
  at SPIE commerce+ defence Program
- **Journal**: None
- **Summary**: The human-object interaction (HOI) detection task refers to localizing humans, localizing objects, and predicting the interactions between each human-object pair. HOI is considered one of the fundamental steps in truly understanding complex visual scenes. For detecting HOI, it is important to utilize relative spatial configurations and object semantics to find salient spatial regions of images that highlight the interactions between human object pairs. This issue is addressed by the novel self-attention based guided transformer network, GTNet. GTNet encodes this spatial contextual information in human and object visual features via self-attention while achieving state of the art results on both the V-COCO and HICO-DET datasets. Code will be made available online.



### Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails
- **Arxiv ID**: http://arxiv.org/abs/2108.00602v6
- **DOI**: 10.1109/TIP.2022.3167280
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00602v6)
- **Published**: 2021-08-02 02:29:24+00:00
- **Updated**: 2022-04-01 12:32:32+00:00
- **Authors**: Yang Zhang, Xin Yu, Xiaobo Lu, Ping Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the task of hallucinating an authentic high-resolution (HR) face from an occluded thumbnail. We propose a multi-stage Progressive Upsampling and Inpainting Generative Adversarial Network, dubbed Pro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*) the occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates facial geometry priors for low-resolution (LR) faces and (2) acquires non-occluded HR face images under the guidance of the estimated priors. Our multi-stage hallucination network super-resolves and inpaints occluded LR faces in a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts significantly. Specifically, we design a novel cross-modal transformer module for facial priors estimation, in which an input face and its landmark features are formulated as queries and keys, respectively. Such a design encourages joint feature learning across the input facial and landmark features, and deep feature correspondences will be discovered by attention. Thus, facial appearance features and facial geometry priors are learned in a mutual promotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves visually pleasing HR faces, reaching superior performance in downstream tasks, i.e., face alignment, face parsing, face recognition and expression classification, compared with other state-of-the-art (SotA) methods.



### Multiple Classifiers Based Maximum Classifier Discrepancy for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2108.00610v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00610v2)
- **Published**: 2021-08-02 03:00:13+00:00
- **Updated**: 2022-04-17 03:37:28+00:00
- **Authors**: Yiju Yang, Taejoon Kim, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial training based on the maximum classifier discrepancy between two classifier structures has achieved great success in unsupervised domain adaptation tasks for image classification. The approach adopts the structure of two classifiers, though simple and intuitive, the learned classification boundary may not well represent the data property in the new domain. In this paper, we propose to extend the structure to multiple classifiers to further boost its performance. To this end, we develop a very straightforward approach to adding more classifiers. We employ the principle that the classifiers are different from each other to construct a discrepancy loss function for multiple classifiers. The proposed construction method of loss function makes it possible to add any number of classifiers to the original framework. The proposed approach is validated through extensive experimental evaluations. We demonstrate that, on average, adopting the structure of three classifiers normally yields the best performance as a trade-off between accuracy and efficiency. With minimum extra computational costs, the proposed approach can significantly improve the performance of the original algorithm. The source code of the proposed approach can be downloaded from \url{https://github.com/rucv/MMCD\_DA}.



### RINDNet: Edge Detection for Discontinuity in Reflectance, Illumination, Normal and Depth
- **Arxiv ID**: http://arxiv.org/abs/2108.00616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00616v1)
- **Published**: 2021-08-02 03:30:01+00:00
- **Updated**: 2021-08-02 03:30:01+00:00
- **Authors**: Mengyang Pu, Yaping Huang, Qingji Guan, Haibin Ling
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: As a fundamental building block in computer vision, edges can be categorised into four types according to the discontinuity in surface-Reflectance, Illumination, surface-Normal or Depth. While great progress has been made in detecting generic or individual types of edges, it remains under-explored to comprehensively study all four edge types together. In this paper, we propose a novel neural network solution, RINDNet, to jointly detect all four types of edges. Taking into consideration the distinct attributes of each type of edges and the relationship between them, RINDNet learns effective representations for each of them and works in three stages. In stage I, RINDNet uses a common backbone to extract features shared by all edges. Then in stage II it branches to prepare discriminative features for each edge type by the corresponding decoder. In stage III, an independent decision head for each type aggregates the features from previous stages to predict the initial results. Additionally, an attention module learns attention maps for all types to capture the underlying relations between them, and these maps are combined with initial results to generate the final edge detection results. For training and evaluation, we construct the first public benchmark, BSDS-RIND, with all four types of edges carefully annotated. In our experiments, RINDNet yields promising results in comparison with state-of-the-art methods. Additional analysis is presented in supplementary material.



### Investigating Attention Mechanism in 3D Point Cloud Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.00620v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00620v2)
- **Published**: 2021-08-02 03:54:39+00:00
- **Updated**: 2021-10-14 07:08:59+00:00
- **Authors**: Shi Qiu, Yunfan Wu, Saeed Anwar, Chongyi Li
- **Comment**: International Conference on 3D Vision (3DV 2021)
- **Journal**: None
- **Summary**: Object detection in three-dimensional (3D) space attracts much interest from academia and industry since it is an essential task in AI-driven applications such as robotics, autonomous driving, and augmented reality. As the basic format of 3D data, the point cloud can provide detailed geometric information about the objects in the original 3D space. However, due to 3D data's sparsity and unorderedness, specially designed networks and modules are needed to process this type of data. Attention mechanism has achieved impressive performance in diverse computer vision tasks; however, it is unclear how attention modules would affect the performance of 3D point cloud object detection and what sort of attention modules could fit with the inherent properties of 3D data. This work investigates the role of the attention mechanism in 3D point cloud object detection and provides insights into the potential of different attention modules. To achieve that, we comprehensively investigate classical 2D attentions, novel 3D attentions, including the latest point cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the detailed experiments and analysis, we conclude the effects of different attention modules. This paper is expected to serve as a reference source for benefiting attention-embedded 3D point cloud object detection. The code and trained models are available at: https://github.com/ShiQiu0419/attentions_in_3D_detection.



### Adversarial Energy Disaggregation for Non-intrusive Load Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2108.01998v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.01998v1)
- **Published**: 2021-08-02 03:56:35+00:00
- **Updated**: 2021-08-02 03:56:35+00:00
- **Authors**: Zhekai Du, Jingjing Li, Lei Zhu, Ke Lu, Heng Tao Shen
- **Comment**: Accepted to ACM/IMS Trans. on Data Science, codes can be found at
  https://github.com/lijin118/AED
- **Journal**: None
- **Summary**: Energy disaggregation, also known as non-intrusive load monitoring (NILM), challenges the problem of separating the whole-home electricity usage into appliance-specific individual consumptions, which is a typical application of data analysis. {NILM aims to help households understand how the energy is used and consequently tell them how to effectively manage the energy, thus allowing energy efficiency which is considered as one of the twin pillars of sustainable energy policy (i.e., energy efficiency and renewable energy).} Although NILM is unidentifiable, it is widely believed that the NILM problem can be addressed by data science. Most of the existing approaches address the energy disaggregation problem by conventional techniques such as sparse coding, non-negative matrix factorization, and hidden Markov model. Recent advances reveal that deep neural networks (DNNs) can get favorable performance for NILM since DNNs can inherently learn the discriminative signatures of the different appliances. In this paper, we propose a novel method named adversarial energy disaggregation (AED) based on DNNs. We introduce the idea of adversarial learning into NILM, which is new for the energy disaggregation task. Our method trains a generator and multiple discriminators via an adversarial fashion. The proposed method not only learns shard representations for different appliances, but captures the specific multimode structures of each appliance. Extensive experiments on real-world datasets verify that our method can achieve new state-of-the-art performance.



### Recurrent Mask Refinement for Few-Shot Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.00622v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00622v2)
- **Published**: 2021-08-02 04:06:12+00:00
- **Updated**: 2021-08-04 04:27:27+00:00
- **Authors**: Hao Tang, Xingwei Liu, Shanlin Sun, Xiangyi Yan, Xiaohui Xie
- **Comment**: Accepted ICCV 2021
- **Journal**: None
- **Summary**: Although having achieved great success in medical image segmentation, deep convolutional neural networks usually require a large dataset with manual annotations for training and are difficult to generalize to unseen classes. Few-shot learning has the potential to address these challenges by learning new classes from only a few labeled examples. In this work, we propose a new framework for few-shot medical image segmentation based on prototypical networks. Our innovation lies in the design of two key modules: 1) a context relation encoder (CRE) that uses correlation to capture local relation features between foreground and background regions; and 2) a recurrent mask refinement module that repeatedly uses the CRE and a prototypical network to recapture the change of context relationship and refine the segmentation mask iteratively. Experiments on two abdomen CT datasets and an abdomen MRI dataset show the proposed method obtains substantial improvement over the state-of-the-art methods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively. Code is publicly available.



### Bespoke Fractal Sampling Patterns for Discrete Fourier Space via the Kaleidoscope Transform
- **Arxiv ID**: http://arxiv.org/abs/2108.00639v1
- **DOI**: 10.1109/LSP.2021.3116510
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.00639v1)
- **Published**: 2021-08-02 05:16:58+00:00
- **Updated**: 2021-08-02 05:16:58+00:00
- **Authors**: Jacob M. White, Stuart Crozier, Shekhar S. Chandra
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: Sampling strategies are important for sparse imaging methodologies, especially those employing the discrete Fourier transform (DFT). Chaotic sensing is one such methodology that employs deterministic, fractal sampling in conjunction with finite, iterative reconstruction schemes to form an image from limited samples. Using a sampling pattern constructed entirely from periodic lines in DFT space, chaotic sensing was found to outperform traditional compressed sensing for magnetic resonance imaging; however, only one such sampling pattern was presented and the reason for its fractal nature was not proven. Through the introduction of a novel image transform known as the kaleidoscope transform, which formalises and extends upon the concept of downsampling and concatenating an image with itself, this paper: (1) demonstrates a fundamental relationship between multiplication in modular arithmetic and downsampling; (2) provides a rigorous mathematical explanation for the fractal nature of the sampling pattern in the DFT; and (3) leverages this understanding to develop a collection of novel fractal sampling patterns for the 2D DFT with customisable properties. The ability to design tailor-made fractal sampling patterns expands the utility of the DFT in chaotic imaging and may form the basis for a bespoke chaotic sensing methodology, in which the fractal sampling matches the imaging task for improved reconstruction.



### Multimodal Feature Fusion for Video Advertisements Tagging Via Stacking Ensemble
- **Arxiv ID**: http://arxiv.org/abs/2108.00679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.00679v1)
- **Published**: 2021-08-02 07:26:28+00:00
- **Updated**: 2021-08-02 07:26:28+00:00
- **Authors**: Qingsong Zhou, Hai Liang, Zhimin Lin, Kele Xu
- **Comment**: 1st place in ACM Multimedia Multimodal Video Ads Tagging Competition
  (2021 Tencent Advertising Algorithm Competition)
- **Journal**: None
- **Summary**: Automated tagging of video advertisements has been a critical yet challenging problem, and it has drawn increasing interests in last years as its applications seem to be evident in many fields. Despite sustainable efforts have been made, the tagging task is still suffered from several challenges, such as, efficiently feature fusion approach is desirable, but under-explored in previous studies. In this paper, we present our approach for Multimodal Video Ads Tagging in the 2021 Tencent Advertising Algorithm Competition. Specifically, we propose a novel multi-modal feature fusion framework, with the goal to combine complementary information from multiple modalities. This framework introduces stacking-based ensembling approach to reduce the influence of varying levels of noise and conflicts between different modalities. Thus, our framework can boost the performance of the tagging task, compared to previous methods. To empirically investigate the effectiveness and robustness of the proposed framework, we conduct extensive experiments on the challenge datasets. The obtained results suggest that our framework can significantly outperform related approaches and our method ranks as the 1st place on the final leaderboard, with a Global Average Precision (GAP) of 82.63%. To better promote the research in this field, we will release our code in the final version.



### Self-supervised Audiovisual Representation Learning for Remote Sensing Data
- **Arxiv ID**: http://arxiv.org/abs/2108.00688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00688v1)
- **Published**: 2021-08-02 07:50:50+00:00
- **Updated**: 2021-08-02 07:50:50+00:00
- **Authors**: Konrad Heidler, Lichao Mou, Di Hu, Pu Jin, Guangyao Li, Chuang Gan, Ji-Rong Wen, Xiao Xiang Zhu
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Many current deep learning approaches make extensive use of backbone networks pre-trained on large datasets like ImageNet, which are then fine-tuned to perform a certain task. In remote sensing, the lack of comparable large annotated datasets and the wide diversity of sensing platforms impedes similar developments. In order to contribute towards the availability of pre-trained backbone networks in remote sensing, we devise a self-supervised approach for pre-training deep neural networks. By exploiting the correspondence between geo-tagged audio recordings and remote sensing imagery, this is done in a completely label-free manner, eliminating the need for laborious manual annotation. For this purpose, we introduce the SoundingEarth dataset, which consists of co-located aerial imagery and audio samples all around the world. Using this dataset, we then pre-train ResNet models to map samples from both modalities into a common embedding space, which encourages the models to understand key properties of a scene that influence both visual and auditory appearance. To validate the usefulness of the proposed approach, we evaluate the transfer learning performance of pre-trained weights obtained against weights obtained through other means. By fine-tuning the models on a number of commonly used remote sensing datasets, we show that our approach outperforms existing pre-training strategies for remote sensing imagery. The dataset, code and pre-trained model weights will be available at https://github.com/khdlr/SoundingEarth.



### LDDMM-Face: Large Deformation Diffeomorphic Metric Learning for Flexible and Consistent Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/2108.00690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00690v1)
- **Published**: 2021-08-02 07:57:15+00:00
- **Updated**: 2021-08-02 07:57:15+00:00
- **Authors**: Huilin Yang, Junyan Lyu, Pujin Cheng, Xiaoying Tang
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: We innovatively propose a flexible and consistent face alignment framework, LDDMM-Face, the key contribution of which is a deformation layer that naturally embeds facial geometry in a diffeomorphic way. Instead of predicting facial landmarks via heatmap or coordinate regression, we formulate this task in a diffeomorphic registration manner and predict momenta that uniquely parameterize the deformation between initial boundary and true boundary, and then perform large deformation diffeomorphic metric mapping (LDDMM) simultaneously for curve and landmark to localize the facial landmarks. Due to the embedding of LDDMM into a deep network, LDDMM-Face can consistently annotate facial landmarks without ambiguity and flexibly handle various annotation schemes, and can even predict dense annotations from sparse ones. Our method can be easily integrated into various face alignment networks. We extensively evaluate LDDMM-Face on four benchmark datasets: 300W, WFLW, HELEN and COFW-68. LDDMM-Face is comparable or superior to state-of-the-art methods for traditional within-dataset and same-annotation settings, but truly distinguishes itself with outstanding performance when dealing with weakly-supervised learning (partial-to-full), challenging cases (e.g., occluded faces), and different training and prediction datasets. In addition, LDDMM-Face shows promising results on the most challenging task of predicting across datasets with different annotation schemes.



### PoseFusion2: Simultaneous Background Reconstruction and Human Shape Recovery in Real-time
- **Arxiv ID**: http://arxiv.org/abs/2108.00695v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.00695v1)
- **Published**: 2021-08-02 08:03:29+00:00
- **Updated**: 2021-08-02 08:03:29+00:00
- **Authors**: Huayan Zhang, Tianwei Zhang, Tin Lun Lam, Sethu Vijayakumar
- **Comment**: Accepted by IROS-2021
- **Journal**: None
- **Summary**: Dynamic environments that include unstructured moving objects pose a hard problem for Simultaneous Localization and Mapping (SLAM) performance. The motion of rigid objects can be typically tracked by exploiting their texture and geometric features. However, humans moving in the scene are often one of the most important, interactive targets - they are very hard to track and reconstruct robustly due to non-rigid shapes. In this work, we present a fast, learning-based human object detector to isolate the dynamic human objects and realise a real-time dense background reconstruction framework. We go further by estimating and reconstructing the human pose and shape. The final output environment maps not only provide the dense static backgrounds but also contain the dynamic human meshes and their trajectories. Our Dynamic SLAM system runs at around 26 frames per second (fps) on GPUs, while additionally turning on accurate human pose estimation can be executed at up to 10 fps.



### An Experimental Urban Case Study with Various Data Sources and a Model for Traffic Estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.07698v1
- **DOI**: 10.3390/s22010144
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.07698v1)
- **Published**: 2021-08-02 08:13:57+00:00
- **Updated**: 2021-08-02 08:13:57+00:00
- **Authors**: Alexander Genser, Noel Hautle, Michail Makridis, Anastasios Kouvelas
- **Comment**: None
- **Journal**: Sensors 2022, 22(1), 144
- **Summary**: Accurate estimation of the traffic state over a network is essential since it is the starting point for designing and implementing any traffic management strategy. Hence, traffic operators and users of a transportation network can make reliable decisions such as influence/change route or mode choice. However, the problem of traffic state estimation from various sensors within an urban environment is very complex for several different reasons, such as availability of sensors, different noise levels, different output quantities, sensor accuracy, heterogeneous data fusion, and many more. To provide a better understanding of this problem, we organized an experimental campaign with video measurement in an area within the urban network of Zurich, Switzerland. We focus on capturing the traffic state in terms of traffic flow and travel times by ensuring measurements from established thermal cameras by the city's authorities, processed video data, and the Google Distance Matrix. We assess the different data sources, and we propose a simple yet efficient Multiple Linear Regression (MLR) model to estimate travel times with fusion of various data sources. Comparative results with ground-truth data (derived from video measurements) show the efficiency and robustness of the proposed methodology.



### Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.00705v2
- **DOI**: 10.1145/3462244.3479892
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.00705v2)
- **Published**: 2021-08-02 08:16:58+00:00
- **Updated**: 2021-08-08 12:50:28+00:00
- **Authors**: Zhongwei Xie, Ling Liu, Lin Li, Luo Zhong
- **Comment**: accepted by ACM ICMI 2021 conference
- **Journal**: None
- **Summary**: This paper introduces a two-phase deep feature calibration framework for efficient learning of semantics enhanced text-image cross-modal joint embedding, which clearly separates the deep feature calibration in data preprocessing from training the joint embedding model. We use the Recipe1M dataset for the technical description and empirical validation. In preprocessing, we perform deep feature calibration by combining deep feature engineering with semantic context features derived from raw text-image input data. We leverage LSTM to identify key terms, NLP methods to produce ranking scores for key terms before generating the key term feature. We leverage wideResNet50 to extract and encode the image category semantics to help semantic alignment of the learned recipe and image embeddings in the joint latent space. In joint embedding learning, we perform deep feature calibration by optimizing the batch-hard triplet loss function with soft-margin and double negative sampling, also utilizing the category-based alignment loss and discriminator-based alignment loss. Extensive experiments demonstrate that our SEJE approach with the deep feature calibration significantly outperforms the state-of-the-art approaches.



### Group Fisher Pruning for Practical Network Compression
- **Arxiv ID**: http://arxiv.org/abs/2108.00708v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.00708v1)
- **Published**: 2021-08-02 08:21:44+00:00
- **Updated**: 2021-08-02 08:21:44+00:00
- **Authors**: Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin Chen, Wenming Yang, Qingmin Liao, Wayne Zhang
- **Comment**: ICML2021; Code: https://github.com/jshilong/FisherPruning
- **Journal**: None
- **Summary**: Network compression has been widely studied since it is able to reduce the memory and computation cost during inference. However, previous methods seldom deal with complicated structures like residual connections, group/depth-wise convolution and feature pyramid network, where channels of multiple layers are coupled and need to be pruned simultaneously. In this paper, we present a general channel pruning approach that can be applied to various complicated structures. Particularly, we propose a layer grouping algorithm to find coupled channels automatically. Then we derive a unified metric based on Fisher information to evaluate the importance of a single channel and coupled channels. Moreover, we find that inference speedup on GPUs is more correlated with the reduction of memory rather than FLOPs, and thus we employ the memory reduction of each channel to normalize the importance. Our method can be used to prune any structures including those with coupled channels. We conduct extensive experiments on various backbones, including the classic ResNet and ResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image classification and object detection which is under-explored. Experimental results validate that our method can effectively prune sophisticated networks, boosting inference speed without sacrificing accuracy.



### Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.00713v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.00713v2)
- **Published**: 2021-08-02 08:32:57+00:00
- **Updated**: 2022-05-18 17:39:50+00:00
- **Authors**: Brennan Nichyporuk, Jillian Cardinell, Justin Szeto, Raghav Mehta, Sotirios Tsaftaris, Douglas L. Arnold, Tal Arbel
- **Comment**: Accepted at DART 2021
- **Journal**: None
- **Summary**: Many automatic machine learning models developed for focal pathology (e.g. lesions, tumours) detection and segmentation perform well, but do not generalize as well to new patient cohorts, impeding their widespread adoption into real clinical contexts. One strategy to create a more diverse, generalizable training set is to naively pool datasets from different cohorts. Surprisingly, training on this \it{big data} does not necessarily increase, and may even reduce, overall performance and model generalizability, due to the existence of cohort biases that affect label distributions. In this paper, we propose a generalized affine conditioning framework to learn and account for cohort biases across multi-source datasets, which we call Source-Conditioned Instance Normalization (SCIN). Through extensive experimentation on three different, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS) clinical trial MRI datasets, we show that our cohort bias adaptation method (1) improves performance of the network on pooled datasets relative to naively pooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the instance normalization parameters, thus learning the new cohort bias with only 10 labelled samples.



### Learning TFIDF Enhanced Joint Embedding for Recipe-Image Cross-Modal Retrieval Service
- **Arxiv ID**: http://arxiv.org/abs/2108.00724v1
- **DOI**: 10.1109/TSC.2021.3098834
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2108.00724v1)
- **Published**: 2021-08-02 08:49:30+00:00
- **Updated**: 2021-08-02 08:49:30+00:00
- **Authors**: Zhongwei Xie, Ling Liu, Yanzhao Wu, Lin Li, Luo Zhong
- **Comment**: accepted by IEEE Transactions on Services Computing
- **Journal**: None
- **Summary**: It is widely acknowledged that learning joint embeddings of recipes with images is challenging due to the diverse composition and deformation of ingredients in cooking procedures. We present a Multi-modal Semantics enhanced Joint Embedding approach (MSJE) for learning a common feature space between the two modalities (text and image), with the ultimate goal of providing high-performance cross-modal retrieval services. Our MSJE approach has three unique features. First, we extract the TFIDF feature from the title, ingredients and cooking instructions of recipes. By determining the significance of word sequences through combining LSTM learned features with their TFIDF features, we encode a recipe into a TFIDF weighted vector for capturing significant key terms and how such key terms are used in the corresponding cooking instructions. Second, we combine the recipe TFIDF feature with the recipe sequence feature extracted through two-stage LSTM networks, which is effective in capturing the unique relationship between a recipe and its associated image(s). Third, we further incorporate TFIDF enhanced category semantics to improve the mapping of image modality and to regulate the similarity loss function during the iterative learning of cross-modal joint embedding. Experiments on the benchmark dataset Recipe1M show the proposed approach outperforms the state-of-the-art approaches.



### Active Perception for Ambiguous Objects Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.00737v1
- **DOI**: 10.1109/IROS51168.2021.9636414
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.00737v1)
- **Published**: 2021-08-02 09:12:34+00:00
- **Updated**: 2021-08-02 09:12:34+00:00
- **Authors**: Evgenii Safronov, Nicola Piga, Michele Colledanchise, Lorenzo Natale
- **Comment**: Accepted version at 2021 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2021)
- **Journal**: 2021 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
- **Summary**: Recent visual pose estimation and tracking solutions provide notable results on popular datasets such as T-LESS and YCB. However, in the real world, we can find ambiguous objects that do not allow exact classification and detection from a single view. In this work, we propose a framework that, given a single view of an object, provides the coordinates of a next viewpoint to discriminate the object against similar ones, if any, and eliminates ambiguities. We also describe a complete pipeline from a real object's scans to the viewpoint selection and classification. We validate our approach with a Franka Emika Panda robot and common household objects featured with ambiguities. We released the source code to reproduce our experiments.



### Flip Learning: Erase to Segment
- **Arxiv ID**: http://arxiv.org/abs/2108.00752v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2108.00752v1)
- **Published**: 2021-08-02 09:56:10+00:00
- **Updated**: 2021-08-02 09:56:10+00:00
- **Authors**: Yuhao Huang, Xin Yang, Yuxin Zou, Chaoyu Chen, Jian Wang, Haoran Dou, Nishant Ravikumar, Alejandro F Frangi, Jianqiao Zhou, Dong Ni
- **Comment**: Accepted by MICCAI 2021
- **Journal**: None
- **Summary**: Nodule segmentation from breast ultrasound images is challenging yet essential for the diagnosis. Weakly-supervised segmentation (WSS) can help reduce time-consuming and cumbersome manual annotation. Unlike existing weakly-supervised approaches, in this study, we propose a novel and general WSS framework called Flip Learning, which only needs the box annotation. Specifically, the target in the label box will be erased gradually to flip the classification tag, and the erased region will be considered as the segmentation result finally. Our contribution is three-fold. First, our proposed approach erases on superpixel level using a Multi-agent Reinforcement Learning framework to exploit the prior boundary knowledge and accelerate the learning process. Second, we design two rewards: classification score and intensity distribution reward, to avoid under- and over-segmentation, respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the residual errors and improve the segmentation performance. Extensively validated on a large dataset, our proposed approach achieves competitive performance and shows great potential to narrow the gap between fully-supervised and weakly-supervised learning.



### BezierSeg: Parametric Shape Representation for Fast Object Segmentation in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2108.00760v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.00760v1)
- **Published**: 2021-08-02 10:10:57+00:00
- **Updated**: 2021-08-02 10:10:57+00:00
- **Authors**: Haichou Chen, Yishu Deng, Bin Li, Zeqin Li, Haohua Chen, Bingzhong Jing, Chaofeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Delineating the lesion area is an important task in image-based diagnosis. Pixel-wise classification is a popular approach to segmenting the region of interest. However, at fuzzy boundaries such methods usually result in glitches, discontinuity, or disconnection, inconsistent with the fact that lesions are solid and smooth. To overcome these undesirable artifacts, we propose the BezierSeg model which outputs bezier curves encompassing the region of interest. Directly modelling the contour with analytic equations ensures that the segmentation is connected, continuous, and the boundary is smooth. In addition, it offers sub-pixel accuracy. Without loss of accuracy, the bezier contour can be resampled and overlaid with images of any resolution. Moreover, a doctor can conveniently adjust the curve's control points to refine the result. Our experiments show that the proposed method runs in real time and achieves accuracy competitive with pixel-wise segmentation models.



### Angle Based Feature Learning in GNN for 3D Object Detection using Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2108.00780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.00780v1)
- **Published**: 2021-08-02 10:56:02+00:00
- **Updated**: 2021-08-02 10:56:02+00:00
- **Authors**: Md Afzal Ansari, Md Meraz, Pavan Chakraborty, Mohammed Javed
- **Comment**: Accepted in MISP2021
- **Journal**: None
- **Summary**: In this paper, we present new feature encoding methods for Detection of 3D objects in point clouds. We used a graph neural network (GNN) for Detection of 3D objects namely cars, pedestrians, and cyclists. Feature encoding is one of the important steps in Detection of 3D objects. The dataset used is point cloud data which is irregular and unstructured and it needs to be encoded in such a way that ensures better feature encapsulation. Earlier works have used relative distance as one of the methods to encode the features. These methods are not resistant to rotation variance problems in Graph Neural Networks. We have included angular-based measures while performing feature encoding in graph neural networks. Along with that, we have performed a comparison between other methods like Absolute, Relative, Euclidean distances, and a combination of the Angle and Relative methods. The model is trained and evaluated on the subset of the KITTI object detection benchmark dataset under resource constraints. Our results demonstrate that a combination of angle measures and relative distance has performed better than other methods. In comparison to the baseline method(relative), it achieved better performance. We also performed time analysis of various feature encoding methods.



### Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling
- **Arxiv ID**: http://arxiv.org/abs/2108.00784v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.00784v2)
- **Published**: 2021-08-02 11:03:39+00:00
- **Updated**: 2021-09-07 16:54:50+00:00
- **Authors**: Natalia Khanzhina, Alexey Lapenok, Andrey Filchenkov
- **Comment**: 9 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: According to recent studies, commonly used computer vision datasets contain about 4% of label errors. For example, the COCO dataset is known for its high level of noise in data labels, which limits its use for training robust neural deep architectures in a real-world scenario. To model such a noise, in this paper we have proposed the homoscedastic aleatoric uncertainty estimation, and present a series of novel loss functions to address the problem of image object detection at scale. Specifically, the proposed functions are based on Bayesian inference and we have incorporated them into the common community-adopted object detection deep learning architecture RetinaNet. We have also shown that modeling of homoscedastic aleatoric uncertainty using our novel functions allows to increase the model interpretability and to improve the object detection performance being evaluated on the COCO dataset.



### Training face verification models from generated face identity data
- **Arxiv ID**: http://arxiv.org/abs/2108.00800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00800v1)
- **Published**: 2021-08-02 12:00:01+00:00
- **Updated**: 2021-08-02 12:00:01+00:00
- **Authors**: Dennis Conway, Loic Simon, Alexis Lechervy, Frederic Jurie
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning tools are becoming increasingly powerful and widely used. Unfortunately membership attacks, which seek to uncover information from data sets used in machine learning, have the potential to limit data sharing. In this paper we consider an approach to increase the privacy protection of data sets, as applied to face recognition. Using an auxiliary face recognition model, we build on the StyleGAN generative adversarial network and feed it with latent codes combining two distinct sub-codes, one encoding visual identity factors, and, the other, non-identity factors. By independently varying these vectors during image generation, we create a synthetic data set of fictitious face identities. We use this data set to train a face recognition model. The model performance degrades in comparison to the state-of-the-art of face verification. When tested with a simple membership attack our model provides good privacy protection, however the model performance degrades in comparison to the state-of-the-art of face verification. We find that the addition of a small amount of private data greatly improves the performance of our model, which highlights the limitations of using synthetic data to train machine learning models.



### Learn to Match: Automatic Matching Network Design for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2108.00803v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00803v1)
- **Published**: 2021-08-02 12:09:23+00:00
- **Updated**: 2021-08-02 12:09:23+00:00
- **Authors**: Zhipeng Zhang, Yihao Liu, Xiao Wang, Bing Li, Weiming Hu
- **Comment**: accepted by ICCV2021
- **Journal**: None
- **Summary**: Siamese tracking has achieved groundbreaking performance in recent years, where the essence is the efficient matching operator cross-correlation and its variants. Besides the remarkable success, it is important to note that the heuristic matching network design relies heavily on expert experience. Moreover, we experimentally find that one sole matching operator is difficult to guarantee stable tracking in all challenging environments. Thus, in this work, we introduce six novel matching operators from the perspective of feature fusion instead of explicit similarity learning, namely Concatenation, Pointwise-Addition, Pairwise-Relation, FiLM, Simple-Transformer and Transductive-Guidance, to explore more feasibility on matching operator selection. The analyses reveal these operators' selective adaptability on different environment degradation types, which inspires us to combine them to explore complementary features. To this end, we propose binary channel manipulation (BCM) to search for the optimal combination of these operators. BCM determines to retrain or discard one operator by learning its contribution to other tracking steps. By inserting the learned matching networks to a strong baseline tracker Ocean, our model achieves favorable gains by $67.2 \rightarrow 71.4$, $52.6 \rightarrow 58.3$, $70.3 \rightarrow 76.0$ success on OTB100, LaSOT, and TrackingNet, respectively. Notably, Our tracker, dubbed AutoMatch, uses less than half of training data/time than the baseline tracker, and runs at 50 FPS using PyTorch. Code and model will be released at https://github.com/JudasDie/SOTS.



### Projective Skip-Connections for Segmentation Along a Subset of Dimensions in Retinal OCT
- **Arxiv ID**: http://arxiv.org/abs/2108.00831v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.00831v1)
- **Published**: 2021-08-02 12:41:58+00:00
- **Updated**: 2021-08-02 12:41:58+00:00
- **Authors**: Dmitrii Lachinov, Philipp Seeboeck, Julia Mai, Ursula Schmidt-Erfurth, Hrvoje Bogunovic
- **Comment**: Submitted to MICCAI 2021
- **Journal**: None
- **Summary**: In medical imaging, there are clinically relevant segmentation tasks where the output mask is a projection to a subset of input image dimensions. In this work, we propose a novel convolutional neural network architecture that can effectively learn to produce a lower-dimensional segmentation mask than the input image. The network restores encoded representation only in a subset of input spatial dimensions and keeps the representation unchanged in the others. The newly proposed projective skip-connections allow linking the encoder and decoder in a UNet-like structure. We evaluated the proposed method on two clinically relevant tasks in retinal Optical Coherence Tomography (OCT): geographic atrophy and retinal blood vessel segmentation. The proposed method outperformed the current state-of-the-art approaches on all the OCT datasets used, consisting of 3D volumes and corresponding 2D en-face masks. The proposed architecture fills the methodological gap between image classification and ND image segmentation.



### Constrained Graphic Layout Generation via Latent Optimization
- **Arxiv ID**: http://arxiv.org/abs/2108.00871v1
- **DOI**: 10.1145/3474085.3475497
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.00871v1)
- **Published**: 2021-08-02 13:04:11+00:00
- **Updated**: 2021-08-02 13:04:11+00:00
- **Authors**: Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, Kota Yamaguchi
- **Comment**: Accepted by ACM Multimedia 2021
- **Journal**: None
- **Summary**: It is common in graphic design humans visually arrange various elements according to their design intent and semantics. For example, a title text almost always appears on top of other elements in a document. In this work, we generate graphic layouts that can flexibly incorporate such design semantics, either specified implicitly or explicitly by a user. We optimize using the latent space of an off-the-shelf layout generation model, allowing our approach to be complementary to and used with existing layout generation models. Our approach builds on a generative layout model based on a Transformer architecture, and formulates the layout generation as a constrained optimization problem where design constraints are used for element alignment, overlap avoidance, or any other user-specified relationship. We show in the experiments that our approach is capable of generating realistic layouts in both constrained and unconstrained generation tasks with a single model. The code is available at https://github.com/ktrk115/const_layout .



### Shallow Feature Matters for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2108.00873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00873v1)
- **Published**: 2021-08-02 13:16:48+00:00
- **Updated**: 2021-08-02 13:16:48+00:00
- **Authors**: Jun Wei, Qin Wang, Zhen Li, Sheng Wang, S. Kevin Zhou, Shuguang Cui
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Weakly supervised object localization (WSOL) aims to localize objects by only utilizing image-level labels. Class activation maps (CAMs) are the commonly used features to achieve WSOL. However, previous CAM-based methods did not take full advantage of the shallow features, despite their importance for WSOL. Because shallow features are easily buried in background noise through conventional fusion. In this paper, we propose a simple but effective Shallow feature-aware Pseudo supervised Object Localization (SPOL) model for accurate WSOL, which makes the utmost of low-level features embedded in shallow layers. In practice, our SPOL model first generates the CAMs through a novel element-wise multiplication of shallow and deep feature maps, which filters the background noise and generates sharper boundaries robustly. Besides, we further propose a general class-agnostic segmentation model to achieve the accurate object mask, by only using the initial CAMs as the pseudo label without any extra annotation. Eventually, a bounding box extractor is applied to the object mask to locate the target. Experiments verify that our SPOL outperforms the state-of-the-art on both CUB-200 and ImageNet-1K benchmarks, achieving 93.44% and 67.15% (i.e., 3.93% and 2.13% improvement) Top-5 localization accuracy, respectively.



### Shallow Attention Network for Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.00882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00882v1)
- **Published**: 2021-08-02 13:29:40+00:00
- **Updated**: 2021-08-02 13:29:40+00:00
- **Authors**: Jun Wei, Yiwen Hu, Ruimao Zhang, Zhen Li, S. Kevin Zhou, Shuguang Cui
- **Comment**: Accepted by MICCAI2021
- **Journal**: None
- **Summary**: Accurate polyp segmentation is of great importance for colorectal cancer diagnosis. However, even with a powerful deep neural network, there still exists three big challenges that impede the development of polyp segmentation. (i) Samples collected under different conditions show inconsistent colors, causing the feature distribution gap and overfitting issue; (ii) Due to repeated feature downsampling, small polyps are easily degraded; (iii) Foreground and background pixels are imbalanced, leading to a biased training. To address the above issues, we propose the Shallow Attention Network (SANet) for polyp segmentation. Specifically, to eliminate the effects of color, we design the color exchange operation to decouple the image contents and colors, and force the model to focus more on the target shape and structure. Furthermore, to enhance the segmentation quality of small polyps, we propose the shallow attention module to filter out the background noise of shallow features. Thanks to the high resolution of shallow features, small polyps can be preserved correctly. In addition, to ease the severe pixel imbalance for small polyps, we propose a probability correction strategy (PCS) during the inference phase. Note that even though PCS is not involved in the training phase, it can still work well on a biased model and consistently improve the segmentation performance. Quantitative and qualitative experimental results on five challenging benchmarks confirm that our proposed SANet outperforms previous state-of-the-art methods by a large margin and achieves a speed about 72FPS.



### Multi-phase Liver Tumor Segmentation with Spatial Aggregation and Uncertain Region Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2108.00911v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.00911v2)
- **Published**: 2021-08-02 14:02:28+00:00
- **Updated**: 2021-08-05 09:45:58+00:00
- **Authors**: Yue Zhang, Chengtao Peng, Liying Peng, Huimin Huang, Ruofeng Tong, Lanfen Lin, Jingsong Li, Yen-Wei Chen, Qingqing Chen, Hongjie Hu, Zhiyi Peng
- **Comment**: To appear in MICCAI 2021
- **Journal**: None
- **Summary**: Multi-phase computed tomography (CT) images provide crucial complementary information for accurate liver tumor segmentation (LiTS). State-of-the-art multi-phase LiTS methods usually fused cross-phase features through phase-weighted summation or channel-attention based concatenation. However, these methods ignored the spatial (pixel-wise) relationships between different phases, hence leading to insufficient feature integration. In addition, the performance of existing methods remains subject to the uncertainty in segmentation, which is particularly acute in tumor boundary regions. In this work, we propose a novel LiTS method to adequately aggregate multi-phase information and refine uncertain region segmentation. To this end, we introduce a spatial aggregation module (SAM), which encourages per-pixel interactions between different phases, to make full use of cross-phase information. Moreover, we devise an uncertain region inpainting module (URIM) to refine uncertain pixels using neighboring discriminative features. Experiments on an in-house multi-phase CT dataset of focal liver lesions (MPCT-FLLs) demonstrate that our method achieves promising liver tumor segmentation and outperforms state-of-the-arts.



### I2V-GAN: Unpaired Infrared-to-Visible Video Translation
- **Arxiv ID**: http://arxiv.org/abs/2108.00913v2
- **DOI**: 10.1145/3474085.3475445
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00913v2)
- **Published**: 2021-08-02 14:04:19+00:00
- **Updated**: 2021-08-04 05:24:30+00:00
- **Authors**: Shuang Li, Bingfeng Han, Zhenjie Yu, Chi Harold Liu, Kai Chen, Shuigen Wang
- **Comment**: Accepted at ACM MM 2021
- **Journal**: None
- **Summary**: Human vision is often adversely affected by complex environmental factors, especially in night vision scenarios. Thus, infrared cameras are often leveraged to help enhance the visual effects via detecting infrared radiation in the surrounding environment, but the infrared videos are undesirable due to the lack of detailed semantic information. In such a case, an effective video-to-video translation method from the infrared domain to the visible light counterpart is strongly needed by overcoming the intrinsic huge gap between infrared and visible fields. To address this challenging problem, we propose an infrared-to-visible (I2V) video translation method I2V-GAN to generate fine-grained and spatial-temporal consistent visible light videos by given unpaired infrared videos. Technically, our model capitalizes on three types of constraints: 1)adversarial constraint to generate synthetic frames that are similar to the real ones, 2)cyclic consistency with the introduced perceptual loss for effective content conversion as well as style preservation, and 3)similarity constraints across and within domains to enhance the content and motion consistency in both spatial and temporal spaces at a fine-grained level. Furthermore, the current public available infrared and visible light datasets are mainly used for object detection or tracking, and some are composed of discontinuous images which are not suitable for video tasks. Thus, we provide a new dataset for I2V video translation, which is named IRVI. Specifically, it has 12 consecutive video clips of vehicle and monitoring scenes, and both infrared and visible light videos could be apart into 24352 frames. Comprehensive experiments validate that I2V-GAN is superior to the compared SOTA methods in the translation of I2V videos with higher fluency and finer semantic details. The code and IRVI dataset are available at https://github.com/BIT-DA/I2V-GAN.



### StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators
- **Arxiv ID**: http://arxiv.org/abs/2108.00946v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.00946v2)
- **Published**: 2021-08-02 14:46:46+00:00
- **Updated**: 2021-12-16 17:05:46+00:00
- **Authors**: Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, Daniel Cohen-Or
- **Comment**: Project page: https://stylegan-nada.github.io/
- **Journal**: None
- **Summary**: Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained "blindly"? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.



### An Applied Deep Learning Approach for Estimating Soybean Relative Maturity from UAV Imagery to Aid Plant Breeding Decisions
- **Arxiv ID**: http://arxiv.org/abs/2108.00952v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.00952v1)
- **Published**: 2021-08-02 14:53:58+00:00
- **Updated**: 2021-08-02 14:53:58+00:00
- **Authors**: Saba Moeinizade, Hieu Pham, Ye Han, Austin Dobbels, Guiping Hu
- **Comment**: 22 pages, 7 figures
- **Journal**: None
- **Summary**: For a global breeding organization, identifying the next generation of superior crops is vital for its success. Recognizing new genetic varieties requires years of in-field testing to gather data about the crop's yield, pest resistance, heat resistance, etc. At the conclusion of the growing season, organizations need to determine which varieties will be advanced to the next growing season (or sold to farmers) and which ones will be discarded from the candidate pool. Specifically for soybeans, identifying their relative maturity is a vital piece of information used for advancement decisions. However, this trait needs to be physically observed, and there are resource limitations (time, money, etc.) that bottleneck the data collection process. To combat this, breeding organizations are moving toward advanced image capturing devices. In this paper, we develop a robust and automatic approach for estimating the relative maturity of soybeans using a time series of UAV images. An end-to-end hybrid model combining Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) is proposed to extract features and capture the sequential behavior of time series data. The proposed deep learning model was tested on six different environments across the United States. Results suggest the effectiveness of our proposed CNN-LSTM model compared to the local regression method. Furthermore, we demonstrate how this newfound information can be used to aid in plant breeding advancement decisions.



### Robust Semantic Segmentation with Superpixel-Mix
- **Arxiv ID**: http://arxiv.org/abs/2108.00968v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2108.00968v2)
- **Published**: 2021-08-02 15:13:52+00:00
- **Updated**: 2021-10-21 16:10:44+00:00
- **Authors**: Gianni Franchi, Nacim Belkhir, Mai Lan Ha, Yufei Hu, Andrei Bursuc, Volker Blanz, Angela Yao
- **Comment**: Accepted to BMVC2021
- **Journal**: None
- **Summary**: Along with predictive performance and runtime speed, reliability is a key requirement for real-world semantic segmentation. Reliability encompasses robustness, predictive uncertainty and reduced bias. To improve reliability, we introduce Superpixel-mix, a new superpixel-based data augmentation method with teacher-student consistency training. Unlike other mixing-based augmentation techniques, mixing superpixels between images is aware of object boundaries, while yielding consistent gains in segmentation accuracy. Our proposed technique achieves state-of-the-art results in semi-supervised semantic segmentation on the Cityscapes dataset. Moreover, Superpixel-mix improves the reliability of semantic segmentation by reducing network uncertainty and bias, as confirmed by competitive results under strong distributions shift (adverse weather, image corruptions) and when facing out-of-distribution data.



### Multilevel Knowledge Transfer for Cross-Domain Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.00977v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.00977v2)
- **Published**: 2021-08-02 15:24:40+00:00
- **Updated**: 2021-08-03 14:09:27+00:00
- **Authors**: Botos Csaba, Xiaojuan Qi, Arslan Chaudhry, Puneet Dokania, Philip Torr
- **Comment**: None
- **Journal**: None
- **Summary**: Domain shift is a well known problem where a model trained on a particular domain (source) does not perform well when exposed to samples from a different domain (target). Unsupervised methods that can adapt to domain shift are highly desirable as they allow effective utilization of the source data without requiring additional annotated training data from the target. Practically, obtaining sufficient amount of annotated data from the target domain can be both infeasible and extremely expensive. In this work, we address the domain shift problem for the object detection task. Our approach relies on gradually removing the domain shift between the source and the target domains. The key ingredients to our approach are -- (a) mapping the source to the target domain on pixel-level; (b) training a teacher network on the mapped source and the unannotated target domain using adversarial feature alignment; and (c) finally training a student network using the pseudo-labels obtained from the teacher. Experimentally, when tested on challenging scenarios involving domain shift, we consistently obtain significantly large performance gains over various recent state of the art approaches.



### My Eyes Are Up Here: Promoting Focus on Uncovered Regions in Masked Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.00996v3
- **DOI**: 10.1109/BIOSIG52210.2021.9548320
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00996v3)
- **Published**: 2021-08-02 15:51:15+00:00
- **Updated**: 2021-08-18 11:25:57+00:00
- **Authors**: Pedro C. Neto, Fadi Boutros, Joo Ribeiro Pinto, Mohsen Saffari, Naser Damer, Ana F. Sequeira, Jaime S. Cardoso
- **Comment**: Accepted at 20th International Conference of the Biometrics Special
  Interest Group (BIOSIG 2021)
- **Journal**: None
- **Summary**: The recent Covid-19 pandemic and the fact that wearing masks in public is now mandatory in several countries, created challenges in the use of face recognition systems (FRS). In this work, we address the challenge of masked face recognition (MFR) and focus on evaluating the verification performance in FRS when verifying masked vs unmasked faces compared to verifying only unmasked faces. We propose a methodology that combines the traditional triplet loss and the mean squared error (MSE) intending to improve the robustness of an MFR system in the masked-unmasked comparison mode. The results obtained by our proposed method show improvements in a detailed step-wise ablation study. The conducted study showed significant performance gains induced by our proposed training paradigm and modified triplet loss on two evaluation databases.



### Wood-leaf classification of tree point cloud based on intensity and geometrical information
- **Arxiv ID**: http://arxiv.org/abs/2108.01002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01002v1)
- **Published**: 2021-08-02 16:04:48+00:00
- **Updated**: 2021-08-02 16:04:48+00:00
- **Authors**: Jingqian Sun, Pei Wang, Zhiyong Gao, Zichu Liu, Yaxin Li, Xiaozheng Gan
- **Comment**: None
- **Journal**: None
- **Summary**: Terrestrial laser scanning (TLS) can obtain tree point cloud with high precision and high density. Efficient classification of wood points and leaf points is essential to study tree structural parameters and ecological characteristics. By using both the intensity and spatial information, a three-step classification and verification method was proposed to achieve automated wood-leaf classification. Tree point cloud was classified into wood points and leaf points by using intensity threshold, neighborhood density and voxelization successively. Experiment was carried in Haidian Park, Beijing, and 24 trees were scanned by using the RIEGL VZ-400 scanner. The tree point clouds were processed by using the proposed method, whose classification results were compared with the manual classification results which were used as standard results. To evaluate the classification accuracy, three indicators were used in the experiment, which are Overall Accuracy (OA), Kappa coefficient (Kappa) and Matthews correlation coefficient (MCC). The ranges of OA, Kappa and MCC of the proposed method are from 0.9167 to 0.9872, from 0.7276 to 0.9191, and from 0.7544 to 0.9211 respectively. The average values of OA, Kappa and MCC are 0.9550, 0.8547 and 0.8627 respectively. Time cost of wood-leaf classification was also recorded to evaluate the algorithm efficiency. The average processing time are 1.4 seconds per million points. The results showed that the proposed method performed well automatically and quickly on wood-leaf classification based on the experimental dataset.



### Bringing AI pipelines onto cloud-HPC: setting a baseline for accuracy of COVID-19 AI diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2108.01033v1
- **DOI**: 10.5281/zenodo.5151511
- **Categories**: **cs.DC**, cs.CV, eess.IV, D.1.3; D.3.2; C.1.3
- **Links**: [PDF](http://arxiv.org/pdf/2108.01033v1)
- **Published**: 2021-08-02 16:45:00+00:00
- **Updated**: 2021-08-02 16:45:00+00:00
- **Authors**: Iacopo Colonnelli, Barbara Cantalupo, Concetto Spampinato, Matteo Pennisi, Marco Aldinucci
- **Comment**: None
- **Journal**: In F. Iannone editor, ENEA CRESCO in the fight against COVID-19,
  pages 66-73. ISBN: 978-88-8286-415-6. June 2021
- **Summary**: HPC is an enabling platform for AI. The introduction of AI workloads in the HPC applications basket has non-trivial consequences both on the way of designing AI applications and on the way of providing HPC computing. This is the leitmotif of the convergence between HPC and AI. The formalized definition of AI pipelines is one of the milestones of HPC-AI convergence. If well conducted, it allows, on the one hand, to obtain portable and scalable applications. On the other hand, it is crucial for the reproducibility of scientific pipelines. In this work, we advocate the StreamFlow Workflow Management System as a crucial ingredient to define a parametric pipeline, called "CLAIRE COVID-19 Universal Pipeline," which is able to explore the optimization space of methods to classify COVID-19 lung lesions from CT scans, compare them for accuracy, and therefore set a performance baseline. The universal pipeline automatizes the training of many different Deep Neural Networks (DNNs) and many different hyperparameters. It, therefore, requires a massive computing power, which is found in traditional HPC infrastructure thanks to the portability-by-design of pipelines designed with StreamFlow. Using the universal pipeline, we identified a DNN reaching over 90% accuracy in detecting COVID-19 lesions in CT scans.



### Musical Speech: A Transformer-based Composition Tool
- **Arxiv ID**: http://arxiv.org/abs/2108.01043v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2108.01043v1)
- **Published**: 2021-08-02 17:03:27+00:00
- **Updated**: 2021-08-02 17:03:27+00:00
- **Authors**: Jason d'Eon, Sri Harsha Dumpala, Chandramouli Shama Sastry, Dani Oore, Sageev Oore
- **Comment**: NeurIPS 2020 Demonstration Track; extended for PMLR
- **Journal**: None
- **Summary**: In this paper, we propose a new compositional tool that will generate a musical outline of speech recorded/provided by the user for use as a musical building block in their compositions. The tool allows any user to use their own speech to generate musical material, while still being able to hear the direct connection between their recorded speech and the resulting music. The tool is built on our proposed pipeline. This pipeline begins with speech-based signal processing, after which some simple musical heuristics are applied, and finally these pre-processed signals are passed through Transformer models trained on new musical tasks. We illustrate the effectiveness of our pipeline -- which does not require a paired dataset for training -- through examples of music created by musicians making use of our tool.



### Distributed Attention for Grounded Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2108.01056v2
- **DOI**: 10.1145/3474085.3475354
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.01056v2)
- **Published**: 2021-08-02 17:28:33+00:00
- **Updated**: 2021-08-22 17:49:50+00:00
- **Authors**: Nenglun Chen, Xingjia Pan, Runnan Chen, Lei Yang, Zhiwen Lin, Yuqiang Ren, Haolei Yuan, Xiaowei Guo, Feiyue Huang, Wenping Wang
- **Comment**: mm21
- **Journal**: None
- **Summary**: We study the problem of weakly supervised grounded image captioning. That is, given an image, the goal is to automatically generate a sentence describing the context of the image with each noun word grounded to the corresponding region in the image. This task is challenging due to the lack of explicit fine-grained region word alignments as supervision. Previous weakly supervised methods mainly explore various kinds of regularization schemes to improve attention accuracy. However, their performances are still far from the fully supervised ones. One main issue that has been ignored is that the attention for generating visually groundable words may only focus on the most discriminate parts and can not cover the whole object. To this end, we propose a simple yet effective method to alleviate the issue, termed as partial grounding problem in our paper. Specifically, we design a distributed attention mechanism to enforce the network to aggregate information from multiple spatially different regions with consistent semantics while generating the words. Therefore, the union of the focused region proposals should form a visual region that encloses the object of interest completely. Extensive experiments have demonstrated the superiority of our proposed method compared with the state-of-the-arts.



### Forward-Looking Sonar Patch Matching: Modern CNNs, Ensembling, and Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2108.01066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.01066v1)
- **Published**: 2021-08-02 17:49:56+00:00
- **Updated**: 2021-08-02 17:49:56+00:00
- **Authors**: Arka Mallick, Paul Plger, Matias Valdenegro-Toro
- **Comment**: Global Oceans 2021 Camera ready, 7 pages, 8 figures
- **Journal**: None
- **Summary**: Application of underwater robots are on the rise, most of them are dependent on sonar for underwater vision, but the lack of strong perception capabilities limits them in this task. An important issue in sonar perception is matching image patches, which can enable other techniques like localization, change detection, and mapping. There is a rich literature for this problem in color images, but for acoustic images, it is lacking, due to the physics that produce these images. In this paper we improve on our previous results for this problem (Valdenegro-Toro et al, 2017), instead of modeling features manually, a Convolutional Neural Network (CNN) learns a similarity function and predicts if two input sonar images are similar or not. With the objective of improving the sonar image matching problem further, three state of the art CNN architectures are evaluated on the Marine Debris dataset, namely DenseNet, and VGG, with a siamese or two-channel architecture, and contrastive loss. To ensure a fair evaluation of each network, thorough hyper-parameter optimization is executed. We find that the best performing models are DenseNet Two-Channel network with 0.955 AUC, VGG-Siamese with contrastive loss at 0.949 AUC and DenseNet Siamese with 0.921 AUC. By ensembling the top performing DenseNet two-channel and DenseNet-Siamese models overall highest prediction accuracy obtained is 0.978 AUC, showing a large improvement over the 0.91 AUC in the state of the art.



### Self-Supervised Disentangled Representation Learning for Third-Person Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.01069v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.01069v1)
- **Published**: 2021-08-02 17:55:03+00:00
- **Updated**: 2021-08-02 17:55:03+00:00
- **Authors**: Jinghuan Shang, Michael S. Ryoo
- **Comment**: Preprint. 8 pages. Accepted at IROS 2021
- **Journal**: None
- **Summary**: Humans learn to imitate by observing others. However, robot imitation learning generally requires expert demonstrations in the first-person view (FPV). Collecting such FPV videos for every robot could be very expensive. Third-person imitation learning (TPIL) is the concept of learning action policies by observing other agents in a third-person view (TPV), similar to what humans do. This ultimately allows utilizing human and robot demonstration videos in TPV from many different data sources, for the policy learning. In this paper, we present a TPIL approach for robot tasks with egomotion. Although many robot tasks with ground/aerial mobility often involve actions with camera egomotion, study on TPIL for such tasks has been limited. Here, FPV and TPV observations are visually very different; FPV shows egomotion while the agent appearance is only observable in TPV. To enable better state learning for TPIL, we propose our disentangled representation learning method. We use a dual auto-encoder structure plus representation permutation loss and time-contrastive loss to ensure the state and viewpoint representations are well disentangled. Our experiments show the effectiveness of our approach.



### Finding Discriminative Filters for Specific Degradations in Blind Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2108.01070v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.01070v1)
- **Published**: 2021-08-02 17:55:27+00:00
- **Updated**: 2021-08-02 17:55:27+00:00
- **Authors**: Liangbin Xie, Xintao Wang, Chao Dong, Zhongang Qi, Ying Shan
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Recent blind super-resolution (SR) methods typically consist of two branches, one for degradation prediction and the other for conditional restoration. However, our experiments show that a one-branch network can achieve comparable performance to the two-branch scheme. Then we wonder: how can one-branch networks automatically learn to distinguish degradations? To find the answer, we propose a new diagnostic tool -- Filter Attribution method based on Integral Gradient (FAIG). Unlike previous integral gradient methods, our FAIG aims at finding the most discriminative filters instead of input pixels/features for degradation removal in blind SR networks. With the discovered filters, we further develop a simple yet effective method to predict the degradation of an input image. Based on FAIG, we show that, in one-branch blind SR networks, 1) we are able to find a very small number of (1%) discriminative filters for each specific degradation; 2) The weights, locations and connections of the discovered filters are all important to determine the specific network function. 3) The task of degradation prediction can be implicitly realized by these discriminative filters without explicit supervised learning. Our findings can not only help us better understand network behaviors inside one-branch blind SR networks, but also provide guidance on designing more efficient architectures and diagnosing networks for blind SR.



### S$^2$-MLPv2: Improved Spatial-Shift MLP Architecture for Vision
- **Arxiv ID**: http://arxiv.org/abs/2108.01072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01072v1)
- **Published**: 2021-08-02 17:59:02+00:00
- **Updated**: 2021-08-02 17:59:02+00:00
- **Authors**: Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, Ping Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, MLP-based vision backbones emerge. MLP-based vision architectures with less inductive bias achieve competitive performance in image recognition compared with CNNs and vision Transformers. Among them, spatial-shift MLP (S$^2$-MLP), adopting the straightforward spatial-shift operation, achieves better performance than the pioneering works including MLP-mixer and ResMLP. More recently, using smaller patches with a pyramid structure, Vision Permutator (ViP) and Global Filter Network (GFNet) achieve better performance than S$^2$-MLP.   In this paper, we improve the S$^2$-MLP vision backbone. We expand the feature map along the channel dimension and split the expanded feature map into several parts. We conduct different spatial-shift operations on split parts.   Meanwhile, we exploit the split-attention operation to fuse these split parts. Moreover, like the counterparts, we adopt smaller-scale patches and use a pyramid structure for boosting the image recognition accuracy. We term the improved spatial-shift MLP vision backbone as S$^2$-MLPv2. Using 55M parameters, our medium-scale model, S$^2$-MLPv2-Medium achieves an $83.6\%$ top-1 accuracy on the ImageNet-1K benchmark using $224\times 224$ images without self-attention and external training data.



### SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations
- **Arxiv ID**: http://arxiv.org/abs/2108.01073v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.01073v2)
- **Published**: 2021-08-02 17:59:47+00:00
- **Updated**: 2022-01-05 00:07:35+00:00
- **Authors**: Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon
- **Comment**: https://sde-image-editing.github.io/
- **Journal**: None
- **Summary**: Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user input (e.g., hand-drawn colored strokes) and realism of the synthesized image. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide of any type, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit significantly outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing.



### Pre-trained Models for Sonar Images
- **Arxiv ID**: http://arxiv.org/abs/2108.01111v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.01111v1)
- **Published**: 2021-08-02 18:17:40+00:00
- **Updated**: 2021-08-02 18:17:40+00:00
- **Authors**: Matias Valdenegro-Toro, Alan Preciado-Grijalva, Bilal Wehbe
- **Comment**: Global Oceans 2021, Camera ready, 8 pages, 9 figures
- **Journal**: None
- **Summary**: Machine learning and neural networks are now ubiquitous in sonar perception, but it lags behind the computer vision field due to the lack of data and pre-trained models specifically for sonar images. In this paper we present the Marine Debris Turntable dataset and produce pre-trained neural networks trained on this dataset, meant to fill the gap of missing pre-trained models for sonar images. We train Resnet 20, MobileNets, DenseNet121, SqueezeNet, MiniXception, and an Autoencoder, over several input image sizes, from 32 x 32 to 96 x 96, on the Marine Debris turntable dataset. We evaluate these models using transfer learning for low-shot classification in the Marine Debris Watertank and another dataset captured using a Gemini 720i sonar. Our results show that in both datasets the pre-trained models produce good features that allow good classification accuracy with low samples (10-30 samples per class). The Gemini dataset validates that the features transfer to other kinds of sonar sensors. We expect that the community benefits from the public release of our pre-trained models and the turntable dataset.



### Consistent Depth of Moving Objects in Video
- **Arxiv ID**: http://arxiv.org/abs/2108.01166v1
- **DOI**: 10.1145/3450626.3459871
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2108.01166v1)
- **Published**: 2021-08-02 20:53:18+00:00
- **Updated**: 2021-08-02 20:53:18+00:00
- **Authors**: Zhoutong Zhang, Forrester Cole, Richard Tucker, William T. Freeman, Tali Dekel
- **Comment**: Published at SIGGRAPH 2021
- **Journal**: ACM Trans. Graph., Vol. 40, No. 4, Article 148, August 2021
- **Summary**: We present a method to estimate depth of a dynamic scene, containing arbitrary moving objects, from an ordinary video captured with a moving camera. We seek a geometrically and temporally consistent solution to this underconstrained problem: the depth predictions of corresponding points across frames should induce plausible, smooth motion in 3D. We formulate this objective in a new test-time training framework where a depth-prediction CNN is trained in tandem with an auxiliary scene-flow prediction MLP over the entire input video. By recursively unrolling the scene-flow prediction MLP over varying time steps, we compute both short-range scene flow to impose local smooth motion priors directly in 3D, and long-range scene flow to impose multi-view consistency constraints with wide baselines. We demonstrate accurate and temporally coherent results on a variety of challenging videos containing diverse moving objects (pets, people, cars), as well as camera motion. Our depth maps give rise to a number of depth-and-motion aware video editing effects such as object and lighting insertion.



### A computational geometry approach for modeling neuronal fiber pathways
- **Arxiv ID**: http://arxiv.org/abs/2108.01175v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01175v1)
- **Published**: 2021-08-02 21:16:29+00:00
- **Updated**: 2021-08-02 21:16:29+00:00
- **Authors**: S. Shailja, Angela Zhang, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel and efficient algorithm to model high-level topological structures of neuronal fibers. Tractography constructs complex neuronal fibers in three dimensions that exhibit the geometry of white matter pathways in the brain. However, most tractography analysis methods are time consuming and intractable. We develop a computational geometry-based tractography representation that aims to simplify the connectivity of white matter fibers. Given the trajectories of neuronal fiber pathways, we model the evolution of trajectories that encodes geometrically significant events and calculate their point correspondence in the 3D brain space. Trajectory inter-distance is used as a parameter to control the granularity of the model that allows local or global representation of the tractogram. Using diffusion MRI data from Alzheimer's patient study, we extract tractography features from our model for distinguishing the Alzheimer's subject from the normal control. Software implementation of our algorithm is available on GitHub.



### Neural Image Representations for Multi-Image Fusion and Layer Separation
- **Arxiv ID**: http://arxiv.org/abs/2108.01199v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01199v4)
- **Published**: 2021-08-02 22:29:35+00:00
- **Updated**: 2022-07-21 04:00:38+00:00
- **Authors**: Seonghyeon Nam, Marcus A. Brubaker, Michael S. Brown
- **Comment**: Project page: https://shnnam.github.io/research/nir
- **Journal**: None
- **Summary**: We propose a framework for aligning and fusing multiple images into a single view using neural image representations (NIRs), also known as implicit or coordinate-based neural representations. Our framework targets burst images that exhibit camera ego motion and potential changes in the scene. We describe different strategies for alignment depending on the nature of the scene motion -- namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. With the neural image representation, our framework effectively combines multiple inputs into a single canonical view without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks. The code and results are available at https://shnnam.github.io/research/nir.



### Multispectral Vineyard Segmentation: A Deep Learning approach
- **Arxiv ID**: http://arxiv.org/abs/2108.01200v3
- **DOI**: 10.1016/j.compag.2022.106782
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.01200v3)
- **Published**: 2021-08-02 22:36:07+00:00
- **Updated**: 2022-03-01 17:59:16+00:00
- **Authors**: T. Barros, P. Conde, G. Gonalves, C. Premebida, M. Monteiro, C. S. S. Ferreira, U. J. Nunes
- **Comment**: Accepted in Computer and Electronics in Agriculture journal
- **Journal**: None
- **Summary**: Digital agriculture has evolved significantly over the last few years due to the technological developments in automation and computational intelligence applied to the agricultural sector, including vineyards which are a relevant crop in the Mediterranean region. In this work, a study is presented of semantic segmentation for vine detection in real-world vineyards by exploring state-of-the-art deep segmentation networks and conventional unsupervised methods. Camera data have been collected on vineyards using an Unmanned Aerial System (UAS) equipped with a dual imaging sensor payload, namely a high-definition RGB camera and a five-band multispectral and thermal camera. Extensive experiments using deep-segmentation networks and unsupervised methods have been performed on multimodal datasets representing four distinct vineyards located in the central region of Portugal. The reported results indicate that SegNet, U-Net, and ModSegNet have equivalent overall performance in vine segmentation. The results also show that multimodality slightly improves the performance of vine segmentation, but the NIR spectrum alone generally is sufficient on most of the datasets. Furthermore, results suggest that high-definition RGB images produce equivalent or higher performance than any lower resolution multispectral band combination. Lastly, Deep Learning (DL) networks have higher overall performance than classical methods. The code and dataset are publicly available at https://github.com/Cybonic/DL_vineyard_segmentation_study.git




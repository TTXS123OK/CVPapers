# Arxiv Papers in cs.CV on 2021-08-05
### Pattern Recognition in Vital Signs Using Spectrograms
- **Arxiv ID**: http://arxiv.org/abs/2108.03168v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2108.03168v2)
- **Published**: 2021-08-05 01:37:45+00:00
- **Updated**: 2021-09-02 20:22:17+00:00
- **Authors**: Sidharth Srivatsav Sribhashyam, Md Sirajus Salekin, Dmitry Goldgof, Ghada Zamzmi, Mark Last, Yu Sun
- **Comment**: Accepted in the IEEE International Conference on Systems, Man, and
  Cybernetics (SMC 2021)
- **Journal**: None
- **Summary**: Spectrograms visualize the frequency components of a given signal which may be an audio signal or even a time-series signal. Audio signals have higher sampling rate and high variability of frequency with time. Spectrograms can capture such variations well. But, vital signs which are time-series signals have less sampling frequency and low-frequency variability due to which, spectrograms fail to express variations and patterns. In this paper, we propose a novel solution to introduce frequency variability using frequency modulation on vital signs. Then we apply spectrograms on frequency modulated signals to capture the patterns. The proposed approach has been evaluated on 4 different medical datasets across both prediction and classification tasks. Significant results are found showing the efficacy of the approach for vital sign signals. The results from the proposed approach are promising with an accuracy of 91.55% and 91.67% in prediction and classification tasks respectively.



### Data Acquisition and Preparation for Dual-reference Deep Learning of Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2108.02348v5
- **DOI**: 10.1109/TIP.2022.3184819
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02348v5)
- **Published**: 2021-08-05 03:31:50+00:00
- **Updated**: 2022-06-19 15:19:47+00:00
- **Authors**: Yanhui Guo, Xiaolin Wu, Xiao Shu
- **Comment**: Accepted by IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: The performance of deep learning based image super-resolution (SR) methods depend on how accurately the paired low and high resolution images for training characterize the sampling process of real cameras. Low and high resolution (LR$\sim$HR) image pairs synthesized by degradation models (e.g., bicubic downsampling) deviate from those in reality; thus the synthetically-trained DCNN SR models work disappointingly when being applied to real-world images. To address this issue, we propose a novel data acquisition process to shoot a large set of LR$\sim$HR image pairs using real cameras. The images are displayed on an ultra-high quality screen and captured at different resolutions. The resulting LR$\sim$HR image pairs can be aligned at very high sub-pixel precision by a novel spatial-frequency dual-domain registration method, and hence they provide more appropriate training data for the learning task of super-resolution. Moreover, the captured HR image and the original digital image offer dual references to strengthen supervised learning. Experimental results show that training a super-resolution DCNN by our LR$\sim$HR dataset achieves higher image quality than training it by other datasets in the literature. Moreover, the proposed screen-capturing data collection process can be automated; it can be carried out for any target camera with ease and low cost, offering a practical way of tailoring the training of a DCNN SR model separately to each of the given cameras.



### Hierarchical Aggregation for 3D Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.02350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02350v1)
- **Published**: 2021-08-05 03:34:34+00:00
- **Updated**: 2021-08-05 03:34:34+00:00
- **Authors**: Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, Xinggang Wang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Instance segmentation on point clouds is a fundamental task in 3D scene perception. In this work, we propose a concise clustering-based framework named HAIS, which makes full use of spatial relation of points and point sets. Considering clustering-based methods may result in over-segmentation or under-segmentation, we introduce the hierarchical aggregation to progressively generate instance proposals, i.e., point aggregation for preliminarily clustering points to sets and set aggregation for generating complete instances from sets. Once the complete 3D instances are obtained, a sub-network of intra-instance prediction is adopted for noisy points filtering and mask quality scoring. HAIS is fast (only 410ms per frame) and does not require non-maximum suppression. It ranks 1st on the ScanNet v2 benchmark, achieving the highest 69.9% AP50 and surpassing previous state-of-the-art (SOTA) methods by a large margin. Besides, the SOTA results on the S3DIS dataset validate the good generalization ability. Code will be available at https://github.com/hustvl/HAIS.



### Alleviating Mode Collapse in GAN via Diversity Penalty Module
- **Arxiv ID**: http://arxiv.org/abs/2108.02353v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02353v4)
- **Published**: 2021-08-05 03:41:14+00:00
- **Updated**: 2021-09-13 11:46:55+00:00
- **Authors**: Sen Pei, Richard Yi Da Xu, Shiming Xiang, Gaofeng Meng
- **Comment**: None
- **Journal**: None
- **Summary**: The vanilla GAN (Goodfellow et al. 2014) suffers from mode collapse deeply, which usually manifests as that the images generated by generators tend to have a high similarity amongst them, even though their corresponding latent vectors have been very different. In this paper, we introduce a pluggable diversity penalty module (DPM) to alleviate mode collapse of GANs. It reduces the similarity of image pairs in feature space, i.e., if two latent vectors are different, then we enforce the generator to generate two images with different features. The normalized Gram matrix is used to measure the similarity. We compare the proposed method with Unrolled GAN (Metz et al. 2016), BourGAN (Xiao, Zhong, and Zheng 2018), PacGAN (Lin et al. 2018), VEEGAN (Srivastava et al. 2017) and ALI (Dumoulin et al. 2016) on 2D synthetic dataset, and results show that the diversity penalty module can help GAN capture much more modes of the data distribution. Further, in classification tasks, we apply this method as image data augmentation on MNIST, Fashion- MNIST and CIFAR-10, and the classification testing accuracy is improved by 0.24%, 1.34% and 0.52% compared with WGAN GP (Gulrajani et al. 2017), respectively. In domain translation, diversity penalty module can help StarGAN (Choi et al. 2018) generate more accurate attention masks and accelarate the convergence process. Finally, we quantitatively evaluate the proposed method with IS and FID on CelebA, CIFAR-10, MNIST and Fashion-MNIST, and the results suggest GAN with diversity penalty module gets much higher IS and lower FID compared with some SOTA GAN architectures.



### Video Abnormal Event Detection by Learning to Complete Visual Cloze Tests
- **Arxiv ID**: http://arxiv.org/abs/2108.02356v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.02356v2)
- **Published**: 2021-08-05 04:05:36+00:00
- **Updated**: 2021-09-17 02:27:02+00:00
- **Authors**: Siqi Wang, Guang Yu, Zhiping Cai, Xinwang Liu, En Zhu, Jianping Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep neural networks (DNNs) enable great progress in video abnormal event detection (VAD), existing solutions typically suffer from two issues: (1) The localization of video events cannot be both precious and comprehensive. (2) The semantics and temporal context are under-explored. To tackle those issues, we are motivated by the prevalent cloze test in education and propose a novel approach named Visual Cloze Completion (VCC), which conducts VAD by learning to complete "visual cloze tests" (VCTs). Specifically, VCC first localizes each video event and encloses it into a spatio-temporal cube (STC). To achieve both precise and comprehensive localization, appearance and motion are used as complementary cues to mark the object region associated with each event. For each marked region, a normalized patch sequence is extracted from current and adjacent frames and stacked into a STC. With each patch and the patch sequence of a STC compared to a visual "word" and "sentence" respectively, we deliberately erase a certain "word" (patch) to yield a VCT. Then, the VCT is completed by training DNNs to infer the erased patch and its optical flow via video semantics. Meanwhile, VCC fully exploits temporal context by alternatively erasing each patch in temporal context and creating multiple VCTs. Furthermore, we propose localization-level, event-level, model-level and decision-level solutions to enhance VCC, which can further exploit VCC's potential and produce significant performance improvement gain. Extensive experiments demonstrate that VCC achieves state-of-the-art VAD performance. Our codes and results are open at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.



### O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2108.02359v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02359v2)
- **Published**: 2021-08-05 04:17:20+00:00
- **Updated**: 2022-01-09 08:49:48+00:00
- **Authors**: Fenglin Liu, Xuancheng Ren, Xian Wu, Bang Yang, Shen Ge, Yuexian Zou, Xu Sun
- **Comment**: Accepted by Findings of ACL 2021 (The Joint Conference of the 59th
  Annual Meeting of the Association for Computational Linguistics and the 11th
  International Joint Conference on Natural Language Processing)
- **Journal**: None
- **Summary**: Video captioning combines video understanding and language generation. Different from image captioning that describes a static image with details of almost every object, video captioning usually considers a sequence of frames and biases towards focused objects, e.g., the objects that stay in focus regardless of the changing background. Therefore, detecting and properly accommodating focused objects is critical in video captioning. To enforce the description of focused objects and achieve controllable video captioning, we propose an Object-Oriented Non-Autoregressive approach (O2NA), which performs caption generation in three steps: 1) identify the focused objects and predict their locations in the target caption; 2) generate the related attribute words and relation words of these focused objects to form a draft caption; and 3) combine video information to refine the draft caption to a fluent final caption. Since the focused objects are generated and located ahead of other words, it is difficult to apply the word-by-word autoregressive generation process; instead, we adopt a non-autoregressive approach. The experiments on two benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness of O2NA, which achieves results competitive with the state-of-the-arts but with both higher diversity and higher inference speed.



### Exploring Structure Consistency for Deep Model Watermarking
- **Arxiv ID**: http://arxiv.org/abs/2108.02360v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02360v1)
- **Published**: 2021-08-05 04:27:15+00:00
- **Updated**: 2021-08-05 04:27:15+00:00
- **Authors**: Jie Zhang, Dongdong Chen, Jing Liao, Han Fang, Zehua Ma, Weiming Zhang, Gang Hua, Nenghai Yu
- **Comment**: None
- **Journal**: None
- **Summary**: The intellectual property (IP) of Deep neural networks (DNNs) can be easily ``stolen'' by surrogate model attack. There has been significant progress in solutions to protect the IP of DNN models in classification tasks. However, little attention has been devoted to the protection of DNNs in image processing tasks. By utilizing consistent invisible spatial watermarks, one recent work first considered model watermarking for deep image processing networks and demonstrated its efficacy in many downstream tasks. Nevertheless, it highly depends on the hypothesis that the embedded watermarks in the network outputs are consistent. When the attacker uses some common data augmentation attacks (e.g., rotate, crop, and resize) during surrogate model training, it will totally fail because the underlying watermark consistency is destroyed. To mitigate this issue, we propose a new watermarking methodology, namely ``structure consistency'', based on which a new deep structure-aligned model watermarking algorithm is designed. Specifically, the embedded watermarks are designed to be aligned with physically consistent image structures, such as edges or semantic regions. Experiments demonstrate that our method is much more robust than the baseline method in resisting data augmentation attacks for model IP protection. Besides that, we further test the generalization ability and robustness of our method to a broader range of circumvention attacks.



### Hybrid Reasoning Network for Video-based Commonsense Captioning
- **Arxiv ID**: http://arxiv.org/abs/2108.02365v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2108.02365v1)
- **Published**: 2021-08-05 04:55:51+00:00
- **Updated**: 2021-08-05 04:55:51+00:00
- **Authors**: Weijiang Yu, Jian Liang, Lei Ji, Lu Li, Yuejian Fang, Nong Xiao, Nan Duan
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: The task of video-based commonsense captioning aims to generate event-wise captions and meanwhile provide multiple commonsense descriptions (e.g., attribute, effect and intention) about the underlying event in the video. Prior works explore the commonsense captions by using separate networks for different commonsense types, which is time-consuming and lacks mining the interaction of different commonsense. In this paper, we propose a Hybrid Reasoning Network (HybridNet) to endow the neural networks with the capability of semantic-level reasoning and word-level reasoning. Firstly, we develop multi-commonsense learning for semantic-level reasoning by jointly training different commonsense types in a unified network, which encourages the interaction between the clues of multiple commonsense descriptions, event-wise captions and videos. Then, there are two steps to achieve the word-level reasoning: (1) a memory module records the history predicted sequence from the previous generation processes; (2) a memory-routed multi-head attention (MMHA) module updates the word-level attention maps by incorporating the history information from the memory module into the transformer decoder for word-level reasoning. Moreover, the multimodal features are used to make full use of diverse knowledge for commonsense reasoning. Experiments and abundant analysis on the large-scale Video-to-Commonsense benchmark show that our HybridNet achieves state-of-the-art performance compared with other methods.



### Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2108.02366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02366v1)
- **Published**: 2021-08-05 04:57:06+00:00
- **Updated**: 2021-08-05 04:57:06+00:00
- **Authors**: Xinzhi Dong, Chengjiang Long, Wenju Xu, Chunxia Xiao
- **Comment**: This paper was accepted to the 29th ACM International Conference on
  Multimedia (ACM MM), Chengdu, Sichuan, China, Oct 20-24, 2021
- **Journal**: None
- **Summary**: Existing image captioning methods just focus on understanding the relationship between objects or instances in a single image, without exploring the contextual correlation existed among contextual image. In this paper, we propose Dual Graph Convolutional Networks (Dual-GCN) with transformer and curriculum learning for image captioning. In particular, we not only use an object-level GCN to capture the object to object spatial relation within a single image, but also adopt an image-level GCN to capture the feature information provided by similar images. With the well-designed Dual-GCN, we can make the linguistic transformer better understand the relationship between different objects in a single image and make full use of similar images as auxiliary information to generate a reasonable caption description for a single image. Meanwhile, with a cross-review strategy introduced to determine difficulty levels, we adopt curriculum learning as the training strategy to increase the robustness and generalization of our proposed model. We conduct extensive experiments on the large-scale MS COCO dataset, and the experimental results powerfully demonstrate that our proposed method outperforms recent state-of-the-art approaches. It achieves a BLEU-1 score of 82.2 and a BLEU-2 score of 67.6. Our source code is available at {\em \color{magenta}{\url{https://github.com/Unbear430/DGCN-for-image-captioning}}}.



### M2IOSR: Maximal Mutual Information Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.02373v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02373v2)
- **Published**: 2021-08-05 05:08:12+00:00
- **Updated**: 2021-08-06 00:37:12+00:00
- **Authors**: Xin Sun, Henghui Ding, Chi Zhang, Guosheng Lin, Keck-Voon Ling
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we aim to address the challenging task of open set recognition (OSR). Many recent OSR methods rely on auto-encoders to extract class-specific features by a reconstruction strategy, requiring the network to restore the input image on pixel-level. This strategy is commonly over-demanding for OSR since class-specific features are generally contained in target objects, not in all pixels. To address this shortcoming, here we discard the pixel-level reconstruction strategy and pay more attention to improving the effectiveness of class-specific feature extraction. We propose a mutual information-based method with a streamlined architecture, Maximal Mutual Information Open Set Recognition (M2IOSR). The proposed M2IOSR only uses an encoder to extract class-specific features by maximizing the mutual information between the given input and its latent features across multiple scales. Meanwhile, to further reduce the open space risk, latent features are constrained to class conditional Gaussian distributions by a KL-divergence loss function. In this way, a strong function is learned to prevent the network from mapping different observations to similar latent features and help the network extract class-specific features with desired statistical characteristics. The proposed method significantly improves the performance of baselines and achieves new state-of-the-art results on several benchmarks consistently.



### Global and Local Texture Randomization for Synthetic-to-Real Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.02376v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02376v2)
- **Published**: 2021-08-05 05:14:49+00:00
- **Updated**: 2021-08-06 03:43:21+00:00
- **Authors**: Duo Peng, Yinjie Lei, Lingqiao Liu, Pingping Zhang, Jun Liu
- **Comment**: 15 pages, 14 figures, accepted by IEEE Transactions on Image
  Processing (TIP 2021)
- **Journal**: None
- **Summary**: Semantic segmentation is a crucial image understanding task, where each pixel of image is categorized into a corresponding label. Since the pixel-wise labeling for ground-truth is tedious and labor intensive, in practical applications, many works exploit the synthetic images to train the model for real-word image semantic segmentation, i.e., Synthetic-to-Real Semantic Segmentation (SRSS). However, Deep Convolutional Neural Networks (CNNs) trained on the source synthetic data may not generalize well to the target real-world data. In this work, we propose two simple yet effective texture randomization mechanisms, Global Texture Randomization (GTR) and Local Texture Randomization (LTR), for Domain Generalization based SRSS. GTR is proposed to randomize the texture of source images into diverse unreal texture styles. It aims to alleviate the reliance of the network on texture while promoting the learning of the domain-invariant cues. In addition, we find the texture difference is not always occurred in entire image and may only appear in some local areas. Therefore, we further propose a LTR mechanism to generate diverse local regions for partially stylizing the source images. Finally, we implement a regularization of Consistency between GTR and LTR (CGL) aiming to harmonize the two proposed mechanisms during training. Extensive experiments on five publicly available datasets (i.e., GTA5, SYNTHIA, Cityscapes, BDDS and Mapillary) with various SRSS settings (i.e., GTA5/SYNTHIA to Cityscapes/BDDS/Mapillary) demonstrate that the proposed method is superior to the state-of-the-art methods for domain generalization based SRSS.



### ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot
- **Arxiv ID**: http://arxiv.org/abs/2108.02385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02385v1)
- **Published**: 2021-08-05 05:31:57+00:00
- **Updated**: 2021-08-05 05:31:57+00:00
- **Authors**: Jiarui Cai, Yizhou Wang, Jenq-Neng Hwang
- **Comment**: ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: One-stage long-tailed recognition methods improve the overall performance in a "seesaw" manner, i.e., either sacrifice the head's accuracy for better tail classification or elevate the head's accuracy even higher but ignore the tail. Existing algorithms bypass such trade-off by a multi-stage training process: pre-training on imbalanced set and fine-tuning on balanced set. Though achieving promising performance, not only are they sensitive to the generalizability of the pre-trained model, but also not easily integrated into other computer vision tasks like detection and segmentation, where pre-training of classifiers solely is not applicable. In this paper, we propose a one-stage long-tailed recognition scheme, ally complementary experts (ACE), where the expert is the most knowledgeable specialist in a sub-set that dominates its training, and is complementary to other experts in the less-seen categories without being disturbed by what it has never seen. We design a distribution-adaptive optimizer to adjust the learning pace of each expert to avoid over-fitting. Without special bells and whistles, the vanilla ACE outperforms the current one-stage SOTA method by 3-10% on CIFAR10-LT, CIFAR100-LT, ImageNet-LT and iNaturalist datasets. It is also shown to be the first one to break the "seesaw" trade-off by improving the accuracy of the majority and minority categories simultaneously in only one stage. Code and trained models are at https://github.com/jrcai/ACE.



### TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/2108.02388v2
- **DOI**: 10.1145/3474085.3475397
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.02388v2)
- **Published**: 2021-08-05 05:47:12+00:00
- **Updated**: 2021-08-11 09:25:23+00:00
- **Authors**: Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, Si Liu
- **Comment**: ACM MM2021
- **Journal**: None
- **Summary**: Recently proposed fine-grained 3D visual grounding is an essential and challenging task, whose goal is to identify the 3D object referred by a natural language sentence from other distractive objects of the same category. Existing works usually adopt dynamic graph networks to indirectly model the intra/inter-modal interactions, making the model difficult to distinguish the referred object from distractors due to the monolithic representations of visual and linguistic contents. In this work, we exploit Transformer for its natural suitability on permutation-invariant 3D point clouds data and propose a TransRefer3D network to extract entity-and-relation aware multimodal context among objects for more discriminative feature learning. Concretely, we devise an Entity-aware Attention (EA) module and a Relation-aware Attention (RA) module to conduct fine-grained cross-modal feature matching. Facilitated by co-attention operation, our EA module matches visual entity features with linguistic entity features while RA module matches pair-wise visual relation features with linguistic relation features, respectively. We further integrate EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and stack several ERCBs to form our TransRefer3D for hierarchical multimodal context modeling. Extensive experiments on both Nr3D and Sr3D datasets demonstrate that our proposed model significantly outperforms existing approaches by up to 10.6% and claims the new state-of-the-art. To the best of our knowledge, this is the first work investigating Transformer architecture for fine-grained 3D visual grounding task.



### Webly Supervised Fine-Grained Recognition: Benchmark Datasets and An Approach
- **Arxiv ID**: http://arxiv.org/abs/2108.02399v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02399v2)
- **Published**: 2021-08-05 06:28:32+00:00
- **Updated**: 2021-08-11 07:43:13+00:00
- **Authors**: Zeren Sun, Yazhou Yao, Xiu-Shen Wei, Yongshun Zhang, Fumin Shen, Jianxin Wu, Jian Zhang, Heng-Tao Shen
- **Comment**: accepted by ICCV 2021
- **Journal**: None
- **Summary**: Learning from the web can ease the extreme dependence of deep learning on large-scale manually labeled datasets. Especially for fine-grained recognition, which targets at distinguishing subordinate categories, it will significantly reduce the labeling costs by leveraging free web data. Despite its significant practical and research value, the webly supervised fine-grained recognition problem is not extensively studied in the computer vision community, largely due to the lack of high-quality datasets. To fill this gap, in this paper we construct two new benchmark webly supervised fine-grained datasets, termed WebFG-496 and WebiNat-5089, respectively. In concretely, WebFG-496 consists of three sub-datasets containing a total of 53,339 web training images with 200 species of birds (Web-bird), 100 types of aircrafts (Web-aircraft), and 196 models of cars (Web-car). For WebiNat-5089, it contains 5089 sub-categories and more than 1.1 million web training images, which is the largest webly supervised fine-grained dataset ever. As a minor contribution, we also propose a novel webly supervised method (termed "{Peer-learning}") for benchmarking these datasets.~Comprehensive experimental results and analyses on two new benchmark datasets demonstrate that the proposed method achieves superior performance over the competing baseline models and states-of-the-art. Our benchmark datasets and the source codes of Peer-learning have been made available at {\url{https://github.com/NUST-Machine-Intelligence-Laboratory/weblyFG-dataset}}.



### Security and Privacy Enhanced Gait Authentication with Random Representation Learning and Digital Lockers
- **Arxiv ID**: http://arxiv.org/abs/2108.02400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02400v1)
- **Published**: 2021-08-05 06:34:42+00:00
- **Updated**: 2021-08-05 06:34:42+00:00
- **Authors**: Lam Tran, Thuc Nguyen, Hyunil Kim, Deokjai Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Gait data captured by inertial sensors have demonstrated promising results on user authentication. However, most existing approaches stored the enrolled gait pattern insecurely for matching with the validating pattern, thus, posed critical security and privacy issues. In this study, we present a gait cryptosystem that generates from gait data the random key for user authentication, meanwhile, secures the gait pattern. First, we propose a revocable and random binary string extraction method using a deep neural network followed by feature-wise binarization. A novel loss function for network optimization is also designed, to tackle not only the intrauser stability but also the inter-user randomness. Second, we propose a new biometric key generation scheme, namely Irreversible Error Correct and Obfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme, to securely generate from the binary string the random and irreversible key. The model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. We showed that our model could generate the key of 139 bits from 5-second data sequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR) smaller than 5.441%. In addition, the security and user privacy analyses showed that our model was secure against existing attacks on biometric template protection, and fulfilled irreversibility and unlinkability.



### Fast Convergence of DETR with Spatially Modulated Co-Attention
- **Arxiv ID**: http://arxiv.org/abs/2108.02404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02404v1)
- **Published**: 2021-08-05 06:53:19+00:00
- **Updated**: 2021-08-05 06:53:19+00:00
- **Authors**: Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai, Hongsheng Li
- **Comment**: Accepted by ICCV2021. arXiv admin note: substantial text overlap with
  arXiv:2101.07448
- **Journal**: None
- **Summary**: The recently proposed Detection Transformer (DETR) model successfully applies Transformer to objects detection and achieves comparable performance with two-stage object detection frameworks, such as Faster-RCNN. However, DETR suffers from its slow convergence. Training DETR from scratch needs 500 epochs to achieve a high accuracy. To accelerate its convergence, we propose a simple yet effective scheme for improving the DETR framework, namely Spatially Modulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct location-aware co-attention in DETR by constraining co-attention responses to be high near initially estimated bounding box locations. Our proposed SMCA increases DETR's convergence speed by replacing the original co-attention mechanism in the decoder while keeping other operations in DETR unchanged. Furthermore, by integrating multi-head and scale-selection attention designs into SMCA, our fully-fledged SMCA can achieve better performance compared to DETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3 mAP at 500 epochs). We perform extensive ablation studies on COCO dataset to validate SMCA. Code is released at https://github.com/gaopengcuhk/SMCA-DETR .



### IDM: An Intermediate Domain Module for Domain Adaptive Person Re-ID
- **Arxiv ID**: http://arxiv.org/abs/2108.02413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02413v1)
- **Published**: 2021-08-05 07:19:46+00:00
- **Updated**: 2021-08-05 07:19:46+00:00
- **Authors**: Yongxing Dai, Jun Liu, Yifan Sun, Zekun Tong, Chi Zhang, Ling-Yu Duan
- **Comment**: Accepted by ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: Unsupervised domain adaptive person re-identification (UDA re-ID) aims at transferring the labeled source domain's knowledge to improve the model's discriminability on the unlabeled target domain. From a novel perspective, we argue that the bridging between the source and target domains can be utilized to tackle the UDA re-ID task, and we focus on explicitly modeling appropriate intermediate domains to characterize this bridging. Specifically, we propose an Intermediate Domain Module (IDM) to generate intermediate domains' representations on-the-fly by mixing the source and target domains' hidden representations using two domain factors. Based on the "shortest geodesic path" definition, i.e., the intermediate domains along the shortest geodesic path between the two extreme domains can play a better bridging role, we propose two properties that these intermediate domains should satisfy. To ensure these two properties to better characterize appropriate intermediate domains, we enforce the bridge losses on intermediate domains' prediction space and feature space, and enforce a diversity loss on the two domain factors. The bridge losses aim at guiding the distribution of appropriate intermediate domains to keep the right distance to the source and target domains. The diversity loss serves as a regularization to prevent the generated intermediate domains from being over-fitting to either of the source and target domains. Our proposed method outperforms the state-of-the-arts by a large margin in all the common UDA re-ID tasks, and the mAP gain is up to 7.7% on the challenging MSMT17 benchmark. Code is available at https://github.com/SikaStar/IDM.



### Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2108.02417v1
- **DOI**: 10.1145/3474085.3475634
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02417v1)
- **Published**: 2021-08-05 07:24:54+00:00
- **Updated**: 2021-08-05 07:24:54+00:00
- **Authors**: Xuri Ge, Fuhai Chen, Joemon M. Jose, Zhilong Ji, Zhongqin Wu, Xiao Liu
- **Comment**: 9 pages, 7 figures, Accepted by ACM MM 2021
- **Journal**: None
- **Summary**: The current state-of-the-art image-sentence retrieval methods implicitly align the visual-textual fragments, like regions in images and words in sentences, and adopt attention modules to highlight the relevance of cross-modal semantic correspondences. However, the retrieval performance remains unsatisfactory due to a lack of consistent representation in both semantics and structural spaces. In this work, we propose to address the above issue from two aspects: (i) constructing intrinsic structure (along with relations) among the fragments of respective modalities, e.g., "dog $\to$ play $\to$ ball" in semantic structure for an image, and (ii) seeking explicit inter-modal structural and semantic correspondence between the visual and textual modalities. In this paper, we propose a novel Structured Multi-modal Feature Embedding and Alignment (SMFEA) model for image-sentence retrieval. In order to jointly and explicitly learn the visual-textual embedding and the cross-modal alignment, SMFEA creates a novel multi-modal structured module with a shared context-aware referral tree. In particular, the relations of the visual and textual fragments are modeled by constructing Visual Context-aware Structured Tree encoder (VCS-Tree) and Textual Context-aware Structured Tree encoder (TCS-Tree) with shared labels, from which visual and textual features can be jointly learned and optimized. We utilize the multi-modal tree structure to explicitly align the heterogeneous image-sentence data by maximizing the semantic and structural similarity between corresponding inter-modal tree nodes. Extensive experiments on Microsoft COCO and Flickr30K benchmarks demonstrate the superiority of the proposed model in comparison to the state-of-the-art methods.



### Intelligent Railway Foreign Object Detection: A Semi-supervised Convolutional Autoencoder Based Method
- **Arxiv ID**: http://arxiv.org/abs/2108.02421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02421v1)
- **Published**: 2021-08-05 07:32:23+00:00
- **Updated**: 2021-08-05 07:32:23+00:00
- **Authors**: Tiange Wang, Zijun Zhang, Fangfang Yang, Kwok-Leung Tsui
- **Comment**: None
- **Journal**: None
- **Summary**: Automated inspection and detection of foreign objects on railways is important for rail transportation safety as it helps prevent potential accidents and trains derailment. Most existing vision-based approaches focus on the detection of frontal intrusion objects with prior labels, such as categories and locations of the objects. In reality, foreign objects with unknown categories can appear anytime on railway tracks. In this paper, we develop a semi-supervised convolutional autoencoder based framework that only requires railway track images without prior knowledge on the foreign objects in the training process. It consists of three different modules, a bottleneck feature generator as encoder, a photographic image generator as decoder, and a reconstruction discriminator developed via adversarial learning. In the proposed framework, the problem of detecting the presence, location, and shape of foreign objects is addressed by comparing the input and reconstructed images as well as setting thresholds based on reconstruction errors. The proposed method is evaluated through comprehensive studies under different performance criteria. The results show that the proposed method outperforms some well-known benchmarking methods. The proposed framework is useful for data analytics via the train Internet-of-Things (IoT) systems



### Automatic Rail Component Detection Based on AttnConv-Net
- **Arxiv ID**: http://arxiv.org/abs/2108.02423v2
- **DOI**: 10.1109/JSEN.2021.3132460
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02423v2)
- **Published**: 2021-08-05 07:38:04+00:00
- **Updated**: 2022-02-10 07:33:30+00:00
- **Authors**: Tiange Wang, Zijun Zhang, Fangfang Yang, Kwok-Leung Tsui
- **Comment**: None
- **Journal**: None
- **Summary**: The automatic detection of major rail components using railway images is beneficial to ensure the rail transport safety. In this paper, we propose an attention-powered deep convolutional network (AttnConv-net) to detect multiple rail components including the rail, clips, and bolts. The proposed method consists of a deep convolutional neural network (DCNN) as the backbone, cascading attention blocks (CAB), and two feed forward networks (FFN). Two types of positional embedding are applied to enrich information in latent features extracted from the backbone. Based on processed latent features, the CAB aims to learn the local context of rail components including their categories and component boundaries. Final categories and bounding boxes are generated via two FFN implemented in parallel. To enhance the detection of small components, various data augmentation methods are employed in the training process. The effectiveness of the proposed AttnConv-net is validated with one real dataset and another synthesized dataset. Compared with classic convolutional neural network based methods, our proposed method simplifies the detection pipeline by eliminating the need of prior- and post-processing, which offers a new speed-quality solution to enable faster and more accurate image-based rail component detections



### Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.02425v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02425v2)
- **Published**: 2021-08-05 07:46:48+00:00
- **Updated**: 2021-09-26 08:26:38+00:00
- **Authors**: Yiming Li, Tao Kong, Ruihang Chu, Yifeng Li, Peng Wang, Lei Li
- **Comment**: International Conference on Intelligent Robots and Systems (IROS)
  2021
- **Journal**: None
- **Summary**: Grasping in cluttered scenes has always been a great challenge for robots, due to the requirement of the ability to well understand the scene and object information. Previous works usually assume that the geometry information of the objects is available, or utilize a step-wise, multi-stage strategy to predict the feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF grasp pose estimation as a simultaneous multi-task learning problem. In a unified framework, we jointly predict the feasible 6-DoF grasp poses, instance semantic segmentation, and collision information. The whole framework is jointly optimized and end-to-end differentiable. Our model is evaluated on large-scale benchmarks as well as the real robot system. On the public dataset, our method outperforms prior state-of-the-art methods by a large margin (+4.08 AP). We also demonstrate the implementation of our model on a real robotic platform and show that the robot can accurately grasp target objects in cluttered scenarios with a high success rate. Project link: https://openbyterobotics.github.io/sscl



### Token Shift Transformer for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.02432v1
- **DOI**: 10.1145/3474085.3475272
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.02432v1)
- **Published**: 2021-08-05 08:04:54+00:00
- **Updated**: 2021-08-05 08:04:54+00:00
- **Authors**: Hao Zhang, Yanbin Hao, Chong-Wah Ngo
- **Comment**: ACM Multimedia 2021, 9 pages, 5 figures
- **Journal**: None
- **Summary**: Transformer achieves remarkable successes in understanding 1 and 2-dimensional signals (e.g., NLP and Image Content Understanding). As a potential alternative to convolutional neural networks, it shares merits of strong interpretability, high discriminative power on hyper-scale data, and flexibility in processing varying length inputs. However, its encoders naturally contain computational intensive operations such as pair-wise self-attention, incurring heavy computational burden when being applied on the complex 3-dimensional video signals.   This paper presents Token Shift Module (i.e., TokShift), a novel, zero-parameter, zero-FLOPs operator, for modeling temporal relations within each transformer encoder. Specifically, the TokShift barely temporally shifts partial [Class] token features back-and-forth across adjacent frames. Then, we densely plug the module into each encoder of a plain 2D vision transformer for learning 3D video representation. It is worth noticing that our TokShift transformer is a pure convolutional-free video transformer pilot with computational efficiency for video understanding. Experiments on standard benchmarks verify its robustness, effectiveness, and efficiency. Particularly, with input clips of 8/12 frames, the TokShift transformer achieves SOTA precision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80% on UCF-101 datasets, comparable or better than existing SOTA convolutional counterparts. Our code is open-sourced in: https://github.com/VideoNetworks/TokShift-Transformer.



### Learning to Design and Construct Bridge without Blueprint
- **Arxiv ID**: http://arxiv.org/abs/2108.02439v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02439v1)
- **Published**: 2021-08-05 08:17:22+00:00
- **Updated**: 2021-08-05 08:17:22+00:00
- **Authors**: Yunfei Li, Tao Kong, Lei Li, Yifeng Li, Yi Wu
- **Comment**: International Conference on Intelligent Robots and Systems (IROS)
  2021
- **Journal**: None
- **Summary**: Autonomous assembly has been a desired functionality of many intelligent robot systems. We study a new challenging assembly task, designing and constructing a bridge without a blueprint. In this task, the robot needs to first design a feasible bridge architecture for arbitrarily wide cliffs and then manipulate the blocks reliably to construct a stable bridge according to the proposed design. In this paper, we propose a bi-level approach to tackle this task. At the high level, the system learns a bridge blueprint policy in a physical simulator using deep reinforcement learning and curriculum learning. A policy is represented as an attention-based neural network with object-centric input, which enables generalization to different numbers of blocks and cliff widths. For low-level control, we implement a motion-planning-based policy for real-robot motion control, which can be directly combined with a trained blueprint policy for real-world bridge construction without tuning. In our field study, our bi-level robot system demonstrates the capability of manipulating blocks to construct a diverse set of bridges with different architectures.



### MFuseNet: Robust Depth Estimation with Learned Multiscopic Fusion
- **Arxiv ID**: http://arxiv.org/abs/2108.02448v2
- **DOI**: 10.1109/LRA.2020.2974422
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.02448v2)
- **Published**: 2021-08-05 08:31:01+00:00
- **Updated**: 2021-08-06 07:31:12+00:00
- **Authors**: Weihao Yuan, Rui Fan, Michael Yu Wang, Qifeng Chen
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA) +
  IEEE Robotics and Automation Letters (RA-L). arXiv admin note: substantial
  text overlap with arXiv:2001.08212
- **Journal**: None
- **Summary**: We design a multiscopic vision system that utilizes a low-cost monocular RGB camera to acquire accurate depth estimation. Unlike multi-view stereo with images captured at unconstrained camera poses, the proposed system controls the motion of a camera to capture a sequence of images in horizontally or vertically aligned positions with the same parallax. In this system, we propose a new heuristic method and a robust learning-based method to fuse multiple cost volumes between the reference image and its surrounding images. To obtain training data, we build a synthetic dataset with multiscopic images. The experiments on the real-world Middlebury dataset and real robot demonstration show that our multiscopic vision system outperforms traditional two-frame stereo matching methods in depth estimation. Our code and dataset are available at https://sites.google.com/view/multiscopic.



### Unifying Nonlocal Blocks for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.02451v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.02451v3)
- **Published**: 2021-08-05 08:34:12+00:00
- **Updated**: 2021-08-17 07:18:59+00:00
- **Authors**: Lei Zhu, Qi She, Duo Li, Yanye Lu, Xuejing Kang, Jie Hu, Changhu Wang
- **Comment**: Accept by ICCV 2021 Conference
- **Journal**: None
- **Summary**: The nonlocal-based blocks are designed for capturing long-range spatial-temporal dependencies in computer vision tasks. Although having shown excellent performance, they still lack the mechanism to encode the rich, structured information among elements in an image or video. In this paper, to theoretically analyze the property of these nonlocal-based blocks, we provide a new perspective to interpret them, where we view them as a set of graph filters generated on a fully-connected graph. Specifically, when choosing the Chebyshev graph filter, a unified formulation can be derived for explaining and analyzing the existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage, double attention block). Furthermore, by concerning the property of spectral, we propose an efficient and robust spectral nonlocal block, which can be more robust and flexible to catch long-range dependencies when inserted into deep neural networks than the existing nonlocal blocks. Experimental results demonstrate the clear-cut improvements and practical applicabilities of our method on image classification, action recognition, semantic segmentation, and person re-identification tasks.



### VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2108.02452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02452v1)
- **Published**: 2021-08-05 08:35:44+00:00
- **Updated**: 2021-08-05 08:35:44+00:00
- **Authors**: Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenyu Liu, Wenjun Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: We present VoxelTrack for multi-person 3D pose estimation and tracking from a few cameras which are separated by wide baselines. It employs a multi-branch network to jointly estimate 3D poses and re-identification (Re-ID) features for all people in the environment. In contrast to previous efforts which require to establish cross-view correspondence based on noisy 2D pose estimates, it directly estimates and tracks 3D poses from a 3D voxel-based representation constructed from multi-view images. We first discretize the 3D space by regular voxels and compute a feature vector for each voxel by averaging the body joint heatmaps that are inversely projected from all views. We estimate 3D poses from the voxel representation by predicting whether each voxel contains a particular body joint. Similarly, a Re-ID feature is computed for each voxel which is used to track the estimated 3D poses over time. The main advantage of the approach is that it avoids making any hard decisions based on individual images. The approach can robustly estimate and track 3D poses even when people are severely occluded in some cameras. It outperforms the state-of-the-art methods by a large margin on three public datasets including Shelf, Campus and CMU Panoptic.



### LSENet: Location and Seasonality Enhanced Network for Multi-Class Ocean Front Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.02455v1
- **DOI**: 10.1109/TGRS.2022.3176635
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.02455v1)
- **Published**: 2021-08-05 08:40:42+00:00
- **Updated**: 2021-08-05 08:40:42+00:00
- **Authors**: Cui Xie, Hao Guo, Junyu Dong
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing
- **Summary**: Ocean fronts can cause the accumulation of nutrients and affect the propagation of underwater sound, so high-precision ocean front detection is of great significance to the marine fishery and national defense fields. However, the current ocean front detection methods either have low detection accuracy or most can only detect the occurrence of ocean front by binary classification, rarely considering the differences of the characteristics of multiple ocean fronts in different sea areas. In order to solve the above problems, we propose a semantic segmentation network called location and seasonality enhanced network (LSENet) for multi-class ocean fronts detection at pixel level. In this network, we first design a channel supervision unit structure, which integrates the seasonal characteristics of the ocean front itself and the contextual information to improve the detection accuracy. We also introduce a location attention mechanism to adaptively assign attention weights to the fronts according to their frequently occurred sea area, which can further improve the accuracy of multi-class ocean front detection. Compared with other semantic segmentation methods and current representative ocean front detection method, the experimental results demonstrate convincingly that our method is more effective.



### Residual Attention: A Simple but Effective Method for Multi-Label Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.02456v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02456v2)
- **Published**: 2021-08-05 08:45:57+00:00
- **Updated**: 2021-08-19 02:38:03+00:00
- **Authors**: Ke Zhu, Jianxin Wu
- **Comment**: ICCV 2021, code: https://github.com/Kevinz-code/CSRA
- **Journal**: None
- **Summary**: Multi-label image recognition is a challenging computer vision task of practical use. Progresses in this area, however, are often characterized by complicated methods, heavy computations, and lack of intuitive explanations. To effectively capture different spatial regions occupied by objects from different categories, we propose an embarrassingly simple module, named class-specific residual attention (CSRA). CSRA generates class-specific features for every category by proposing a simple spatial attention score, and then combines it with the class-agnostic average pooling feature. CSRA achieves state-of-the-art results on multilabel recognition, and at the same time is much simpler than them. Furthermore, with only 4 lines of code, CSRA also leads to consistent improvement across many diverse pretrained models and datasets without any extra training. CSRA is both easy to implement and light in computations, which also enjoys intuitive explanations and visualizations.



### Colorectal Polyp Classification from White-light Colonoscopy Images via Domain Alignment
- **Arxiv ID**: http://arxiv.org/abs/2108.02476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02476v1)
- **Published**: 2021-08-05 09:31:46+00:00
- **Updated**: 2021-08-05 09:31:46+00:00
- **Authors**: Qin Wang, Hui Che, Weizhen Ding, Li Xiang, Guanbin Li, Zhen Li, Shuguang Cui
- **Comment**: Accepted in MICCAI-21
- **Journal**: None
- **Summary**: Differentiation of colorectal polyps is an important clinical examination. A computer-aided diagnosis system is required to assist accurate diagnosis from colonoscopy images. Most previous studies at-tempt to develop models for polyp differentiation using Narrow-Band Imaging (NBI) or other enhanced images. However, the wide range of these models' applications for clinical work has been limited by the lagging of imaging techniques. Thus, we propose a novel framework based on a teacher-student architecture for the accurate colorectal polyp classification (CPC) through directly using white-light (WL) colonoscopy images in the examination. In practice, during training, the auxiliary NBI images are utilized to train a teacher network and guide the student network to acquire richer feature representation from WL images. The feature transfer is realized by domain alignment and contrastive learning. Eventually the final student network has the ability to extract aligned features from only WL images to facilitate the CPC task. Besides, we release the first public-available paired CPC dataset containing WL-NBI pairs for the alignment training. Quantitative and qualitative evaluation indicates that the proposed method outperforms the previous methods in CPC, improving the accuracy by 5.6%with very fast speed.



### MixMicrobleed: Multi-stage detection and segmentation of cerebral microbleeds
- **Arxiv ID**: http://arxiv.org/abs/2108.02482v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02482v1)
- **Published**: 2021-08-05 09:41:08+00:00
- **Updated**: 2021-08-05 09:41:08+00:00
- **Authors**: Marta Girones Sanguesa, Denis Kutnar, Bas H. M. van der Velden, Hugo J. Kuijf
- **Comment**: Submitted to the "Where is VALDO?" challenge, MICCAI 2021
- **Journal**: None
- **Summary**: Cerebral microbleeds are small, dark, round lesions that can be visualised on T2*-weighted MRI or other sequences sensitive to susceptibility effects. In this work, we propose a multi-stage approach to both microbleed detection and segmentation. First, possible microbleed locations are detected with a Mask R-CNN technique. Second, at each possible microbleed location, a simple U-Net performs the final segmentation. This work used the 72 subjects as training data provided by the "Where is VALDO?" challenge of MICCAI 2021.



### MixLacune: Segmentation of lacunes of presumed vascular origin
- **Arxiv ID**: http://arxiv.org/abs/2108.02483v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02483v1)
- **Published**: 2021-08-05 09:41:35+00:00
- **Updated**: 2021-08-05 09:41:35+00:00
- **Authors**: Denis Kutnar, Bas H. M. van der Velden, Marta Girones Sanguesa, Mirjam I. Geerlings, J. Matthijs Biesbroek, Hugo J. Kuijf
- **Comment**: Submitted to the "Where is VALDO?" challenge, MICCAI 2021
- **Journal**: None
- **Summary**: Lacunes of presumed vascular origin are fluid-filled cavities of between 3 - 15 mm in diameter, visible on T1 and FLAIR brain MRI. Quantification of lacunes relies on manual annotation or semi-automatic / interactive approaches; and almost no automatic methods exist for this task. In this work, we present a two-stage approach to segment lacunes of presumed vascular origin: (1) detection with Mask R-CNN followed by (2) segmentation with a U-Net CNN. Data originates from Task 3 of the "Where is VALDO?" challenge and consists of 40 training subjects. We report the mean DICE on the training set of 0.83 and on the validation set of 0.84. Source code is available at: https://github.com/hjkuijf/MixLacune . The docker container hjkuijf/mixlacune can be pulled from https://hub.docker.com/r/hjkuijf/mixlacune .



### Poison Ink: Robust and Invisible Backdoor Attack
- **Arxiv ID**: http://arxiv.org/abs/2108.02488v3
- **DOI**: 10.1109/TIP.2022.3201472
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02488v3)
- **Published**: 2021-08-05 09:52:49+00:00
- **Updated**: 2022-08-13 11:17:49+00:00
- **Authors**: Jie Zhang, Dongdong Chen, Qidong Huang, Jing Liao, Weiming Zhang, Huamin Feng, Gang Hua, Nenghai Yu
- **Comment**: IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: Recent research shows deep neural networks are vulnerable to different types of attacks, such as adversarial attack, data poisoning attack and backdoor attack. Among them, backdoor attack is the most cunning one and can occur in almost every stage of deep learning pipeline. Therefore, backdoor attack has attracted lots of interests from both academia and industry. However, most existing backdoor attack methods are either visible or fragile to some effortless pre-processing such as common data transformations. To address these limitations, we propose a robust and invisible backdoor attack called "Poison Ink". Concretely, we first leverage the image structures as target poisoning areas, and fill them with poison ink (information) to generate the trigger pattern. As the image structure can keep its semantic meaning during the data transformation, such trigger pattern is inherently robust to data transformations. Then we leverage a deep injection network to embed such trigger pattern into the cover image to achieve stealthiness. Compared to existing popular backdoor attack methods, Poison Ink outperforms both in stealthiness and robustness. Through extensive experiments, we demonstrate Poison Ink is not only general to different datasets and network architectures, but also flexible for different attack scenarios. Besides, it also has very strong resistance against many state-of-the-art defense techniques.



### Imperceptible Adversarial Examples by Spatial Chroma-Shift
- **Arxiv ID**: http://arxiv.org/abs/2108.02502v2
- **DOI**: 10.1145/3475724.3483604
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.02502v2)
- **Published**: 2021-08-05 10:19:23+00:00
- **Updated**: 2021-09-02 17:19:30+00:00
- **Authors**: Ayberk Aydin, Deniz Sen, Berat Tuna Karli, Oguz Hanoglu, Alptekin Temizel
- **Comment**: 10 pages, 5 figures. Accepted to International Workshop on
  Adversarial Learning for Multimedia, ACM Multimedia 2021
- **Journal**: None
- **Summary**: Deep Neural Networks have been shown to be vulnerable to various kinds of adversarial perturbations. In addition to widely studied additive noise based perturbations, adversarial examples can also be created by applying a per pixel spatial drift on input images. While spatial transformation based adversarial examples look more natural to human observers due to absence of additive noise, they still possess visible distortions caused by spatial transformations. Since the human vision is more sensitive to the distortions in the luminance compared to those in chrominance channels, which is one of the main ideas behind the lossy visual multimedia compression standards, we propose a spatial transformation based perturbation method to create adversarial examples by only modifying the color components of an input image. While having competitive fooling rates on CIFAR-10 and NIPS2017 Adversarial Learning Challenge datasets, examples created with the proposed method have better scores with regards to various perceptual quality metrics. Human visual perception studies validate that the examples are more natural looking and often indistinguishable from their original counterparts.



### RCA-IUnet: A residual cross-spatial attention guided inception U-Net model for tumor segmentation in breast ultrasound imaging
- **Arxiv ID**: http://arxiv.org/abs/2108.02508v4
- **DOI**: 10.1007/s00138-022-01280-3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02508v4)
- **Published**: 2021-08-05 10:35:06+00:00
- **Updated**: 2022-01-02 12:21:15+00:00
- **Authors**: Narinder Singh Punn, Sonali Agarwal
- **Comment**: None
- **Journal**: Machine Vision and Applications, Springer, 2022
- **Summary**: The advancements in deep learning technologies have produced immense contributions to biomedical image analysis applications. With breast cancer being the common deadliest disease among women, early detection is the key means to improve survivability. Medical imaging like ultrasound presents an excellent visual representation of the functioning of the organs; however, for any radiologist analysing such scans is challenging and time consuming which delays the diagnosis process. Although various deep learning based approaches are proposed that achieved promising results, the present article introduces an efficient residual cross-spatial attention guided inception U-Net (RCA-IUnet) model with minimal training parameters for tumor segmentation using breast ultrasound imaging to further improve the segmentation performance of varying tumor sizes. The RCA-IUnet model follows U-Net topology with residual inception depth-wise separable convolution and hybrid pooling (max pooling and spectral pooling) layers. In addition, cross-spatial attention filters are added to suppress the irrelevant features and focus on the target structure. The segmentation performance of the proposed model is validated on two publicly available datasets using standard segmentation evaluation metrics, where it outperformed the other state-of-the-art segmentation models.



### Object-Augmented RGB-D SLAM for Wide-Disparity Relocalisation
- **Arxiv ID**: http://arxiv.org/abs/2108.02522v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.02522v1)
- **Published**: 2021-08-05 11:02:25+00:00
- **Updated**: 2021-08-05 11:02:25+00:00
- **Authors**: Yuhang Ming, Xingrui Yang, Andrew Calway
- **Comment**: Accepted by 2021 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2021)
- **Journal**: None
- **Summary**: We propose a novel object-augmented RGB-D SLAM system that is capable of constructing a consistent object map and performing relocalisation based on centroids of objects in the map. The approach aims to overcome the view dependence of appearance-based relocalisation methods using point features or images. During the map construction, we use a pre-trained neural network to detect objects and estimate 6D poses from RGB-D data. An incremental probabilistic model is used to aggregate estimates over time to create the object map. Then in relocalisation, we use the same network to extract objects-of-interest in the `lost' frames. Pairwise geometric matching finds correspondences between map and frame objects, and probabilistic absolute orientation followed by application of iterative closest point to dense depth maps and object centroids gives relocalisation. Results of experiments in desktop environments demonstrate very high success rates even for frames with widely different viewpoints from those used to construct the map, significantly outperforming two appearance-based methods.



### SimpModeling: Sketching Implicit Field to Guide Mesh Modeling for 3D Animalmorphic Head Design
- **Arxiv ID**: http://arxiv.org/abs/2108.02548v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2108.02548v1)
- **Published**: 2021-08-05 12:17:36+00:00
- **Updated**: 2021-08-05 12:17:36+00:00
- **Authors**: Zhongjin Luo, Jie Zhou, Heming Zhu, Dong Du, Xiaoguang Han, Hongbo Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Head shapes play an important role in 3D character design. In this work, we propose SimpModeling, a novel sketch-based system for helping users, especially amateur users, easily model 3D animalmorphic heads - a prevalent kind of heads in character design. Although sketching provides an easy way to depict desired shapes, it is challenging to infer dense geometric information from sparse line drawings. Recently, deepnet-based approaches have been taken to address this challenge and try to produce rich geometric details from very few strokes. However, while such methods reduce users' workload, they would cause less controllability of target shapes. This is mainly due to the uncertainty of the neural prediction. Our system tackles this issue and provides good controllability from three aspects: 1) we separate coarse shape design and geometric detail specification into two stages and respectively provide different sketching means; 2) in coarse shape designing, sketches are used for both shape inference and geometric constraints to determine global geometry, and in geometric detail crafting, sketches are used for carving surface details; 3) in both stages, we use the advanced implicit-based shape inference methods, which have strong ability to handle the domain gap between freehand sketches and synthetic ones used for training. Experimental results confirm the effectiveness of our method and the usability of our interactive system. We also contribute to a dataset of high-quality 3D animal heads, which are manually created by artists.



### MS-KD: Multi-Organ Segmentation with Multiple Binary-Labeled Datasets
- **Arxiv ID**: http://arxiv.org/abs/2108.02559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02559v1)
- **Published**: 2021-08-05 12:29:26+00:00
- **Updated**: 2021-08-05 12:29:26+00:00
- **Authors**: Shixiang Feng, Yuhang Zhou, Xiaoman Zhang, Ya Zhang, Yanfeng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Annotating multiple organs in 3D medical images is time-consuming and costly. Meanwhile, there exist many single-organ datasets with one specific organ annotated. This paper investigates how to learn a multi-organ segmentation model leveraging a set of binary-labeled datasets. A novel Multi-teacher Single-student Knowledge Distillation (MS-KD) framework is proposed, where the teacher models are pre-trained single-organ segmentation networks, and the student model is a multi-organ segmentation network. Considering that each teacher focuses on different organs, a region-based supervision method, consisting of logits-wise supervision and feature-wise supervision, is proposed. Each teacher supervises the student in two regions, the organ region where the teacher is considered as an expert and the background region where all teachers agree. Extensive experiments on three public single-organ datasets and a multi-organ dataset have demonstrated the effectiveness of the proposed MS-KD framework.



### Tikhonov Regularization of Circle-Valued Signals
- **Arxiv ID**: http://arxiv.org/abs/2108.02602v3
- **DOI**: 10.1109/TSP.2022.3179816
- **Categories**: **math.OC**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2108.02602v3)
- **Published**: 2021-08-05 13:20:10+00:00
- **Updated**: 2022-06-07 12:51:28+00:00
- **Authors**: Laurent Condat
- **Comment**: None
- **Journal**: None
- **Summary**: It is common to have to process signals or images whose values are cyclic and can be represented as points on the complex circle, like wrapped phases, angles, orientations, or color hues. We consider a Tikhonov-type regularization model to smoothen or interpolate circle-valued signals defined on arbitrary graphs. We propose a convex relaxation of this nonconvex problem as a semidefinite program, and an efficient algorithm to solve it.



### UniCon: Unified Context Network for Robust Active Speaker Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.02607v1
- **DOI**: 10.1145/3474085.3475275
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02607v1)
- **Published**: 2021-08-05 13:25:44+00:00
- **Updated**: 2021-08-05 13:25:44+00:00
- **Authors**: Yuanhang Zhang, Susan Liang, Shuang Yang, Xiao Liu, Zhongqin Wu, Shiguang Shan, Xilin Chen
- **Comment**: 10 pages, 6 figures; to appear at ACM Multimedia 2021
- **Journal**: None
- **Summary**: We introduce a new efficient framework, the Unified Context Network (UniCon), for robust active speaker detection (ASD). Traditional methods for ASD usually operate on each candidate's pre-cropped face track separately and do not sufficiently consider the relationships among the candidates. This potentially limits performance, especially in challenging scenarios with low-resolution faces, multiple candidates, etc. Our solution is a novel, unified framework that focuses on jointly modeling multiple types of contextual information: spatial context to indicate the position and scale of each candidate's face, relational context to capture the visual relationships among the candidates and contrast audio-visual affinities with each other, and temporal context to aggregate long-term information and smooth out local uncertainties. Based on such information, our model optimizes all candidates in a unified process for robust and reliable ASD. A thorough ablation study is performed on several challenging ASD benchmarks under different settings. In particular, our method outperforms the state-of-the-art by a large margin of about 15% mean Average Precision (mAP) absolute on two challenging subsets: one with three candidate speakers, and the other with faces smaller than 64 pixels. Together, our UniCon achieves 92.0% mAP on the AVA-ActiveSpeaker validation set, surpassing 90% for the first time on this challenging dataset at the time of submission. Project website: https://unicon-asd.github.io/.



### Planning with Learned Dynamic Model for Unsupervised Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2108.02613v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.02613v2)
- **Published**: 2021-08-05 13:47:11+00:00
- **Updated**: 2021-08-19 16:43:01+00:00
- **Authors**: Haobo Jiang, Jin Xie, Jianjun Qian, Jian Yang
- **Comment**: Accepted by IJCAI-2021
- **Journal**: None
- **Summary**: Point cloud registration is a fundamental problem in 3D computer vision. In this paper, we cast point cloud registration into a planning problem in reinforcement learning, which can seek the transformation between the source and target point clouds through trial and error. By modeling the point cloud registration process as a Markov decision process (MDP), we develop a latent dynamic model of point clouds, consisting of a transformation network and evaluation network. The transformation network aims to predict the new transformed feature of the point cloud after performing a rigid transformation (i.e., action) on it while the evaluation network aims to predict the alignment precision between the transformed source point cloud and target point cloud as the reward signal. Once the dynamic model of the point cloud is trained, we employ the cross-entropy method (CEM) to iteratively update the planning policy by maximizing the rewards in the point cloud registration process. Thus, the optimal policy, i.e., the transformation between the source and target point clouds, can be obtained via gradually narrowing the search space of the transformation. Experimental results on ModelNet40 and 7Scene benchmark datasets demonstrate that our method can yield good registration performance in an unsupervised manner.



### Parallel Capsule Networks for Classification of White Blood Cells
- **Arxiv ID**: http://arxiv.org/abs/2108.02644v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02644v2)
- **Published**: 2021-08-05 14:30:44+00:00
- **Updated**: 2021-09-06 08:18:44+00:00
- **Authors**: Juan P. Vigueras-Guilln, Arijit Patra, Ola Engkvist, Frank Seeliger
- **Comment**: Accepted for the International Conference on Medical Image Computing
  and Computer Assisted Intervention (MICCAI) 2021
- **Journal**: None
- **Summary**: Capsule Networks (CapsNets) is a machine learning architecture proposed to overcome some of the shortcomings of convolutional neural networks (CNNs). However, CapsNets have mainly outperformed CNNs in datasets where images are small and/or the objects to identify have minimal background noise. In this work, we present a new architecture, parallel CapsNets, which exploits the concept of branching the network to isolate certain capsules, allowing each branch to identify different entities. We applied our concept to the two current types of CapsNet architectures, studying the performance for networks with different layers of capsules. We tested our design in a public, highly unbalanced dataset of acute myeloid leukaemia images (15 classes). Our experiments showed that conventional CapsNets show similar performance than our baseline CNN (ResNeXt-50) but depict instability problems. In contrast, parallel CapsNets can outperform ResNeXt-50, is more stable, and shows better rotational invariance than both, conventional CapsNets and ResNeXt-50.



### A Computer-Aided Diagnosis System for Breast Pathology: A Deep Learning Approach with Model Interpretability from Pathological Perspective
- **Arxiv ID**: http://arxiv.org/abs/2108.02656v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02656v1)
- **Published**: 2021-08-05 14:43:59+00:00
- **Updated**: 2021-08-05 14:43:59+00:00
- **Authors**: Wei-Wen Hsu, Yongfang Wu, Chang Hao, Yu-Ling Hou, Xiang Gao, Yun Shao, Xueli Zhang, Tao He, Yanhong Tai
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: We develop a computer-aided diagnosis (CAD) system using deep learning approaches for lesion detection and classification on whole-slide images (WSIs) with breast cancer. The deep features being distinguishing in classification from the convolutional neural networks (CNN) are demonstrated in this study to provide comprehensive interpretability for the proposed CAD system using pathological knowledge. Methods: In the experiment, a total of 186 slides of WSIs were collected and classified into three categories: Non-Carcinoma, Ductal Carcinoma in Situ (DCIS), and Invasive Ductal Carcinoma (IDC). Instead of conducting pixel-wise classification into three classes directly, we designed a hierarchical framework with the multi-view scheme that performs lesion detection for region proposal at higher magnification first and then conducts lesion classification at lower magnification for each detected lesion. Results: The slide-level accuracy rate for three-category classification reaches 90.8% (99/109) through 5-fold cross-validation and achieves 94.8% (73/77) on the testing set. The experimental results show that the morphological characteristics and co-occurrence properties learned by the deep learning models for lesion classification are accordant with the clinical rules in diagnosis. Conclusion: The pathological interpretability of the deep features not only enhances the reliability of the proposed CAD system to gain acceptance from medical specialists, but also facilitates the development of deep learning frameworks for various tasks in pathology. Significance: This paper presents a CAD system for pathological image analysis, which fills the clinical requirements and can be accepted by medical specialists with providing its interpretability from the pathological perspective.



### Adaptive Normalized Representation Learning for Generalizable Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2108.02667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02667v1)
- **Published**: 2021-08-05 15:04:33+00:00
- **Updated**: 2021-08-05 15:04:33+00:00
- **Authors**: Shubao Liu, Ke-Yue Zhang, Taiping Yao, Mingwei Bi, Shouhong Ding, Jilin Li, Feiyue Huang, Lizhuang Ma
- **Comment**: accepted on ACM MM 2021
- **Journal**: None
- **Summary**: With various face presentation attacks arising under unseen scenarios, face anti-spoofing (FAS) based on domain generalization (DG) has drawn growing attention due to its robustness. Most existing methods utilize DG frameworks to align the features to seek a compact and generalized feature space. However, little attention has been paid to the feature extraction process for the FAS task, especially the influence of normalization, which also has a great impact on the generalization of the learned representation. To address this issue, we propose a novel perspective of face anti-spoofing that focuses on the normalization selection in the feature extraction process. Concretely, an Adaptive Normalized Representation Learning (ANRL) framework is devised, which adaptively selects feature normalization methods according to the inputs, aiming to learn domain-agnostic and discriminative representation. Moreover, to facilitate the representation learning, Dual Calibration Constraints are designed, including Inter-Domain Compatible loss and Inter-Class Separable loss, which provide a better optimization direction for generalizable representation. Extensive experiments and visualizations are presented to demonstrate the effectiveness of our method against the SOTA competitors.



### Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware
- **Arxiv ID**: http://arxiv.org/abs/2108.02671v2
- **DOI**: 10.1109/ICCVW54120.2021.00111
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02671v2)
- **Published**: 2021-08-05 15:10:00+00:00
- **Updated**: 2022-05-05 14:16:53+00:00
- **Authors**: Julia Hornauer, Lazaros Nalpantidis, Vasileios Belagiannis
- **Comment**: Accepted to ICCV 2021 Workshop on Embedded and Real-World Computer
  Vision in Autonomous Driving
- **Journal**: 2021 IEEE/CVF International Conference on Computer Vision
  Workshops (ICCVW), 2021, pp. 954-962
- **Summary**: Real-world perception systems in many cases build on hardware with limited resources to adhere to cost and power limitations of their carrying system. Deploying deep neural networks on resource-constrained hardware became possible with model compression techniques, as well as efficient and hardware-aware architecture design. However, model adaptation is additionally required due to the diverse operation environments. In this work, we address the problem of training deep neural networks on resource-constrained hardware in the context of visual domain adaptation. We select the task of monocular depth estimation where our goal is to transform a pre-trained model to the target's domain data. While the source domain includes labels, we assume an unlabelled target domain, as it happens in real-world applications. Then, we present an adversarial learning approach that is adapted for training on the device with limited resources. Since visual domain adaptation, i.e. neural network training, has not been previously explored for resource-constrained hardware, we present the first feasibility study for image-based depth estimation. Our experiments show that visual domain adaptation is relevant only for efficient network architectures and training sets at the order of a few hundred samples. Models and code are publicly available.



### Redesigning Fully Convolutional DenseUNets for Large Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2108.02676v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02676v1)
- **Published**: 2021-08-05 15:14:20+00:00
- **Updated**: 2021-08-05 15:14:20+00:00
- **Authors**: Juan P. Vigueras-Guilln, Joan Lasenby, Frank Seeliger
- **Comment**: 15 pages, 6 figures. Originally submitted to the European Conference
  on Computer Vision 2020 (but did not get enough grade to pass the acceptance
  threshold)
- **Journal**: None
- **Summary**: The automated segmentation of cancer tissue in histopathology images can help clinicians to detect, diagnose, and analyze such disease. Different from other natural images used in many convolutional networks for benchmark, histopathology images can be extremely large, and the cancerous patterns can reach beyond 1000 pixels. Therefore, the well-known networks in the literature were never conceived to handle these peculiarities. In this work, we propose a Fully Convolutional DenseUNet that is particularly designed to solve histopathology problems. We evaluated our network in two public pathology datasets published as challenges in the recent MICCAI 2019: binary segmentation in colon cancer images (DigestPath2019), and multi-class segmentation in prostate cancer images (Gleason2019), achieving similar and better results than the winners of the challenges, respectively. Furthermore, we discussed some good practices in the training setup to yield the best performance and the main challenges in these histopathology datasets.



### A Low Rank Promoting Prior for Unsupervised Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.02696v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02696v1)
- **Published**: 2021-08-05 15:58:25+00:00
- **Updated**: 2021-08-05 15:58:25+00:00
- **Authors**: Yu Wang, Jingyang Lin, Qi Cai, Yingwei Pan, Ting Yao, Hongyang Chao, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised learning is just at a tipping point where it could really take off. Among these approaches, contrastive learning has seen tremendous progress and led to state-of-the-art performance. In this paper, we construct a novel probabilistic graphical model that effectively incorporates the low rank promoting prior into the framework of contrastive learning, referred to as LORAC. In contrast to the existing conventional self-supervised approaches that only considers independent learning, our hypothesis explicitly requires that all the samples belonging to the same instance class lie on the same subspace with small dimension. This heuristic poses particular joint learning constraints to reduce the degree of freedom of the problem during the search of the optimal network parameterization. Most importantly, we argue that the low rank prior employed here is not unique, and many different priors can be invoked in a similar probabilistic way, corresponding to different hypotheses about underlying truth behind the contrastive features. Empirical evidences show that the proposed algorithm clearly surpasses the state-of-the-art approaches on multiple benchmarks, including image classification, object detection, instance segmentation and keypoint detection.



### Rotaflip: A New CNN Layer for Regularization and Rotational Invariance in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2108.02704v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02704v1)
- **Published**: 2021-08-05 16:13:36+00:00
- **Updated**: 2021-08-05 16:13:36+00:00
- **Authors**: Juan P. Vigueras-Guilln, Joan Lasenby, Frank Seeliger
- **Comment**: 13 pages, 6 images. Originally submitted to the European Conference
  on Computer Vision 2020 (but did not get enough grade to pass the acceptance
  threshold)
- **Journal**: None
- **Summary**: Regularization in convolutional neural networks (CNNs) is usually addressed with dropout layers. However, dropout is sometimes detrimental in the convolutional part of a CNN as it simply sets to zero a percentage of pixels in the feature maps, adding unrepresentative examples during training. Here, we propose a CNN layer that performs regularization by applying random rotations of reflections to a small percentage of feature maps after every convolutional layer. We prove how this concept is beneficial for images with orientational symmetries, such as in medical images, as it provides a certain degree of rotational invariance. We tested this method in two datasets, a patch-based set of histopathology images (PatchCamelyon) to perform classification using a generic DenseNet, and a set of specular microscopy images of the corneal endothelium to perform segmentation using a tailored U-net, improving the performance in both cases.



### Fairness Properties of Face Recognition and Obfuscation Systems
- **Arxiv ID**: http://arxiv.org/abs/2108.02707v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.02707v3)
- **Published**: 2021-08-05 16:18:15+00:00
- **Updated**: 2022-09-16 17:46:37+00:00
- **Authors**: Harrison Rosenberg, Brian Tang, Kassem Fawaz, Somesh Jha
- **Comment**: None
- **Journal**: None
- **Summary**: The proliferation of automated face recognition in the commercial and government sectors has caused significant privacy concerns for individuals. One approach to address these privacy concerns is to employ evasion attacks against the metric embedding networks powering face recognition systems: Face obfuscation systems generate imperceptibly perturbed images that cause face recognition systems to misidentify the user. Perturbed faces are generated on metric embedding networks, which are known to be unfair in the context of face recognition. A question of demographic fairness naturally follows: are there demographic disparities in face obfuscation system performance? We answer this question with an analytical and empirical exploration of recent face obfuscation systems. Metric embedding networks are found to be demographically aware: face embeddings are clustered by demographic. We show how this clustering behavior leads to reduced face obfuscation utility for faces in minority groups. An intuitive analytical model yields insight into these phenomena.



### Object Wake-up: 3D Object Rigging from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2108.02708v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02708v3)
- **Published**: 2021-08-05 16:20:12+00:00
- **Updated**: 2022-07-06 00:12:18+00:00
- **Authors**: Ji Yang, Xinxin Zuo, Sen Wang, Zhenbo Yu, Xingyu Li, Bingbing Ni, Minglun Gong, Li Cheng
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Given a single image of a general object such as a chair, could we also restore its articulated 3D shape similar to human modeling, so as to animate its plausible articulations and diverse motions? This is an interesting new question that may have numerous downstream augmented reality and virtual reality applications. Comparing with previous efforts on object manipulation, our work goes beyond 2D manipulation and rigid deformation, and involves articulated manipulation. To achieve this goal, we propose an automated approach to build such 3D generic objects from single images and embed articulated skeletons in them. Specifically, our framework starts by reconstructing the 3D object from an input image. Afterwards, to extract skeletons for generic 3D objects, we develop a novel skeleton prediction method with a multi-head structure for skeleton probability field estimation by utilizing the deep implicit functions. A dataset of generic 3D objects with ground-truth annotated skeletons is collected. Empirically our approach is demonstrated with satisfactory performance on public datasets as well as our in-house dataset; our results surpass those of the state-of-the-arts by a noticeable margin on both 3D reconstruction and skeleton prediction.



### Generalizable Mixed-Precision Quantization via Attribution Rank Preservation
- **Arxiv ID**: http://arxiv.org/abs/2108.02720v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02720v1)
- **Published**: 2021-08-05 16:41:57+00:00
- **Updated**: 2021-08-05 16:41:57+00:00
- **Authors**: Ziwei Wang, Han Xiao, Jiwen Lu, Jie Zhou
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: In this paper, we propose a generalizable mixed-precision quantization (GMPQ) method for efficient inference. Conventional methods require the consistency of datasets for bitwidth search and model deployment to guarantee the policy optimality, leading to heavy search cost on challenging largescale datasets in realistic applications. On the contrary, our GMPQ searches the mixed-quantization policy that can be generalized to largescale datasets with only a small amount of data, so that the search cost is significantly reduced without performance degradation. Specifically, we observe that locating network attribution correctly is general ability for accurate visual analysis across different data distribution. Therefore, despite of pursuing higher model accuracy and complexity, we preserve attribution rank consistency between the quantized models and their full-precision counterparts via efficient capacity-aware attribution imitation for generalizable mixed-precision quantization strategy search. Extensive experiments show that our method obtains competitive accuracy-complexity trade-off compared with the state-of-the-art mixed-precision networks in significantly reduced search cost. The code is available at https://github.com/ZiweiWangTHU/GMPQ.git.



### Instance Similarity Learning for Unsupervised Feature Representation
- **Arxiv ID**: http://arxiv.org/abs/2108.02721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02721v1)
- **Published**: 2021-08-05 16:42:06+00:00
- **Updated**: 2021-08-05 16:42:06+00:00
- **Authors**: Ziwei Wang, Yunsong Wang, Ziyi Wu, Jiwen Lu, Jie Zhou
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: In this paper, we propose an instance similarity learning (ISL) method for unsupervised feature representation. Conventional methods assign close instance pairs in the feature space with high similarity, which usually leads to wrong pairwise relationship for large neighborhoods because the Euclidean distance fails to depict the true semantic similarity on the feature manifold. On the contrary, our method mines the feature manifold in an unsupervised manner, through which the semantic similarity among instances is learned in order to obtain discriminative representations. Specifically, we employ the Generative Adversarial Networks (GAN) to mine the underlying feature manifold, where the generated features are applied as the proxies to progressively explore the feature manifold so that the semantic similarity among instances is acquired as reliable pseudo supervision. Extensive experiments on image classification demonstrate the superiority of our method compared with the state-of-the-art methods. The code is available at https://github.com/ZiweiWangTHU/ISL.git.



### Video Contrastive Learning with Global Context
- **Arxiv ID**: http://arxiv.org/abs/2108.02722v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02722v1)
- **Published**: 2021-08-05 16:42:38+00:00
- **Updated**: 2021-08-05 16:42:38+00:00
- **Authors**: Haofei Kuang, Yi Zhu, Zhi Zhang, Xinyu Li, Joseph Tighe, Sren Schwertfeger, Cyrill Stachniss, Mu Li
- **Comment**: Code is publicly available at:
  https://github.com/amazon-research/video-contrastive-learning
- **Journal**: None
- **Summary**: Contrastive learning has revolutionized self-supervised image representation learning field, and recently been adapted to video domain. One of the greatest advantages of contrastive learning is that it allows us to flexibly define powerful loss objectives as long as we can find a reasonable way to formulate positive and negative samples to contrast. However, existing approaches rely heavily on the short-range spatiotemporal salience to form clip-level contrastive signals, thus limit themselves from using global context. In this paper, we propose a new video-level contrastive learning method based on segments to formulate positive pairs. Our formulation is able to capture global context in a video, thus robust to temporal content change. We also incorporate a temporal order regularization term to enforce the inherent sequential structure of videos. Extensive experiments show that our video-level contrastive learning framework (VCLR) is able to outperform previous state-of-the-arts on five video datasets for downstream action classification, action localization and video retrieval. Code is available at https://github.com/amazon-research/video-contrastive-learning.



### VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search
- **Arxiv ID**: http://arxiv.org/abs/2108.02725v1
- **DOI**: 10.1145/1122445.1122456
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02725v1)
- **Published**: 2021-08-05 16:47:21+00:00
- **Updated**: 2021-08-05 16:47:21+00:00
- **Authors**: Shaunak Mishra, Mikhail Kuznetsov, Gaurav Srivastava, Maxim Sviridenko
- **Comment**: Accepted for publication at KDD 2021
- **Journal**: None
- **Summary**: Numerous online stock image libraries offer high quality yet copyright free images for use in marketing campaigns. To assist advertisers in navigating such third party libraries, we study the problem of automatically fetching relevant ad images given the ad text (via a short textual query for images). Motivated by our observations in logged data on ad image search queries (given ad text), we formulate a keyword extraction problem, where a keyword extracted from the ad text (or its augmented version) serves as the ad image query. In this context, we propose VisualTextRank: an unsupervised method to (i) augment input ad text using semantically similar ads, and (ii) extract the image query from the augmented ad text. VisualTextRank builds on prior work on graph based context extraction (biased TextRank in particular) by leveraging both the text and image of similar ads for better keyword extraction, and using advertiser category specific biasing with sentence-BERT embeddings. Using data collected from the Verizon Media Native (Yahoo Gemini) ad platform's stock image search feature for onboarding advertisers, we demonstrate the superiority of VisualTextRank compared to competitive keyword extraction baselines (including an $11\%$ accuracy lift over biased TextRank). For the case when the stock image library is restricted to English queries, we show the effectiveness of VisualTextRank on multilingual ads (translated to English) while leveraging semantically similar English ads. Online tests with a simplified version of VisualTextRank led to a 28.7% increase in the usage of stock image search, and a 41.6% increase in the advertiser onboarding rate in the Verizon Media Native ad platform.



### COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/2108.03131v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.03131v1)
- **Published**: 2021-08-05 16:47:33+00:00
- **Updated**: 2021-08-05 16:47:33+00:00
- **Authors**: Alexander MacLean, Saad Abbasi, Ashkan Ebadi, Andy Zhao, Maya Pavlova, Hayden Gunraj, Pengcheng Xi, Sonny Kohli, Alexander Wong
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of life globally, and a critical factor in mitigating its effects is screening individuals for infections, thereby allowing for both proper treatment for those individuals as well as action to be taken to prevent further spread of the virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a screening tool as it is a much cheaper and easier to apply imaging modality than others that are traditionally used for pulmonary examinations, namely chest x-ray and computed tomography. Given the scarcity of expert radiologists for interpreting POCUS examinations in many highly affected regions around the world, low-cost deep learning-driven clinical decision support solutions can have a large impact during the on-going pandemic. Motivated by this, we introduce COVID-Net US, a highly efficient, self-attention deep convolutional neural network design tailored for COVID-19 screening from lung POCUS images. Experimental results show that the proposed COVID-Net US can achieve an AUC of over 0.98 while achieving 353X lower architectural complexity, 62X lower computational complexity, and 14.3X faster inference times on a Raspberry Pi. Clinical validation was also conducted, where select cases were reviewed and reported on by a practicing clinician (20 years of clinical practice) specializing in intensive care (ICU) and 15 years of expertise in POCUS interpretation. To advocate affordable healthcare and artificial intelligence for resource-constrained environments, we have made COVID-Net US open source and publicly available as part of the COVID-Net open source initiative.



### WSDesc: Weakly Supervised 3D Local Descriptor Learning for Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2108.02740v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2108.02740v2)
- **Published**: 2021-08-05 17:11:08+00:00
- **Updated**: 2022-03-14 13:28:07+00:00
- **Authors**: Lei Li, Hongbo Fu, Maks Ovsjanikov
- **Comment**: To appear in IEEE TVCG
- **Journal**: None
- **Summary**: In this work, we present a novel method called WSDesc to learn 3D local descriptors in a weakly supervised manner for robust point cloud registration. Our work builds upon recent 3D CNN-based descriptor extractors, which leverage a voxel-based representation to parameterize local geometry of 3D points. Instead of using a predefined fixed-size local support in voxelization, we propose to learn the optimal support in a data-driven manner. To this end, we design a novel differentiable voxelization layer that can back-propagate the gradient to the support size optimization. To train the extracted descriptors, we propose a novel registration loss based on the deviation from rigidity of 3D transformations, and the loss is weakly supervised by the prior knowledge that the input point clouds have partial overlap, without requiring ground-truth alignment information. Through extensive experiments, we show that our learned descriptors yield superior performance on existing geometric registration benchmarks.



### Semi- and Self-Supervised Multi-View Fusion of 3D Microscopy Images using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.02743v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02743v1)
- **Published**: 2021-08-05 17:21:01+00:00
- **Updated**: 2021-08-05 17:21:01+00:00
- **Authors**: Canyu Yang, Dennis Eschweiler, Johannes Stegmaier
- **Comment**: 11 pages, 3 figures, 1 table, accepted for publication at MLMIR 2021
- **Journal**: None
- **Summary**: Recent developments in fluorescence microscopy allow capturing high-resolution 3D images over time for living model organisms. To be able to image even large specimens, techniques like multi-view light-sheet imaging record different orientations at each time point that can then be fused into a single high-quality volume. Based on measured point spread functions (PSF), deconvolution and content fusion are able to largely revert the inevitable degradation occurring during the imaging process. Classical multi-view deconvolution and fusion methods mainly use iterative procedures and content-based averaging. Lately, Convolutional Neural Networks (CNNs) have been deployed to approach 3D single-view deconvolution microscopy, but the multi-view case waits to be studied. We investigated the efficacy of CNN-based multi-view deconvolution and fusion with two synthetic data sets that mimic developing embryos and involve either two or four complementary 3D views. Compared with classical state-of-the-art methods, the proposed semi- and self-supervised models achieve competitive and superior deconvolution and fusion quality in the two-view and quad-view cases, respectively.



### Unifying Global-Local Representations in Salient Object Detection with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2108.02759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02759v1)
- **Published**: 2021-08-05 17:51:32+00:00
- **Updated**: 2021-08-05 17:51:32+00:00
- **Authors**: Sucheng Ren, Qiang Wen, Nanxuan Zhao, Guoqiang Han, Shengfeng He
- **Comment**: None
- **Journal**: None
- **Summary**: The fully convolutional network (FCN) has dominated salient object detection for a long period. However, the locality of CNN requires the model deep enough to have a global receptive field and such a deep model always leads to the loss of local details. In this paper, we introduce a new attention-based encoder, vision transformer, into salient object detection to ensure the globalization of the representations from shallow to deep layers. With the global view in very shallow layers, the transformer encoder preserves more local representations to recover the spatial details in final saliency maps. Besides, as each layer can capture a global view of its previous layer, adjacent layers can implicitly maximize the representation differences and minimize the redundant features, making that every output feature of transformer layers contributes uniquely for final prediction. To decode features from the transformer, we propose a simple yet effective deeply-transformed decoder. The decoder densely decodes and upsamples the transformer features, generating the final saliency map with less noise injection. Experimental results demonstrate that our method significantly outperforms other FCN-based and transformer-based methods in five benchmarks by a large margin, with an average of 12.17% improvement in terms of Mean Absolute Error (MAE). Code will be available at https://github.com/OliverRensu/GLSTR.



### SLAMP: Stochastic Latent Appearance and Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2108.02760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02760v1)
- **Published**: 2021-08-05 17:52:18+00:00
- **Updated**: 2021-08-05 17:52:18+00:00
- **Authors**: Adil Kaan Akan, Erkut Erdem, Aykut Erdem, Fatma Gney
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Motion is an important cue for video prediction and often utilized by separating video content into static and dynamic components. Most of the previous work utilizing motion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explicitly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without history already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the state-of-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challenging real-world autonomous driving datasets with complex motion and dynamic background.



### Sketch Your Own GAN
- **Arxiv ID**: http://arxiv.org/abs/2108.02774v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02774v2)
- **Published**: 2021-08-05 17:59:42+00:00
- **Updated**: 2021-09-20 15:07:52+00:00
- **Authors**: Sheng-Yu Wang, David Bau, Jun-Yan Zhu
- **Comment**: ICCV 2021 website: https://peterwang512.github.io/GANSketching code:
  https://github.com/PeterWang512/GANSketching
- **Journal**: None
- **Summary**: Can a user create a deep generative model by sketching a single example? Traditionally, creating a GAN model has required the collection of a large-scale dataset of exemplars and specialized knowledge in deep learning. In contrast, sketching is possibly the most universally accessible way to convey a visual concept. In this work, we present a method, GAN Sketching, for rewriting GANs with one or more sketches, to make GANs training easier for novice users. In particular, we change the weights of an original GAN model according to user sketches. We encourage the model's output to match the user sketches through a cross-domain adversarial loss. Furthermore, we explore different regularization methods to preserve the original model's diversity and image quality. Experiments have shown that our method can mold GANs to match shapes and poses specified by sketches while maintaining realism and diversity. Finally, we demonstrate a few applications of the resulting GAN, including latent space interpolation and image editing.



### Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina
- **Arxiv ID**: http://arxiv.org/abs/2108.02798v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02798v1)
- **Published**: 2021-08-05 18:02:56+00:00
- **Updated**: 2021-08-05 18:02:56+00:00
- **Authors**: Jan Kukaka, Anja Zenz, Marcel Kollovieh, Dominik Jstel, Vasilis Ntziachristos
- **Comment**: None
- **Journal**: None
- **Summary**: Fundus photography is the primary method for retinal imaging and essential for diabetic retinopathy prevention. Automated segmentation of fundus photographs would improve the quality, capacity, and cost-effectiveness of eye care screening programs. However, current segmentation methods are not robust towards the diversity in imaging conditions and pathologies typical for real-world clinical applications. To overcome these limitations, we utilized contrastive self-supervised learning to exploit the large variety of unlabeled fundus images in the publicly available EyePACS dataset. We pre-trained an encoder of a U-Net, which we later fine-tuned on several retinal vessel and lesion segmentation datasets. We demonstrate for the first time that by using contrastive self-supervised learning, the pre-trained network can recognize blood vessels, optic disc, fovea, and various lesions without being provided any labels. Furthermore, when fine-tuned on a downstream blood vessel segmentation task, such pre-trained networks achieve state-of-the-art performance on images from different datasets. Additionally, the pre-training also leads to shorter training times and an improved few-shot performance on both blood vessel and lesion segmentation tasks. Altogether, our results showcase the benefits of contrastive self-supervised pre-training which can play a crucial role in real-world clinical applications requiring robust models able to adapt to new devices with only a few annotated samples.



### A volumetric change detection framework using UAV oblique photogrammetry - A case study of ultra-high-resolution monitoring of progressive building collapse
- **Arxiv ID**: http://arxiv.org/abs/2108.02800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02800v1)
- **Published**: 2021-08-05 18:20:29+00:00
- **Updated**: 2021-08-05 18:20:29+00:00
- **Authors**: Ningli Xu, Debao Huang, Shuang Song, Xiao Ling, Chris Strasbaugh, Alper Yilmaz, Halil Sezen, Rongjun Qin
- **Comment**: 28 pages, 9 figures
- **Journal**: None
- **Summary**: In this paper, we present a case study that performs an unmanned aerial vehicle (UAV) based fine-scale 3D change detection and monitoring of progressive collapse performance of a building during a demolition event. Multi-temporal oblique photogrammetry images are collected with 3D point clouds generated at different stages of the demolition. The geometric accuracy of the generated point clouds has been evaluated against both airborne and terrestrial LiDAR point clouds, achieving an average distance of 12 cm and 16 cm for roof and facade respectively. We propose a hierarchical volumetric change detection framework that unifies multi-temporal UAV images for pose estimation (free of ground control points), reconstruction, and a coarse-to-fine 3D density change analysis. This work has provided a solution capable of addressing change detection on full 3D time-series datasets where dramatic scene content changes are presented progressively. Our change detection results on the building demolition event have been evaluated against the manually marked ground-truth changes and have achieved an F-1 score varying from 0.78 to 0.92, with consistently high precision (0.92 - 0.99). Volumetric changes through the demolition progress are derived from change detection and have shown to favorably reflect the qualitative and quantitative building demolition progression.



### Neural Twins Talk & Alternative Calculations
- **Arxiv ID**: http://arxiv.org/abs/2108.02807v1
- **DOI**: 10.1142/S1793351X21500045
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02807v1)
- **Published**: 2021-08-05 18:41:34+00:00
- **Updated**: 2021-08-05 18:41:34+00:00
- **Authors**: Zanyar Zohourianshahzadi, Jugal K. Kalita
- **Comment**: This paper was published at World Scientific Journal, International
  Journal of Semantic Computing. This is a preprint version that was submitted
  to the journal before final publication. arXiv admin note: substantial text
  overlap with arXiv:2009.12524
- **Journal**: International Journal of Semantic Computing, 2021, 93-116
- **Summary**: Inspired by how the human brain employs a higher number of neural pathways when describing a highly focused subject, we show that deep attentive models used for the main vision-language task of image captioning, could be extended to achieve better performance. Image captioning bridges a gap between computer vision and natural language processing. Automated image captioning is used as a tool to eliminate the need for human agent for creating descriptive captions for unseen images.Automated image captioning is challenging and yet interesting. One reason is that AI based systems capable of generating sentences that describe an input image could be used in a wide variety of tasks beyond generating captions for unseen images found on web or uploaded to social media. For example, in biology and medical sciences, these systems could provide researchers and physicians with a brief linguistic description of relevant images, potentially expediting their work.



### Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications
- **Arxiv ID**: http://arxiv.org/abs/2108.02818v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2108.02818v1)
- **Published**: 2021-08-05 19:05:57+00:00
- **Updated**: 2021-08-05 19:05:57+00:00
- **Authors**: Sandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim, Miles Brundage
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2103.00020
- **Journal**: None
- **Summary**: Recently, there have been breakthroughs in computer vision ("CV") models that are more generalizable with the advent of models such as CLIP and ALIGN. In this paper, we analyze CLIP and highlight some of the challenges such models pose. CLIP reduces the need for task specific training data, potentially opening up many niche tasks to automation. CLIP also allows its users to flexibly specify image classification classes in natural language, which we find can shift how biases manifest. Additionally, through some preliminary probes we find that CLIP can inherit biases found in prior computer vision systems. Given the wide and unpredictable domain of uses for such models, this raises questions regarding what sufficiently safe behaviour for such systems may look like. These results add evidence to the growing body of work calling for a change in the notion of a 'better' model--to move beyond simply looking at higher accuracy at task-oriented capability evaluations, and towards a broader 'better' that takes into account deployment-critical features such as different use contexts, and people who interact with the model when thinking about model deployment.



### End-to-end Neural Video Coding Using a Compound Spatiotemporal Representation
- **Arxiv ID**: http://arxiv.org/abs/2108.04103v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04103v1)
- **Published**: 2021-08-05 19:43:32+00:00
- **Updated**: 2021-08-05 19:43:32+00:00
- **Authors**: Haojie Liu, Ming Lu, Zhiqi Chen, Xun Cao, Zhan Ma, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed rapid advances in learnt video coding. Most algorithms have solely relied on the vector-based motion representation and resampling (e.g., optical flow based bilinear sampling) for exploiting the inter frame redundancy. In spite of the great success of adaptive kernel-based resampling (e.g., adaptive convolutions and deformable convolutions) in video prediction for uncompressed videos, integrating such approaches with rate-distortion optimization for inter frame coding has been less successful. Recognizing that each resampling solution offers unique advantages in regions with different motion and texture characteristics, we propose a hybrid motion compensation (HMC) method that adaptively combines the predictions generated by these two approaches. Specifically, we generate a compound spatiotemporal representation (CSTR) through a recurrent information aggregation (RIA) module using information from the current and multiple past frames. We further design a one-to-many decoder pipeline to generate multiple predictions from the CSTR, including vector-based resampling, adaptive kernel-based resampling, compensation mode selection maps and texture enhancements, and combines them adaptively to achieve more accurate inter prediction. Experiments show that our proposed inter coding system can provide better motion-compensated prediction and is more robust to occlusions and complex motions. Together with jointly trained intra coder and residual coder, the overall learnt hybrid coder yields the state-of-the-art coding efficiency in low-delay scenario, compared to the traditional H.264/AVC and H.265/HEVC, as well as recently published learning-based methods, in terms of both PSNR and MS-SSIM metrics.



### Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.02832v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02832v1)
- **Published**: 2021-08-05 19:59:26+00:00
- **Updated**: 2021-08-05 19:59:26+00:00
- **Authors**: Akash Gupta, Padmaja Jonnalagedda, Bir Bhanu, Amit K. Roy-Chowdhury
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing works in supervised spatio-temporal video super-resolution (STVSR) heavily rely on a large-scale external dataset consisting of paired low-resolution low-frame rate (LR-LFR)and high-resolution high-frame-rate (HR-HFR) videos. Despite their remarkable performance, these methods make a prior assumption that the low-resolution video is obtained by down-scaling the high-resolution video using a known degradation kernel, which does not hold in practical settings. Another problem with these methods is that they cannot exploit instance-specific internal information of video at testing time. Recently, deep internal learning approaches have gained attention due to their ability to utilize the instance-specific statistics of a video. However, these methods have a large inference time as they require thousands of gradient updates to learn the intrinsic structure of the data. In this work, we presentAdaptiveVideoSuper-Resolution (Ada-VSR) which leverages external, as well as internal, information through meta-transfer learning and internal learning, respectively. Specifically, meta-learning is employed to obtain adaptive parameters, using a large-scale external dataset, that can adapt quickly to the novel condition (degradation model) of the given test video during the internal learning task, thereby exploiting external and internal information of a video for super-resolution. The model trained using our approach can quickly adapt to a specific video condition with only a few gradient updates, which reduces the inference time significantly. Extensive experiments on standard datasets demonstrate that our method performs favorably against various state-of-the-art approaches.



### Elaborative Rehearsal for Zero-shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.02833v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02833v2)
- **Published**: 2021-08-05 20:02:46+00:00
- **Updated**: 2021-08-18 18:33:46+00:00
- **Authors**: Shizhe Chen, Dong Huang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: The growing number of action classes has posed a new challenge for video understanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction. The ZSAR task aims to recognize target (unseen) actions without training examples by leveraging semantic representations to bridge seen and unseen actions. However, due to the complexity and diversity of actions, it remains challenging to semantically represent action classes and transfer knowledge from seen data. In this work, we propose an ER-enhanced ZSAR model inspired by an effective human memory technique Elaborative Rehearsal (ER), which involves elaborating a new concept and relating it to known concepts. Specifically, we expand each action class as an Elaborative Description (ED) sentence, which is more discriminative than a class name and less costly than manual-defined attributes. Besides directly aligning class semantics with videos, we incorporate objects from the video as Elaborative Concepts (EC) to improve video semantics and generalization from seen actions to unseen actions. Our ER-enhanced ZSAR model achieves state-of-the-art results on three existing benchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics dataset to overcome limitations of current benchmarks and demonstrate the first case where ZSAR performance is comparable to few-shot learning baselines on this more realistic setting. We will release our codes and collected EDs at https://github.com/DeLightCMU/ElaborativeRehearsal.



### Attention-based fusion of semantic boundary and non-boundary information to improve semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.02840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02840v1)
- **Published**: 2021-08-05 20:46:53+00:00
- **Updated**: 2021-08-05 20:46:53+00:00
- **Authors**: Jefferson Fontinele, Gabriel Lefundes, Luciano Oliveira
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a method for image semantic segmentation grounded on a novel fusion scheme, which takes place inside a deep convolutional neural network. The main goal of our proposal is to explore object boundary information to improve the overall segmentation performance. Unlike previous works that combine boundary and segmentation features, or those that use boundary information to regularize semantic segmentation, we instead propose a novel approach that embodies boundary information onto segmentation. For that, our semantic segmentation method uses two streams, which are combined through an attention gate, forming an end-to-end Y-model. To the best of our knowledge, ours is the first work to show that boundary detection can improve semantic segmentation when fused through a semantic fusion gate (attention model). We performed an extensive evaluation of our method over public data sets. We found competitive results on all data sets after comparing our proposed model with other twelve state-of-the-art segmenters, considering the same training conditions. Our proposed model achieved the best mIoU on the CityScapes, CamVid, and Pascal Context data sets, and the second best on Mapillary Vistas.



### Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene
- **Arxiv ID**: http://arxiv.org/abs/2108.02846v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.02846v1)
- **Published**: 2021-08-05 20:56:47+00:00
- **Updated**: 2021-08-05 20:56:47+00:00
- **Authors**: Qi Wu, Cheng-Ju Wu, Yixin Zhu, Jungseock Joo
- **Comment**: To appear in IROS 2021
- **Journal**: None
- **Summary**: Human-robot collaboration is an essential research topic in artificial intelligence (AI), enabling researchers to devise cognitive AI systems and affords an intuitive means for users to interact with the robot. Of note, communication plays a central role. To date, prior studies in embodied agent navigation have only demonstrated that human languages facilitate communication by instructions in natural languages. Nevertheless, a plethora of other forms of communication is left unexplored. In fact, human communication originated in gestures and oftentimes is delivered through multimodal cues, e.g. "go there" with a pointing gesture. To bridge the gap and fill in the missing dimension of communication in embodied agent navigation, we propose investigating the effects of using gestures as the communicative interface instead of verbal cues. Specifically, we develop a VR-based 3D simulation environment, named Ges-THOR, based on AI2-THOR platform. In this virtual environment, a human player is placed in the same virtual scene and shepherds the artificial agent using only gestures. The agent is tasked to solve the navigation problem guided by natural gestures with unknown semantics; we do not use any predefined gestures due to the diversity and versatile nature of human gestures. We argue that learning the semantics of natural gestures is mutually beneficial to learning the navigation task--learn to communicate and communicate to learn. In a series of experiments, we demonstrate that human gesture cues, even without predefined semantics, improve the object-goal navigation for an embodied agent, outperforming various state-of-the-art methods.



### Hyperparameter Analysis for Derivative Compressive Sampling
- **Arxiv ID**: http://arxiv.org/abs/2108.04355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04355v1)
- **Published**: 2021-08-05 20:58:18+00:00
- **Updated**: 2021-08-05 20:58:18+00:00
- **Authors**: Md Fazle Rabbi
- **Comment**: None
- **Journal**: None
- **Summary**: Derivative compressive sampling (DCS) is a signal reconstruction method from measurements of the spatial gradient with sub-Nyquist sampling rate. Applications of DCS include optical image reconstruction, photometric stereo, and shape-from-shading. In this work, we study the sensitivity of DCS with respect to algorithmic hyperparameters using a brute-force search algorithm. We perform experiments on a dataset of surface images and deduce guidelines for the user to setup values for the hyperparameters for improved signal recovery performance.



### 3DRIMR: 3D Reconstruction and Imaging via mmWave Radar based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.02858v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02858v1)
- **Published**: 2021-08-05 21:24:57+00:00
- **Updated**: 2021-08-05 21:24:57+00:00
- **Authors**: Yue Sun, Zhuoming Huang, Honggang Zhang, Zhi Cao, Deqiang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: mmWave radar has been shown as an effective sensing technique in low visibility, smoke, dusty, and dense fog environment. However tapping the potential of radar sensing to reconstruct 3D object shapes remains a great challenge, due to the characteristics of radar data such as sparsity, low resolution, specularity, high noise, and multi-path induced shadow reflections and artifacts. In this paper we propose 3D Reconstruction and Imaging via mmWave Radar (3DRIMR), a deep learning based architecture that reconstructs 3D shape of an object in dense detailed point cloud format, based on sparse raw mmWave radar intensity data. The architecture consists of two back-to-back conditional GAN deep neural networks: the first generator network generates 2D depth images based on raw radar intensity data, and the second generator network outputs 3D point clouds based on the results of the first generator. The architecture exploits both convolutional neural network's convolutional operation (that extracts local structure neighborhood information) and the efficiency and detailed geometry capture capability of point clouds (other than costly voxelization of 3D space or distance fields). Our experiments have demonstrated 3DRIMR's effectiveness in reconstructing 3D objects, and its performance improvement over standard techniques.



### A Data Augmented Approach to Transfer Learning for Covid-19 Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.02870v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02870v1)
- **Published**: 2021-08-05 22:23:23+00:00
- **Updated**: 2021-08-05 22:23:23+00:00
- **Authors**: Shagufta Henna, Aparna Reji
- **Comment**: None
- **Journal**: None
- **Summary**: Covid-19 detection at an early stage can aid in an effective treatment and isolation plan to prevent its spread. Recently, transfer learning has been used for Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major limitations inherent to these proposed methods is limited labeled dataset size that affects the reliability of Covid-19 diagnosis and disease progression. In this work, we demonstrate that how we can augment limited X-ray images data by using Contrast limited adaptive histogram equalization (CLAHE) to train the last layer of the pre-trained deep learning models to mitigate the bias of transfer learning for Covid-19 detection. We transfer learned various pre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18, and GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset. The experiment results reveal that the CLAHE-based augmentation to various pre-trained deep learning models significantly improves the model efficiency. The pre-trained VCG-16 model with CLAHEbased augmented images achieves a sensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when trained on non-augmented data. Other models demonstrate a value of less than 60% when trained on non-augmented data. Our results reveal that the sample bias can negatively impact the performance of transfer learning which is significantly improved by using CLAHE-based augmentation.



### Disentangled Lifespan Face Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2108.02874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02874v2)
- **Published**: 2021-08-05 22:33:14+00:00
- **Updated**: 2021-08-13 11:24:24+00:00
- **Authors**: Sen He, Wentong Liao, Michael Ying Yang, Yi-Zhe Song, Bodo Rosenhahn, Tao Xiang
- **Comment**: To appear in ICCV 2021
- **Journal**: None
- **Summary**: A lifespan face synthesis (LFS) model aims to generate a set of photo-realistic face images of a person's whole life, given only one snapshot as reference. The generated face image given a target age code is expected to be age-sensitive reflected by bio-plausible transformations of shape and texture, while being identity preserving. This is extremely challenging because the shape and texture characteristics of a face undergo separate and highly nonlinear transformations w.r.t. age. Most recent LFS models are based on generative adversarial networks (GANs) whereby age code conditional transformations are applied to a latent face representation. They benefit greatly from the recent advancements of GANs. However, without explicitly disentangling their latent representations into the texture, shape and identity factors, they are fundamentally limited in modeling the nonlinear age-related transformation on texture and shape whilst preserving identity. In this work, a novel LFS model is proposed to disentangle the key face characteristics including shape, texture and identity so that the unique shape and texture age transformations can be modeled effectively. This is achieved by extracting shape, texture and identity features separately from an encoder. Critically, two transformation modules, one conditional convolution based and the other channel attention based, are designed for modeling the nonlinear shape and texture feature transformations respectively. This is to accommodate their rather distinct aging processes and ensure that our synthesized images are both age-sensitive and identity preserving. Extensive experiments show that our LFS model is clearly superior to the state-of-the-art alternatives. Codes and demo are available on our project website: \url{https://senhe.github.io/projects/iccv_2021_lifespan_face}.



### Out-of-Domain Generalization from a Single Source: An Uncertainty Quantification Approach
- **Arxiv ID**: http://arxiv.org/abs/2108.02888v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.02888v2)
- **Published**: 2021-08-05 23:53:55+00:00
- **Updated**: 2022-06-16 17:19:44+00:00
- **Authors**: Xi Peng, Fengchun Qiao, Long Zhao
- **Comment**: 13 pages, 11 figures, accepted by IEEE Transactions on Pattern
  Analysis and Machine Intelligence. arXiv admin note: substantial text overlap
  with arXiv:2003.13216
- **Journal**: None
- **Summary**: We are concerned with a worst-case scenario in model generalization, in the sense that a model aims to perform well on many unseen domains while there is only one single domain available for training. We propose Meta-Learning based Adversarial Domain Augmentation to solve this Out-of-Domain generalization problem. The key idea is to leverage adversarial training to create "fictitious" yet "challenging" populations, from which a model can learn to generalize with theoretical guarantees. To facilitate fast and desirable domain augmentation, we cast the model training in a meta-learning scheme and use a Wasserstein Auto-Encoder to relax the widely used worst-case constraint. We further improve our method by integrating uncertainty quantification for efficient domain generalization. Extensive experiments on multiple benchmark datasets indicate its superior performance in tackling single domain generalization.




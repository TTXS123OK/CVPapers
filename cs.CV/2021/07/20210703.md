# Arxiv Papers in cs.CV on 2021-07-03
### A study of CNN capacity applied to Left Venticle Segmentation in Cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/2107.01318v2
- **DOI**: 10.1007/s42979-021-00897-x
- **Categories**: **eess.IV**, cs.CV, cs.NE, 68T07, 92B20, I.2.6; I.5.1; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2107.01318v2)
- **Published**: 2021-07-03 00:56:21+00:00
- **Updated**: 2021-10-13 10:51:28+00:00
- **Authors**: Marcelo Toledo, Daniel Lima, José Krieger, Marco Gutierrez
- **Comment**: None
- **Journal**: SN COMPUT. SCI. 2, 480 (2021)
- **Summary**: CNN (Convolutional Neural Network) models have been successfully used for segmentation of the left ventricle (LV) in cardiac MRI (Magnetic Resonance Imaging), providing clinical measurements. In practice, two questions arise with deployment of CNNs: 1) when is it better to use a shallow model instead of a deeper one? 2) how the size of a dataset might change the network performance? We propose a framework to answer them, by experimenting with deep and shallow versions of three U-Net families, trained from scratch in six subsets varying from 100 to 10,000 images, different network sizes, learning rates and regularization values. 1620 models were evaluated using 5-fold cross-validation by loss and DICE. The results indicate that: sample size affects performance more than architecture or hyper-parameters; in small samples the performance is more sensitive to hyper-parameters than architecture; the performance difference between shallow and deeper networks is not the same across families.



### Learning Hierarchical Graph Neural Networks for Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/2107.01319v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.01319v2)
- **Published**: 2021-07-03 01:28:42+00:00
- **Updated**: 2021-07-17 14:50:19+00:00
- **Authors**: Yifan Xing, Tong He, Tianjun Xiao, Yongxin Wang, Yuanjun Xiong, Wei Xia, David Wipf, Zheng Zhang, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a hierarchical graph neural network (GNN) model that learns how to cluster a set of images into an unknown number of identities using a training set of images annotated with labels belonging to a disjoint set of identities. Our hierarchical GNN uses a novel approach to merge connected components predicted at each level of the hierarchy to form a new graph at the next level. Unlike fully unsupervised hierarchical clustering, the choice of grouping and complexity criteria stems naturally from supervision in the training set. The resulting method, Hi-LANDER, achieves an average of 54% improvement in F-score and 8% increase in Normalized Mutual Information (NMI) relative to current GNN-based clustering algorithms. Additionally, state-of-the-art GNN-based methods rely on separate models to predict linkage probabilities and node densities as intermediate steps of the clustering process. In contrast, our unified framework achieves a seven-fold decrease in computational cost. We release our training and inference code at https://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.



### VinDr-RibCXR: A Benchmark Dataset for Automatic Segmentation and Labeling of Individual Ribs on Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2107.01327v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.01327v1)
- **Published**: 2021-07-03 02:36:09+00:00
- **Updated**: 2021-07-03 02:36:09+00:00
- **Authors**: Hoang C. Nguyen, Tung T. Le, Hieu H. Pham, Ha Q. Nguyen
- **Comment**: This is a preprint of our paper, which was accepted for publication
  by Medical Imaging with Deep Learning (MIDL 2021)
- **Journal**: None
- **Summary**: We introduce a new benchmark dataset, namely VinDr-RibCXR, for automatic segmentation and labeling of individual ribs from chest X-ray (CXR) scans. The VinDr-RibCXR contains 245 CXRs with corresponding ground truth annotations provided by human experts. A set of state-of-the-art segmentation models are trained on 196 images from the VinDr-RibCXR to segment and label 20 individual ribs. Our best performing model obtains a Dice score of 0.834 (95% CI, 0.810--0.853) on an independent test set of 49 images. Our study, therefore, serves as a proof of concept and baseline performance for future research.



### SPI-GAN: Towards Single-Pixel Imaging through Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2107.01330v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2107.01330v1)
- **Published**: 2021-07-03 03:06:09+00:00
- **Updated**: 2021-07-03 03:06:09+00:00
- **Authors**: Nazmul Karim, Nazanin Rahnavard
- **Comment**: None
- **Journal**: None
- **Summary**: Single-pixel imaging is a novel imaging scheme that has gained popularity due to its huge computational gain and potential for a low-cost alternative to imaging beyond the visible spectrum. The traditional reconstruction methods struggle to produce a clear recovery when one limits the number of illumination patterns from a spatial light modulator. As a remedy, several deep-learning-based solutions have been proposed which lack good generalization ability due to the architectural setup and loss functions. In this paper, we propose a generative adversarial network-based reconstruction framework for single-pixel imaging, referred to as SPI-GAN. Our method can reconstruct images with 17.92 dB PSNR and 0.487 SSIM, even if the sampling ratio drops to 5%. This facilitates much faster reconstruction making our method suitable for single-pixel video. Furthermore, our ResNet-like architecture for the generator leads to useful representation learning that allows us to reconstruct completely unseen objects. The experimental results demonstrate that SPI-GAN achieves significant performance gain, e.g. near 3dB PSNR gain, over the current state-of-the-art method.



### CT Image Harmonization for Enhancing Radiomics Studies
- **Arxiv ID**: http://arxiv.org/abs/2107.01337v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.01337v1)
- **Published**: 2021-07-03 04:03:42+00:00
- **Updated**: 2021-07-03 04:03:42+00:00
- **Authors**: Md Selim, Jie Zhang, Baowei Fei, Guo-Qiang Zhang, Jin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: While remarkable advances have been made in Computed Tomography (CT), capturing CT images with non-standardized protocols causes low reproducibility regarding radiomic features, forming a barrier on CT image analysis in a large scale. RadiomicGAN is developed to effectively mitigate the discrepancy caused by using non-standard reconstruction kernels. RadiomicGAN consists of hybrid neural blocks including both pre-trained and trainable layers adopted to learn radiomic feature distributions efficiently. A novel training approach, called Dynamic Window-based Training, has been developed to smoothly transform the pre-trained model to the medical imaging domain. Model performance evaluated using 1401 radiomic features show that RadiomicGAN clearly outperforms the state-of-art image standardization models.



### Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2107.01349v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.01349v1)
- **Published**: 2021-07-03 05:51:53+00:00
- **Updated**: 2021-07-03 05:51:53+00:00
- **Authors**: Jong-Yeong Kim, Dong-Wan Choi
- **Comment**: In AAAI-2021
- **Journal**: In Proceedings of the AAAI Conference on Artificial Intelligence
  (Vol. 35, No. 9, pp. 8137-8145) 2021
- **Summary**: Continual learning has been a major problem in the deep learning community, where the main challenge is how to effectively learn a series of newly arriving tasks without forgetting the knowledge of previous tasks. Initiated by Learning without Forgetting (LwF), many of the existing works report that knowledge distillation is effective to preserve the previous knowledge, and hence they commonly use a soft label for the old task, namely a knowledge distillation (KD) loss, together with a class label for the new task, namely a cross entropy (CE) loss, to form a composite loss for a single neural network. However, this approach suffers from learning the knowledge by a CE loss as a KD loss often more strongly influences the objective function when they are in a competitive situation within a single network. This could be a critical problem particularly in a class incremental scenario, where the knowledge across tasks as well as within the new task, both of which can only be acquired by a CE loss, is essentially learned due to the existence of a unified classifier. In this paper, we propose a novel continual learning method, called Split-and-Bridge, which can successfully address the above problem by partially splitting a neural network into two partitions for training the new task separated from the old task and re-connecting them for learning the knowledge across tasks. In our thorough experimental analysis, our Split-and-Bridge method outperforms the state-of-the-art competitors in KD-based continual learning.



### EAR-NET: Error Attention Refining Network For Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.01351v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.01351v2)
- **Published**: 2021-07-03 06:03:46+00:00
- **Updated**: 2021-09-22 05:02:56+00:00
- **Authors**: Jun Wang, Yang Zhao, Linglong Qian, Xiaohan Yu, Yongsheng Gao
- **Comment**: Accepted to DICTA2021
- **Journal**: None
- **Summary**: The precise detection of blood vessels in retinal images is crucial to the early diagnosis of the retinal vascular diseases, e.g., diabetic, hypertensive and solar retinopathies. Existing works often fail in predicting the abnormal areas, e.g, sudden brighter and darker areas and are inclined to predict a pixel to background due to the significant class imbalance, leading to high accuracy and specificity while low sensitivity. To that end, we propose a novel error attention refining network (ERA-Net) that is capable of learning and predicting the potential false predictions in a two-stage manner for effective retinal vessel segmentation. The proposed ERA-Net in the refine stage drives the model to focus on and refine the segmentation errors produced in the initial training stage. To achieve this, unlike most previous attention approaches that run in an unsupervised manner, we introduce a novel error attention mechanism which considers the differences between the ground truth and the initial segmentation masks as the ground truth to supervise the attention map learning. Experimental results demonstrate that our method achieves state-of-the-art performance on two common retinal blood vessel datasets.



### CInC Flow: Characterizable Invertible 3x3 Convolution
- **Arxiv ID**: http://arxiv.org/abs/2107.01358v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.01358v1)
- **Published**: 2021-07-03 06:55:24+00:00
- **Updated**: 2021-07-03 06:55:24+00:00
- **Authors**: Sandeep Nagar, Marius Dufraisse, Girish Varma
- **Comment**: Accepted for the 4th Workshop on Tractable Probabilistic
  Modeling,(UAI 2021)
- **Journal**: The 4th Workshop on Tractable Probabilistic Modeling, 2021.
  https://openreview.net/forum?id=kl1ds_AeLRM
- **Summary**: Normalizing flows are an essential alternative to GANs for generative modelling, which can be optimized directly on the maximum likelihood of the dataset. They also allow computation of the exact latent vector corresponding to an image since they are composed of invertible transformations. However, the requirement of invertibility of the transformation prevents standard and expressive neural network models such as CNNs from being directly used. Emergent convolutions were proposed to construct an invertible 3$\times$3 CNN layer using a pair of masked CNN layers, making them inefficient. We study conditions such that 3$\times$3 CNNs are invertible, allowing them to construct expressive normalizing flows. We derive necessary and sufficient conditions on a padded CNN for it to be invertible. Our conditions for invertibility are simple, can easily be maintained during the training process. Since we require only a single CNN layer for every effective invertible CNN layer, our approach is more efficient than emerging convolutions. We also proposed a coupling method, Quad-coupling. We benchmark our approach and show similar performance results to emergent convolutions while improving the model's efficiency.



### Sensor-invariant Fingerprint ROI Segmentation Using Recurrent Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.01361v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.01361v1)
- **Published**: 2021-07-03 07:16:39+00:00
- **Updated**: 2021-07-03 07:16:39+00:00
- **Authors**: Indu Joshi, Ayush Utkarsh, Riya Kothari, Vinod K Kurmi, Antitza Dantcheva, Sumantra Dutta Roy, Prem Kumar Kalra
- **Comment**: IJCNN 2021 (Accepted)
- **Journal**: IJCNN 2021
- **Summary**: A fingerprint region of interest (roi) segmentation algorithm is designed to separate the foreground fingerprint from the background noise. All the learning based state-of-the-art fingerprint roi segmentation algorithms proposed in the literature are benchmarked on scenarios when both training and testing databases consist of fingerprint images acquired from the same sensors. However, when testing is conducted on a different sensor, the segmentation performance obtained is often unsatisfactory. As a result, every time a new fingerprint sensor is used for testing, the fingerprint roi segmentation model needs to be re-trained with the fingerprint image acquired from the new sensor and its corresponding manually marked ROI. Manually marking fingerprint ROI is expensive because firstly, it is time consuming and more importantly, requires domain expertise. In order to save the human effort in generating annotations required by state-of-the-art, we propose a fingerprint roi segmentation model which aligns the features of fingerprint images derived from the unseen sensor such that they are similar to the ones obtained from the fingerprints whose ground truth roi masks are available for training. Specifically, we propose a recurrent adversarial learning based feature alignment network that helps the fingerprint roi segmentation model to learn sensor-invariant features. Consequently, sensor-invariant features learnt by the proposed roi segmentation model help it to achieve improved segmentation performance on fingerprints acquired from the new sensor. Experiments on publicly available FVC databases demonstrate the efficacy of the proposed work.



### Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation
- **Arxiv ID**: http://arxiv.org/abs/2107.01378v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01378v4)
- **Published**: 2021-07-03 08:28:34+00:00
- **Updated**: 2022-06-02 12:16:28+00:00
- **Authors**: Zhiwei Hao, Jianyuan Guo, Ding Jia, Kai Han, Yehui Tang, Chao Zhang, Han Hu, Yunhe Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In the past few years, transformers have achieved promising performances on various computer vision tasks. Unfortunately, the immense inference overhead of most existing vision transformers withholds their from being deployed on edge devices such as cell phones and smart watches. Knowledge distillation is a widely used paradigm for compressing cumbersome architectures via transferring information to a compact student. However, most of them are designed for convolutional neural networks (CNNs), which do not fully investigate the character of vision transformer (ViT). In this paper, we utilize the patch-level information and propose a fine-grained manifold distillation method. Specifically, we train a tiny student model to match a pre-trained teacher model in the patch-level manifold space. Then, we decouple the manifold matching loss into three terms with careful design to further reduce the computational costs for the patch relationship. Equipped with the proposed method, a DeiT-Tiny model containing 5M parameters achieves 76.5% top-1 accuracy on ImageNet-1k, which is +2.0% higher than previous distillation approaches. Transfer learning results on other classification benchmarks and downstream vision tasks also demonstrate the superiority of our method over the state-of-the-art algorithms.



### Person-MinkUNet: 3D Person Detection with LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2107.06780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.06780v1)
- **Published**: 2021-07-03 09:41:53+00:00
- **Updated**: 2021-07-03 09:41:53+00:00
- **Authors**: Dan Jia, Bastian Leibe
- **Comment**: accepted as an extended abstract in JRDB-ACT Workshop at CVPR21
- **Journal**: None
- **Summary**: In this preliminary work we attempt to apply submanifold sparse convolution to the task of 3D person detection. In particular, we present Person-MinkUNet, a single-stage 3D person detection network based on Minkowski Engine with U-Net architecture. The network achieves a 76.4% average precision (AP) on the JRDB 3D detection benchmark.



### WisdomNet: Prognosis of COVID-19 with Slender Prospect of False Negative Cases and Vaticinating the Probability of Maturation to ARDS using Posteroanterior Chest X-Rays
- **Arxiv ID**: http://arxiv.org/abs/2107.01392v1
- **DOI**: 10.22207/JPAM.14.SPL1.24
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, 68T07 (Primary), 68T10 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2107.01392v1)
- **Published**: 2021-07-03 09:55:28+00:00
- **Updated**: 2021-07-03 09:55:28+00:00
- **Authors**: Peeyush Kumar, Ayushe Gangal, Sunita Kumari
- **Comment**: 10 pages, 4 figures, 1 table
- **Journal**: J Pure Appl Microbiol. 2020;14(suppl 1):869-878, Article Number:
  6236
- **Summary**: Coronavirus is a large virus family consisting of diverse viruses, some of which disseminate among mammals and others cause sickness among humans. COVID-19 is highly contagious and is rapidly spreading, rendering its early diagnosis of preeminent status. Researchers, medical specialists and organizations all over the globe have been working tirelessly to combat this virus and help in its containment. In this paper, a novel neural network called WisdomNet has been proposed, for the diagnosis of COVID-19 using chest X-rays. The WisdomNet uses the concept of Wisdom of Crowds as its founding idea. It is a two-layered convolutional Neural Network (CNN), which takes chest x-ray images as input. Both layers of the proposed neural network consist of a number of neural networks each. The dataset used for this study consists of chest x-ray images of COVID-19 positive patients, compiled and shared by Dr. Cohen on GitHub, and the chest x-ray images of healthy lungs and lungs affected by viral and bacterial pneumonia were obtained from Kaggle. The network not only pinpoints the presence of COVID-19, but also gives the probability of the disease maturing into Acute Respiratory Distress Syndrome (ARDS). Thus, predicting the progression of the disease in the COVID-19 positive patients. The network also slender the occurrences of false negative cases by employing a high threshold value, thus aids in curbing the spread of the disease and gives an accuracy of 100% for successfully predicting COVID-19 among the chest x-rays of patients affected with COVID-19, bacterial and viral pneumonia.



### Demiguise Attack: Crafting Invisible Semantic Adversarial Perturbations with Perceptual Similarity
- **Arxiv ID**: http://arxiv.org/abs/2107.01396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.01396v1)
- **Published**: 2021-07-03 10:14:01+00:00
- **Updated**: 2021-07-03 10:14:01+00:00
- **Authors**: Yajie Wang, Shangbo Wu, Wenyi Jiang, Shengang Hao, Yu-an Tan, Quanxin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples. Adversarial examples are malicious images with visually imperceptible perturbations. While these carefully crafted perturbations restricted with tight $\Lp$ norm bounds are small, they are still easily perceivable by humans. These perturbations also have limited success rates when attacking black-box models or models with defenses like noise reduction filters. To solve these problems, we propose Demiguise Attack, crafting ``unrestricted'' perturbations with Perceptual Similarity. Specifically, we can create powerful and photorealistic adversarial examples by manipulating semantic information based on Perceptual Similarity. Adversarial examples we generate are friendly to the human visual system (HVS), although the perturbations are of large magnitudes. We extend widely-used attacks with our approach, enhancing adversarial effectiveness impressively while contributing to imperceptibility. Extensive experiments show that the proposed method not only outperforms various state-of-the-art attacks in terms of fooling rate, transferability, and robustness against defenses but can also improve attacks effectively. In addition, we also notice that our implementation can simulate illumination and contrast changes that occur in real-world scenarios, which will contribute to exposing the blind spots of DNNs.



### Learning from scarce information: using synthetic data to classify Roman fine ware pottery
- **Arxiv ID**: http://arxiv.org/abs/2107.01401v1
- **DOI**: 10.3390/e23091140
- **Categories**: **cs.CV**, 68T07, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2107.01401v1)
- **Published**: 2021-07-03 10:30:46+00:00
- **Updated**: 2021-07-03 10:30:46+00:00
- **Authors**: Santos J. Núñez Jareño, Daniël P. van Helden, Evgeny M. Mirkes, Ivan Y. Tyukin, Penelope M. Allison
- **Comment**: None
- **Journal**: None
- **Summary**: In this article we consider a version of the challenging problem of learning from datasets whose size is too limited to allow generalisation beyond the training set. To address the challenge we propose to use a transfer learning approach whereby the model is first trained on a synthetic dataset replicating features of the original objects. In this study the objects were smartphone photographs of near-complete Roman terra sigillata pottery vessels from the collection of the Museum of London. Taking the replicated features from published profile drawings of pottery forms allowed the integration of expert knowledge into the process through our synthetic data generator. After this first initial training the model was fine-tuned with data from photographs of real vessels. We show, through exhaustive experiments across several popular deep learning architectures, different test priors, and considering the impact of the photograph viewpoint and excessive damage to the vessels, that the proposed hybrid approach enables the creation of classifiers with appropriate generalisation performance. This performance is significantly better than that of classifiers trained exclusively on the original data which shows the promise of the approach to alleviate the fundamental issue of learning from small datasets.



### Imaging dynamics beneath turbid media via parallelized single-photon detection
- **Arxiv ID**: http://arxiv.org/abs/2107.01422v4
- **DOI**: None
- **Categories**: **physics.optics**, cs.CV, eess.IV, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2107.01422v4)
- **Published**: 2021-07-03 12:32:21+00:00
- **Updated**: 2022-06-13 00:08:05+00:00
- **Authors**: Shiqi Xu, Xi Yang, Wenhui Liu, Joakim Jonsson, Ruobing Qian, Pavan Chandra Konda, Kevin C. Zhou, Lucas Kreiss, Qionghai Dai, Haoqian Wang, Edouard Berrocal, Roarke Horstmeyer
- **Comment**: None
- **Journal**: None
- **Summary**: Noninvasive optical imaging through dynamic scattering media has numerous important biomedical applications but still remains a challenging task. While standard diffuse imaging methods measure optical absorption or fluorescent emission, it is also well-established that the temporal correlation of scattered coherent light diffuses through tissue much like optical intensity. Few works to date, however, have aimed to experimentally measure and process such temporal correlation data to demonstrate deep-tissue video reconstruction of decorrelation dynamics. In this work, we utilize a single-photon avalanche diode (SPAD) array camera to simultaneously monitor the temporal dynamics of speckle fluctuations at the single-photon level from 12 different phantom tissue surface locations delivered via a customized fiber bundle array. We then apply a deep neural network to convert the acquired single-photon measurements into video of scattering dynamics beneath rapidly decorrelating tissue phantoms. We demonstrate the ability to reconstruct images of transient (0.1-0.4s) dynamic events occurring up to 8 mm beneath a decorrelating tissue phantom with millimeter-scale resolution, and highlight how our model can flexibly extend to monitor flow speed within buried phantom vessels.



### Drone Detection Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.01435v1
- **DOI**: 10.1109/ICSPIS51611.2020.9349620
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01435v1)
- **Published**: 2021-07-03 13:26:06+00:00
- **Updated**: 2021-07-03 13:26:06+00:00
- **Authors**: Fatemeh Mahdavi, Roozbeh Rajabi
- **Comment**: 5 pages, conference
- **Journal**: None
- **Summary**: In image processing, it is essential to detect and track air targets, especially UAVs. In this paper, we detect the flying drone using a fisheye camera. In the field of diagnosis and classification of objects, there are always many problems that prevent the development of rapid and significant progress in this area. During the previous decades, a couple of advanced classification methods such as convolutional neural networks and support vector machines have been developed. In this study, the drone was detected using three methods of classification of convolutional neural network (CNN), support vector machine (SVM), and nearest neighbor. The outcomes show that CNN, SVM, and nearest neighbor have total accuracy of 95%, 88%, and 80%, respectively. Compared with other classifiers with the same experimental conditions, the accuracy of the convolutional neural network classifier is satisfactory.



### Custom Deep Neural Network for 3D Covid Chest CT-scan Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.01456v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.01456v1)
- **Published**: 2021-07-03 15:54:38+00:00
- **Updated**: 2021-07-03 15:54:38+00:00
- **Authors**: Quoc Huy Trinh, Minh Van Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: 3D CT-scan base on chest is one of the controversial topisc of the researcher nowadays. There are many tasks to diagnose the disease through CT-scan images, include Covid19. In this paper, we propose a method that custom and combine Deep Neural Network to classify the series of 3D CT-scans chest images. In our methods, we experiment with 2 backbones is DenseNet 121 and ResNet 101. In this proposal, we separate the experiment into 2 tasks, one is for 2 backbones combination of ResNet and DenseNet, one is for DenseNet backbones combination.



### Scene-aware Learning Network for Radar Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.01469v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01469v1)
- **Published**: 2021-07-03 17:19:56+00:00
- **Updated**: 2021-07-03 17:19:56+00:00
- **Authors**: Zangwei Zheng, Xiangyu Yue, Kurt Keutzer, Alberto Sangiovanni Vincentelli
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is essential to safe autonomous or assisted driving. Previous works usually utilize RGB images or LiDAR point clouds to identify and localize multiple objects in self-driving. However, cameras tend to fail in bad driving conditions, e.g. bad weather or weak lighting, while LiDAR scanners are too expensive to get widely deployed in commercial applications. Radar has been drawing more and more attention due to its robustness and low cost. In this paper, we propose a scene-aware radar learning framework for accurate and robust object detection. First, the learning framework contains branches conditioning on the scene category of the radar sequence; with each branch optimized for a specific type of scene. Second, three different 3D autoencoder-based architectures are proposed for radar object detection and ensemble learning is performed over the different architectures to further boost the final performance. Third, we propose novel scene-aware sequence mix augmentation (SceneMix) and scene-specific post-processing to generate more robust detection results. In the ROD2021 Challenge, we achieved a final result of average precision of 75.0% and an average recall of 81.0%. Moreover, in the parking lot scene, our framework ranks first with an average precision of 97.8% and an average recall of 98.6%, which demonstrates the effectiveness of our framework.



### A Generalized Lottery Ticket Hypothesis
- **Arxiv ID**: http://arxiv.org/abs/2107.06825v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68T05, I.2.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2107.06825v2)
- **Published**: 2021-07-03 20:01:24+00:00
- **Updated**: 2021-07-26 09:28:51+00:00
- **Authors**: Ibrahim Alabdulmohsin, Larisa Markeeva, Daniel Keysers, Ilya Tolstikhin
- **Comment**: Workshop on Sparsity in Neural Networks: Advancing Understanding and
  Practice (SNN'21). Updates: New curve on Figure 2(left) and discussion on Li
  et al
- **Journal**: None
- **Summary**: We introduce a generalization to the lottery ticket hypothesis in which the notion of "sparsity" is relaxed by choosing an arbitrary basis in the space of parameters. We present evidence that the original results reported for the canonical basis continue to hold in this broader setting. We describe how structured pruning methods, including pruning units or factorizing fully-connected layers into products of low-rank matrices, can be cast as particular instances of this "generalized" lottery ticket hypothesis. The investigations reported here are preliminary and are provided to encourage further research along this direction.



### Pulmonary Vessel Segmentation based on Orthogonal Fused U-Net++ of Chest CT Images
- **Arxiv ID**: http://arxiv.org/abs/2107.01502v1
- **DOI**: 10.1007/978-3-030-32226-7_33
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T45, 68T07, I.2.10; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2107.01502v1)
- **Published**: 2021-07-03 21:46:29+00:00
- **Updated**: 2021-07-03 21:46:29+00:00
- **Authors**: Hejie Cui, Xinglong Liu, Ning Huang
- **Comment**: Published in Medical Image Computing and Computer Assisted
  Intervention (MICCAI 2019)
- **Journal**: None
- **Summary**: Pulmonary vessel segmentation is important for clinical diagnosis of pulmonary diseases, while is also challenging due to the complicated structure. In this work, we present an effective framework and refinement process of pulmonary vessel segmentation from chest computed tomographic (CT) images. The key to our approach is a 2.5D segmentation network applied from three orthogonal axes, which presents a robust and fully automated pulmonary vessel segmentation result with lower network complexity and memory usage compared to 3D networks. The slice radius is introduced to convolve the adjacent information of the center slice and the multi-planar fusion optimizes the presentation of intra- and inter- slice features. Besides, the tree-like structure of the pulmonary vessel is extracted in the post-processing process, which is used for segmentation refining and pruning. In the evaluation experiments, three fusion methods are tested and the most promising one is compared with the state-of-the-art 2D and 3D structures on 300 cases of lung images randomly selected from LIDC dataset. Our method outperforms other network structures by a large margin and achieves by far the highest average DICE score of 0.9272 and precision of 0.9310, as per our knowledge from the pulmonary vessel segmentation models available in the literature.




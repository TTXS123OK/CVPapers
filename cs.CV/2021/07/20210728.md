# Arxiv Papers in cs.CV on 2021-07-28
### Experimenting with Self-Supervision using Rotation Prediction for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2107.13111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13111v1)
- **Published**: 2021-07-28 00:46:27+00:00
- **Updated**: 2021-07-28 00:46:27+00:00
- **Authors**: Ahmed Elhagry, Karima Kadaoui
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning is a task in the field of Artificial Intelligence that merges between computer vision and natural language processing. It is responsible for generating legends that describe images, and has various applications like descriptions used by assistive technology or indexing images (for search engines for instance). This makes it a crucial topic in AI that is undergoing a lot of research. This task however, like many others, is trained on large images labeled via human annotation, which can be very cumbersome: it needs manual effort, both financial and temporal costs, it is error-prone and potentially difficult to execute in some cases (e.g. medical images). To mitigate the need for labels, we attempt to use self-supervised learning, a type of learning where models use the data contained within the images themselves as labels. It is challenging to accomplish though, since the task is two-fold: the images and captions come from two different modalities and usually handled by different types of networks. It is thus not obvious what a completely self-supervised solution would look like. How it would achieve captioning in a comparable way to how self-supervision is applied today on image recognition tasks is still an ongoing research topic. In this project, we are using an encoder-decoder architecture where the encoder is a convolutional neural network (CNN) trained on OpenImages dataset and learns image features in a self-supervised fashion using the rotation pretext task. The decoder is a Long Short-Term Memory (LSTM), and it is trained, along within the image captioning model, on MS COCO dataset and is responsible of generating captions. Our GitHub repository can be found: https://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction



### A Thorough Review on Recent Deep Learning Methodologies for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2107.13114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13114v1)
- **Published**: 2021-07-28 00:54:59+00:00
- **Updated**: 2021-07-28 00:54:59+00:00
- **Authors**: Ahmed Elhagry, Karima Kadaoui
- **Comment**: None
- **Journal**: None
- **Summary**: Image Captioning is a task that combines computer vision and natural language processing, where it aims to generate descriptive legends for images. It is a two-fold process relying on accurate image understanding and correct language understanding both syntactically and semantically. It is becoming increasingly difficult to keep up with the latest research and findings in the field of image captioning due to the growing amount of knowledge available on the topic. There is not, however, enough coverage of those findings in the available review papers. We perform in this paper a run-through of the current techniques, datasets, benchmarks and evaluation metrics used in image captioning. The current research on the field is mostly focused on deep learning-based methods, where attention mechanisms along with deep reinforcement and adversarial learning appear to be in the forefront of this research topic. In this paper, we review recent methodologies such as UpDown, OSCAR, VIVO, Meta Learning and a model that uses conditional generative adversarial nets. Although the GAN-based model achieves the highest score, UpDown represents an important basis for image captioning and OSCAR and VIVO are more useful as they use novel object captioning. This review paper serves as a roadmap for researchers to keep up to date with the latest contributions made in the field of image caption generation.



### Image color correction, enhancement, and editing
- **Arxiv ID**: http://arxiv.org/abs/2107.13117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13117v1)
- **Published**: 2021-07-28 01:14:12+00:00
- **Updated**: 2021-07-28 01:14:12+00:00
- **Authors**: Mahmoud Afifi
- **Comment**: PhD dissertation
- **Journal**: None
- **Summary**: This thesis presents methods and approaches to image color correction, color enhancement, and color editing. To begin, we study the color correction problem from the standpoint of the camera's image signal processor (ISP). A camera's ISP is hardware that applies a series of in-camera image processing and color manipulation steps, many of which are nonlinear in nature, to render the initial sensor image to its final photo-finished representation saved in the 8-bit standard RGB (sRGB) color space. As white balance (WB) is one of the major procedures applied by the ISP for color correction, this thesis presents two different methods for ISP white balancing. Afterward, we discuss another scenario of correcting and editing image colors, where we present a set of methods to correct and edit WB settings for images that have been improperly white-balanced by the ISP. Then, we explore another factor that has a significant impact on the quality of camera-rendered colors, in which we outline two different methods to correct exposure errors in camera-rendered images. Lastly, we discuss post-capture auto color editing and manipulation. In particular, we propose auto image recoloring methods to generate different realistic versions of the same camera-rendered image with new colors. Through extensive evaluations, we demonstrate that our methods provide superior solutions compared to existing alternatives targeting color correction, color enhancement, and color editing.



### Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.13118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13118v1)
- **Published**: 2021-07-28 01:14:32+00:00
- **Updated**: 2021-07-28 01:14:32+00:00
- **Authors**: Jinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, Hong Zhou
- **Comment**: accepted by ICCV 2021
- **Journal**: None
- **Summary**: Reconstruction-based methods play an important role in unsupervised anomaly detection in images. Ideally, we expect a perfect reconstruction for normal samples and poor reconstruction for abnormal samples. Since the generalizability of deep neural networks is difficult to control, existing models such as autoencoder do not work well. In this work, we interpret the reconstruction of an image as a divide-and-assemble procedure. Surprisingly, by varying the granularity of division on feature maps, we are able to modulate the reconstruction capability of the model for both normal and abnormal samples. That is, finer granularity leads to better reconstruction, while coarser granularity leads to poorer reconstruction. With proper granularity, the gap between the reconstruction error of normal and abnormal samples can be maximized. The divide-and-assemble framework is implemented by embedding a novel multi-scale block-wise memory module into an autoencoder network. Besides, we introduce adversarial learning and explore the semantic latent representation of the discriminator, which improves the detection of subtle anomaly. We achieve state-of-the-art performance on the challenging MVTec AD dataset. Remarkably, we improve the vanilla autoencoder model by 10.1% in terms of the AUROC score.



### "Excavating AI" Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset
- **Arxiv ID**: http://arxiv.org/abs/2107.13998v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.AI, cs.CV, cs.LG, 68T01, K.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2107.13998v1)
- **Published**: 2021-07-28 01:31:59+00:00
- **Updated**: 2021-07-28 01:31:59+00:00
- **Authors**: Michael J. Lyons
- **Comment**: 20 pages, 4 figures
- **Journal**: None
- **Summary**: Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I designed and photographed JAFFE, a set of facial expression images intended for use in a study of face perception. In 2019, without seeking permission or informing us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely publicized art shows. In addition, they published a nonfactual account of the images in the essay "Excavating AI: The Politics of Images in Machine Learning Training Sets." The present article recounts the creation of the JAFFE dataset and unravels each of Crawford and Paglen's fallacious statements. I also discuss JAFFE more broadly in connection with research on facial expression, affective computing, and human-computer interaction.



### Subjective evaluation of traditional and learning-based image coding methods
- **Arxiv ID**: http://arxiv.org/abs/2107.13122v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.13122v1)
- **Published**: 2021-07-28 01:37:13+00:00
- **Updated**: 2021-07-28 01:37:13+00:00
- **Authors**: Zhigao Fang, Jiaqi Zhang, Lu Yu, Yin Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: We conduct a subjective experiment to compare the performance of traditional image coding methods and learning-based image coding methods. HEVC and VVC, the state-of-the-art traditional coding methods, are used as the representative traditional methods. The learning-based methods used contain not only CNN-based methods, but also a GAN-based method, all of which are advanced or typical. Single Stimuli (SS), which is also called Absolute Category Rating (ACR), is adopted as the methodology of the experiment to obtain perceptual quality of images. Additionally, we utilize some typical and frequently used objective quality metrics to evaluate the coding methods in the experiment as comparison. The experiment shows that CNN-based and GAN-based methods can perform better than traditional methods in low bit-rates. In high bit-rates, however, it is hard to verify whether CNN-based methods are superior to traditional methods. Because the GAN method does not provide models with high target bit-rates, we cannot exactly tell the performance of the GAN method in high bit-rates. Furthermore, some popular objective quality metrics have not shown the ability well to measure quality of images generated by learning-based coding methods, especially the GAN-based one.



### Insights from Generative Modeling for Neural Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2107.13136v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.13136v2)
- **Published**: 2021-07-28 02:19:39+00:00
- **Updated**: 2023-07-09 23:05:59+00:00
- **Authors**: Ruihan Yang, Yibo Yang, Joseph Marino, Stephan Mandt
- **Comment**: This work has been submitted to the IEEE for publication as an
  extension work of arXiv:2010.10258. Copyright may be transferred without
  notice, after which this version may no longer be accessible. arXiv admin
  note: text overlap with arXiv:2010.10258
- **Journal**: None
- **Summary**: While recent machine learning research has revealed connections between deep generative models such as VAEs and rate-distortion losses used in learned compression, most of this work has focused on images. In a similar spirit, we view recently proposed neural video coding algorithms through the lens of deep autoregressive and latent variable modeling. We present these codecs as instances of a generalized stochastic temporal autoregressive transform, and propose new avenues for further improvements inspired by normalizing flows and structured priors. We propose several architectures that yield state-of-the-art video compression performance on high-resolution video and discuss their tradeoffs and ablations. In particular, we propose (i) improved temporal autoregressive transforms, (ii) improved entropy models with structured and temporal dependencies, and (iii) variable bitrate versions of our algorithms. Since our improvements are compatible with a large class of existing models, we provide further evidence that the generative modeling viewpoint can advance the neural video coding field.



### Unsupervised Monocular Depth Estimation in Highly Complex Environments
- **Arxiv ID**: http://arxiv.org/abs/2107.13137v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13137v2)
- **Published**: 2021-07-28 02:35:38+00:00
- **Updated**: 2022-06-09 10:19:01+00:00
- **Authors**: Chaoqiang Zhao, Yang Tang, Qiyu Sun
- **Comment**: Accepted by IEEE Transactions on Emerging Topics in Computational
  Intelligence
- **Journal**: None
- **Summary**: With the development of computational intelligence algorithms, unsupervised monocular depth and pose estimation framework, which is driven by warped photometric consistency, has shown great performance in the daytime scenario. While in some challenging environments, like night and rainy night, the essential photometric consistency hypothesis is untenable because of the complex lighting and reflection, so that the above unsupervised framework cannot be directly applied to these complex scenarios. In this paper, we investigate the problem of unsupervised monocular depth estimation in highly complex scenarios and address this challenging problem by adopting an image transfer-based domain adaptation framework. We adapt the depth model trained on day-time scenarios to be applicable to night-time scenarios, and constraints on both feature space and output space promote the framework to learn the key features for depth decoding. Meanwhile, we further tackle the effects of unstable image transfer quality on domain adaptation, and an image adaptation approach is proposed to evaluate the quality of transferred images and re-weight the corresponding losses, so as to improve the performance of the adapted depth model. Extensive experiments show the effectiveness of the proposed unsupervised framework in estimating the dense depth map from highly complex images.



### Content-aware Directed Propagation Network with Pixel Adaptive Kernel Attention
- **Arxiv ID**: http://arxiv.org/abs/2107.13144v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.13144v3)
- **Published**: 2021-07-28 02:59:19+00:00
- **Updated**: 2022-09-14 01:16:34+00:00
- **Authors**: Min-Cheol Sagong, Yoon-Jae Yeo, Seung-Won Jung, Sung-Jea Ko
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been not only widespread but also achieved noticeable results on numerous applications including image classification, restoration, and generation. Although the weight-sharing property of convolutions makes them widely adopted in various tasks, its content-agnostic characteristic can also be considered a major drawback. To solve this problem, in this paper, we propose a novel operation, called pixel adaptive kernel attention (PAKA). PAKA provides directivity to the filter weights by multiplying spatially varying attention from learnable features. The proposed method infers pixel-adaptive attention maps along the channel and spatial directions separately to address the decomposed model with fewer parameters. Our method is trainable in an end-to-end manner and applicable to any CNN-based models. In addition, we propose an improved information aggregation module with PAKA, called the hierarchical PAKA module (HPM). We demonstrate the superiority of our HPM by presenting state-of-the-art performance on semantic segmentation compared to the conventional information aggregation modules. We validate the proposed method through additional ablation studies and visualizing the effect of PAKA providing directivity to the weights of convolutions. We also show the generalizability of the proposed method by applying it to multi-modal tasks especially color-guided depth map super-resolution.



### Multi Point-Voxel Convolution (MPVConv) for Deep Learning on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2107.13152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13152v1)
- **Published**: 2021-07-28 03:42:59+00:00
- **Updated**: 2021-07-28 03:42:59+00:00
- **Authors**: Wei Zhou, Xin Cao, Xiaodan Zhang, Xingxing Hao, Dekui Wang, Ying He
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2104.14834
- **Journal**: None
- **Summary**: The existing 3D deep learning methods adopt either individual point-based features or local-neighboring voxel-based features, and demonstrate great potential for processing 3D data. However, the point based models are inefficient due to the unordered nature of point clouds and the voxel-based models suffer from large information loss. Motivated by the success of recent point-voxel representation, such as PVCNN, we propose a new convolutional neural network, called Multi Point-Voxel Convolution (MPVConv), for deep learning on point clouds. Integrating both the advantages of voxel and point-based methods, MPVConv can effectively increase the neighboring collection between point-based features and also promote independence among voxel-based features. Moreover, most of the existing approaches aim at solving one specific task, and only a few of them can handle a variety of tasks. Simply replacing the corresponding convolution module with MPVConv, we show that MPVConv can fit in different backbones to solve a wide range of 3D tasks. Extensive experiments on benchmark datasets such as ShapeNet Part, S3DIS and KITTI for various tasks show that MPVConv improves the accuracy of the backbone (PointNet) by up to \textbf{36\%}, and achieves higher accuracy than the voxel-based model with up to \textbf{34}$\times$ speedups. In addition, MPVConv outperforms the state-of-the-art point-based models with up to \textbf{8}$\times$ speedups. Notably, our MPVConv achieves better accuracy than the newest point-voxel-based model PVCNN (a model more efficient than PointNet) with lower latency.



### Global Aggregation then Local Distribution for Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/2107.13154v1
- **DOI**: 10.1109/TIP.2021.3099366
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13154v1)
- **Published**: 2021-07-28 03:46:57+00:00
- **Updated**: 2021-07-28 03:46:57+00:00
- **Authors**: Xiangtai Li, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Xiatian Zhu, Tao Xiang
- **Comment**: Accepted by IEEE-TIP-2021. arXiv admin note: text overlap with
  arXiv:1909.07229
- **Journal**: None
- **Summary**: Modelling long-range contextual relationships is critical for pixel-wise prediction tasks such as semantic segmentation. However, convolutional neural networks (CNNs) are inherently limited to model such dependencies due to the naive structure in its building modules (\eg, local convolution kernel). While recent global aggregation methods are beneficial for long-range structure information modelling, they would oversmooth and bring noise to the regions containing fine details (\eg,~boundaries and small objects), which are very much cared for the semantic segmentation task. To alleviate this problem, we propose to explore the local context for making the aggregated long-range relationship being distributed more accurately in local regions. In particular, we design a novel local distribution module which models the affinity map between global and local relationship for each pixel adaptively. Integrating existing global aggregation modules, we show that our approach can be modularized as an end-to-end trainable block and easily plugged into existing semantic segmentation networks, giving rise to the \emph{GALD} networks. Despite its simplicity and versatility, our approach allows us to build new state of the art on major semantic segmentation benchmarks including Cityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained models are released at \url{https://github.com/lxtGH/GALD-DGCNet} to foster further research.



### Improving Video Instance Segmentation via Temporal Pyramid Routing
- **Arxiv ID**: http://arxiv.org/abs/2107.13155v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13155v2)
- **Published**: 2021-07-28 03:57:12+00:00
- **Updated**: 2022-09-22 18:50:03+00:00
- **Authors**: Xiangtai Li, Hao He, Yibo Yang, Henghui Ding, Kuiyuan Yang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
- **Comment**: T-PAMI-2022
- **Journal**: None
- **Summary**: Video Instance Segmentation (VIS) is a new and inherently multi-task problem, which aims to detect, segment, and track each instance in a video sequence. Existing approaches are mainly based on single-frame features or single-scale features of multiple frames, where either temporal information or multi-scale information is ignored. To incorporate both temporal and scale information, we propose a Temporal Pyramid Routing (TPR) strategy to conditionally align and conduct pixel-level aggregation from a feature pyramid pair of two adjacent frames. Specifically, TPR contains two novel components, including Dynamic Aligned Cell Routing (DACR) and Cross Pyramid Routing (CPR), where DACR is designed for aligning and gating pyramid features across temporal dimension, while CPR transfers temporally aggregated features across scale dimension. Moreover, our approach is a light-weight and plug-and-play module and can be easily applied to existing instance segmentation methods. Extensive experiments on three datasets including YouTube-VIS (2019, 2021) and Cityscapes-VPS demonstrate the effectiveness and efficiency of the proposed approach on several state-of-the-art video instance and panoptic segmentation methods. Codes will be publicly available at \url{https://github.com/lxtGH/TemporalPyramidRouting}.



### Shape Controllable Virtual Try-on for Underwear Models
- **Arxiv ID**: http://arxiv.org/abs/2107.13156v1
- **DOI**: 10.1145/3474085.3475210
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2107.13156v1)
- **Published**: 2021-07-28 04:01:01+00:00
- **Updated**: 2021-07-28 04:01:01+00:00
- **Authors**: Xin Gao, Zhenjiang Liu, Zunlei Feng, Chengji Shen, Kairi Ou, Haihong Tang, Mingli Song
- **Comment**: 10 pages, 9 figures, conference
- **Journal**: None
- **Summary**: Image virtual try-on task has abundant applications and has become a hot research topic recently. Existing 2D image-based virtual try-on methods aim to transfer a target clothing image onto a reference person, which has two main disadvantages: cannot control the size and length precisely; unable to accurately estimate the user's figure in the case of users wearing thick clothes, resulting in inaccurate dressing effect. In this paper, we put forward an akin task that aims to dress clothing for underwear models. %, which is also an urgent need in e-commerce scenarios. To solve the above drawbacks, we propose a Shape Controllable Virtual Try-On Network (SC-VTON), where a graph attention network integrates the information of model and clothing to generate the warped clothing image. In addition, the control points are incorporated into SC-VTON for the desired clothing shape. Furthermore, by adding a Splitting Network and a Synthesis Network, we can use clothing/model pair data to help optimize the deformation module and generalize the task to the typical virtual try-on task. Extensive experiments show that the proposed method can achieve accurate shape control. Meanwhile, compared with other methods, our method can generate high-resolution results with detailed textures.



### Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases
- **Arxiv ID**: http://arxiv.org/abs/2107.13157v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.13157v1)
- **Published**: 2021-07-28 04:03:57+00:00
- **Updated**: 2021-07-28 04:03:57+00:00
- **Authors**: Anusua Trivedi, Jocelyn Desbiens, Ron Gross, Sunil Gupta, Rahul Dodhia, Juan Lavista Ferres
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To demonstrate that retinal microvasculature per se is a reliable biomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular diseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to color fundus images for semantic segmentation of the blood vessels and severity classification on both vascular and full images. Vessel reconstruction through harmonic descriptors is also used as a smoothing and de-noising tool. The mathematical background of the theory is also outlined. Results: For diabetic patients, at least 93.8% of DR No-Refer vs. Refer classification can be related to vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening case, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the disease biomarkers are related topologically to the vasculature. Translational Relevance: Experiments conducted on eye blood vasculature reconstruction as a biomarker shows a strong correlation between vasculature shape and later stages of DR.



### Unsupervised Segmentation for Terracotta Warrior with Seed-Region-Growing CNN (SRG-Net)
- **Arxiv ID**: http://arxiv.org/abs/2107.13167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13167v1)
- **Published**: 2021-07-28 04:50:27+00:00
- **Updated**: 2021-07-28 04:50:27+00:00
- **Authors**: Yao Hu, Guohua Geng, Kang Li, Wei Zhou, Xingxing Hao, Xin Cao
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2012.00433
- **Journal**: None
- **Summary**: The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum Site Museum is handcrafted by experts, and the increasing amounts of unearthed pieces of terracotta warriors make the archaeologists too challenging to conduct the restoration of terracotta warriors efficiently. We hope to segment the 3D point cloud data of the terracotta warriors automatically and store the fragment data in the database to assist the archaeologists in matching the actual fragments with the ones in the database, which could result in higher repairing efficiency of terracotta warriors. Moreover, the existing 3D neural network research is mainly focusing on supervised classification, clustering, unsupervised representation, and reconstruction. There are few pieces of researches concentrating on unsupervised point cloud part segmentation. In this paper, we present SRG-Net for 3D point clouds of terracotta warriors to address these problems. Firstly, we adopt a customized seed-region-growing algorithm to segment the point cloud coarsely. Then we present a supervised segmentation and unsupervised reconstruction networks to learn the characteristics of 3D point clouds. Finally, we combine the SRG algorithm with our improved CNN using a refinement method. This pipeline is called SRG-Net, which aims at conducting segmentation tasks on the terracotta warriors. Our proposed SRG-Net is evaluated on the terracotta warriors data and ShapeNet dataset by measuring the accuracy and the latency. The experimental results show that our SRG-Net outperforms the state-of-the-art methods. Our code is shown in Code File 1~\cite{Srgnet_2021}.



### Accurate Grid Keypoint Learning for Efficient Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.13170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13170v1)
- **Published**: 2021-07-28 05:04:30+00:00
- **Updated**: 2021-07-28 05:04:30+00:00
- **Authors**: Xiaojie Gao, Yueming Jin, Qi Dou, Chi-Wing Fu, Pheng-Ann Heng
- **Comment**: IROS2021
- **Journal**: None
- **Summary**: Video prediction methods generally consume substantial computing resources in training and deployment, among which keypoint-based approaches show promising improvement in efficiency by simplifying dense image prediction to light keypoint prediction. However, keypoint locations are often modeled only as continuous coordinates, so noise from semantically insignificant deviations in videos easily disrupt learning stability, leading to inaccurate keypoint modeling. In this paper, we design a new grid keypoint learning framework, aiming at a robust and explainable intermediate keypoint representation for long-term efficient video prediction. We have two major technical contributions. First, we detect keypoints by jumping among candidate locations in our raised grid space and formulate a condensation loss to encourage meaningful keypoints with strong representative capability. Second, we introduce a 2D binary map to represent the detected grid keypoints and then suggest propagating keypoint locations with stochasticity by selecting entries in the discrete grid space, thus preserving the spatial structure of keypoints in the longterm horizon for better future frame generation. Extensive experiments verify that our method outperforms the state-ofthe-art stochastic video prediction methods while saves more than 98% of computing resources. We also demonstrate our method on a robotic-assisted surgery dataset with promising results. Our code is available at https://github.com/xjgaocs/Grid-Keypoint-Learning.



### Squeeze-Excitation Convolutional Recurrent Neural Networks for Audio-Visual Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.13180v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.13180v1)
- **Published**: 2021-07-28 06:10:10+00:00
- **Updated**: 2021-07-28 06:10:10+00:00
- **Authors**: Javier Naranjo-Alcazar, Sergi Perez-Castanos, Aaron Lopez-Garcia, Pedro Zuccarello, Maximo Cobos, Francesc J. Ferri
- **Comment**: None
- **Journal**: None
- **Summary**: The use of multiple and semantically correlated sources can provide complementary information to each other that may not be evident when working with individual modalities on their own. In this context, multi-modal models can help producing more accurate and robust predictions in machine learning tasks where audio-visual data is available. This paper presents a multi-modal model for automatic scene classification that exploits simultaneously auditory and visual information. The proposed approach makes use of two separate networks which are respectively trained in isolation on audio and visual data, so that each network specializes in a given modality. The visual subnetwork is a pre-trained VGG16 model followed by a bidiretional recurrent layer, while the residual audio subnetwork is based on stacked squeeze-excitation convolutional blocks trained from scratch. After training each subnetwork, the fusion of information from the audio and visual streams is performed at two different stages. The early fusion stage combines features resulting from the last convolutional block of the respective subnetworks at different time steps to feed a bidirectional recurrent structure. The late fusion stage combines the output of the early fusion stage with the independent predictions provided by the two subnetworks, resulting in the final prediction. We evaluate the method using the recently published TAU Audio-Visual Urban Scenes 2021, which contains synchronized audio and video recordings from 12 European cities in 10 different scene classes. The proposed model has been shown to provide an excellent trade-off between prediction performance (86.5%) and system complexity (15M parameters) in the evaluation results of the DCASE 2021 Challenge.



### Assessment of Deep Learning-based Heart Rate Estimation using Remote Photoplethysmography under Different Illuminations
- **Arxiv ID**: http://arxiv.org/abs/2107.13193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13193v2)
- **Published**: 2021-07-28 06:50:52+00:00
- **Updated**: 2022-05-11 03:17:08+00:00
- **Authors**: Ze Yang, Haofei Wang, Feng Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Remote photoplethysmography (rPPG) monitors heart rate without requiring physical contact, which allows for a wide variety of applications. Deep learning-based rPPG have demonstrated superior performance over the traditional approaches in controlled context. However, the lighting situation in indoor space is typically complex, with uneven light distribution and frequent variations in illumination. It lacks a fair comparison of different methods under different illuminations using the same dataset. In this paper, we present a public dataset, namely the BH-rPPG dataset, which contains data from thirty five subjects under three illuminations: low, medium, and high illumination. We also provide the ground truth heart rate measured by an oximeter. We evaluate the performance of three deep learning-based methods (Deepphys, rPPGNet, and Physnet) to that of four traditional methods (CHROM, GREEN, ICA, and POS) using two public datasets: the UBFC-rPPG dataset and the BH-rPPG dataset. The experimental results demonstrate that traditional methods are generally more resistant to fluctuating illuminations. We found that the Physnet achieves lowest mean absolute error (MAE) among deep learning-based method under medium illumination, whereas the CHROM achieves 1.04 beats per minute (BPM), outperforming the Physnet by 80$\%$. Additionally, we investigate potential methods for improving performance of deep learning-based methods. We find that brightness augmentation make model more robust to variation illumination. These findings suggest that while developing deep learning-based heart rate estimation algorithms, illumination variation should be taken into account. This work serves as a benchmark for rPPG performance evaluation and it opens a pathway for future investigation into deep learning-based rPPG under illumination variations.



### An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization
- **Arxiv ID**: http://arxiv.org/abs/2107.13200v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.13200v1)
- **Published**: 2021-07-28 07:19:00+00:00
- **Updated**: 2021-07-28 07:19:00+00:00
- **Authors**: Fan Zhang, Bo Pan, Pengfei Shao, Peng Liu, Shuwei Shen, Peng Yao, Ronald X. Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal period mild cognitive impairment (MCI) is essential for the delayed disease progression and the improved quality of patients'life. The emerging computer-aided diagnostic methods that combine deep learning with structural magnetic resonance imaging (sMRI) have achieved encouraging results, but some of them are limit of issues such as data leakage and unexplainable diagnosis. In this research, we propose a novel end-to-end deep learning approach for automated diagnosis of AD and localization of important brain regions related to the disease from sMRI data. This approach is based on a 2D single model strategy and has the following differences from the current approaches: 1) Convolutional Neural Network (CNN) models of different structures and capacities are evaluated systemically and the most suitable model is adopted for AD diagnosis; 2) a data augmentation strategy named Two-stage Random RandAugment (TRRA) is proposed to alleviate the overfitting issue caused by limited training data and to improve the classification performance in AD diagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the visually explainable heatmaps that localize and highlight the brain regions that our model focuses on and to make our model more transparent. Our approach has been evaluated on two publicly accessible datasets for two classification tasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable MCI (sMCI). The experimental results indicate that our approach outperforms the state-of-the-art approaches, including those using multi-model and 3D CNN methods. The resultant localization heatmaps from our approach also highlight the lateral ventricle and some disease-relevant regions of cortex, coincident with the commonly affected regions during the development of AD.



### DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices
- **Arxiv ID**: http://arxiv.org/abs/2107.13217v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.13217v1)
- **Published**: 2021-07-28 08:00:09+00:00
- **Updated**: 2021-07-28 08:00:09+00:00
- **Authors**: Geetika Arora, Rohit K Bharadwaj, Kamlesh Tiwari
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes teeth-photo, a new biometric modality for human authentication on mobile and hand held devices. Biometrics samples are acquired using the camera mounted on mobile device with the help of a mobile application having specific markers to register the teeth area. Region of interest (RoI) is then extracted using the markers and the obtained sample is enhanced using contrast limited adaptive histogram equalization (CLAHE) for better visual clarity. We propose a deep learning architecture and novel regularization scheme to obtain highly discriminative embedding for small size RoI. Proposed custom loss function was able to achieve perfect classification for the tiny RoI of $75\times 75$ size. The model is end-to-end and few-shot and therefore is very efficient in terms of time and energy requirements. The system can be used in many ways including device unlocking and secure authentication. To the best of our understanding, this is the first work on teeth-photo based authentication for mobile device. Experiments have been conducted on an in-house teeth-photo database collected using our application. The database is made publicly available. Results have shown that the proposed system has perfect accuracy.



### Normalization Matters in Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2107.13221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13221v1)
- **Published**: 2021-07-28 08:14:49+00:00
- **Updated**: 2021-07-28 08:14:49+00:00
- **Authors**: Jeesoo Kim, Junsuk Choe, Sangdoo Yun, Nojun Kwak
- **Comment**: Accepted at ICCV 2021. 16 pages, 10 figures
- **Journal**: None
- **Summary**: Weakly-supervised object localization (WSOL) enables finding an object using a dataset without any localization information. By simply training a classification model using only image-level annotations, the feature map of the model can be utilized as a score map for localization. In spite of many WSOL methods proposing novel strategies, there has not been any de facto standard about how to normalize the class activation map (CAM). Consequently, many WSOL methods have failed to fully exploit their own capacity because of the misuse of a normalization method. In this paper, we review many existing normalization methods and point out that they should be used according to the property of the given dataset. Additionally, we propose a new normalization method which substantially enhances the performance of any CAM-based WSOL methods. Using the proposed normalization method, we provide a comprehensive evaluation over three datasets (CUB, ImageNet and OpenImages) on three different architectures and observe significant performance gains over the conventional min-max normalization method in all the evaluated cases.



### C^3Net: End-to-End deep learning for efficient real-time visual active camera control
- **Arxiv ID**: http://arxiv.org/abs/2107.13233v1
- **DOI**: 10.1007/s11554-021-01077-z
- **Categories**: **cs.CV**, cs.RO, 65D19, 68T45, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2107.13233v1)
- **Published**: 2021-07-28 09:31:46+00:00
- **Updated**: 2021-07-28 09:31:46+00:00
- **Authors**: Christos Kyrkou
- **Comment**: Journal of Real-Time Image Processing , 2021. Real-time active
  vision, Smart camera, Deep learning, End-to-end learning
  https://www.youtube.com/watch?v=UuepDtWUpsg&ab_channel=ChristosKyrkou. arXiv
  admin note: text overlap with arXiv:2012.06428
- **Journal**: None
- **Summary**: The need for automated real-time visual systems in applications such as smart camera surveillance, smart environments, and drones necessitates the improvement of methods for visual active monitoring and control. Traditionally, the active monitoring task has been handled through a pipeline of modules such as detection, filtering, and control. However, such methods are difficult to jointly optimize and tune their various parameters for real-time processing in resource constraint systems. In this paper a deep Convolutional Camera Controller Neural Network is proposed to go directly from visual information to camera movement to provide an efficient solution to the active vision problem. It is trained end-to-end without bounding box annotations to control a camera and follow multiple targets from raw pixel values. Evaluation through both a simulation framework and real experimental setup, indicate that the proposed solution is robust to varying conditions and able to achieve better monitoring performance than traditional approaches both in terms of number of targets monitored as well as in effective monitoring time. The advantage of the proposed approach is that it is computationally less demanding and can run at over 10 FPS (~4x speedup) on an embedded smart camera providing a practical and affordable solution to real-time active monitoring.



### A Visual Domain Transfer Learning Approach for Heartbeat Sound Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.13237v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2107.13237v2)
- **Published**: 2021-07-28 09:41:38+00:00
- **Updated**: 2021-10-04 07:05:36+00:00
- **Authors**: Uddipan Mukherjee, Sidharth Pancholi
- **Comment**: None
- **Journal**: None
- **Summary**: Heart disease is the most common reason for human mortality that causes almost one-third of deaths throughout the world. Detecting the disease early increases the chances of survival of the patient and there are several ways a sign of heart disease can be detected early. This research proposes to convert cleansed and normalized heart sound into visual mel scale spectrograms and then using visual domain transfer learning approaches to automatically extract features and categorize between heart sounds. Some of the previous studies found that the spectrogram of various types of heart sounds is visually distinguishable to human eyes, which motivated this study to experiment on visual domain classification approaches for automated heart sound classification. It will use convolution neural network-based architectures i.e. ResNet, MobileNetV2, etc as the automated feature extractors from spectrograms. These well-accepted models in the image domain showed to learn generalized feature representations of cardiac sounds collected from different environments with varying amplitude and noise levels. Model evaluation criteria used were categorical accuracy, precision, recall, and AUROC as the chosen dataset is unbalanced. The proposed approach has been implemented on datasets A and B of the PASCAL heart sound collection and resulted in ~ 90% categorical accuracy and AUROC of ~0.97 for both sets.



### TransAction: ICL-SJTU Submission to EPIC-Kitchens Action Anticipation Challenge 2021
- **Arxiv ID**: http://arxiv.org/abs/2107.13259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13259v1)
- **Published**: 2021-07-28 10:42:47+00:00
- **Updated**: 2021-07-28 10:42:47+00:00
- **Authors**: Xiao Gu, Jianing Qiu, Yao Guo, Benny Lo, Guang-Zhong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, the technical details of our submission to the EPIC-Kitchens Action Anticipation Challenge 2021 are given. We developed a hierarchical attention model for action anticipation, which leverages Transformer-based attention mechanism to aggregate features across temporal dimension, modalities, symbiotic branches respectively. In terms of Mean Top-5 Recall of action, our submission with team name ICL-SJTU achieved 13.39% for overall testing set, 10.05% for unseen subsets and 11.88% for tailed subsets. Additionally, it is noteworthy that our submission ranked 1st in terms of verb class in all three (sub)sets.



### Improving Multi-View Stereo via Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2107.13261v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.13261v1)
- **Published**: 2021-07-28 10:45:05+00:00
- **Updated**: 2021-07-28 10:45:05+00:00
- **Authors**: Eugenio Lomurno, Andrea Romanoni, Matteo Matteucci
- **Comment**: None
- **Journal**: None
- **Summary**: Today, Multi-View Stereo techniques are able to reconstruct robust and detailed 3D models, especially when starting from high-resolution images. However, there are cases in which the resolution of input images is relatively low, for instance, when dealing with old photos, or when hardware constrains the amount of data that can be acquired. In this paper, we investigate if, how, and how much increasing the resolution of such input images through Super-Resolution techniques reflects in quality improvements of the reconstructed 3D models, despite the artifacts that sometimes this may generate. We show that applying a Super-Resolution step before recovering the depth maps in most cases leads to a better 3D model both in the case of PatchMatch-based and deep-learning-based algorithms. The use of Super-Resolution improves especially the completeness of reconstructed models and turns out to be particularly effective in the case of textured scenes.



### Learning-Based Depth and Pose Estimation for Monocular Endoscope with Loss Generalization
- **Arxiv ID**: http://arxiv.org/abs/2107.13263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13263v1)
- **Published**: 2021-07-28 10:51:06+00:00
- **Updated**: 2021-07-28 10:51:06+00:00
- **Authors**: Aji Resindra Widya, Yusuke Monno, Masatoshi Okutomi, Sho Suzuki, Takuji Gotoda, Kenji Miki
- **Comment**: Accepted for EMBC 2021
- **Journal**: None
- **Summary**: Gastroendoscopy has been a clinical standard for diagnosing and treating conditions that affect a part of a patient's digestive system, such as the stomach. Despite the fact that gastroendoscopy has a lot of advantages for patients, there exist some challenges for practitioners, such as the lack of 3D perception, including the depth and the endoscope pose information. Such challenges make navigating the endoscope and localizing any found lesion in a digestive tract difficult. To tackle these problems, deep learning-based approaches have been proposed to provide monocular gastroendoscopy with additional yet important depth and pose information. In this paper, we propose a novel supervised approach to train depth and pose estimation networks using consecutive endoscopy images to assist the endoscope navigation in the stomach. We firstly generate real depth and pose training data using our previously proposed whole stomach 3D reconstruction pipeline to avoid poor generalization ability between computer-generated (CG) models and real data for the stomach. In addition, we propose a novel generalized photometric loss function to avoid the complicated process of finding proper weights for balancing the depth and the pose loss terms, which is required for existing direct depth and pose supervision approaches. We then experimentally show that our proposed generalized loss performs better than existing direct supervision losses.



### Aug3D-RPN: Improving Monocular 3D Object Detection by Synthetic Images with Virtual Depth
- **Arxiv ID**: http://arxiv.org/abs/2107.13269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13269v1)
- **Published**: 2021-07-28 11:00:47+00:00
- **Updated**: 2021-07-28 11:00:47+00:00
- **Authors**: Chenhang He, Jianqiang Huang, Xian-Sheng Hua, Lei Zhang
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Current geometry-based monocular 3D object detection models can efficiently detect objects by leveraging perspective geometry, but their performance is limited due to the absence of accurate depth information. Though this issue can be alleviated in a depth-based model where a depth estimation module is plugged to predict depth information before 3D box reasoning, the introduction of such module dramatically reduces the detection speed. Instead of training a costly depth estimator, we propose a rendering module to augment the training data by synthesizing images with virtual-depths. The rendering module takes as input the RGB image and its corresponding sparse depth image, outputs a variety of photo-realistic synthetic images, from which the detection model can learn more discriminative features to adapt to the depth changes of the objects. Besides, we introduce an auxiliary module to improve the detection model by jointly optimizing it through a depth estimation task. Both modules are working in the training time and no extra computation will be introduced to the detection model. Experiments show that by working with our proposed modules, a geometry-based model can represent the leading accuracy on the KITTI 3D detection benchmark.



### Spatial Uncertainty-Aware Semi-Supervised Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2107.13271v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.13271v2)
- **Published**: 2021-07-28 11:06:52+00:00
- **Updated**: 2021-08-02 09:07:45+00:00
- **Authors**: Yanda Meng, Hongrun Zhang, Yitian Zhao, Xiaoyun Yang, Xuesheng Qian, Xiaowei Huang, Yalin Zheng
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: Semi-supervised approaches for crowd counting attract attention, as the fully supervised paradigm is expensive and laborious due to its request for a large number of images of dense crowd scenarios and their annotations. This paper proposes a spatial uncertainty-aware semi-supervised approach via regularized surrogate task (binary segmentation) for crowd counting problems. Different from existing semi-supervised learning-based crowd counting methods, to exploit the unlabeled data, our proposed spatial uncertainty-aware teacher-student framework focuses on high confident regions' information while addressing the noisy supervision from the unlabeled data in an end-to-end manner. Specifically, we estimate the spatial uncertainty maps from the teacher model's surrogate task to guide the feature learning of the main task (density regression) and the surrogate task of the student model at the same time. Besides, we introduce a simple yet effective differential transformation layer to enforce the inherent spatial consistency regularization between the main task and the surrogate task in the student model, which helps the surrogate task to yield more reliable predictions and generates high-quality uncertainty maps. Thus, our model can also address the task-level perturbation problems that occur spatial inconsistency between the primary and surrogate tasks in the student model. Experimental results on four challenging crowd counting datasets demonstrate that our method achieves superior performance to the state-of-the-art semi-supervised methods.



### Rank-based verification for long-term face tracking in crowded scenes
- **Arxiv ID**: http://arxiv.org/abs/2107.13273v1
- **DOI**: 10.1109/TBIOM.2021.3099568
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13273v1)
- **Published**: 2021-07-28 11:15:04+00:00
- **Updated**: 2021-07-28 11:15:04+00:00
- **Authors**: Germn Barquero, Isabelle Hupont, Carles Fernndez
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2010.08675
- **Journal**: IEEE Transactions on Biometrics, Behavior, and Identity Science,
  2021
- **Summary**: Most current multi-object trackers focus on short-term tracking, and are based on deep and complex systems that often cannot operate in real-time, making them impractical for video-surveillance. In this paper we present a long-term, multi-face tracking architecture conceived for working in crowded contexts where faces are often the only visible part of a person. Our system benefits from advances in the fields of face detection and face recognition to achieve long-term tracking, and is particularly unconstrained to the motion and occlusions of people. It follows a tracking-by-detection approach, combining a fast short-term visual tracker with a novel online tracklet reconnection strategy grounded on rank-based face verification. The proposed rank-based constraint favours higher inter-class distance among tracklets, and reduces the propagation of errors due to wrong reconnections. Additionally, a correction module is included to correct past assignments with no extra computational cost. We present a series of experiments introducing novel specialized metrics for the evaluation of long-term tracking capabilities, and publicly release a video dataset with 10 manually annotated videos and a total length of 8' 54". Our findings validate the robustness of each of the proposed modules, and demonstrate that, in these challenging contexts, our approach yields up to 50% longer tracks than state-of-the-art deep learning trackers.



### A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/2107.13277v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.13277v1)
- **Published**: 2021-07-28 11:18:48+00:00
- **Updated**: 2021-07-28 11:18:48+00:00
- **Authors**: Yue Shi, Liangxiu Han, Anthony Kleerekoper, Sheng Chang, Tongle Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Late blight disease is one of the most destructive diseases in potato crop, leading to serious yield losses globally. Accurate diagnosis of the disease at early stage is critical for precision disease control and management. Current farm practices in crop disease diagnosis are based on manual visual inspection, which is costly, time consuming, subject to individual bias. Recent advances in imaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote sensing and machine learning offer the opportunity to address this challenge. Particularly, hyperspectral imagery (HSI) combining with machine learning/deep learning approaches is preferable for accurately identifying specific plant diseases because the HSI consists of a wide range of high-quality reflectance information beyond human vision, capable of capturing both spectral-spatial information. The proposed method considers the potential disease specific reflectance radiation variance caused by the canopy structural diversity, introduces the multiple capsule layers to model the hierarchical structure of the spectral-spatial disease attributes with the encapsulated features to represent the various classes and the rotation invariance of the disease attributes in the feature space. We have evaluated the proposed method with the real UAV-based HSI data under the controlled field conditions. The effectiveness of the hierarchical features has been quantitatively assessed and compared with the existing representative machine learning/deep learning methods. The experiment results show that the proposed model significantly improves the accuracy performance when considering hierarchical-structure of spectral-spatial features, comparing to the existing methods only using spectral, or spatial or spectral-spatial features without consider hierarchical-structure of spectral-spatial features.



### Pseudo-LiDAR Based Road Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.13279v2
- **DOI**: 10.1109/TCSVT.2022.3146305
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.13279v2)
- **Published**: 2021-07-28 11:21:42+00:00
- **Updated**: 2022-03-11 02:32:41+00:00
- **Authors**: Libo Sun, Haokui Zhang, Wei Yin
- **Comment**: 13 pages, 11 figures. This paper has been accepted by TCSVT. IEEE
  Trans. Circuit Syst.Video Technol. 2022
- **Journal**: None
- **Summary**: Road detection is a critically important task for self-driving cars. By employing LiDAR data, recent works have significantly improved the accuracy of road detection. Relying on LiDAR sensors limits the wide application of those methods when only cameras are available. In this paper, we propose a novel road detection approach with RGB being the only input during inference. Specifically, we exploit pseudo-LiDAR using depth estimation, and propose a feature fusion network where RGB and learned depth information are fused for improved road detection. To further optimize the network structure and improve the efficiency of the network. we search for the network structure of the feature fusion module using NAS techniques. Finally, be aware of that generating pseudo-LiDAR from RGB via depth estimation introduces extra computational costs and relies on depth estimation networks, we design a modality distillation strategy and leverage it to further free our network from these extra computational cost and dependencies during inference. The proposed method achieves state-of-the-art performance on two challenging benchmarks, KITTI and R2D.



### Development and evaluation of intraoperative ultrasound segmentation with negative image frames and multiple observer labels
- **Arxiv ID**: http://arxiv.org/abs/2108.04114v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04114v1)
- **Published**: 2021-07-28 12:15:49+00:00
- **Updated**: 2021-07-28 12:15:49+00:00
- **Authors**: Liam F Chalcroft, Jiongqi Qu, Sophie A Martin, Iani JMB Gayo, Giulio V Minore, Imraj RD Singh, Shaheer U Saeed, Qianye Yang, Zachary MC Baum, Andre Altmann, Yipeng Hu
- **Comment**: Accepted to ASMUS@MICCAI 2021
- **Journal**: None
- **Summary**: When developing deep neural networks for segmenting intraoperative ultrasound images, several practical issues are encountered frequently, such as the presence of ultrasound frames that do not contain regions of interest and the high variance in ground-truth labels. In this study, we evaluate the utility of a pre-screening classification network prior to the segmentation network. Experimental results demonstrate that such a classifier, minimising frame classification errors, was able to directly impact the number of false positive and false negative frames. Importantly, the segmentation accuracy on the classifier-selected frames, that would be segmented, remains comparable to or better than those from standalone segmentation networks. Interestingly, the efficacy of the pre-screening classifier was affected by the sampling methods for training labels from multiple observers, a seemingly independent problem. We show experimentally that a previously proposed approach, combining random sampling and consensus labels, may need to be adapted to perform well in our application. Furthermore, this work aims to share practical experience in developing a machine learning application that assists highly variable interventional imaging for prostate cancer patients, to present robust and reproducible open-source implementations, and to report a set of comprehensive results and analysis comparing these practical, yet important, options in a real-world clinical application.



### WaveCNet: Wavelet Integrated CNNs to Suppress Aliasing Effect for Noise-Robust Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.13335v1
- **DOI**: 10.1109/TIP.2021.3101395
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13335v1)
- **Published**: 2021-07-28 12:59:15+00:00
- **Updated**: 2021-07-28 12:59:15+00:00
- **Authors**: Qiufu Li, Linlin Shen, Sheng Guo, Zhihui Lai
- **Comment**: IEEE TIP accepted paper. arXiv admin note: substantial text overlap
  with arXiv:2005.03337
- **Journal**: None
- **Summary**: Though widely used in image classification, convolutional neural networks (CNNs) are prone to noise interruptions, i.e. the CNN output can be drastically changed by small image noise. To improve the noise robustness, we try to integrate CNNs with wavelet by replacing the common down-sampling (max-pooling, strided-convolution, and average pooling) with discrete wavelet transform (DWT). We firstly propose general DWT and inverse DWT (IDWT) layers applicable to various orthogonal and biorthogonal discrete wavelets like Haar, Daubechies, and Cohen, etc., and then design wavelet integrated CNNs (WaveCNets) by integrating DWT into the commonly used CNNs (VGG, ResNets, and DenseNet). During the down-sampling, WaveCNets apply DWT to decompose the feature maps into the low-frequency and high-frequency components. Containing the main information including the basic object structures, the low-frequency component is transmitted into the following layers to generate robust high-level features. The high-frequency components are dropped to remove most of the data noises. The experimental results show that %wavelet accelerates the CNN training, and WaveCNets achieve higher accuracy on ImageNet than various vanilla CNNs. We have also tested the performance of WaveCNets on the noisy version of ImageNet, ImageNet-C and six adversarial attacks, the results suggest that the proposed DWT/IDWT layers could provide better noise-robustness and adversarial robustness. When applying WaveCNets as backbones, the performance of object detectors (i.e., faster R-CNN and RetinaNet) on COCO detection dataset are consistently improved. We believe that suppression of aliasing effect, i.e. separation of low frequency and high frequency information, is the main advantages of our approach. The code of our DWT/IDWT layer and different WaveCNets are available at https://github.com/CVI-SZU/WaveCNet.



### A Computer Vision-Based Approach for Driver Distraction Recognition using Deep Learning and Genetic Algorithm Based Ensemble
- **Arxiv ID**: http://arxiv.org/abs/2107.13355v1
- **DOI**: 10.1007/978-3-030-87897-9_5
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.13355v1)
- **Published**: 2021-07-28 13:39:31+00:00
- **Updated**: 2021-07-28 13:39:31+00:00
- **Authors**: Ashlesha Kumar, Kuldip Singh Sangwan, Dhiraj
- **Comment**: 12 pages, Presented in 20th International Conference on Artificial
  Intelligence and Soft Computing (ICAISC 2021)
- **Journal**: None
- **Summary**: As the proportion of road accidents increases each year, driver distraction continues to be an important risk component in road traffic injuries and deaths. The distractions caused by the increasing use of mobile phones and other wireless devices pose a potential risk to road safety. Our current study aims to aid the already existing techniques in driver posture recognition by improving the performance in the driver distraction classification problem. We present an approach using a genetic algorithm-based ensemble of six independent deep neural architectures, namely, AlexNet, VGG-16, EfficientNet B0, Vanilla CNN, Modified DenseNet, and InceptionV3 + BiLSTM. We test it on two comprehensive datasets, the AUC Distracted Driver Dataset, on which our technique achieves an accuracy of 96.37%, surpassing the previously obtained 95.98%, and on the State Farm Driver Distraction Dataset, on which we attain an accuracy of 99.75%. The 6-Model Ensemble gave an inference time of 0.024 seconds as measured on our machine with Ubuntu 20.04(64-bit) and GPU as GeForce GTX 1080.



### Graph Constrained Data Representation Learning for Human Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.13362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13362v2)
- **Published**: 2021-07-28 13:49:16+00:00
- **Updated**: 2021-08-27 11:59:39+00:00
- **Authors**: Mariella Dimiccoli, Llus Garrido, Guillem Rodriguez-Corominas, Herwig Wendt
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Recently, transfer subspace learning based approaches have shown to be a valid alternative to unsupervised subspace clustering and temporal data clustering for human motion segmentation (HMS). These approaches leverage prior knowledge from a source domain to improve clustering performance on a target domain, and currently they represent the state of the art in HMS. Bucking this trend, in this paper, we propose a novel unsupervised model that learns a representation of the data and digs clustering information from the data itself. Our model is reminiscent of temporal subspace clustering, but presents two critical differences. First, we learn an auxiliary data matrix that can deviate from the initial data, hence confer more degrees of freedom to the coding matrix. Second, we introduce a regularization term for this auxiliary data matrix that preserves the local geometrical structure present in the high-dimensional space. The proposed model is efficiently optimized by using an original Alternating Direction Method of Multipliers (ADMM) formulation allowing to learn jointly the auxiliary data representation, a nonnegative dictionary and a coding matrix. Experimental results on four benchmark datasets for HMS demonstrate that our approach achieves significantly better clustering performance then state-of-the-art methods, including both unsupervised and more recent semi-supervised transfer learning approaches.



### Evaluating the Use of Reconstruction Error for Novelty Localization
- **Arxiv ID**: http://arxiv.org/abs/2107.13379v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.13379v1)
- **Published**: 2021-07-28 14:10:55+00:00
- **Updated**: 2021-07-28 14:10:55+00:00
- **Authors**: Patrick Feeney, Michael C. Hughes
- **Comment**: None
- **Journal**: None
- **Summary**: The pixelwise reconstruction error of deep autoencoders is often utilized for image novelty detection and localization under the assumption that pixels with high error indicate which parts of the input image are unfamiliar and therefore likely to be novel. This assumed correlation between pixels with high reconstruction error and novel regions of input images has not been verified and may limit the accuracy of these methods. In this paper we utilize saliency maps to evaluate whether this correlation exists. Saliency maps reveal directly how much a change in each input pixel would affect reconstruction loss, while each pixel's reconstruction error may be attributed to many input pixels when layers are fully connected. We compare saliency maps to reconstruction error maps via qualitative visualizations as well as quantitative correspondence between the top K elements of the maps for both novel and normal images. Our results indicate that reconstruction error maps do not closely correlate with the importance of pixels in the input images, making them insufficient for novelty localization.



### VirtualConductor: Music-driven Conducting Video Generation System
- **Arxiv ID**: http://arxiv.org/abs/2108.04350v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.04350v1)
- **Published**: 2021-07-28 14:17:45+00:00
- **Updated**: 2021-07-28 14:17:45+00:00
- **Authors**: Delong Chen, Fan Liu, Zewen Li, Feng Xu
- **Comment**: Accepted by IEEE International Conference on Multimedia and Expo
  (ICME) 2021, demo track. Best demo
- **Journal**: None
- **Summary**: In this demo, we present VirtualConductor, a system that can generate conducting video from any given music and a single user's image. First, a large-scale conductor motion dataset is collected and constructed. Then, we propose Audio Motion Correspondence Network (AMCNet) and adversarial-perceptual learning to learn the cross-modal relationship and generate diverse, plausible, music-synchronized motion. Finally, we combine 3D animation rendering and a pose transfer model to synthesize conducting video from a single given user's image. Therefore, any user can become a virtual conductor through the system.



### SimROD: A Simple Adaptation Method for Robust Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.13389v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.13389v1)
- **Published**: 2021-07-28 14:28:32+00:00
- **Updated**: 2021-07-28 14:28:32+00:00
- **Authors**: Rindra Ramamonjison, Amin Banitalebi-Dehkordi, Xinyu Kang, Xiaolong Bai, Yong Zhang
- **Comment**: Accepted to ICCV 2021 conference for full oral presentation
- **Journal**: None
- **Summary**: This paper presents a Simple and effective unsupervised adaptation method for Robust Object Detection (SimROD). To overcome the challenging issues of domain shift and pseudo-label noise, our method integrates a novel domain-centric augmentation method, a gradual self-labeling adaptation procedure, and a teacher-guided fine-tuning mechanism. Using our method, target domain samples can be leveraged to adapt object detection models without changing the model architecture or generating synthetic data. When applied to image corruptions and high-level cross-domain adaptation benchmarks, our method outperforms prior baselines on multiple domain adaptation benchmarks. SimROD achieves new state-of-the-art on standard real-to-synthetic and cross-camera setup benchmarks. On the image corruption benchmark, models adapted with our method achieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6% AP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method outperformed the best baseline performance by up to 8% AP50 on Comic dataset and up to 4% on Watercolor dataset.



### High-speed object detection with a single-photon time-of-flight image sensor
- **Arxiv ID**: http://arxiv.org/abs/2107.13407v1
- **DOI**: 10.1364/OE.435619
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2107.13407v1)
- **Published**: 2021-07-28 14:53:44+00:00
- **Updated**: 2021-07-28 14:53:44+00:00
- **Authors**: Germn Mora-Martn, Alex Turpin, Alice Ruget, Abderrahim Halimi, Robert Henderson, Jonathan Leach, Istvan Gyongy
- **Comment**: 13 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: 3D time-of-flight (ToF) imaging is used in a variety of applications such as augmented reality (AR), computer interfaces, robotics and autonomous systems. Single-photon avalanche diodes (SPADs) are one of the enabling technologies providing accurate depth data even over long ranges. By developing SPADs in array format with integrated processing combined with pulsed, flood-type illumination, high-speed 3D capture is possible. However, array sizes tend to be relatively small, limiting the lateral resolution of the resulting depth maps, and, consequently, the information that can be extracted from the image for applications such as object detection. In this paper, we demonstrate that these limitations can be overcome through the use of convolutional neural networks (CNNs) for high-performance object detection. We present outdoor results from a portable SPAD camera system that outputs 16-bin photon timing histograms with 64x32 spatial resolution. The results, obtained with exposure times down to 2 ms (equivalent to 500 FPS) and in signal-to-background (SBR) ratios as low as 0.05, point to the advantages of providing the CNN with full histogram data rather than point clouds alone. Alternatively, a combination of point cloud and active intensity data may be used as input, for a similar level of performance. In either case, the GPU-accelerated processing time is less than 1 ms per frame, leading to an overall latency (image acquisition plus processing) in the millisecond range, making the results relevant for safety-critical computer vision applications which would benefit from faster than human reaction times.



### Predicting the Future from First Person (Egocentric) Vision: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2107.13411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13411v1)
- **Published**: 2021-07-28 14:58:13+00:00
- **Updated**: 2021-07-28 14:58:13+00:00
- **Authors**: Ivan Rodin, Antonino Furnari, Dimitrios Mavroedis, Giovanni Maria Farinella
- **Comment**: Computer Vision and Image Understanding, 2021
- **Journal**: None
- **Summary**: Egocentric videos can bring a lot of information about how humans perceive the world and interact with the environment, which can be beneficial for the analysis of human behaviour. The research in egocentric video analysis is developing rapidly thanks to the increasing availability of wearable devices and the opportunities offered by new large-scale egocentric datasets. As computer vision techniques continue to develop at an increasing pace, the tasks related to the prediction of future are starting to evolve from the need of understanding the present. Predicting future human activities, trajectories and interactions with objects is crucial in applications such as human-robot interaction, assistive wearable technologies for both industrial and daily living scenarios, entertainment and virtual or augmented reality. This survey summarises the evolution of studies in the context of future prediction from egocentric vision making an overview of applications, devices, existing problems, commonly used datasets, models and input modalities. Our analysis highlights that methods for future prediction from egocentric vision can have a significant impact in a range of applications and that further research efforts should be devoted to the standardisation of tasks and the proposal of datasets considering real-world scenarios such as the ones with an industrial vocation.



### Neural Rays for Occlusion-aware Image-based Rendering
- **Arxiv ID**: http://arxiv.org/abs/2107.13421v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2107.13421v2)
- **Published**: 2021-07-28 15:09:40+00:00
- **Updated**: 2022-03-28 12:29:59+00:00
- **Authors**: Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng Wang, Christian Theobalt, Xiaowei Zhou, Wenping Wang
- **Comment**: CVPR2022. Project page https://liuyuan-pal.github.io/NeuRay/ Codes
  available at https://github.com/liuyuan-pal/NeuRay
- **Journal**: None
- **Summary**: We present a new neural representation, called Neural Ray (NeuRay), for the novel view synthesis task. Recent works construct radiance fields from image features of input views to render novel view images, which enables the generalization to new scenes. However, due to occlusions, a 3D point may be invisible to some input views. On such a 3D point, these generalization methods will include inconsistent image features from invisible views, which interfere with the radiance field construction. To solve this problem, we predict the visibility of 3D points to input views within our NeuRay representation. This visibility enables the radiance field construction to focus on visible image features, which significantly improves its rendering quality. Meanwhile, a novel consistency loss is proposed to refine the visibility in NeuRay when finetuning on a specific scene. Experiments demonstrate that our approach achieves state-of-the-art performance on the novel view synthesis task when generalizing to unseen scenes and outperforms per-scene optimization methods after finetuning.



### Task-Specific Normalization for Continual Learning of Blind Image Quality Models
- **Arxiv ID**: http://arxiv.org/abs/2107.13429v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13429v2)
- **Published**: 2021-07-28 15:21:01+00:00
- **Updated**: 2023-03-02 06:39:53+00:00
- **Authors**: Weixia Zhang, Kede Ma, Guangtao Zhai, Xiaokang Yang
- **Comment**: Revise the performance metrics, methodological updates, and new
  experimental results
- **Journal**: None
- **Summary**: The computational vision community has recently paid attention to continual learning for blind image quality assessment (BIQA). The primary challenge is to combat catastrophic forgetting of previously-seen IQA datasets (i.e., tasks). In this paper, we present a simple yet effective continual learning method for BIQA with improved quality prediction accuracy, plasticity-stability trade-off, and task-order/-length robustness. The key step in our approach is to freeze all convolution filters of a pre-trained deep neural network (DNN) for an explicit promise of stability, and learn task-specific normalization parameters for plasticity. We assign each new task a prediction head, and load the corresponding normalization parameters to produce a quality score. The final quality estimate is computed by a weighted summation of predictions from all heads with a lightweight K-means gating mechanism, without leveraging the test-time oracle. Extensive experiments on six IQA datasets demonstrate the advantages of the proposed method in comparison to previous training techniques for BIQA.



### AI assisted method for efficiently generating breast ultrasound screening reports
- **Arxiv ID**: http://arxiv.org/abs/2107.13431v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.13431v2)
- **Published**: 2021-07-28 15:21:57+00:00
- **Updated**: 2022-05-22 07:26:24+00:00
- **Authors**: Shuang Ge, Qiongyu Ye, Wenquan Xie, Desheng Sun, Huabin Zhang, Xiaobo Zhou, Kehong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Background: Ultrasound is one of the preferred choices for early screening of dense breast cancer. Clinically, doctors have to manually write the screening report which is time-consuming and laborious, and it is easy to miss and miswrite. Aim: We proposed a new pipeline to automatically generate AI breast ultrasound screening reports based on ultrasound images, aiming to assist doctors in improving the efficiency of clinical screening and reducing repetitive report writing. Methods: AI was used to efficiently generate personalized breast ultrasound screening preliminary reports, especially for benign and normal cases which account for the majority. Based on the preliminary AI report, doctors then make simple adjustments or corrections to quickly generate the final report. The approach has been trained and tested using a database of 4809 breast tumor instances. Results: Experimental results indicate that this pipeline improves doctors' work efficiency by up to 90%, which greatly reduces repetitive work. Conclusion: Personalized report generation is more widely recognized by doctors in clinical practice compared with non-intelligent reports based on fixed templates or containing options to fill in the blanks.



### CarveNet: Carving Point-Block for Complex 3D Shape Completion
- **Arxiv ID**: http://arxiv.org/abs/2107.13452v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2107.13452v1)
- **Published**: 2021-07-28 16:07:20+00:00
- **Updated**: 2021-07-28 16:07:20+00:00
- **Authors**: Qing Guo, Zhijie Wang, Felix Juefei-Xu, Di Lin, Lei Ma, Wei Feng, Yang Liu
- **Comment**: 10 pages and 10 figures
- **Journal**: None
- **Summary**: 3D point cloud completion is very challenging because it heavily relies on the accurate understanding of the complex 3D shapes (e.g., high-curvature, concave/convex, and hollowed-out 3D shapes) and the unknown & diverse patterns of the partially available point clouds. In this paper, we propose a novel solution,i.e., Point-block Carving (PC), for completing the complex 3D point cloud completion. Given the partial point cloud as the guidance, we carve a3D block that contains the uniformly distributed 3D points, yielding the entire point cloud. To achieve PC, we propose a new network architecture, i.e., CarveNet. This network conducts the exclusive convolution on each point of the block, where the convolutional kernels are trained on the 3D shape data. CarveNet determines which point should be carved, for effectively recovering the details of the complete shapes. Furthermore, we propose a sensor-aware method for data augmentation,i.e., SensorAug, for training CarveNet on richer patterns of partial point clouds, thus enhancing the completion power of the network. The extensive evaluations on the ShapeNet and KITTI datasets demonstrate the generality of our approach on the partial point clouds with diverse patterns. On these datasets, CarveNet successfully outperforms the state-of-the-art methods.



### Surrogate Model-Based Explainability Methods for Point Cloud NNs
- **Arxiv ID**: http://arxiv.org/abs/2107.13459v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.13459v3)
- **Published**: 2021-07-28 16:13:20+00:00
- **Updated**: 2021-08-18 10:18:21+00:00
- **Authors**: Hanxiao Tan, Helena Kotthaus
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of autonomous driving and robotics, point clouds are showing their excellent real-time performance as raw data from most of the mainstream 3D sensors. Therefore, point cloud neural networks have become a popular research direction in recent years. So far, however, there has been little discussion about the explainability of deep neural networks for point clouds. In this paper, we propose a point cloud-applicable explainability approach based on local surrogate model-based method to show which components contribute to the classification. Moreover, we propose quantitative fidelity validations for generated explanations that enhance the persuasive power of explainability and compare the plausibility of different existing point cloud-applicable explainability methods. Our new explainability approach provides a fairly accurate, more semantically coherent and widely applicable explanation for point cloud classification tasks. Our code is available at https://github.com/Explain3D/LIME-3D



### Learning the shape of female breasts: an open-access 3D statistical shape model of the female breast built from 110 breast scans
- **Arxiv ID**: http://arxiv.org/abs/2107.13463v2
- **DOI**: 10.1007/s00371-022-02431-3
- **Categories**: **cs.CV**, I.4.m; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2107.13463v2)
- **Published**: 2021-07-28 16:14:49+00:00
- **Updated**: 2022-02-01 15:48:27+00:00
- **Authors**: Maximilian Weiherer, Andreas Eigenberger, Bernhard Egger, Vanessa Brbant, Lukas Prantl, Christoph Palm
- **Comment**: 16 pages, 14 figures, accepted for publication in The Visual Computer
- **Journal**: None
- **Summary**: We present the Regensburg Breast Shape Model (RBSM) -- a 3D statistical shape model of the female breast built from 110 breast scans acquired in a standing position, and the first publicly available. Together with the model, a fully automated, pairwise surface registration pipeline used to establish dense correspondence among 3D breast scans is introduced. Our method is computationally efficient and requires only four landmarks to guide the registration process. A major challenge when modeling female breasts from surface-only 3D breast scans is the non-separability of breast and thorax. In order to weaken the strong coupling between breast and surrounding areas, we propose to minimize the variance outside the breast region as much as possible. To achieve this goal, a novel concept called breast probability masks (BPMs) is introduced. A BPM assigns probabilities to each point of a 3D breast scan, telling how likely it is that a particular point belongs to the breast area. During registration, we use BPMs to align the template to the target as accurately as possible inside the breast region and only roughly outside. This simple yet effective strategy significantly reduces the unwanted variance outside the breast region, leading to better statistical shape models in which breast shapes are quite well decoupled from the thorax. The RBSM is thus able to produce a variety of different breast shapes as independently as possible from the shape of the thorax. Our systematic experimental evaluation reveals a generalization ability of 0.17 mm and a specificity of 2.8 mm. To underline the expressiveness of the proposed model, we finally demonstrate in two showcase applications how the RBSM can be used for surgical outcome simulation and the prediction of a missing breast from the remaining one. Our model is available at https://www.rbsm.re-mic.de/.



### A Proof-of-Concept Study of Artificial Intelligence Assisted Contour Revision
- **Arxiv ID**: http://arxiv.org/abs/2107.13465v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.13465v1)
- **Published**: 2021-07-28 16:18:29+00:00
- **Updated**: 2021-07-28 16:18:29+00:00
- **Authors**: Ti Bai, Anjali Balagopal, Michael Dohopolski, Howard E. Morgan, Rafe McBeth, Jun Tan, Mu-Han Lin, David J. Sher, Dan Nguyen, Steve Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of anatomical structures is critical for many medical applications. However, the results are not always clinically acceptable and require tedious manual revision. Here, we present a novel concept called artificial intelligence assisted contour revision (AIACR) and demonstrate its feasibility. The proposed clinical workflow of AIACR is as follows given an initial contour that requires a clinicians revision, the clinician indicates where a large revision is needed, and a trained deep learning (DL) model takes this input to update the contour. This process repeats until a clinically acceptable contour is achieved. The DL model is designed to minimize the clinicians input at each iteration and to minimize the number of iterations needed to reach acceptance. In this proof-of-concept study, we demonstrated the concept on 2D axial images of three head-and-neck cancer datasets, with the clinicians input at each iteration being one mouse click on the desired location of the contour segment. The performance of the model is quantified with Dice Similarity Coefficient (DSC) and 95th percentile of Hausdorff Distance (HD95). The average DSC/HD95 (mm) of the auto-generated initial contours were 0.82/4.3, 0.73/5.6 and 0.67/11.4 for three datasets, which were improved to 0.91/2.1, 0.86/2.4 and 0.86/4.7 with three mouse clicks, respectively. Each DL-based contour update requires around 20 ms. We proposed a novel AIACR concept that uses DL models to assist clinicians in revising contours in an efficient and effective way, and we demonstrated its feasibility by using 2D axial CT images from three head-and-neck cancer datasets.



### Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2107.13467v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.13467v2)
- **Published**: 2021-07-28 16:26:46+00:00
- **Updated**: 2021-08-17 19:08:30+00:00
- **Authors**: Xiaofeng Liu, Site Li, Yubin Ge, Pengyi Ye, Jane You, Jun Lu
- **Comment**: Accepted to ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: The unsupervised domain adaptation (UDA) has been widely adopted to alleviate the data scalability issue, while the existing works usually focus on classifying independently discrete labels. However, in many tasks (e.g., medical diagnosis), the labels are discrete and successively distributed. The UDA for ordinal classification requires inducing non-trivial ordinal distribution prior to the latent space. Target for this, the partially ordered set (poset) is defined for constraining the latent vector. Instead of the typically i.i.d. Gaussian latent prior, in this work, a recursively conditional Gaussian (RCG) set is adapted for ordered constraint modeling, which admits a tractable joint distribution prior. Furthermore, we are able to control the density of content vector that violates the poset constraints by a simple "three-sigma rule". We explicitly disentangle the cross-domain images into a shared ordinal prior induced ordinal content space and two separate source/target ordinal-unrelated spaces, and the self-training is worked on the shared space exclusively for ordinal-aware domain alignment. Extensive experiments on UDA medical diagnoses and facial age estimation demonstrate its effectiveness.



### Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate
- **Arxiv ID**: http://arxiv.org/abs/2107.13469v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2107.13469v2)
- **Published**: 2021-07-28 16:28:01+00:00
- **Updated**: 2021-08-02 03:27:42+00:00
- **Authors**: Xiaofeng Liu, Zhenhua Guo, Site Li, Fangxu Xing, Jane You, C. -C. Jay Kuo, Georges El Fakhri, Jonghye Woo
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: In this work, we propose an adversarial unsupervised domain adaptation (UDA) approach with the inherent conditional and label shifts, in which we aim to align the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is inaccessible in the target domain, the conventional adversarial UDA assumes $p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an alternative to the $p(x|y)$ alignment. To address this, we provide a thorough theoretical and empirical analysis of the conventional adversarial UDA methods under both conditional and label shifts, and propose a novel and practical alternative optimization scheme for adversarial UDA. Specifically, we infer the marginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely align the posterior $p(y|x)$ in testing. Our experimental results demonstrate its effectiveness on both classification and segmentation UDA, and partial UDA.



### Inferring bias and uncertainty in camera calibration
- **Arxiv ID**: http://arxiv.org/abs/2107.13484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13484v1)
- **Published**: 2021-07-28 16:49:39+00:00
- **Updated**: 2021-07-28 16:49:39+00:00
- **Authors**: Annika Hagemann, Moritz Knorr, Holger Janssen, Christoph Stiller
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate camera calibration is a precondition for many computer vision applications. Calibration errors, such as wrong model assumptions or imprecise parameter estimation, can deteriorate a system's overall performance, making the reliable detection and quantification of these errors critical. In this work, we introduce an evaluation scheme to capture the fundamental error sources in camera calibration: systematic errors (biases) and uncertainty (variance). The proposed bias detection method uncovers smallest systematic errors and thereby reveals imperfections of the calibration setup and provides the basis for camera model selection. A novel resampling-based uncertainty estimator enables uncertainty estimation under non-ideal conditions and thereby extends the classical covariance estimator. Furthermore, we derive a simple uncertainty metric that is independent of the camera model. In combination, the proposed methods can be used to assess the accuracy of individual calibrations, but also to benchmark new calibration algorithms, camera models, or calibration setups. We evaluate the proposed methods with simulations and real cameras.



### CRD-CGAN: Category-Consistent and Relativistic Constraints for Diverse Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2107.13516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13516v1)
- **Published**: 2021-07-28 17:38:33+00:00
- **Updated**: 2021-07-28 17:38:33+00:00
- **Authors**: Tao Hu, Chengjiang Long, Chunxia Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Generating photo-realistic images from a text description is a challenging problem in computer vision. Previous works have shown promising performance to generate synthetic images conditional on text by Generative Adversarial Networks (GANs). In this paper, we focus on the category-consistent and relativistic diverse constraints to optimize the diversity of synthetic images. Based on those constraints, a category-consistent and relativistic diverse conditional GAN (CRD-CGAN) is proposed to synthesize $K$ photo-realistic images simultaneously. We use the attention loss and diversity loss to improve the sensitivity of the GAN to word attention and noises. Then, we employ the relativistic conditional loss to estimate the probability of relatively real or fake for synthetic images, which can improve the performance of basic conditional loss. Finally, we introduce a category-consistent loss to alleviate the over-category issues between K synthetic images. We evaluate our approach using the Birds-200-2011, Oxford-102 flower and MSCOCO 2014 datasets, and the extensive experiments demonstrate superiority of the proposed method in comparison with state-of-the-art methods in terms of photorealistic and diversity of the generated synthetic images.



### TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations
- **Arxiv ID**: http://arxiv.org/abs/2107.13542v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.13542v1)
- **Published**: 2021-07-28 17:55:56+00:00
- **Updated**: 2021-07-28 17:55:56+00:00
- **Authors**: Madeleine K. Wyburd, Nicola K. Dinsdale, Ana I. L. Namburete, Mark Jenkinson
- **Comment**: International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI) 2021
- **Journal**: None
- **Summary**: Accurate topology is key when performing meaningful anatomical segmentations, however, it is often overlooked in traditional deep learning methods. In this work we propose TEDS-Net: a novel segmentation method that guarantees accurate topology. Our method is built upon a continuous diffeomorphic framework, which enforces topology preservation. However, in practice, diffeomorphic fields are represented using a finite number of parameters and sampled using methods such as linear interpolation, violating the theoretical guarantees. We therefore introduce additional modifications to more strictly enforce it. Our network learns how to warp a binary prior, with the desired topological characteristics, to complete the segmentation task. We tested our method on myocardium segmentation from an open-source 2D heart dataset. TEDS-Net preserved topology in 100% of the cases, compared to 90% from the U-Net, without sacrificing on Hausdorff Distance or Dice performance. Code will be made available at: www.github.com/mwyburd/TEDS-Net



### Social Processes: Self-Supervised Meta-Learning over Conversational Groups for Forecasting Nonverbal Social Cues
- **Arxiv ID**: http://arxiv.org/abs/2107.13576v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.13576v3)
- **Published**: 2021-07-28 18:01:08+00:00
- **Updated**: 2022-08-26 18:00:05+00:00
- **Authors**: Chirag Raman, Hayley Hung, Marco Loog
- **Comment**: In Proceedings of the European Conference on Computer Vision (ECCV)
  2022 Workshop on Computer Vision for Metaverse (cv4metaverse)
- **Journal**: None
- **Summary**: Free-standing social conversations constitute a yet underexplored setting for human behavior forecasting. While the task of predicting pedestrian trajectories has received much recent attention, an intrinsic difference between these settings is how groups form and disband. Evidence from social psychology suggests that group members in a conversation explicitly self-organize to sustain the interaction by adapting to one another's behaviors. Crucially, the same individual is unlikely to adapt similarly across different groups; contextual factors such as perceived relationships, attraction, rapport, etc., influence the entire spectrum of participants' behaviors. A question arises: how can we jointly forecast the mutually dependent futures of conversation partners by modeling the dynamics unique to every group? In this paper, we propose the Social Process (SP) models, taking a novel meta-learning and stochastic perspective of group dynamics. Training group-specific forecasting models hinders generalization to unseen groups and is challenging given limited conversation data. In contrast, our SP models treat interaction sequences from a single group as a meta-dataset: we condition forecasts for a sequence from a given group on other observed-future sequence pairs from the same group. In this way, an SP model learns to adapt its forecasts to the unique dynamics of the interacting partners, generalizing to unseen groups in a data-efficient manner. Additionally, we first rethink the task formulation itself, motivating task requirements from social science literature that prior formulations have overlooked. For our formulation of Social Cue Forecasting, we evaluate the empirical performance of our SP models against both non-meta-learning and meta-learning approaches with similar assumptions. The SP models yield improved performance on synthetic and real-world behavior datasets.



### Fast and Scalable Image Search For Histology
- **Arxiv ID**: http://arxiv.org/abs/2107.13587v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2107.13587v1)
- **Published**: 2021-07-28 18:15:03+00:00
- **Updated**: 2021-07-28 18:15:03+00:00
- **Authors**: Chengkuan Chen, Ming Y. Lu, Drew F. K. Williamson, Tiffany Y. Chen, Andrew J. Schaumberg, Faisal Mahmood
- **Comment**: None
- **Journal**: None
- **Summary**: The expanding adoption of digital pathology has enabled the curation of large repositories of histology whole slide images (WSIs), which contain a wealth of information. Similar pathology image search offers the opportunity to comb through large historical repositories of gigapixel WSIs to identify cases with similar morphological features and can be particularly useful for diagnosing rare diseases, identifying similar cases for predicting prognosis, treatment outcomes, and potential clinical trial success. A critical challenge in developing a WSI search and retrieval system is scalability, which is uniquely challenging given the need to search a growing number of slides that each can consist of billions of pixels and are several gigabytes in size. Such systems are typically slow and retrieval speed often scales with the size of the repository they search through, making their clinical adoption tedious and are not feasible for repositories that are constantly growing. Here we present Fast Image Search for Histopathology (FISH), a histology image search pipeline that is infinitely scalable and achieves constant search speed that is independent of the image database size while being interpretable and without requiring detailed annotations. FISH uses self-supervised deep learning to encode meaningful representations from WSIs and a Van Emde Boas tree for fast search, followed by an uncertainty-based ranking algorithm to retrieve similar WSIs. We evaluated FISH on multiple tasks and datasets with over 22,000 patient cases spanning 56 disease subtypes. We additionally demonstrate that FISH can be used to assist with the diagnosis of rare cancer types where sufficient cases may not be available to train traditional supervised deep models. FISH is available as an easy-to-use, open-source software package (https://github.com/mahmoodlab/FISH).



### Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2108.04351v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04351v2)
- **Published**: 2021-07-28 18:21:20+00:00
- **Updated**: 2021-08-19 06:12:42+00:00
- **Authors**: Amey Thakur, Mega Satish
- **Comment**: This was an undergraduate research effort, and in retrospect, it
  isn't comprehensive enough
- **Journal**: None
- **Summary**: This paper aims to demonstrate the efficiency of the Adversarial Open Domain Adaption framework for sketch-to-photo synthesis. The unsupervised open domain adaption for generating realistic photos from a hand-drawn sketch is challenging as there is no such sketch of that class for training data. The absence of learning supervision and the huge domain gap between both the freehand drawing and picture domains make it hard. We present an approach that learns both sketch-to-photo and photo-to-sketch generation to synthesise the missing freehand drawings from pictures. Due to the domain gap between synthetic sketches and genuine ones, the generator trained on false drawings may produce unsatisfactory results when dealing with drawings of lacking classes. To address this problem, we offer a simple but effective open-domain sampling and optimization method that tricks the generator into considering false drawings as genuine. Our approach generalises the learnt sketch-to-photo and photo-to-sketch mappings from in-domain input to open-domain categories. On the Scribble and SketchyCOCO datasets, we compared our technique to the most current competing methods. For many types of open-domain drawings, our model outperforms impressive results in synthesising accurate colour, substance, and retaining the structural layout.



### Adding Visibility to Visibility Graphs: Weighting Visibility Analysis with Attenuation Coefficients
- **Arxiv ID**: http://arxiv.org/abs/2108.04231v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.HC, I.6
- **Links**: [PDF](http://arxiv.org/pdf/2108.04231v1)
- **Published**: 2021-07-28 18:36:56+00:00
- **Updated**: 2021-07-28 18:36:56+00:00
- **Authors**: Mathew Schwartz, Margarita Vinnikov, John Federici
- **Comment**: 9 pages, 12 figures, accepted to SIMAUD conference 2021
- **Journal**: None
- **Summary**: Evaluating the built environment based on visibility has been long used as a tool for human-centric design. The origins of isovists and visibility graphs are within interior spaces, while more recently, these evaluation techniques have been applied in the urban context. One of the key differentiators of an outside environment is the weather, which has largely been ignored in the design computation and space-syntax research areas. While a visibility graph is a straightforward metric for determining connectivity between regions of space through a line of sight calculation, this approach largely ignores the actual visibility of one point to another. This paper introduces a new method for weighting a visibility graph based on weather conditions (i.e. rain, fog, snow). These new factors are integrated into visibility graphs and applied to sample environments to demonstrate the variance between assuming a straight line of sight and reduced visibility.



### Adaptation and Generalization for Unknown Sensitive Factors of Variations
- **Arxiv ID**: http://arxiv.org/abs/2107.13625v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2107.13625v3)
- **Published**: 2021-07-28 20:18:08+00:00
- **Updated**: 2021-11-17 20:18:41+00:00
- **Authors**: William Paul, Philippe Burlina
- **Comment**: None
- **Journal**: None
- **Summary**: Assured AI in unrestricted settings is a critical problem. Our framework addresses AI assurance challenges lying at the intersection of domain adaptation, fairness, and counterfactuals analysis, operating via the discovery and intervention on factors of variations in data (e.g. weather or illumination conditions) that significantly affect the robustness of AI models. Robustness is understood here as insensitivity of the model performance to variations in sensitive factors. Sensitive factors are traditionally set in a supervised setting, whereby factors are known a-priori (e.g. for fairness this could be factors like sex or race). In contrast, our motivation is real-life scenarios where less, or nothing, is actually known a-priori about certain factors that cause models to fail. This leads us to consider various settings (unsupervised, domain generalization, semi-supervised) that correspond to different degrees of incomplete knowledge about those factors. Therefore, our two step approach works by a) discovering sensitive factors that cause AI systems to fail in a unsupervised fashion, and then b) intervening models to lessen these factor's influence. Our method considers 3 interventions consisting of Augmentation, Coherence, and Adversarial Interventions (ACAI). We demonstrate the ability for interventions on discovered/source factors to generalize to target/real factors. We also demonstrate how adaptation to real factors of variations can be performed in the semi-supervised case where some target factor labels are known, via automated intervention selection. Experiments show that our approach improves on baseline models, with regard to achieving optimal utility vs. sensitivity/robustness tradeoffs.



### United We Learn Better: Harvesting Learning Improvements From Class Hierarchies Across Tasks
- **Arxiv ID**: http://arxiv.org/abs/2107.13627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13627v1)
- **Published**: 2021-07-28 20:25:37+00:00
- **Updated**: 2021-07-28 20:25:37+00:00
- **Authors**: Sindi Shkodrani, Yu Wang, Marco Manfredi, Nra Baka
- **Comment**: None
- **Journal**: None
- **Summary**: Attempts of learning from hierarchical taxonomies in computer vision have been mostly focusing on image classification. Though ways of best harvesting learning improvements from hierarchies in classification are far from being solved, there is a need to target these problems in other vision tasks such as object detection. As progress on the classification side is often dependent on hierarchical cross-entropy losses, novel detection architectures using sigmoid as an output function instead of softmax cannot easily apply these advances, requiring novel methods in detection. In this work we establish a theoretical framework based on probability and set theory for extracting parent predictions and a hierarchical loss that can be used across tasks, showing results across classification and detection benchmarks and opening up the possibility of hierarchical learning for sigmoid-based detection architectures.



### Underwater inspection and intervention dataset
- **Arxiv ID**: http://arxiv.org/abs/2107.13628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13628v1)
- **Published**: 2021-07-28 20:29:14+00:00
- **Updated**: 2021-07-28 20:29:14+00:00
- **Authors**: Tomasz Luczynski, Jonatan Scharff Willners, Elizabeth Vargas, Joshua Roe, Shida Xu, Yu Cao, Yvan Petillot, Sen Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel dataset for the development of visual navigation and simultaneous localisation and mapping (SLAM) algorithms as well as for underwater intervention tasks. It differs from existing datasets as it contains ground truth for the vehicle's position captured by an underwater motion tracking system. The dataset contains distortion-free and rectified stereo images along with the calibration parameters of the stereo camera setup. Furthermore, the experiments were performed and recorded in a controlled environment, where current and waves could be generated allowing the dataset to cover a wide range of conditions - from calm water to waves and currents of significant strength.



### Discovering 3D Parts from Image Collections
- **Arxiv ID**: http://arxiv.org/abs/2107.13629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13629v1)
- **Published**: 2021-07-28 20:29:16+00:00
- **Updated**: 2021-07-28 20:29:16+00:00
- **Authors**: Chun-Han Yao, Wei-Chih Hung, Varun Jampani, Ming-Hsuan Yang
- **Comment**: Accepted by ICCV 2021. Project page: https://chhankyao.github.io/lpd/
- **Journal**: None
- **Summary**: Reasoning 3D shapes from 2D images is an essential yet challenging task, especially when only single-view images are at our disposal. While an object can have a complicated shape, individual parts are usually close to geometric primitives and thus are easier to model. Furthermore, parts provide a mid-level representation that is robust to appearance variations across objects in a particular category. In this work, we tackle the problem of 3D part discovery from only 2D image collections. Instead of relying on manually annotated parts for supervision, we propose a self-supervised approach, latent part discovery (LPD). Our key insight is to learn a novel part shape prior that allows each part to fit an object shape faithfully while constrained to have simple geometry. Extensive experiments on the synthetic ShapeNet, PartNet, and real-world Pascal 3D+ datasets show that our method discovers consistent object parts and achieves favorable reconstruction accuracy compared to the existing methods with the same level of supervision.



### Sign and Search: Sign Search Functionality for Sign Language Lexica
- **Arxiv ID**: http://arxiv.org/abs/2107.13637v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM, I.4.9; I.5.4; H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2107.13637v1)
- **Published**: 2021-07-28 20:48:53+00:00
- **Updated**: 2021-07-28 20:48:53+00:00
- **Authors**: Manolis Fragkiadakis, Peter van der Putten
- **Comment**: Accepted for the 1st International Workshop on Automatic Translation
  for Signed and Spoken Languages (ATS4SSL), August 20, 2021
- **Journal**: None
- **Summary**: Sign language lexica are a useful resource for researchers and people learning sign languages. Current implementations allow a user to search a sign either by its gloss or by selecting its primary features such as handshape and location. This study focuses on exploring a reverse search functionality where a user can sign a query sign in front of a webcam and retrieve a set of matching signs. By extracting different body joints combinations (upper body, dominant hand's arm and wrist) using the pose estimation framework OpenPose, we compare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance metrics between 20 query signs, each performed by eight participants on a 1200 sign lexicon. The results show that UMAP and DTW can predict a matching sign with an 80\% and 71\% accuracy respectively at the top-20 retrieved signs using the movement of the dominant hand arm. Using DTW and adding more sign instances from other participants in the lexicon, the accuracy can be raised to 90\% at the top-10 ranking. Our results suggest that our methodology can be used with no training in any sign language lexicon regardless of its size.



### Lighter Stacked Hourglass Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.13643v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.13643v1)
- **Published**: 2021-07-28 21:05:34+00:00
- **Updated**: 2021-07-28 21:05:34+00:00
- **Authors**: Ahmed Elhagry, Mohamed Saeed, Musie Araia
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose estimation (HPE) is one of the most challenging tasks in computer vision as humans are deformable by nature and thus their pose has so much variance. HPE aims to correctly identify the main joint locations of a single person or multiple people in a given image or video. Locating joints of a person in images or videos is an important task that can be applied in action recognition and object tracking. As have many computer vision tasks, HPE has advanced massively with the introduction of deep learning to the field. In this paper, we focus on one of the deep learning-based approaches of HPE proposed by Newell et al., which they named the stacked hourglass network. Their approach is widely used in many applications and is regarded as one of the best works in this area. The main focus of their approach is to capture as much information as it can at all possible scales so that a coherent understanding of the local features and full-body location is achieved. Their findings demonstrate that important cues such as orientation of a person, arrangement of limbs, and adjacent joints' relative location can be identified from multiple scales at different resolutions. To do so, they makes use of a single pipeline to process images in multiple resolutions, which comprises a skip layer to not lose spatial information at each resolution. The resolution of the images stretches as lower as 4x4 to make sure that a smaller spatial feature is included. In this study, we study the effect of architectural modifications on the computational speed and accuracy of the network.



### Egyptian Sign Language Recognition Using CNN and LSTM
- **Arxiv ID**: http://arxiv.org/abs/2107.13647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.13647v1)
- **Published**: 2021-07-28 21:33:35+00:00
- **Updated**: 2021-07-28 21:33:35+00:00
- **Authors**: Ahmed Elhagry, Rawan Glalal Elrayes
- **Comment**: None
- **Journal**: None
- **Summary**: Sign language is a set of gestures that deaf people use to communicate. Unfortunately, normal people don't understand it, which creates a communication gap that needs to be filled. Because of the variations in (Egyptian Sign Language) ESL from one region to another, ESL provides a challenging research problem. In this work, we are providing applied research with its video-based Egyptian sign language recognition system that serves the local community of deaf people in Egypt, with a moderate and reasonable accuracy. We present a computer vision system with two different neural networks architectures. The first is a Convolutional Neural Network (CNN) for extracting spatial features. The CNN model was retrained on the inception mod. The second architecture is a CNN followed by a Long Short-Term Memory (LSTM) for extracting both spatial and temporal features. The two models achieved an accuracy of 90% and 72%, respectively. We examined the power of these two architectures to distinguish between 9 common words (with similar signs) among some deaf people community in Egypt.



### Spot What Matters: Learning Context Using Graph Convolutional Networks for Weakly-Supervised Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.13648v1
- **DOI**: 10.1007/978-3-030-68799-1_9
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.13648v1)
- **Published**: 2021-07-28 21:37:18+00:00
- **Updated**: 2021-07-28 21:37:18+00:00
- **Authors**: Michail Tsiaousis, Gertjan Burghouts, Fieke Hillerstrm, Peter van der Putten
- **Comment**: Paper presented at the International Workshop on Deep Learning for
  Human-Centric Activity Understanding (DL-HAU2020), January 11, 2021
- **Journal**: International Workshop on Deep Learning for Human-Centric Activity
  Understanding (DL-HAU2020), January 11, 2021
- **Summary**: The dominant paradigm in spatiotemporal action detection is to classify actions using spatiotemporal features learned by 2D or 3D Convolutional Networks. We argue that several actions are characterized by their context, such as relevant objects and actors present in the video. To this end, we introduce an architecture based on self-attention and Graph Convolutional Networks in order to model contextual cues, such as actor-actor and actor-object interactions, to improve human action detection in video. We are interested in achieving this in a weakly-supervised setting, i.e. using as less annotations as possible in terms of action bounding boxes. Our model aids explainability by visualizing the learned context as an attention map, even for actions and objects unseen during training. We evaluate how well our model highlights the relevant context by introducing a quantitative metric based on recall of objects retrieved by attention maps. Our model relies on a 3D convolutional RGB stream, and does not require expensive optical flow computation. We evaluate our models on the DALY dataset, which consists of human-object interaction actions. Experimental results show that our contextualized approach outperforms a baseline action detection approach by more than 2 points in Video-mAP. Code is available at \url{https://github.com/micts/acgcn}



### Similarity and symmetry measures based on fuzzy descriptors of image objects` composition
- **Arxiv ID**: http://arxiv.org/abs/2107.13651v1
- **DOI**: None
- **Categories**: **cs.CV**, 03B52, 94A08, I.4.8; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2107.13651v1)
- **Published**: 2021-07-28 21:42:19+00:00
- **Updated**: 2021-07-28 21:42:19+00:00
- **Authors**: Marcin Iwanowski, Marcin Grzabka
- **Comment**: 10 pages
- **Journal**: WSCG 2021 29. International Conference in Central Europe on
  Computer Graphics, Visualization and Computer Vision
- **Summary**: The paper describes a method for measuring the similarity and symmetry of an image annotated with bounding boxes indicating image objects. The latter representation became popular recently due to the rapid development of fast and efficient deep-learning-based object-detection methods. The proposed approach allows for comparing sets of bounding boxes to estimate the degree of similarity of their underlying images. It is based on the fuzzy approach that uses the fuzzy mutual position (FMP) matrix to describe spatial composition and relations between bounding boxes within an image. A method of computing the similarity of two images described by their FMP matrices is proposed and the algorithm of its computation. It outputs the single scalar value describing the degree of content-based image similarity. By modifying the method`s parameters, instead of similarity, the reflectional symmetry of object composition may also be measured. The proposed approach allows for measuring differences in objects` composition of various intensities. It is also invariant to translation and scaling and - in case of symmetry detection - position and orientation of the symmetry axis. A couple of examples illustrate the method.




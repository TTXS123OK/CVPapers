# Arxiv Papers in cs.CV on 2021-03-21
### Structural Textile Pattern Recognition and Processing Based on Hypergraphs
- **Arxiv ID**: http://arxiv.org/abs/2103.11271v1
- **DOI**: 10.1007/s10791-020-09384-y
- **Categories**: **cs.IR**, cs.CC, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.11271v1)
- **Published**: 2021-03-21 00:44:40+00:00
- **Updated**: 2021-03-21 00:44:40+00:00
- **Authors**: Vuong M. Ngo, Sven Helmer, Nhien-An Le-Khac, M-Tahar Kechadi
- **Comment**: 38 pages, 23 figures
- **Journal**: Information Retrieval Journal, Springer, 2021
- **Summary**: The humanities, like many other areas of society, are currently undergoing major changes in the wake of digital transformation. However, in order to make collection of digitised material in this area easily accessible, we often still lack adequate search functionality. For instance, digital archives for textiles offer keyword search, which is fairly well understood, and arrange their content following a certain taxonomy, but search functionality at the level of thread structure is still missing. To facilitate the clustering and search, we introduce an approach for recognising similar weaving patterns based on their structures for textile archives. We first represent textile structures using hypergraphs and extract multisets of k-neighbourhoods describing weaving patterns from these graphs. Then, the resulting multisets are clustered using various distance measures and various clustering algorithms (K-Means for simplicity and hierarchical agglomerative algorithms for precision). We evaluate the different variants of our approach experimentally, showing that this can be implemented efficiently (meaning it has linear complexity), and demonstrate its quality to query and cluster datasets containing large textile samples. As, to the est of our knowledge, this is the first practical approach for explicitly modelling complex and irregular weaving patterns usable for retrieval, we aim at establishing a solid baseline.



### High precision control and deep learning-based corn stand counting algorithms for agricultural robot
- **Arxiv ID**: http://arxiv.org/abs/2103.11276v1
- **DOI**: 10.1007/s10514-020-09915-y
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2103.11276v1)
- **Published**: 2021-03-21 01:13:38+00:00
- **Updated**: 2021-03-21 01:13:38+00:00
- **Authors**: Zhongzhong Zhang, Erkan Kayacan, Benjamin Thompson, Girish Chowdhary
- **Comment**: 14 pages, 9 figures
- **Journal**: Autonomous Robots, volume 44, pages 1289-1302, 2020
- **Summary**: This paper presents high precision control and deep learning-based corn stand counting algorithms for a low-cost, ultra-compact 3D printed and autonomous field robot for agricultural operations. Currently, plant traits, such as emergence rate, biomass, vigor, and stand counting, are measured manually. This is highly labor-intensive and prone to errors. The robot, termed TerraSentia, is designed to automate the measurement of plant traits for efficient phenotyping as an alternative to manual measurements. In this paper, we formulate a Nonlinear Moving Horizon Estimator (NMHE) that identifies key terrain parameters using onboard robot sensors and a learning-based Nonlinear Model Predictive Control (NMPC) that ensures high precision path tracking in the presence of unknown wheel-terrain interaction. Moreover, we develop a machine vision algorithm designed to enable an ultra-compact ground robot to count corn stands by driving through the fields autonomously. The algorithm leverages a deep network to detect corn plants in images, and a visual tracking model to re-identify detected objects at different time steps. We collected data from 53 corn plots in various fields for corn plants around 14 days after emergence (stage V3 - V4). The robot predictions have agreed well with the ground truth with $C_{robot}=1.02 \times C_{human}-0.86$ and a correlation coefficient $R=0.96$. The mean relative error given by the algorithm is $-3.78\%$, and the standard deviation is $6.76\%$. These results indicate a first and significant step towards autonomous robot-based real-time phenotyping using low-cost, ultra-compact ground robots for corn and potentially other crops.



### Geo-Spatiotemporal Features and Shape-Based Prior Knowledge for Fine-grained Imbalanced Data Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.11285v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.11285v1)
- **Published**: 2021-03-21 02:01:38+00:00
- **Updated**: 2021-03-21 02:01:38+00:00
- **Authors**: Charles A. Kantor, Marta Skreta, Brice Rauby, Léonard Boussioux, Emmanuel Jehanno, Alexandra Luccioni, David Rolnick, Hugues Talbot
- **Comment**: Copyright by the authors. All rights reserved to authors only.
  Correspondence to: ckantor (at) stanford [dot] edu
- **Journal**: Proc. IJCAI 2021, Workshop on AI for Social Good, Harvard
  University (2021)
- **Summary**: Fine-grained classification aims at distinguishing between items with similar global perception and patterns, but that differ by minute details. Our primary challenges come from both small inter-class variations and large intra-class variations. In this article, we propose to combine several innovations to improve fine-grained classification within the use-case of wildlife, which is of practical interest for experts. We utilize geo-spatiotemporal data to enrich the picture information and further improve the performance. We also investigate state-of-the-art methods for handling the imbalanced data issue.



### Deep Dense Multi-scale Network for Snow Removal Using Semantic and Geometric Priors
- **Arxiv ID**: http://arxiv.org/abs/2103.11298v1
- **DOI**: 10.1109/TIP.2021.3104166
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11298v1)
- **Published**: 2021-03-21 03:30:30+00:00
- **Updated**: 2021-03-21 03:30:30+00:00
- **Authors**: Kaihao Zhang, Rongqing Li, Yanjiang Yu, Wenhan Luo, Changsheng Li, Hongdong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Images captured in snowy days suffer from noticeable degradation of scene visibility, which degenerates the performance of current vision-based intelligent systems. Removing snow from images thus is an important topic in computer vision. In this paper, we propose a Deep Dense Multi-Scale Network (\textbf{DDMSNet}) for snow removal by exploiting semantic and geometric priors. As images captured in outdoor often share similar scenes and their visibility varies with depth from camera, such semantic and geometric information provides a strong prior for snowy image restoration. We incorporate the semantic and geometric maps as input and learn the semantic-aware and geometry-aware representation to remove snow. In particular, we first create a coarse network to remove snow from the input images. Then, the coarsely desnowed images are fed into another network to obtain the semantic and geometric labels. Finally, we design a DDMSNet to learn semantic-aware and geometry-aware representation via a self-attention mechanism to produce the final clean images. Experiments evaluated on public synthetic and real-world snowy images verify the superiority of the proposed method, offering better results both quantitatively and qualitatively.



### A Modular and Unified Framework for Detecting and Localizing Video Anomalies
- **Arxiv ID**: http://arxiv.org/abs/2103.11299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11299v1)
- **Published**: 2021-03-21 04:16:51+00:00
- **Updated**: 2021-03-21 04:16:51+00:00
- **Authors**: Keval Doshi, Yasin Yilmaz
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection in videos has been attracting an increasing amount of attention. Despite the competitive performance of recent methods on benchmark datasets, they typically lack desirable features such as modularity, cross-domain adaptivity, interpretability, and real-time anomalous event detection. Furthermore, current state-of-the-art approaches are evaluated using the standard instance-based detection metric by considering video frames as independent instances, which is not ideal for video anomaly detection. Motivated by these research gaps, we propose a modular and unified approach to the online video anomaly detection and localization problem, called MOVAD, which consists of a novel transfer learning based plug-and-play architecture, a sequential anomaly detector, a mathematical framework for selecting the detection threshold, and a suitable performance metric for real-time anomalous event detection in videos. Extensive performance evaluations on benchmark datasets show that the proposed framework significantly outperforms the current state-of-the-art approaches.



### An Unsupervised Sampling Approach for Image-Sentence Matching Using Document-Level Structural Information
- **Arxiv ID**: http://arxiv.org/abs/2104.02605v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.02605v1)
- **Published**: 2021-03-21 05:43:29+00:00
- **Updated**: 2021-03-21 05:43:29+00:00
- **Authors**: Zejun Li, Zhongyu Wei, Zhihao Fan, Haijun Shan, Xuanjing Huang
- **Comment**: To be published in AAAI2021
- **Journal**: None
- **Summary**: In this paper, we focus on the problem of unsupervised image-sentence matching. Existing research explores to utilize document-level structural information to sample positive and negative instances for model training. Although the approach achieves positive results, it introduces a sampling bias and fails to distinguish instances with high semantic similarity. To alleviate the bias, we propose a new sampling strategy to select additional intra-document image-sentence pairs as positive or negative samples. Furthermore, to recognize the complex pattern in intra-document samples, we propose a Transformer based model to capture fine-grained features and implicitly construct a graph for each document, where concepts in a document are introduced to bridge the representation learning of images and sentences in the context of a document. Experimental results show the effectiveness of our approach to alleviate the bias and learn well-aligned multimodal representations.



### PGT: A Progressive Method for Training Models on Long Videos
- **Arxiv ID**: http://arxiv.org/abs/2103.11313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11313v1)
- **Published**: 2021-03-21 06:15:20+00:00
- **Updated**: 2021-03-21 06:15:20+00:00
- **Authors**: Bo Pang, Gao Peng, Yizhuo Li, Cewu Lu
- **Comment**: CVPR21, Oral
- **Journal**: CVPR2021 oral
- **Summary**: Convolutional video models have an order of magnitude larger computational complexity than their counterpart image-level models. Constrained by computational resources, there is no model or training method that can train long video sequences end-to-end. Currently, the main-stream method is to split a raw video into clips, leading to incomplete fragmentary temporal information flow. Inspired by natural language processing techniques dealing with long sentences, we propose to treat videos as serial fragments satisfying Markov property, and train it as a whole by progressively propagating information through the temporal dimension in multiple steps. This progressive training (PGT) method is able to train long videos end-to-end with limited resources and ensures the effective transmission of information. As a general and robust training method, we empirically demonstrate that it yields significant performance improvements on different models and datasets. As an illustrative example, the proposed method improves SlowOnly network by 3.7 mAP on Charades and 1.9 top-1 accuracy on Kinetics with negligible parameter and computation overhead. Code is available at https://github.com/BoPang1996/PGT.



### A Learned Compact and Editable Light Field Representation
- **Arxiv ID**: http://arxiv.org/abs/2103.11314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11314v1)
- **Published**: 2021-03-21 06:16:11+00:00
- **Updated**: 2021-03-21 06:16:11+00:00
- **Authors**: Menghan Xia, Jose Echevarria, Minshan Xie, Tien-Tsin Wong
- **Comment**: submitted to TIP since 2020.08.03
- **Journal**: None
- **Summary**: Light fields are 4D scene representation typically structured as arrays of views, or several directional samples per pixel in a single view. This highly correlated structure is not very efficient to transmit and manipulate (especially for editing), though. To tackle these problems, we present a novel compact and editable light field representation, consisting of a set of visual channels (i.e. the central RGB view) and a complementary meta channel that encodes the residual geometric and appearance information. The visual channels in this representation can be edited using existing 2D image editing tools, before accurately reconstructing the whole edited light field back. We propose to learn this representation via an autoencoder framework, consisting of an encoder for learning the representation, and a decoder for reconstructing the light field. To handle the challenging occlusions and propagation of edits, we specifically designed an editing-aware decoding network and its associated training strategy, so that the edits to the visual channels can be consistently propagated to the whole light field upon reconstruction.Experimental results show that our proposed method outperforms related existing methods in reconstruction accuracy, and achieves visually pleasant performance in editing propagation.



### Reference-Aided Part-Aligned Feature Disentangling for Video Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2103.11319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11319v1)
- **Published**: 2021-03-21 06:53:57+00:00
- **Updated**: 2021-03-21 06:53:57+00:00
- **Authors**: Guoqing Zhang, Yuhao Chen, Yang Dai, Yuhui Zheng, Yi Wu
- **Comment**: 6 pages, accepted by ICME 2021
- **Journal**: None
- **Summary**: Recently, video-based person re-identification (re-ID) has drawn increasing attention in compute vision community because of its practical application prospects. Due to the inaccurate person detections and pose changes, pedestrian misalignment significantly increases the difficulty of feature extraction and matching. To address this problem, in this paper, we propose a \textbf{R}eference-\textbf{A}ided \textbf{P}art-\textbf{A}ligned (\textbf{RAPA}) framework to disentangle robust features of different parts. Firstly, in order to obtain better references between different videos, a pose-based reference feature learning module is introduced. Secondly, an effective relation-based part feature disentangling module is explored to align frames within each video. By means of using both modules, the informative parts of pedestrian in videos are well aligned and more discriminative feature representation is generated. Comprehensive experiments on three widely-used benchmarks, i.e. iLIDS-VID, PRID-2011 and MARS datasets verify the effectiveness of the proposed framework. Our code will be made publicly available.



### Unsupervised Learning of Depth Estimation and Visual Odometry for Sparse Light Field Cameras
- **Arxiv ID**: http://arxiv.org/abs/2103.11322v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, I.4.8; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2103.11322v1)
- **Published**: 2021-03-21 07:13:14+00:00
- **Updated**: 2021-03-21 07:13:14+00:00
- **Authors**: S. Tejaswi Digumarti, Joseph Daniel, Ahalya Ravendran, Donald G. Dansereau
- **Comment**: Submitted to IROS 2021, 8 pages, 6 figures, 2 tables, for associated
  project page, see https://roboticimaging.org/Projects/LearnLFOdo/
- **Journal**: None
- **Summary**: While an exciting diversity of new imaging devices is emerging that could dramatically improve robotic perception, the challenges of calibrating and interpreting these cameras have limited their uptake in the robotics community. In this work we generalise techniques from unsupervised learning to allow a robot to autonomously interpret new kinds of cameras. We consider emerging sparse light field (LF) cameras, which capture a subset of the 4D LF function describing the set of light rays passing through a plane. We introduce a generalised encoding of sparse LFs that allows unsupervised learning of odometry and depth. We demonstrate the proposed approach outperforming monocular and conventional techniques for dealing with 4D imagery, yielding more accurate odometry and depth maps and delivering these with metric scale. We anticipate our technique to generalise to a broad class of LF and sparse LF cameras, and to enable unsupervised recalibration for coping with shifts in camera behaviour over the lifetime of a robot. This work represents a first step toward streamlining the integration of new kinds of imaging devices in robotics applications.



### Cross-Dataset Collaborative Learning for Semantic Segmentation in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2103.11351v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11351v3)
- **Published**: 2021-03-21 09:59:47+00:00
- **Updated**: 2021-11-02 01:55:40+00:00
- **Authors**: Li Wang, Dong Li, Han Liu, Jinzhang Peng, Lu Tian, Yi Shan
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is an important task for scene understanding in self-driving cars and robotics, which aims to assign dense labels for all pixels in the image. Existing work typically improves semantic segmentation performance by exploring different network architectures on a target dataset. Little attention has been paid to build a unified system by simultaneously learning from multiple datasets due to the inherent distribution shift across different datasets. In this paper, we propose a simple, flexible, and general method for semantic segmentation, termed Cross-Dataset Collaborative Learning (CDCL). Our goal is to train a unified model for improving the performance in each dataset by leveraging information from all the datasets. Specifically, we first introduce a family of Dataset-Aware Blocks (DAB) as the fundamental computing units of the network, which help capture homogeneous convolutional representations and heterogeneous statistics across different datasets. Second, we present a Dataset Alternation Training (DAT) mechanism to facilitate the collaborative optimization procedure. We conduct extensive evaluations on diverse semantic segmentation datasets for autonomous driving. Experiments demonstrate that our method consistently achieves notable improvements over prior single-dataset and cross-dataset training methods without introducing extra FLOPs. Particularly, with the same architecture of PSPNet (ResNet-18), our method outperforms the single-dataset baseline by 5.65\%, 6.57\%, and 5.79\% mIoU on the validation sets of Cityscapes, BDD100K, CamVid, respectively. We also apply CDCL for point cloud 3D semantic segmentation and achieve improved performance, which further validates the superiority and generality of our method. Code and models will be released.



### A new public Alsat-2B dataset for single-image super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2103.12547v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.12547v1)
- **Published**: 2021-03-21 10:47:38+00:00
- **Updated**: 2021-03-21 10:47:38+00:00
- **Authors**: Achraf Djerida, Khelifa Djerriri, Moussa Sofiane Karoui, Mohammed El Amin larabi
- **Comment**: This paper has been Accepted for publication in the International
  Geoscience and Remote Sensing Symposium (IGARSS 2021)
- **Journal**: None
- **Summary**: Currently, when reliable training datasets are available, deep learning methods dominate the proposed solutions for image super-resolution. However, for remote sensing benchmarks, it is very expensive to obtain high spatial resolution images. Most of the super-resolution methods use down-sampling techniques to simulate low and high spatial resolution pairs and construct the training samples. To solve this issue, the paper introduces a novel public remote sensing dataset (Alsat2B) of low and high spatial resolution images (10m and 2.5m respectively) for the single-image super-resolution task. The high-resolution images are obtained through pan-sharpening. Besides, the performance of some super-resolution methods on the dataset is assessed based on common criteria. The obtained results reveal that the proposed scheme is promising and highlight the challenges in the dataset which shows the need for advanced methods to grasp the relationship between the low and high-resolution patches.



### Self adversarial attack as an augmentation method for immunohistochemical stainings
- **Arxiv ID**: http://arxiv.org/abs/2103.11362v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.11362v1)
- **Published**: 2021-03-21 10:48:40+00:00
- **Updated**: 2021-03-21 10:48:40+00:00
- **Authors**: Jelica Vasiljević, Friedrich Feuerhake, Cédric Wemmert, Thomas Lampert
- **Comment**: Accepted to ISBI 2021
- **Journal**: None
- **Summary**: It has been shown that unpaired image-to-image translation methods constrained by cycle-consistency hide the information necessary for accurate input reconstruction as imperceptible noise. We demonstrate that, when applied to histopathology data, this hidden noise appears to be related to stain specific features and show that this is the case with two immunohistochemical stainings during translation to Periodic acid- Schiff (PAS), a histochemical staining method commonly applied in renal pathology. Moreover, by perturbing this hidden information, the translation models produce different, plausible outputs. We demonstrate that this property can be used as an augmentation method which, in a case of supervised glomeruli segmentation, leads to improved performance.



### Natural Perturbed Training for General Robustness of Neural Network Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2103.11372v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.11372v1)
- **Published**: 2021-03-21 11:47:38+00:00
- **Updated**: 2021-03-21 11:47:38+00:00
- **Authors**: Sadaf Gulshad, Arnold Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on the robustness of neural networks for classification. To permit a fair comparison between methods to achieve robustness, we first introduce a standard based on the mensuration of a classifier's degradation. Then, we propose natural perturbed training to robustify the network. Natural perturbations will be encountered in practice: the difference of two images of the same object may be approximated by an elastic deformation (when they have slightly different viewing angles), by occlusions (when they hide differently behind objects), or by saturation, Gaussian noise etc. Training some fraction of the epochs on random versions of such variations will help the classifier to learn better. We conduct extensive experiments on six datasets of varying sizes and granularity. Natural perturbed learning show better and much faster performance than adversarial training on clean, adversarial as well as natural perturbed images. It even improves general robustness on perturbations not seen during the training. For Cifar-10 and STL-10 natural perturbed training even improves the accuracy for clean data and reaches the state of the art performance. Ablation studies verify the effectiveness of natural perturbed training.



### ProgressiveSpinalNet architecture for FC layers
- **Arxiv ID**: http://arxiv.org/abs/2103.11373v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.11373v1)
- **Published**: 2021-03-21 11:54:50+00:00
- **Updated**: 2021-03-21 11:54:50+00:00
- **Authors**: Praveen Chopra
- **Comment**: None
- **Journal**: None
- **Summary**: In deeplearning models the FC (fully connected) layer has biggest important role for classification of the input based on the learned features from previous layers. The FC layers has highest numbers of parameters and fine-tuning these large numbers of parameters, consumes most of the computational resources, so in this paper it is aimed to reduce these large numbers of parameters significantly with improved performance. The motivation is inspired from SpinalNet and other biological architecture. The proposed architecture has a gradient highway between input to output layers and this solves the problem of diminishing gradient in deep networks. In this all the layers receives the input from previous layers as well as the CNN layer output and this way all layers contribute in decision making with last layer. This approach has improved classification performance over the SpinalNet architecture and has SOTA performance on many datasets such as Caltech101, KMNIST, QMNIST and EMNIST. The source code is available at https://github.com/praveenchopra/ProgressiveSpinalNet.



### MaAST: Map Attention with Semantic Transformersfor Efficient Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2103.11374v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.11374v1)
- **Published**: 2021-03-21 12:01:23+00:00
- **Updated**: 2021-03-21 12:01:23+00:00
- **Authors**: Zachary Seymour, Kowshik Thopalli, Niluthpol Mithun, Han-Pang Chiu, Supun Samarasekera, Rakesh Kumar
- **Comment**: 6 pages, 5 figures, accepted at ICRA 2021
- **Journal**: None
- **Summary**: Visual navigation for autonomous agents is a core task in the fields of computer vision and robotics. Learning-based methods, such as deep reinforcement learning, have the potential to outperform the classical solutions developed for this task; however, they come at a significantly increased computational load. Through this work, we design a novel approach that focuses on performing better or comparable to the existing learning-based solutions but under a clear time/computational budget. To this end, we propose a method to encode vital scene semantics such as traversable paths, unexplored areas, and observed scene objects -- alongside raw visual streams such as RGB, depth, and semantic segmentation masks -- into a semantically informed, top-down egocentric map representation. Further, to enable the effective use of this information, we introduce a novel 2-D map attention mechanism, based on the successful multi-layer Transformer networks. We conduct experiments on 3-D reconstructed indoor PointGoal visual navigation and demonstrate the effectiveness of our approach. We show that by using our novel attention schema and auxiliary rewards to better utilize scene semantics, we outperform multiple baselines trained with only raw inputs or implicit semantic information while operating with an 80% decrease in the agent's experience.



### Multi-level Metric Learning for Few-shot Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.11383v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11383v4)
- **Published**: 2021-03-21 12:49:07+00:00
- **Updated**: 2021-12-06 01:58:01+00:00
- **Authors**: Haoxing Chen, Huaxiong Li, Yaohui Li, Chunlin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning is devoted to training a model on few samples. Most of these approaches learn a model based on a pixel-level or global-level feature representation. However, using global features may lose local information, and using pixel-level features may lose the contextual semantics of the image. Moreover, such works can only measure the relations between them on a single level, which is not comprehensive and effective. And if query images can simultaneously be well classified via three distinct level similarity metrics, the query images within a class can be more tightly distributed in a smaller feature space, generating more discriminative feature maps. Motivated by this, we propose a novel Part-level Embedding Adaptation with Graph (PEAG) method to generate task-specific features. Moreover, a Multi-level Metric Learning (MML) method is proposed, which not only calculates the pixel-level similarity but also considers the similarity of part-level features and global-level features. Extensive experiments on popular few-shot image recognition datasets prove the effectiveness of our method compared with the state-of-the-art methods. Our code is available at \url{https://github.com/chenhaoxing/M2L}.



### Hierarchical Representation based Query-Specific Prototypical Network for Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.11384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11384v1)
- **Published**: 2021-03-21 12:50:05+00:00
- **Updated**: 2021-03-21 12:50:05+00:00
- **Authors**: Yaohui Li, Huaxiong Li, Haoxing Chen, Chunlin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot image classification aims at recognizing unseen categories with a small number of labeled training data. Recent metric-based frameworks tend to represent a support class by a fixed prototype (e.g., the mean of the support category) and make classification according to the similarities between query instances and support prototypes. However, discriminative dominant regions may locate uncertain areas of images and have various scales, which leads to the misaligned metric. Besides, a fixed prototype for one support category cannot fit for all query instances to accurately reflect their distances with this category, which lowers the efficiency of metric. Therefore, query-specific dominant regions in support samples should be extracted for a high-quality metric. To address these problems, we propose a Hierarchical Representation based Query-Specific Prototypical Network (QPN) to tackle the limitations by generating a region-level prototype for each query sample, which achieves both positional and dimensional semantic alignment simultaneously. Extensive experiments conducted on five benchmark datasets (including three fine-grained datasets) show that our proposed method outperforms the current state-of-the-art methods.



### Multi-view analysis of unregistered medical images using cross-view transformers
- **Arxiv ID**: http://arxiv.org/abs/2103.11390v2
- **DOI**: 10.1007/978-3-030-87199-4_10
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11390v2)
- **Published**: 2021-03-21 13:29:51+00:00
- **Updated**: 2021-09-23 17:14:21+00:00
- **Authors**: Gijs van Tulder, Yao Tong, Elena Marchiori
- **Comment**: Conference paper presented at MICCAI 2021. Code available via
  https://vantulder.net/code/2021/miccai-transformers/
- **Journal**: In: M. de Bruijne et al. (Eds.): MICCAI 2021, LNCS 12903, pp.
  104-113, Springer Nature Switzerland, 2021
- **Summary**: Multi-view medical image analysis often depends on the combination of information from multiple views. However, differences in perspective or other forms of misalignment can make it difficult to combine views effectively, as registration is not always possible. Without registration, views can only be combined at a global feature level, by joining feature vectors after global pooling. We present a novel cross-view transformer method to transfer information between unregistered views at the level of spatial feature maps. We demonstrate this method on multi-view mammography and chest X-ray datasets. On both datasets, we find that a cross-view transformer that links spatial feature maps can outperform a baseline model that joins feature vectors after global pooling.



### ScanMix: Learning from Severe Label Noise via Semantic Clustering and Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.11395v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.11395v3)
- **Published**: 2021-03-21 13:43:09+00:00
- **Updated**: 2022-10-16 16:23:13+00:00
- **Authors**: Ragav Sachdeva, Filipe R Cordeiro, Vasileios Belagiannis, Ian Reid, Gustavo Carneiro
- **Comment**: Paper accepted at Pattern Recognition
- **Journal**: None
- **Summary**: We propose a new training algorithm, ScanMix, that explores semantic clustering and semi-supervised learning (SSL) to allow superior robustness to severe label noise and competitive robustness to non-severe label noise problems, in comparison to the state of the art (SOTA) methods. ScanMix is based on the expectation maximisation framework, where the E-step estimates the latent variable to cluster the training images based on their appearance and classification results, and the M-step optimises the SSL classification and learns effective feature representations via semantic clustering. We present a theoretical result that shows the correctness and convergence of ScanMix, and an empirical result that shows that ScanMix has SOTA results on CIFAR-10/-100 (with symmetric, asymmetric and semantic label noise), Red Mini-ImageNet (from the Controlled Noisy Web Labels), Clothing1M and WebVision. In all benchmarks with severe label noise, our results are competitive to the current SOTA.



### Learning Calibrated-Guidance for Object Detection in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2103.11399v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11399v4)
- **Published**: 2021-03-21 13:55:46+00:00
- **Updated**: 2022-04-06 08:11:41+00:00
- **Authors**: Zongqi Wei, Dong Liang, Dong Zhang, Liyan Zhang, Qixiang Geng, Mingqiang Wei, Huiyu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is one of the most fundamental yet challenging research topics in the domain of computer vision. Recently, the study on this topic in aerial images has made tremendous progress. However, complex background and worse imaging quality are obvious problems in aerial object detection. Most state-of-the-art approaches tend to develop elaborate attention mechanisms for the space-time feature calibrations with arduous computational complexity, while surprisingly ignoring the importance of feature calibrations in channel-wise. In this work, we propose a simple yet effective Calibrated-Guidance (CG) scheme to enhance channel communications in a feature transformer fashion, which can adaptively determine the calibration weights for each channel based on the global feature affinity correlations. Specifically, for a given set of feature maps, CG first computes the feature similarity between each channel and the remaining channels as the intermediary calibration guidance. Then, re-representing each channel by aggregating all the channels weighted together via the guidance operation. Our CG is a general module that can be plugged into any deep neural networks, which is named as CG-Net. To demonstrate its effectiveness and efficiency, extensive experiments are carried out on both oriented object detection task and horizontal object detection task in aerial images. Experimental results on two challenging benchmarks (DOTA and HRSC2016) demonstrate that our CG-Net can achieve the new state-of-the-art performance in accuracy with a fair computational overhead. The source code has been open sourced at https://github.com/WeiZongqi/CG-Net



### Instant-Teaching: An End-to-End Semi-Supervised Object Detection Framework
- **Arxiv ID**: http://arxiv.org/abs/2103.11402v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.11402v1)
- **Published**: 2021-03-21 14:03:36+00:00
- **Updated**: 2021-03-21 14:03:36+00:00
- **Authors**: Qiang Zhou, Chaohui Yu, Zhibin Wang, Qi Qian, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised learning based object detection frameworks demand plenty of laborious manual annotations, which may not be practical in real applications. Semi-supervised object detection (SSOD) can effectively leverage unlabeled data to improve the model performance, which is of great significance for the application of object detection models. In this paper, we revisit SSOD and propose Instant-Teaching, a completely end-to-end and effective SSOD framework, which uses instant pseudo labeling with extended weak-strong data augmentations for teaching during each training iteration. To alleviate the confirmation bias problem and improve the quality of pseudo annotations, we further propose a co-rectify scheme based on Instant-Teaching, denoted as Instant-Teaching$^*$. Extensive experiments on both MS-COCO and PASCAL VOC datasets substantiate the superiority of our framework. Specifically, our method surpasses state-of-the-art methods by 4.2 mAP on MS-COCO when using $2\%$ labeled data. Even with full supervised information of MS-COCO, the proposed method still outperforms state-of-the-art methods by about 1.0 mAP. On PASCAL VOC, we can achieve more than 5 mAP improvement by applying VOC07 as labeled data and VOC12 as unlabeled data.



### Deep Distribution-preserving Incomplete Clustering with Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2103.11424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.11424v1)
- **Published**: 2021-03-21 15:43:17+00:00
- **Updated**: 2021-03-21 15:43:17+00:00
- **Authors**: Mingjie Luo, Siwei Wang, Xinwang Liu, Wenxuan Tu, Yi Zhang, Xifeng Guo, Sihang Zhou, En Zhu
- **Comment**: Data are provided at
  https://github.com/wangsiwei2010/Single-view-incomplete-datasets-for-deep-clustering
- **Journal**: None
- **Summary**: Clustering is a fundamental task in the computer vision and machine learning community. Although various methods have been proposed, the performance of existing approaches drops dramatically when handling incomplete high-dimensional data (which is common in real world applications). To solve the problem, we propose a novel deep incomplete clustering method, named Deep Distribution-preserving Incomplete Clustering with Optimal Transport (DDIC-OT). To avoid insufficient sample utilization in existing methods limited by few fully-observed samples, we propose to measure distribution distance with the optimal transport for reconstruction evaluation instead of traditional pixel-wise loss function. Moreover, the clustering loss of the latent feature is introduced to regularize the embedding with more discrimination capability. As a consequence, the network becomes more robust against missing features and the unified framework which combines clustering and sample imputation enables the two procedures to negotiate to better serve for each other. Extensive experiments demonstrate that the proposed network achieves superior and stable clustering performance improvement against existing state-of-the-art incomplete clustering methods over different missing ratios.



### Responsible AI: Gender bias assessment in emotion recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.11436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11436v1)
- **Published**: 2021-03-21 17:00:21+00:00
- **Updated**: 2021-03-21 17:00:21+00:00
- **Authors**: Artem Domnich, Gholamreza Anbarjafari
- **Comment**: 19 pages, 31 figures
- **Journal**: None
- **Summary**: Rapid development of artificial intelligence (AI) systems amplify many concerns in society. These AI algorithms inherit different biases from humans due to mysterious operational flow and because of that it is becoming adverse in usage. As a result, researchers have started to address the issue by investigating deeper in the direction towards Responsible and Explainable AI. Among variety of applications of AI, facial expression recognition might not be the most important one, yet is considered as a valuable part of human-AI interaction. Evolution of facial expression recognition from the feature based methods to deep learning drastically improve quality of such algorithms. This research work aims to study a gender bias in deep learning methods for facial expression recognition by investigating six distinct neural networks, training them, and further analysed on the presence of bias, according to the three definition of fairness. The main outcomes show which models are gender biased, which are not and how gender of subject affects its emotion recognition. More biased neural networks show bigger accuracy gap in emotion recognition between male and female test sets. Furthermore, this trend keeps for true positive and false positive rates. In addition, due to the nature of the research, we can observe which types of emotions are better classified for men and which for women. Since the topic of biases in facial expression recognition is not well studied, a spectrum of continuation of this research is truly extensive, and may comprise detail analysis of state-of-the-art methods, as well as targeting other biases.



### Traffic Camera Calibration via Vehicle Vanishing Point Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.11438v1
- **DOI**: 10.1007/978-3-030-86383-8_50
- **Categories**: **cs.CV**, I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2103.11438v1)
- **Published**: 2021-03-21 17:11:59+00:00
- **Updated**: 2021-03-21 17:11:59+00:00
- **Authors**: Viktor Kocur, Milan Ftáčnik
- **Comment**: submitted to ICANN 2021
- **Journal**: None
- **Summary**: In this paper we propose a traffic surveillance camera calibration method based on detection of pairs of vanishing points associated with vehicles in the traffic surveillance footage. To detect the vanishing points we propose a CNN which outputs heatmaps in which the positions of vanishing points are represented using the diamond space parametrization which enables us to detect vanishing points from the whole infinite projective space. From the detected pairs of vanishing points for multiple vehicles in a scene we establish the scene geometry by estimating the focal length of the camera and the orientation of the road plane. We show that our method achieves competitive results on the BrnoCarPark dataset while having fewer requirements than the current state of the art approach.



### MONAIfbs: MONAI-based fetal brain MRI deep learning segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.13314v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.13314v1)
- **Published**: 2021-03-21 18:35:25+00:00
- **Updated**: 2021-03-21 18:35:25+00:00
- **Authors**: Marta B. M. Ranzini, Lucas Fidon, Sébastien Ourselin, Marc Modat, Tom Vercauteren
- **Comment**: Abstract accepted at IEEE International Symposium on Biomedical
  Imaging (ISBI) 2021
- **Journal**: None
- **Summary**: In fetal Magnetic Resonance Imaging, Super Resolution Reconstruction (SRR) algorithms are becoming popular tools to obtain high-resolution 3D volume reconstructions from low-resolution stacks of 2D slices, acquired at different orientations. To be effective, these algorithms often require accurate segmentation of the region of interest, such as the fetal brain in suspected pathological cases. In the case of Spina Bifida, Ebner, Wang et al. (NeuroImage, 2020) combined their SRR algorithm with a 2-step segmentation pipeline (2D localisation followed by a 2D segmentation network). However, if the localisation step fails, the second network is not able to recover a correct brain mask, thus requiring manual corrections for an effective SRR. In this work, we aim at improving the fetal brain segmentation for SRR in Spina Bifida. We hypothesise that a well-trained single-step UNet can achieve accurate performance, avoiding the need of a 2-step approach. We propose a new tool for fetal brain segmentation called MONAIfbs, which takes advantage of the Medical Open Network for Artificial Intelligence (MONAI) framework. Our network is based on the dynamic UNet (dynUNet), an adaptation of the nnU-Net framework. When compared to the original 2-step approach proposed in Ebner-Wang, and the same Ebner-Wang approach retrained with the expanded dataset available for this work, the dynUNet showed to achieve higher performance using a single step only. It also showed to reduce the number of outliers, as only 28 stacks obtained Dice score less than 0.9, compared to 68 for Ebner-Wang and 53 Ebner-Wang expanded. The proposed dynUNet model thus provides an improvement of the state-of-the-art fetal brain segmentation techniques, reducing the need for manual correction in automated SRR pipelines. Our code and our trained model are made publicly available at https://github.com/gift-surg/MONAIfbs.



### UAV Images Dataset for Moving Object Detection from Moving Cameras
- **Arxiv ID**: http://arxiv.org/abs/2103.11460v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.11460v2)
- **Published**: 2021-03-21 18:44:38+00:00
- **Updated**: 2021-03-24 09:42:45+00:00
- **Authors**: Ibrahim Delibasoglu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new high resolution aerial images dataset in which moving objects are labelled manually. It aims to contribute to the evaluation of the moving object detection methods for moving cameras. The problem of recognizing moving objects from aerial images is one of the important issues in computer vision. The biggest problem in the images taken by UAV is that the background is constantly variable due to camera movement. There are various datasets in the literature in which proposed methods for motion detection are evaluated. Prepared dataset consists of challenging images containing small targets compared to other datasets. Two methods in the literature have been tested for the prepared dataset. In addition, a simpler method compared to these methods has been proposed for moving object object in this paper.



### Learning Multi-Scene Absolute Pose Regression with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2103.11468v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11468v2)
- **Published**: 2021-03-21 19:21:44+00:00
- **Updated**: 2021-07-26 10:11:11+00:00
- **Authors**: Yoli Shavit, Ron Ferens, Yosi Keller
- **Comment**: None
- **Journal**: None
- **Summary**: Absolute camera pose regressors estimate the position and orientation of a camera from the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron head is trained with images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended for learning multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into candidate pose predictions. This mechanism allows our model to focus on general features that are informative for localization while embedding multiple scenes in parallel. We evaluate our method on commonly benchmarked indoor and outdoor datasets and show that it surpasses both multi-scene and state-of-the-art single-scene absolute pose regressors. We make our code publicly available from https://github.com/yolish/multi-scene-pose-transformer.



### Conditional Generative Adversarial Networks for Speed Control in Trajectory Simulation
- **Arxiv ID**: http://arxiv.org/abs/2103.11471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11471v1)
- **Published**: 2021-03-21 19:47:03+00:00
- **Updated**: 2021-03-21 19:47:03+00:00
- **Authors**: Sahib Julka, Vishal Sowrirajan, Joerg Schloetterer, Michael Granitzer
- **Comment**: None
- **Journal**: None
- **Summary**: Motion behaviour is driven by several factors -- goals, presence and actions of neighbouring agents, social relations, physical and social norms, the environment with its variable characteristics, and further. Most factors are not directly observable and must be modelled from context. Trajectory prediction, is thus a hard problem, and has seen increasing attention from researchers in the recent years. Prediction of motion, in application, must be realistic, diverse and controllable. In spite of increasing focus on multimodal trajectory generation, most methods still lack means for explicitly controlling different modes of the data generation. Further, most endeavours invest heavily in designing special mechanisms to learn the interactions in latent space. We present Conditional Speed GAN (CSG), that allows controlled generation of diverse and socially acceptable trajectories, based on user controlled speed. During prediction, CSG forecasts future speed from latent space and conditions its generation based on it. CSG is comparable to state-of-the-art GAN methods in terms of the benchmark distance metrics, while being simple and useful for simulation and data augmentation for different contexts such as fast or slow paced environments. Additionally, we compare the effect of different aggregation mechanisms and show that a naive approach of concatenation works comparable to its attention and pooling alternatives.



### #PraCegoVer: A Large Dataset for Image Captioning in Portuguese
- **Arxiv ID**: http://arxiv.org/abs/2103.11474v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2103.11474v2)
- **Published**: 2021-03-21 19:55:46+00:00
- **Updated**: 2021-10-28 01:27:51+00:00
- **Authors**: Gabriel Oliveira dos Santos, Esther Luna Colombini, Sandra Avila
- **Comment**: 23 pages, 21 figures, 2 tables
- **Journal**: None
- **Summary**: Automatically describing images using natural sentences is an important task to support visually impaired people's inclusion onto the Internet. It is still a big challenge that requires understanding the relation of the objects present in the image and their attributes and actions they are involved in. Then, visual interpretation methods are needed, but linguistic models are also necessary to verbally describe the semantic relations. This problem is known as Image Captioning. Although many datasets were proposed in the literature, the majority contains only English captions, whereas datasets with captions described in other languages are scarce. Recently, a movement called PraCegoVer arose on the Internet, stimulating users from social media to publish images, tag #PraCegoVer and add a short description of their content. Thus, inspired by this movement, we have proposed the #PraCegoVer, a multi-modal dataset with Portuguese captions based on posts from Instagram. It is the first large dataset for image captioning in Portuguese with freely annotated images. Further, the captions in our dataset bring additional challenges to the problem: first, in contrast to popular datasets such as MS COCO Captions, #PraCegoVer has only one reference to each image; also, both mean and variance of our reference sentence length are significantly greater than those in the MS COCO Captions. These two characteristics contribute to making our dataset interesting due to the linguistic aspect and the challenges that it introduces to the image captioning problem. We publicly-share the dataset at https://github.com/gabrielsantosrv/PraCegoVer.



### Paying Attention to Activation Maps in Camera Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/2103.11477v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.11477v2)
- **Published**: 2021-03-21 20:10:15+00:00
- **Updated**: 2021-04-11 19:55:34+00:00
- **Authors**: Yoli Shavit, Ron Ferens, Yosi Keller
- **Comment**: None
- **Journal**: None
- **Summary**: Camera pose regression methods apply a single forward pass to the query image to estimate the camera pose. As such, they offer a fast and light-weight alternative to traditional localization schemes based on image retrieval. Pose regression approaches simultaneously learn two regression tasks, aiming to jointly estimate the camera position and orientation using a single embedding vector computed by a convolutional backbone. We propose an attention-based approach for pose regression, where the convolutional activation maps are used as sequential inputs. Transformers are applied to encode the sequential activation maps as latent vectors, used for camera pose regression. This allows us to pay attention to spatially-varying deep features. Using two Transformer heads, we separately focus on the features for camera position and orientation, based on how informative they are per task. Our proposed approach is shown to compare favorably to contemporary pose regressors schemes and achieves state-of-the-art accuracy across multiple outdoor and indoor benchmarks. In particular, to the best of our knowledge, our approach is the only method to attain sub-meter average accuracy across outdoor scenes. We make our code publicly available from here.



### MoViNets: Mobile Video Networks for Efficient Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.11511v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.11511v2)
- **Published**: 2021-03-21 23:06:38+00:00
- **Updated**: 2021-04-18 18:04:58+00:00
- **Authors**: Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew Brown, Boqing Gong
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We present Mobile Video Networks (MoViNets), a family of computation and memory efficient video networks that can operate on streaming video for online inference. 3D convolutional neural networks (CNNs) are accurate at video recognition but require large computation and memory budgets and do not support online inference, making them difficult to work on mobile devices. We propose a three-step approach to improve computational efficiency while substantially reducing the peak memory usage of 3D CNNs. First, we design a video network search space and employ neural architecture search to generate efficient and diverse 3D CNN architectures. Second, we introduce the Stream Buffer technique that decouples memory from video clip duration, allowing 3D CNNs to embed arbitrary-length streaming video sequences for both training and inference with a small constant memory footprint. Third, we propose a simple ensembling technique to improve accuracy further without sacrificing efficiency. These three progressive techniques allow MoViNets to achieve state-of-the-art accuracy and efficiency on the Kinetics, Moments in Time, and Charades video action recognition datasets. For instance, MoViNet-A5-Stream achieves the same accuracy as X3D-XL on Kinetics 600 while requiring 80% fewer FLOPs and 65% less memory. Code will be made available at https://github.com/tensorflow/models/tree/master/official/vision.



### Unsupervised and self-adaptative techniques for cross-domain person re-identification
- **Arxiv ID**: http://arxiv.org/abs/2103.11520v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.11520v3)
- **Published**: 2021-03-21 23:58:39+00:00
- **Updated**: 2022-02-07 13:29:38+00:00
- **Authors**: Gabriel Bertocco, Fernanda Andaló, Anderson Rocha
- **Comment**: Published on IEEE Transactions on Information Forensics and Security
- **Journal**: None
- **Summary**: Person Re-Identification (ReID) across non-overlapping cameras is a challenging task and, for this reason, most works in the prior art rely on supervised feature learning from a labeled dataset to match the same person in different views. However, it demands the time-consuming task of labeling the acquired data, prohibiting its fast deployment, specially in forensic scenarios. Unsupervised Domain Adaptation (UDA) emerges as a promising alternative, as it performs feature-learning adaptation from a model trained on a source to a target domain without identity-label annotation. However, most UDA-based algorithms rely upon a complex loss function with several hyper-parameters, which hinders the generalization to different scenarios. Moreover, as UDA depends on the translation between domains, it is important to select the most reliable data from the unseen domain, thus avoiding error propagation caused by noisy examples on the target data -- an often overlooked problem. In this sense, we propose a novel UDA-based ReID method that optimizes a simple loss function with only one hyper-parameter and that takes advantage of triplets of samples created by a new offline strategy based on the diversity of cameras within a cluster. This new strategy adapts the model and also regularizes it, avoiding overfitting on the target domain. We also introduce a new self-ensembling strategy, in which weights from different iterations are aggregated to create a final model combining knowledge from distinct moments of the adaptation. For evaluation, we consider three well-known deep learning architectures and combine them for final decision-making. The proposed method does not use person re-ranking nor any label on the target domain, and outperforms the state of the art, with a much simpler setup, on the Market to Duke, the challenging Market1501 to MSMT17, and Duke to MSMT17 adaptation scenarios.



### Conditional Frechet Inception Distance
- **Arxiv ID**: http://arxiv.org/abs/2103.11521v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.11521v2)
- **Published**: 2021-03-21 23:59:19+00:00
- **Updated**: 2022-02-28 09:31:42+00:00
- **Authors**: Michael Soloveitchik, Tzvi Diskin, Efrat Morin, Ami Wiesel
- **Comment**: None
- **Journal**: None
- **Summary**: We consider distance functions between conditional distributions. We focus on the Wasserstein metric and its Gaussian case known as the Frechet Inception Distance (FID). We develop conditional versions of these metrics, analyze their relations and provide a closed form solution to the conditional FID (CFID) metric. We numerically compare the metrics in the context of performance evaluation of modern conditional generative models. Our results show the advantages of CFID compared to the classical FID and mean squared error (MSE) measures. In contrast to FID, CFID is useful in identifying failures where realistic outputs which are not related to their inputs are generated. On the other hand, compared to MSE, CFID is useful in identifying failures where a single realistic output is generated even though there is a diverse set of equally probable outputs.




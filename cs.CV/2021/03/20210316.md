# Arxiv Papers in cs.CV on 2021-03-16
### Unconstrained Face-Mask & Face-Hand Datasets: Building a Computer Vision System to Help Prevent the Transmission of COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2103.08773v3
- **DOI**: 10.1007/s11760-022-02308-x
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.08773v3)
- **Published**: 2021-03-16 00:00:04+00:00
- **Updated**: 2021-12-08 12:54:18+00:00
- **Authors**: Fevziye Irem Eyiokur, Hazım Kemal Ekenel, Alexander Waibel
- **Comment**: 9 pages, 4 figures
- **Journal**: SIViP (2022)
- **Summary**: Health organizations advise social distancing, wearing face mask, and avoiding touching face to prevent the spread of coronavirus. Based on these protective measures, we developed a computer vision system to help prevent the transmission of COVID-19. Specifically, the developed system performs face mask detection, face-hand interaction detection, and measures social distance. To train and evaluate the developed system, we collected and annotated images that represent face mask usage and face-hand interaction in the real world. Besides assessing the performance of the developed system on our own datasets, we also tested it on existing datasets in the literature without performing any adaptation on them. In addition, we proposed a module to track social distance between people. Experimental results indicate that our datasets represent the real-world's diversity well. The proposed system achieved very high performance and generalization capacity for face mask usage detection, face-hand interaction detection, and measuring social distance in a real-world scenario on unseen data. The datasets will be available at https://github.com/iremeyiokur/COVID-19-Preventions-Control-System.



### LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2103.08784v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.08784v2)
- **Published**: 2021-03-16 00:35:28+00:00
- **Updated**: 2021-04-11 21:53:08+00:00
- **Authors**: Siqi Sun, Yen-Chun Chen, Linjie Li, Shuohang Wang, Yuwei Fang, Jingjing Liu
- **Comment**: NAACL 2021
- **Journal**: None
- **Summary**: Multimodal pre-training has propelled great advancement in vision-and-language research. These large-scale pre-trained models, although successful, fatefully suffer from slow inference speed due to enormous computation cost mainly from cross-modal attention in Transformer architecture. When applied to real-life applications, such latency and computation demand severely deter the practical use of pre-trained models. In this paper, we study Image-text retrieval (ITR), the most mature scenario of V+L application, which has been widely studied even prior to the emergence of recent pre-trained models. We propose a simple yet highly effective approach, LightningDOT that accelerates the inference time of ITR by thousands of times, without sacrificing accuracy. LightningDOT removes the time-consuming cross-modal attention by pre-training on three novel learning objectives, extracting feature indexes offline, and employing instant dot-product matching with further re-ranking, which significantly speeds up retrieval process. In fact, LightningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that consume 1000x magnitude of computational hours. Code and pre-training checkpoints are available at https://github.com/intersun/LightningDOT.



### Track to Detect and Segment: An Online Multi-Object Tracker
- **Arxiv ID**: http://arxiv.org/abs/2103.08808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.08808v1)
- **Published**: 2021-03-16 02:34:06+00:00
- **Updated**: 2021-03-16 02:34:06+00:00
- **Authors**: Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, Junsong Yuan
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Most online multi-object trackers perform object detection stand-alone in a neural net without any input from tracking. In this paper, we present a new online joint detection and tracking model, TraDeS (TRAck to DEtect and Segment), exploiting tracking clues to assist detection end-to-end. TraDeS infers object tracking offset by a cost volume, which is used to propagate previous object features for improving current object detection and segmentation. Effectiveness and superiority of TraDeS are shown on 4 datasets, including MOT (2D tracking), nuScenes (3D tracking), MOTS and Youtube-VIS (instance segmentation tracking). Project page: https://jialianwu.com/projects/TraDeS.html.



### Towards Indirect Top-Down Road Transport Emissions Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.08829v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.08829v1)
- **Published**: 2021-03-16 03:30:53+00:00
- **Updated**: 2021-03-16 03:30:53+00:00
- **Authors**: Ryan Mukherjee, Derek Rollend, Gordon Christie, Armin Hadzic, Sally Matson, Anshu Saksena, Marisa Hughes
- **Comment**: None
- **Journal**: None
- **Summary**: Road transportation is one of the largest sectors of greenhouse gas (GHG) emissions affecting climate change. Tackling climate change as a global community will require new capabilities to measure and inventory road transport emissions. However, the large scale and distributed nature of vehicle emissions make this sector especially challenging for existing inventory methods. In this work, we develop machine learning models that use satellite imagery to perform indirect top-down estimation of road transport emissions. Our initial experiments focus on the United States, where a bottom-up inventory was available for training our models. We achieved a mean absolute error (MAE) of 39.5 kg CO$_{2}$ of annual road transport emissions, calculated on a pixel-by-pixel (100 m$^{2}$) basis in Sentinel-2 imagery. We also discuss key model assumptions and challenges that need to be addressed to develop models capable of generalizing to global geography. We believe this work is the first published approach for automated indirect top-down estimation of road transport sector emissions using visual imagery and represents a critical step towards scalable, global, near-real-time road transportation emissions inventories that are measured both independently and objectively.



### Skeleton Aware Multi-modal Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.08833v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.08833v5)
- **Published**: 2021-03-16 03:38:17+00:00
- **Updated**: 2021-05-02 20:49:40+00:00
- **Authors**: Songyao Jiang, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, Yun Fu
- **Comment**: This is a preprint version of our work SAM-SLR that ranked 1st at
  CVPR2021 Challenge on Large Scale Signer Independent Isolated Sign Language
  Recognition
- **Journal**: None
- **Summary**: Sign language is commonly used by deaf or speech impaired people to communicate but requires significant effort to master. Sign Language Recognition (SLR) aims to bridge the gap between sign language users and others by recognizing signs from given videos. It is an essential yet challenging task since sign language is performed with the fast and complex movement of hand gestures, body posture, and even facial expressions. Recently, skeleton-based action recognition attracts increasing attention due to the independence between the subject and background variation. However, skeleton-based SLR is still under exploration due to the lack of annotations on hand keypoints. Some efforts have been made to use hand detectors with pose estimators to extract hand key points and learn to recognize sign language via Neural Networks, but none of them outperforms RGB-based methods. To this end, we propose a novel Skeleton Aware Multi-modal SLR framework (SAM-SLR) to take advantage of multi-modal information towards a higher recognition rate. Specifically, we propose a Sign Language Graph Convolution Network (SL-GCN) to model the embedded dynamics and a novel Separable Spatial-Temporal Convolution Network (SSTCN) to exploit skeleton features. RGB and depth modalities are also incorporated and assembled into our framework to provide global information that is complementary to the skeleton-based methods SL-GCN and SSTCN. As a result, SAM-SLR achieves the highest performance in both RGB (98.42\%) and RGB-D (98.53\%) tracks in 2021 Looking at People Large Scale Signer Independent Isolated SLR Challenge. Our code is available at https://github.com/jackyjsy/CVPR21Chal-SLR



### GSVNet: Guided Spatially-Varying Convolution for Fast Semantic Segmentation on Video
- **Arxiv ID**: http://arxiv.org/abs/2103.08834v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.08834v2)
- **Published**: 2021-03-16 03:38:59+00:00
- **Updated**: 2021-06-08 02:02:18+00:00
- **Authors**: Shih-Po Lee, Si-Cun Chen, Wen-Hsiao Peng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses fast semantic segmentation on video.Video segmentation often calls for real-time, or even fasterthan real-time, processing. One common recipe for conserving computation arising from feature extraction is to propagate features of few selected keyframes. However, recent advances in fast image segmentation make these solutions less attractive. To leverage fast image segmentation for furthering video segmentation, we propose a simple yet efficient propagation framework. Specifically, we perform lightweight flow estimation in 1/8-downscaled image space for temporal warping in segmentation outpace space. Moreover, we introduce a guided spatially-varying convolution for fusing segmentations derived from the previous and current frames, to mitigate propagation error and enable lightweight feature extraction on non-keyframes. Experimental results on Cityscapes and CamVid show that our scheme achieves the state-of-the-art accuracy-throughput trade-off on video segmentation.



### Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2103.08849v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2103.08849v3)
- **Published**: 2021-03-16 04:37:40+00:00
- **Updated**: 2021-04-15 02:01:38+00:00
- **Authors**: Po-Yao Huang, Mandela Patrick, Junjie Hu, Graham Neubig, Florian Metze, Alexander Hauptmann
- **Comment**: accepted by NAACL 2021
- **Journal**: None
- **Summary**: This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextualized multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (MultiHowTo100M) for pre-training. Experiments on VTT show that our method significantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at http://github.com/berniebear/Multi-HT100M.



### Lite-HDSeg: LiDAR Semantic Segmentation Using Lite Harmonic Dense Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2103.08852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.08852v1)
- **Published**: 2021-03-16 04:54:57+00:00
- **Updated**: 2021-03-16 04:54:57+00:00
- **Authors**: Ryan Razani, Ran Cheng, Ehsan Taghavi, Liu Bingbing
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous driving vehicles and robotic systems rely on accurate perception of their surroundings. Scene understanding is one of the crucial components of perception modules. Among all available sensors, LiDARs are one of the essential sensing modalities of autonomous driving systems due to their active sensing nature with high resolution of sensor readings. Accurate and fast semantic segmentation methods are needed to fully utilize LiDAR sensors for scene understanding. In this paper, we present Lite-HDSeg, a novel real-time convolutional neural network for semantic segmentation of full $3$D LiDAR point clouds. Lite-HDSeg can achieve the best accuracy vs. computational complexity trade-off in SemanticKitti benchmark and is designed on the basis of a new encoder-decoder architecture with light-weight harmonic dense convolutions as its core. Moreover, we introduce ICM, an improved global contextual module to capture multi-scale contextual features, and MCSPN, a multi-class Spatial Propagation Network to further refine the semantic boundaries. Our experimental results show that the proposed method outperforms state-of-the-art semantic segmentation approaches which can run real-time, thus is suitable for robotic and autonomous driving applications.



### Adversarial YOLO: Defense Human Detection Patch Attacks via Detecting Adversarial Patches
- **Arxiv ID**: http://arxiv.org/abs/2103.08860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.08860v1)
- **Published**: 2021-03-16 05:41:39+00:00
- **Updated**: 2021-03-16 05:41:39+00:00
- **Authors**: Nan Ji, YanFei Feng, Haidong Xie, Xueshuang Xiang, Naijin Liu
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: The security of object detection systems has attracted increasing attention, especially when facing adversarial patch attacks. Since patch attacks change the pixels in a restricted area on objects, they are easy to implement in the physical world, especially for attacking human detection systems. The existing defenses against patch attacks are mostly applied for image classification problems and have difficulty resisting human detection attacks. Towards this critical issue, we propose an efficient and effective plug-in defense component on the YOLO detection system, which we name Ad-YOLO. The main idea is to add a patch class on the YOLO architecture, which has a negligible inference increment. Thus, Ad-YOLO is expected to directly detect both the objects of interest and adversarial patches. To the best of our knowledge, our approach is the first defense strategy against human detection attacks.   We investigate Ad-YOLO's performance on the YOLOv2 baseline. To improve the ability of Ad-YOLO to detect variety patches, we first use an adversarial training process to develop a patch dataset based on the Inria dataset, which we name Inria-Patch. Then, we train Ad-YOLO by a combination of Pascal VOC, Inria, and Inria-Patch datasets. With a slight drop of $0.70\%$ mAP on VOC 2007 test set, Ad-YOLO achieves $80.31\%$ AP of persons, which highly outperforms $33.93\%$ AP for YOLOv2 when facing white-box patch attacks. Furthermore, compared with YOLOv2, the results facing a physical-world attack are also included to demonstrate Ad-YOLO's excellent generalization ability.



### Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar
- **Arxiv ID**: http://arxiv.org/abs/2103.08863v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.08863v2)
- **Published**: 2021-03-16 05:47:26+00:00
- **Updated**: 2021-08-17 11:41:57+00:00
- **Authors**: Peike Li, Xin Yu, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional face super-resolution methods usually assume testing low-resolution (LR) images lie in the same domain as the training ones. Due to different lighting conditions and imaging hardware, domain gaps between training and testing images inevitably occur in many real-world scenarios. Neglecting those domain gaps would lead to inferior face super-resolution (FSR) performance. However, how to transfer a trained FSR model to a target domain efficiently and effectively has not been investigated. To tackle this problem, we develop a Domain-Aware Pyramid-based Face Super-Resolution network, named DAP-FSR network. Our DAP-FSR is the first attempt to super-resolve LR faces from a target domain by exploiting only a pair of high-resolution (HR) and LR exemplar in the target domain. To be specific, our DAP-FSR firstly employs its encoder to extract the multi-scale latent representations of the input LR face. Considering only one target domain example is available, we propose to augment the target domain data by mixing the latent representations of the target domain face and source domain ones, and then feed the mixed representations to the decoder of our DAP-FSR. The decoder will generate new face images resembling the target domain image style. The generated HR faces in turn are used to optimize our decoder to reduce the domain gap. By iteratively updating the latent representations and our decoder, our DAP-FSR will be adapted to the target domain, thus achieving authentic and high-quality upsampled HR faces. Extensive experiments on three newly constructed benchmarks validate the effectiveness and superior performance of our DAP-FSR compared to the state-of-the-art.



### Spatial Dependency Networks: Neural Layers for Improved Generative Image Modeling
- **Arxiv ID**: http://arxiv.org/abs/2103.08877v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.08877v1)
- **Published**: 2021-03-16 07:01:08+00:00
- **Updated**: 2021-03-16 07:01:08+00:00
- **Authors**: Đorđe Miladinović, Aleksandar Stanić, Stefan Bauer, Jürgen Schmidhuber, Joachim M. Buhmann
- **Comment**: None
- **Journal**: International Conference on Learning Representations (2021);
- **Summary**: How to improve generative modeling by better exploiting spatial regularities and coherence in images? We introduce a novel neural network for building image generators (decoders) and apply it to variational autoencoders (VAEs). In our spatial dependency networks (SDNs), feature maps at each level of a deep neural net are computed in a spatially coherent way, using a sequential gating-based mechanism that distributes contextual information across 2-D space. We show that augmenting the decoder of a hierarchical VAE by spatial dependency layers considerably improves density estimation over baseline convolutional architectures and the state-of-the-art among the models within the same class. Furthermore, we demonstrate that SDN can be applied to large images by synthesizing samples of high quality and coherence. In a vanilla VAE setting, we find that a powerful SDN decoder also improves learning disentangled representations, indicating that neural architectures play an important role in this task. Our results suggest favoring spatial dependency over convolutional layers in various VAE settings. The accompanying source code is given at https://github.com/djordjemila/sdn.



### Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.08896v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.08896v1)
- **Published**: 2021-03-16 07:39:06+00:00
- **Updated**: 2021-03-16 07:39:06+00:00
- **Authors**: Jungbeom Lee, Eunji Kim, Sungroh Yoon
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation produces a pixel-level localization from a classifier, but it is likely to restrict its focus to a small discriminative region of the target object. AdvCAM is an attribution map of an image that is manipulated to increase the classification score. This manipulation is realized in an anti-adversarial manner, which perturbs the images along pixel gradients in the opposite direction from those used in an adversarial attack. It forces regions initially considered not to be discriminative to become involved in subsequent classifications, and produces attribution maps that successively identify more regions of the target object. In addition, we introduce a new regularization procedure that inhibits the incorrect attribution of regions unrelated to the target object and limits the attributions of the regions that already have high scores. On PASCAL VOC 2012 test images, we achieve mIoUs of 68.0 and 76.9 for weakly and semi-supervised semantic segmentation respectively, which represent a new state-of-the-art.



### BBAM: Bounding Box Attribution Map for Weakly Supervised Semantic and Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.08907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.08907v1)
- **Published**: 2021-03-16 08:29:33+00:00
- **Updated**: 2021-03-16 08:29:33+00:00
- **Authors**: Jungbeom Lee, Jihun Yi, Chaehun Shin, Sungroh Yoon
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Weakly supervised segmentation methods using bounding box annotations focus on obtaining a pixel-level mask from each box containing an object. Existing methods typically depend on a class-agnostic mask generator, which operates on the low-level information intrinsic to an image. In this work, we utilize higher-level information from the behavior of a trained object detector, by seeking the smallest areas of the image from which the object detector produces almost the same result as it does from the whole image. These areas constitute a bounding-box attribution map (BBAM), which identifies the target object in its bounding box and thus serves as pseudo ground-truth for weakly supervised semantic and instance segmentation. This approach significantly outperforms recent comparable techniques on both the PASCAL VOC and MS COCO benchmarks in weakly supervised semantic and instance segmentation. In addition, we provide a detailed analysis of our method, offering deeper insight into the behavior of the BBAM.



### EADNet: Efficient Asymmetric Dilated Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.08914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.08914v1)
- **Published**: 2021-03-16 08:46:57+00:00
- **Updated**: 2021-03-16 08:46:57+00:00
- **Authors**: Qihang Yang, Tao Chen, Jiayuan Fan, Ye Lu, Chongyan Zuo, Qinghua Chi
- **Comment**: None
- **Journal**: None
- **Summary**: Due to real-time image semantic segmentation needs on power constrained edge devices, there has been an increasing desire to design lightweight semantic segmentation neural network, to simultaneously reduce computational cost and increase inference speed. In this paper, we propose an efficient asymmetric dilated semantic segmentation network, named EADNet, which consists of multiple developed asymmetric convolution branches with different dilation rates to capture the variable shapes and scales information of an image. Specially, a multi-scale multi-shape receptive field convolution (MMRFC) block with only a few parameters is designed to capture such information. Experimental results on the Cityscapes dataset demonstrate that our proposed EADNet achieves segmentation mIoU of 67.1 with smallest number of parameters (only 0.35M) among mainstream lightweight semantic segmentation networks.



### Combining Morphological and Histogram based Text Line Segmentation in the OCR Context
- **Arxiv ID**: http://arxiv.org/abs/2103.08922v4
- **DOI**: 10.46298/jdmdh.7277
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2103.08922v4)
- **Published**: 2021-03-16 09:06:25+00:00
- **Updated**: 2021-11-01 12:56:57+00:00
- **Authors**: Pit Schneider
- **Comment**: Journal of Data Mining and Digital Humanities; Small adjustments
- **Journal**: Journal of Data Mining & Digital Humanities, 2021,
  HistoInformatics (November 4, 2021) jdmdh:7277
- **Summary**: Text line segmentation is one of the pre-stages of modern optical character recognition systems. The algorithmic approach proposed by this paper has been designed for this exact purpose. Its main characteristic is the combination of two different techniques, morphological image operations and horizontal histogram projections. The method was developed to be applied on a historic data collection that commonly features quality issues, such as degraded paper, blurred text, or presence of noise. For that reason, the segmenter in question could be of particular interest for cultural institutions, that want access to robust line bounding boxes for a given historic document. Because of the promising segmentation results that are joined by low computational cost, the algorithm was incorporated into the OCR pipeline of the National Library of Luxembourg, in the context of the initiative of reprocessing their historic newspaper collection. The general contribution of this paper is to outline the approach and to evaluate the gains in terms of accuracy and speed, comparing it to the segmentation algorithm bundled with the used open source OCR software.



### Unsupervised anomaly detection in digital pathology using GANs
- **Arxiv ID**: http://arxiv.org/abs/2103.08945v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.08945v1)
- **Published**: 2021-03-16 10:10:12+00:00
- **Updated**: 2021-03-16 10:10:12+00:00
- **Authors**: Milda Pocevičiūtė, Gabriel Eilertsen, Claes Lundström
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning (ML) algorithms are optimized for the distribution represented by the training data. For outlier data, they often deliver predictions with equal confidence, even though these should not be trusted. In order to deploy ML-based digital pathology solutions in clinical practice, effective methods for detecting anomalous data are crucial to avoid incorrect decisions in the outlier scenario. We propose a new unsupervised learning approach for anomaly detection in histopathology data based on generative adversarial networks (GANs). Compared to the existing GAN-based methods that have been used in medical imaging, the proposed approach improves significantly on performance for pathology data. Our results indicate that histopathology imagery is substantially more complex than the data targeted by the previous methods. This complexity requires not only a more advanced GAN architecture but also an appropriate anomaly metric to capture the quality of the reconstructed images.



### Modulating Localization and Classification for Harmonized Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.08958v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.08958v2)
- **Published**: 2021-03-16 10:36:02+00:00
- **Updated**: 2021-03-25 07:53:10+00:00
- **Authors**: Taiheng Zhang, Qiaoyong Zhong, Shiliang Pu, Di Xie
- **Comment**: Accepted by ICME 2021
- **Journal**: None
- **Summary**: Object detection involves two sub-tasks, i.e. localizing objects in an image and classifying them into various categories. For existing CNN-based detectors, we notice the widespread divergence between localization and classification, which leads to degradation in performance. In this work, we propose a mutual learning framework to modulate the two tasks. In particular, the two tasks are forced to learn from each other with a novel mutual labeling strategy. Besides, we introduce a simple yet effective IoU rescoring scheme, which further reduces the divergence. Moreover, we define a Spearman rank correlation-based metric to quantify the divergence, which correlates well with the detection performance. The proposed approach is general-purpose and can be easily injected into existing detectors such as FCOS and RetinaNet. We achieve a significant performance gain over the baseline detectors on the COCO dataset.



### Hebbian Semi-Supervised Learning in a Sample Efficiency Setting
- **Arxiv ID**: http://arxiv.org/abs/2103.09002v2
- **DOI**: 10.1016/j.neunet.2021.08.003
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09002v2)
- **Published**: 2021-03-16 11:57:52+00:00
- **Updated**: 2021-09-17 08:29:06+00:00
- **Authors**: Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato
- **Comment**: 18 pages, 9 figures, 3 tables, accepted by Elsevier Neural Networks
- **Journal**: Neural Networks, Volume 143, November 2021, Pages 719-731,
  Elsevier
- **Summary**: We propose to address the issue of sample efficiency, in Deep Convolutional Neural Networks (DCNN), with a semi-supervised training strategy that combines Hebbian learning with gradient descent: all internal layers (both convolutional and fully connected) are pre-trained using an unsupervised approach based on Hebbian learning, and the last fully connected layer (the classification layer) is trained using Stochastic Gradient Descent (SGD). In fact, as Hebbian learning is an unsupervised learning method, its potential lies in the possibility of training the internal layers of a DCNN without labels. Only the final fully connected layer has to be trained with labeled examples.   We performed experiments on various object recognition datasets, in different regimes of sample efficiency, comparing our semi-supervised (Hebbian for internal layers + SGD for the final fully connected layer) approach with end-to-end supervised backprop training, and with semi-supervised learning based on Variational Auto-Encoder (VAE). The results show that, in regimes where the number of available labeled samples is low, our semi-supervised approach outperforms the other approaches in almost all the cases.



### PC-HMR: Pose Calibration for 3D Human Mesh Recovery from 2D Images/Videos
- **Arxiv ID**: http://arxiv.org/abs/2103.09009v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09009v2)
- **Published**: 2021-03-16 12:12:45+00:00
- **Updated**: 2021-03-18 17:13:37+00:00
- **Authors**: Tianyu Luan, Yali Wang, Junhao Zhang, Zhe Wang, Zhipeng Zhou, Yu Qiao
- **Comment**: 9 pages, 7 figures. AAAI2021
- **Journal**: None
- **Summary**: The end-to-end Human Mesh Recovery (HMR) approach has been successfully used for 3D body reconstruction. However, most HMR-based frameworks reconstruct human body by directly learning mesh parameters from images or videos, while lacking explicit guidance of 3D human pose in visual data. As a result, the generated mesh often exhibits incorrect pose for complex activities. To tackle this problem, we propose to exploit 3D pose to calibrate human mesh. Specifically, we develop two novel Pose Calibration frameworks, i.e., Serial PC-HMR and Parallel PC-HMR. By coupling advanced 3D pose estimators and HMR in a serial or parallel manner, these two frameworks can effectively correct human mesh with guidance of a concise pose calibration module. Furthermore, since the calibration module is designed via non-rigid pose transformation, our PC-HMR frameworks can flexibly tackle bone length variations to alleviate misplacement in the calibrated mesh. Finally, our frameworks are based on generic and complementary integration of data-driven learning and geometrical modeling. Via plug-and-play modules, they can be efficiently adapted for both image/video-based human mesh recovery. Additionally, they have no requirement of extra 3D pose annotations in the testing phase, which releases inference difficulties in practice. We perform extensive experiments on the popular bench-marks, i.e., Human3.6M, 3DPW and SURREAL, where our PC-HMR frameworks achieve the SOTA results.



### Dense Interaction Learning for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2103.09013v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09013v3)
- **Published**: 2021-03-16 12:22:08+00:00
- **Updated**: 2021-08-17 03:19:16+00:00
- **Authors**: Tianyu He, Xin Jin, Xu Shen, Jianqiang Huang, Zhibo Chen, Xian-Sheng Hua
- **Comment**: ICCV 2021, Oral
- **Journal**: None
- **Summary**: Video-based person re-identification (re-ID) aims at matching the same person across video clips. Efficiently exploiting multi-scale fine-grained features while building the structural interaction among them is pivotal for its success. In this paper, we propose a hybrid framework, Dense Interaction Learning (DenseIL), that takes the principal advantages of both CNN-based and Attention-based architectures to tackle video-based person re-ID difficulties. DenseIL contains a CNN encoder and a Dense Interaction (DI) decoder. The CNN encoder is responsible for efficiently extracting discriminative spatial features while the DI decoder is designed to densely model spatial-temporal inherent interaction across frames. Different from previous works, we additionally let the DI decoder densely attends to intermediate fine-grained CNN features and that naturally yields multi-grained spatial-temporal representation for each video clip. Moreover, we introduce Spatio-TEmporal Positional Embedding (STEP-Emb) into the DI decoder to investigate the positional relation among the spatial-temporal inputs. Our experiments consistently and significantly outperform all the state-of-the-art methods on multiple standard video-based person re-ID datasets.



### Missing Cone Artifacts Removal in ODT using Unsupervised Deep Learning in Projection Domain
- **Arxiv ID**: http://arxiv.org/abs/2103.09022v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09022v2)
- **Published**: 2021-03-16 12:41:33+00:00
- **Updated**: 2021-07-18 09:27:43+00:00
- **Authors**: Hyungjin Chung, Jaeyoung Huh, Geon Kim, Yong Keun Park, Jong Chul Ye
- **Comment**: This will appear in IEEE Trans. on Computational Imaging
- **Journal**: None
- **Summary**: Optical diffraction tomography (ODT) produces three dimensional distribution of refractive index (RI) by measuring scattering fields at various angles. Although the distribution of RI index is highly informative, due to the missing cone problem stemming from the limited-angle acquisition of holograms, reconstructions have very poor resolution along axial direction compared to the horizontal imaging plane. To solve this issue, here we present a novel unsupervised deep learning framework, which learns the probability distribution of missing projection views through optimal transport driven cycleGAN. Experimental results show that missing cone artifact in ODT can be significantly resolved by the proposed method.



### Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.09027v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09027v1)
- **Published**: 2021-03-16 12:53:09+00:00
- **Updated**: 2021-03-16 12:53:09+00:00
- **Authors**: Namyeong Kwon, Hwidong Na, Gabriel Huang, Simon Lacoste-Julien
- **Comment**: Appears in: Proceedings of the Ninth International Conference on
  Learning Representations (ICLR 2021). 20 pages
- **Journal**: None
- **Summary**: Model-agnostic meta-learning (MAML) is a popular method for few-shot learning but assumes that we have access to the meta-training set. In practice, training on the meta-training set may not always be an option due to data privacy concerns, intellectual property issues, or merely lack of computing resources. In this paper, we consider the novel problem of repurposing pretrained MAML checkpoints to solve new few-shot classification tasks. Because of the potential distribution mismatch, the original MAML steps may no longer be optimal. Therefore we propose an alternative meta-testing procedure and combine MAML gradient steps with adversarial training and uncertainty-based stepsize adaptation. Our method outperforms "vanilla" MAML on same-domain and cross-domains benchmarks using both SGD and Adam optimizers and shows improved robustness to the choice of base stepsize.



### A Large-Scale Dataset for Benchmarking Elevator Button Segmentation and Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.09030v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09030v2)
- **Published**: 2021-03-16 12:57:58+00:00
- **Updated**: 2021-03-22 07:17:47+00:00
- **Authors**: Jianbang Liu, Yuqi Fang, Delong Zhu, Nachuan Ma, Jin Pan, Max Q. -H. Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Human activities are hugely restricted by COVID-19, recently. Robots that can conduct inter-floor navigation attract much public attention, since they can substitute human workers to conduct the service work. However, current robots either depend on human assistance or elevator retrofitting, and fully autonomous inter-floor navigation is still not available. As the very first step of inter-floor navigation, elevator button segmentation and recognition hold an important position. Therefore, we release the first large-scale publicly available elevator panel dataset in this work, containing 3,718 panel images with 35,100 button labels, to facilitate more powerful algorithms on autonomous elevator operation. Together with the dataset, a number of deep learning based implementations for button segmentation and recognition are also released to benchmark future methods in the community. The dataset will be available at \url{https://github.com/zhudelong/elevator_button_recognition



### Invertible Residual Network with Regularization for Effective Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.09042v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09042v1)
- **Published**: 2021-03-16 13:19:59+00:00
- **Updated**: 2021-03-16 13:19:59+00:00
- **Authors**: Kashu Yamazaki, Vidhiwar Singh Rathour, T. Hoang Ngan Le
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) i.e. Residual Networks (ResNets) have been used successfully for many computer vision tasks, but are difficult to scale to 3D volumetric medical data. Memory is increasingly often the bottleneck when training 3D Convolutional Neural Networks (CNNs). Recently, invertible neural networks have been applied to significantly reduce activation memory footprint when training neural networks with backpropagation thanks to the invertible functions that allow retrieving its input from its output without storing intermediate activations in memory to perform the backpropagation.   Among many successful network architectures, 3D Unet has been established as a standard architecture for volumetric medical segmentation. Thus, we choose 3D Unet as a baseline for a non-invertible network and we then extend it with the invertible residual network. In this paper, we proposed two versions of the invertible Residual Network, namely Partially Invertible Residual Network (Partially-InvRes) and Fully Invertible Residual Network (Fully-InvRes). In Partially-InvRes, the invertible residual layer is defined by a technique called additive coupling whereas in Fully-InvRes, both invertible upsampling and downsampling operations are learned based on squeezing (known as pixel shuffle). Furthermore, to avoid the overfitting problem because of less training data, a variational auto-encoder (VAE) branch is added to reconstruct the input volumetric data itself. Our results indicate that by using partially/fully invertible networks as the central workhorse in volumetric segmentation, we not only reduce memory overhead but also achieve compatible segmentation performance compared against the non-invertible 3D Unet. We have demonstrated the proposed networks on various volumetric datasets such as iSeg 2019 and BraTS 2020.



### Unsupervised Anomaly Segmentation using Image-Semantic Cycle Translation
- **Arxiv ID**: http://arxiv.org/abs/2103.09094v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09094v1)
- **Published**: 2021-03-16 14:15:30+00:00
- **Updated**: 2021-03-16 14:15:30+00:00
- **Authors**: Chenxin Li, Yunlong Zhang, Jiongcheng Li, Yue Huang, Xinghao Ding
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of unsupervised anomaly segmentation (UAS) is to detect the pixel-level anomalies unseen during training. It is a promising field in the medical imaging community, e.g, we can use the model trained with only healthy data to segment the lesions of rare diseases. Existing methods are mainly based on Information Bottleneck, whose underlying principle is modeling the distribution of normal anatomy via learning to compress and recover the healthy data with a low-dimensional manifold, and then detecting lesions as the outlier from this learned distribution. However, this dimensionality reduction inevitably damages the localization information, which is especially essential for pixel-level anomaly detection. In this paper, to alleviate this issue, we introduce the semantic space of healthy anatomy in the process of modeling healthy-data distribution. More precisely, we view the couple of segmentation and synthesis as a special Autoencoder, and propose a novel cycle translation framework with a journey of 'image->semantic->image'. Experimental results on the BraTS and ISLES databases show that the proposed approach achieves significantly superior performance compared to several prior methods and segments the anomalies more accurately.



### Frequency-aware Discriminative Feature Learning Supervised by Single-Center Loss for Face Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.09096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09096v1)
- **Published**: 2021-03-16 14:17:17+00:00
- **Updated**: 2021-03-16 14:17:17+00:00
- **Authors**: Jiaming Li, Hongtao Xie, Jiahong Li, Zhongyuan Wang, Yongdong Zhang
- **Comment**: 10 pages,6 figures;cvpr accept
- **Journal**: None
- **Summary**: Face forgery detection is raising ever-increasing interest in computer vision since facial manipulation technologies cause serious worries. Though recent works have reached sound achievements, there are still unignorable problems: a) learned features supervised by softmax loss are separable but not discriminative enough, since softmax loss does not explicitly encourage intra-class compactness and interclass separability; and b) fixed filter banks and hand-crafted features are insufficient to capture forgery patterns of frequency from diverse inputs. To compensate for such limitations, a novel frequency-aware discriminative feature learning framework is proposed in this paper. Specifically, we design a novel single-center loss (SCL) that only compresses intra-class variations of natural faces while boosting inter-class differences in the embedding space. In such a case, the network can learn more discriminative features with less optimization difficulty. Besides, an adaptive frequency feature generation module is developed to mine frequency clues in a completely data-driven fashion. With the above two modules, the whole framework can learn more discriminative features in an end-to-end manner. Extensive experiments demonstrate the effectiveness and superiority of our framework on three versions of the FF++ dataset.



### Consistent Posterior Distributions under Vessel-Mixing: A Regularization for Cross-Domain Retinal Artery/Vein Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.09097v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09097v2)
- **Published**: 2021-03-16 14:18:35+00:00
- **Updated**: 2021-06-18 12:55:59+00:00
- **Authors**: Chenxin Li, Yunlong Zhang, Zhehan Liang, Wenao Ma, Yue Huang, Xinghao Ding
- **Comment**: Accepted to ICIP2021
- **Journal**: None
- **Summary**: Retinal artery/vein (A/V) classification is a critical technique for diagnosing diabetes and cardiovascular diseases. Although deep learning based methods achieve impressive results in A/V classification, their performances usually degrade severely when being directly applied to another database, due to the domain shift, e.g., caused by the variations in imaging protocols. In this paper, we propose a novel vessel-mixing based consistency regularization framework, for cross-domain learning in retinal A/V classification. Specially, to alleviate the severe bias to source domain, based on the label smooth prior, the model is regularized to give consistent predictions for unlabeled target-domain inputs that are under perturbation. This consistency regularization implicitly introduces a mechanism where the model and the perturbation is opponent to each other, where the model is pushed to be robust enough to cope with the perturbation. Thus, we investigate a more difficult opponent to further inspire the robustness of model, in the scenario of retinal A/V, called vessel-mixing perturbation. Specially, it effectively disturbs the fundus images especially the vessel structures by mixing two images regionally. We conduct extensive experiments on cross-domain A/V classification using four public datasets, which are collected by diverse institutions and imaging devices. The results demonstrate that our method achieves the state-of-the-art cross-domain performance, which is also close to the upper bound obtained by fully supervised learning on target domain.



### Is it enough to optimize CNN architectures on ImageNet?
- **Arxiv ID**: http://arxiv.org/abs/2103.09108v4
- **DOI**: 10.3389/fcomp.2022.1041703
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09108v4)
- **Published**: 2021-03-16 14:42:01+00:00
- **Updated**: 2023-03-06 14:50:44+00:00
- **Authors**: Lukas Tuggener, Jürgen Schmidhuber, Thilo Stadelmann
- **Comment**: None
- **Journal**: Frontiers in Computer Science, Volume 4, 2022
- **Summary**: Classification performance based on ImageNet is the de-facto standard metric for CNN development. In this work we challenge the notion that CNN architecture design solely based on ImageNet leads to generally effective convolutional neural network (CNN) architectures that perform well on a diverse set of datasets and application domains. To this end, we investigate and ultimately improve ImageNet as a basis for deriving such architectures. We conduct an extensive empirical study for which we train $500$ CNN architectures, sampled from the broad AnyNetX design space, on ImageNet as well as $8$ additional well known image classification benchmark datasets from a diverse array of application domains. We observe that the performances of the architectures are highly dataset dependent. Some datasets even exhibit a negative error correlation with ImageNet across all architectures. We show how to significantly increase these correlations by utilizing ImageNet subsets restricted to fewer classes. These contributions can have a profound impact on the way we design future CNN architectures and help alleviate the tilt we see currently in our community with respect to over-reliance on one dataset.



### Balancing Biases and Preserving Privacy on Balanced Faces in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2103.09118v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.09118v5)
- **Published**: 2021-03-16 15:05:49+00:00
- **Updated**: 2023-07-05 20:06:22+00:00
- **Authors**: Joseph P Robinson, Can Qin, Yann Henon, Samson Timoner, Yun Fu
- **Comment**: arXiv admin note: text overlap with arXiv:2102.08941
- **Journal**: None
- **Summary**: There are demographic biases present in current facial recognition (FR) models. To measure these biases across different ethnic and gender subgroups, we introduce our Balanced Faces in the Wild (BFW) dataset. This dataset allows for the characterization of FR performance per subgroup. We found that relying on a single score threshold to differentiate between genuine and imposters sample pairs leads to suboptimal results. Additionally, performance within subgroups often varies significantly from the global average. Therefore, specific error rates only hold for populations that match the validation data. To mitigate imbalanced performances, we propose a novel domain adaptation learning scheme that uses facial features extracted from state-of-the-art neural networks. This scheme boosts the average performance and preserves identity information while removing demographic knowledge. Removing demographic knowledge prevents potential biases from affecting decision-making and protects privacy by eliminating demographic information. We explore the proposed method and demonstrate that subgroup classifiers can no longer learn from features projected using our domain adaptation scheme. For access to the source code and data, please visit https://github.com/visionjo/facerec-bias-bfw.



### QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.09136v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09136v2)
- **Published**: 2021-03-16 15:30:20+00:00
- **Updated**: 2022-03-24 12:19:20+00:00
- **Authors**: Chenhongyi Yang, Zehao Huang, Naiyan Wang
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: While general object detection with deep learning has achieved great success in the past few years, the performance and efficiency of detecting small objects are far from satisfactory. The most common and effective way to promote small object detection is to use high-resolution images or feature maps. However, both approaches induce costly computation since the computational cost grows squarely as the size of images and features increases. To get the best of two worlds, we propose QueryDet that uses a novel query mechanism to accelerate the inference speed of feature-pyramid based object detectors. The pipeline composes two steps: it first predicts the coarse locations of small objects on low-resolution features and then computes the accurate detection results using high-resolution features sparsely guided by those coarse positions. In this way, we can not only harvest the benefit of high-resolution feature maps but also avoid useless computation for the background area. On the popular COCO dataset, the proposed method improves the detection mAP by 1.0 and mAP-small by 2.0, and the high-resolution inference speed is improved to 3.0x on average. On VisDrone dataset, which contains more small objects, we create a new state-of-the-art while gaining a 2.3x high-resolution acceleration on average. Code is available at https://github.com/ChenhongyiYang/QueryDet-PyTorch.



### Simultaneous Multi-View Camera Pose Estimation and Object Tracking with Square Planar Markers
- **Arxiv ID**: http://arxiv.org/abs/2103.09141v1
- **DOI**: 10.1109/ACCESS.2019.2896648
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09141v1)
- **Published**: 2021-03-16 15:33:58+00:00
- **Updated**: 2021-03-16 15:33:58+00:00
- **Authors**: Hamid Sarmadi, Rafael Muñoz-Salinas, M. A. Berbís, R. Medina-Carnicer
- **Comment**: Some errors in the IEEE Access version (regarding object's rotational
  accuracy, and the definition of Equation 14) have been corrected in this
  version. IEEE Access paper: https://doi.org/10.1109/ACCESS.2019.2896648
- **Journal**: None
- **Summary**: Object tracking is a key aspect in many applications such as augmented reality in medicine (e.g. tracking a surgical instrument) or robotics. Squared planar markers have become popular tools for tracking since their pose can be estimated from their four corners. While using a single marker and a single camera limits the working area considerably, using multiple markers attached to an object requires estimating their relative position, which is not trivial, for high accuracy tracking. Likewise, using multiple cameras requires estimating their extrinsic parameters, also a tedious process that must be repeated whenever a camera is moved.   This work proposes a novel method to simultaneously solve the above-mentioned problems. From a video sequence showing a rigid set of planar markers recorded from multiple cameras, the proposed method is able to automatically obtain the three-dimensional configuration of the markers, the extrinsic parameters of the cameras, and the relative pose between the markers and the cameras at each frame. Our experiments show that our approach can obtain highly accurate results for estimating these parameters using low resolution cameras.   Once the parameters are obtained, tracking of the object can be done in real time with a low computational cost. The proposed method is a step forward in the development of cost-effective solutions for object tracking.



### Adversarial Driving: Attacking End-to-End Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2103.09151v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09151v7)
- **Published**: 2021-03-16 15:47:34+00:00
- **Updated**: 2023-05-31 10:51:04+00:00
- **Authors**: Han Wu, Syed Yunas, Sareh Rowlands, Wenjie Ruan, Johan Wahlstrom
- **Comment**: Accepted by IEEE Intelligent Vehicle Symposium, 2023
- **Journal**: None
- **Summary**: As research in deep neural networks advances, deep convolutional networks become promising for autonomous driving tasks. In particular, there is an emerging trend of employing end-to-end neural network models for autonomous driving. However, previous research has shown that deep neural network classifiers are vulnerable to adversarial attacks. While for regression tasks, the effect of adversarial attacks is not as well understood. In this research, we devise two white-box targeted attacks against end-to-end autonomous driving models. Our attacks manipulate the behavior of the autonomous driving system by perturbing the input image. In an average of 800 attacks with the same attack strength (epsilon=1), the image-specific and image-agnostic attack deviates the steering angle from the original output by 0.478 and 0.111, respectively, which is much stronger than random noises that only perturbs the steering angle by 0.002 (The steering angle ranges from [-1, 1]). Both attacks can be initiated in real-time on CPUs without employing GPUs. Demo video: https://youtu.be/I0i8uN2oOP0.



### Leveraging Recent Advances in Deep Learning for Audio-Visual Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.09154v2
- **DOI**: 10.1016/j.patrec.2021.03.007
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2103.09154v2)
- **Published**: 2021-03-16 15:49:15+00:00
- **Updated**: 2021-09-14 08:26:09+00:00
- **Authors**: Liam Schoneveld, Alice Othmani, Hazem Abdelkawy
- **Comment**: 8 pages, 3 figures, Pattern Recognition Letters
- **Journal**: None
- **Summary**: Emotional expressions are the behaviors that communicate our emotional state or attitude to others. They are expressed through verbal and non-verbal communication. Complex human behavior can be understood by studying physical features from multiple modalities; mainly facial, vocal and physical gestures. Recently, spontaneous multi-modal emotion recognition has been extensively studied for human behavior analysis. In this paper, we propose a new deep learning-based approach for audio-visual emotion recognition. Our approach leverages recent advances in deep learning like knowledge distillation and high-performing deep architectures. The deep feature representations of the audio and visual modalities are fused based on a model-level fusion strategy. A recurrent neural network is then used to capture the temporal dynamics. Our proposed approach substantially outperforms state-of-the-art approaches in predicting valence on the RECOLA dataset. Moreover, our proposed visual facial expression feature extraction network outperforms state-of-the-art results on the AffectNet and Google Facial Expression Comparison datasets.



### LRGNet: Learnable Region Growing for Class-Agnostic Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.09160v1
- **DOI**: 10.1109/LRA.2021.3062607
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.09160v1)
- **Published**: 2021-03-16 15:58:01+00:00
- **Updated**: 2021-03-16 15:58:01+00:00
- **Authors**: Jingdao Chen, Zsolt Kira, Yong K. Cho
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters 2021
- **Summary**: 3D point cloud segmentation is an important function that helps robots understand the layout of their surrounding environment and perform tasks such as grasping objects, avoiding obstacles, and finding landmarks. Current segmentation methods are mostly class-specific, many of which are tuned to work with specific object categories and may not be generalizable to different types of scenes. This research proposes a learnable region growing method for class-agnostic point cloud segmentation, specifically for the task of instance label prediction. The proposed method is able to segment any class of objects using a single deep neural network without any assumptions about their shapes and sizes. The deep neural network is trained to predict how to add or remove points from a point cloud region to morph it into incrementally more complete regions of an object instance. Segmentation results on the S3DIS and ScanNet datasets show that the proposed method outperforms competing methods by 1%-9% on 6 different evaluation metrics.



### Monocular Multi-Layer Layout Estimation for Warehouse Racks
- **Arxiv ID**: http://arxiv.org/abs/2103.09174v3
- **DOI**: 10.1145/3490035.3490263
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.09174v3)
- **Published**: 2021-03-16 16:22:31+00:00
- **Updated**: 2021-10-28 17:00:21+00:00
- **Authors**: Meher Shashwat Nigam, Avinash Prabhu, Anurag Sahu, Puru Gupta, Tanvi Karandikar, N. Sai Shankar, Ravi Kiran Sarvadevabhatla, K. Madhava Krishna
- **Comment**: Visit our project repository at
  https://github.com/Avinash2468/RackLay
- **Journal**: None
- **Summary**: Given a monocular colour image of a warehouse rack, we aim to predict the bird's-eye view layout for each shelf in the rack, which we term as multi-layer layout prediction. To this end, we present RackLay, a deep neural network for real-time shelf layout estimation from a single image. Unlike previous layout estimation methods, which provide a single layout for the dominant ground plane alone, RackLay estimates the top-view and front-view layout for each shelf in the considered rack populated with objects. RackLay's architecture and its variants are versatile and estimate accurate layouts for diverse scenes characterized by varying number of visible shelves in an image, large range in shelf occupancy factor and varied background clutter. Given the extreme paucity of datasets in this space and the difficulty involved in acquiring real data from warehouses, we additionally release a flexible synthetic dataset generation pipeline WareSynth which allows users to control the generation process and tailor the dataset according to contingent application. The ablations across architectural variants and comparison with strong prior baselines vindicate the efficacy of RackLay as an apt architecture for the novel problem of multi-layered layout estimation. We also show that fusing the top-view and front-view enables 3D reasoning applications such as metric free space estimation for the considered rack.



### Conceptual Text Region Network: Cognition-Inspired Accurate Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.09179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09179v1)
- **Published**: 2021-03-16 16:28:33+00:00
- **Updated**: 2021-03-16 16:28:33+00:00
- **Authors**: Chenwei Cui, Liangfu Lu, Zhiyuan Tan, Amir Hussain
- **Comment**: Preprint submitted to Neurocomputing
- **Journal**: None
- **Summary**: Segmentation-based methods are widely used for scene text detection due to their superiority in describing arbitrary-shaped text instances. However, two major problems still exist: 1) current label generation techniques are mostly empirical and lack theoretical support, discouraging elaborate label design; 2) as a result, most methods rely heavily on text kernel segmentation which is unstable and requires deliberate tuning. To address these challenges, we propose a human cognition-inspired framework, termed, Conceptual Text Region Network (CTRNet). The framework utilizes Conceptual Text Regions (CTRs), which is a class of cognition-based tools inheriting good mathematical properties, allowing for sophisticated label design. Another component of CTRNet is an inference pipeline that, with the help of CTRs, completely omits the need for text kernel segmentation. Compared with previous segmentation-based methods, our approach is not only more interpretable but also more accurate. Experimental results show that CTRNet achieves state-of-the-art performance on benchmark CTW1500, Total-Text, MSRA-TD500, and ICDAR 2015 datasets, yielding performance gains of up to 2.0%. Notably, to the best of our knowledge, CTRNet is among the first detection models to achieve F-measures higher than 85.0% on all four of the benchmarks, with remarkable consistency and stability.



### Goal-constrained Sparse Reinforcement Learning for End-to-End Driving
- **Arxiv ID**: http://arxiv.org/abs/2103.09189v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09189v2)
- **Published**: 2021-03-16 16:39:09+00:00
- **Updated**: 2021-07-31 16:31:56+00:00
- **Authors**: Pranav Agarwal, Pierre de Beaucorps, Raoul de Charette
- **Comment**: Conference submission 6 pages, 8 figures
- **Journal**: None
- **Summary**: Deep reinforcement Learning for end-to-end driving is limited by the need of complex reward engineering. Sparse rewards can circumvent this challenge but suffers from long training time and leads to sub-optimal policy. In this work, we explore full-control driving with only goal-constrained sparse reward and propose a curriculum learning approach for end-to-end driving using only navigation view maps that benefit from small virtual-to-real domain gap. To address the complexity of multiple driving policies, we learn concurrent individual policies selected at inference by a navigation system. We demonstrate the ability of our proposal to generalize on unseen road layout, and to drive significantly longer than in the training.



### Back to the Feature: Learning Robust Camera Localization from Pixels to Pose
- **Arxiv ID**: http://arxiv.org/abs/2103.09213v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09213v2)
- **Published**: 2021-03-16 17:40:12+00:00
- **Updated**: 2021-04-07 21:48:06+00:00
- **Authors**: Paul-Edouard Sarlin, Ajaykumar Unagar, Måns Larsson, Hugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys, Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, Torsten Sattler
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Camera pose estimation in known scenes is a 3D geometry task recently tackled by multiple learning algorithms. Many regress precise geometric quantities, like poses or 3D points, from an input image. This either fails to generalize to new viewpoints or ties the model parameters to a specific scene. In this paper, we go Back to the Feature: we argue that deep networks should focus on learning robust and invariant visual features, while the geometric estimation should be left to principled algorithms. We introduce PixLoc, a scene-agnostic neural network that estimates an accurate 6-DoF pose from an image and a 3D model. Our approach is based on the direct alignment of multiscale deep features, casting camera localization as metric learning. PixLoc learns strong data priors by end-to-end training from pixels to pose and exhibits exceptional generalization to new scenes by separating model parameters and scene geometry. The system can localize in large environments given coarse pose priors but also improve the accuracy of sparse feature matching by jointly refining keypoints and poses with little overhead. The code will be publicly available at https://github.com/cvg/pixloc.



### Design and Development of Autonomous Delivery Robot
- **Arxiv ID**: http://arxiv.org/abs/2103.09229v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09229v1)
- **Published**: 2021-03-16 17:57:44+00:00
- **Updated**: 2021-03-16 17:57:44+00:00
- **Authors**: Aniket Gujarathi, Akshay Kulkarni, Unmesh Patil, Yogesh Phalak, Rajeshree Deotalu, Aman Jain, Navid Panchi, Ashwin Dhabale, Shital Chiddarwar
- **Comment**: 56 pages, Bachelor Thesis
- **Journal**: None
- **Summary**: The field of autonomous robotics is growing at a rapid rate. The trend to use increasingly more sensors in vehicles is driven both by legislation and consumer demands for higher safety and reliable service. Nowadays, robots are found everywhere, ranging from homes, hospitals to industries, and military operations. Autonomous robots are developed to be robust enough to work beside humans and to carry out jobs efficiently. Humans have a natural sense of understanding of the physical forces acting around them like gravity, sense of motion, etc. which are not taught explicitly but are developed naturally. However, this is not the case with robots. To make the robot fully autonomous and competent to work with humans, the robot must be able to perceive the situation and devise a plan for smooth operation, considering all the adversities that may occur while carrying out the tasks. In this thesis, we present an autonomous mobile robot platform that delivers the package within the VNIT campus without any human intercommunication. From an initial user-supplied geographic target location, the system plans an optimized path and autonomously navigates through it. The entire pipeline of an autonomous robot working in outdoor environments is explained in detail in this thesis.



### Bio-inspired Robustness: A Review
- **Arxiv ID**: http://arxiv.org/abs/2103.09265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09265v1)
- **Published**: 2021-03-16 18:20:29+00:00
- **Updated**: 2021-03-16 18:20:29+00:00
- **Authors**: Harshitha Machiraju, Oh-Hyeon Choung, Pascal Frossard, Michael. H Herzog
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have revolutionized computer vision and are often advocated as good models of the human visual system. However, there are currently many shortcomings of DCNNs, which preclude them as a model of human vision. For example, in the case of adversarial attacks, where adding small amounts of noise to an image, including an object, can lead to strong misclassification of that object. But for humans, the noise is often invisible. If vulnerability to adversarial noise cannot be fixed, DCNNs cannot be taken as serious models of human vision. Many studies have tried to add features of the human visual system to DCNNs to make them robust against adversarial attacks. However, it is not fully clear whether human vision inspired components increase robustness because performance evaluations of these novel components in DCNNs are often inconclusive. We propose a set of criteria for proper evaluation and analyze different models according to these criteria. We finally sketch future efforts to make DCCNs one step closer to the model of human vision.



### Co-Generation and Segmentation for Generalized Surgical Instrument Segmentation on Unlabelled Data
- **Arxiv ID**: http://arxiv.org/abs/2103.09276v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09276v1)
- **Published**: 2021-03-16 18:41:18+00:00
- **Updated**: 2021-03-16 18:41:18+00:00
- **Authors**: Megha Kalia, Tajwar Abrar Aleef, Nassir Navab, Septimiu E. Salcudean
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Surgical instrument segmentation for robot-assisted surgery is needed for accurate instrument tracking and augmented reality overlays. Therefore, the topic has been the subject of a number of recent papers in the CAI community. Deep learning-based methods have shown state-of-the-art performance for surgical instrument segmentation, but their results depend on labelled data. However, labelled surgical data is of limited availability and is a bottleneck in surgical translation of these methods. In this paper, we demonstrate the limited generalizability of these methods on different datasets, including human robot-assisted surgeries. We then propose a novel joint generation and segmentation strategy to learn a segmentation model with better generalization capability to domains that have no labelled data. The method leverages the availability of labelled data in a different domain. The generator does the domain translation from the labelled domain to the unlabelled domain and simultaneously, the segmentation model learns using the generated data while regularizing the generative model. We compared our method with state-of-the-art methods and showed its generalizability on publicly available datasets and on our own recorded video frames from robot-assisted prostatectomies. Our method shows consistently high mean Dice scores on both labelled and unlabelled domains when data is available only for one of the domains.   *M. Kalia and T. Aleef contributed equally to the manuscript



### Colorectal Cancer Segmentation using Atrous Convolution and Residual Enhanced UNet
- **Arxiv ID**: http://arxiv.org/abs/2103.09289v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09289v1)
- **Published**: 2021-03-16 19:20:20+00:00
- **Updated**: 2021-03-16 19:20:20+00:00
- **Authors**: Nisarg A. Shah, Divij Gupta, Romil Lodaya, Ujjwal Baid, Sanjay Talbar
- **Comment**: 5th IAPR International Conference on Computer Vision and Image
  Processing, 12 pages
- **Journal**: None
- **Summary**: Colorectal cancer is a leading cause of death worldwide. However, early diagnosis dramatically increases the chances of survival, for which it is crucial to identify the tumor in the body. Since its imaging uses high-resolution techniques, annotating the tumor is time-consuming and requires particular expertise. Lately, methods built upon Convolutional Neural Networks(CNNs) have proven to be at par, if not better in many biomedical segmentation tasks. For the task at hand, we propose another CNN-based approach, which uses atrous convolutions and residual connections besides the conventional filters. The training and inference were made using an efficient patch-based approach, which significantly reduced unnecessary computations. The proposed AtResUNet was trained on the DigestPath 2019 Challenge dataset for colorectal cancer segmentation with results having a Dice Coefficient of 0.748.



### The impact of data volume on performance of deep learning based building rooftop extraction using very high spatial resolution aerial images
- **Arxiv ID**: http://arxiv.org/abs/2103.09300v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09300v2)
- **Published**: 2021-03-16 20:03:50+00:00
- **Updated**: 2021-10-04 20:50:24+00:00
- **Authors**: Hongjie He, Ke Yang, Yuwei Cai, Zijian Jiang, Qiutong Yu, Kun Zhao, Junbo Wang, Sarah Narges Fatholahi, Yan Liu, Hasti Andon Petrosians, Bingxu Hu, Liyuan Qing, Zhehan Zhang, Hongzhang Xu, Siyu Li, Kyle Gao, Linlin Xu, Jonathan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Building rooftop data are of importance in several urban applications and in natural disaster management. In contrast to traditional surveying and mapping, by using high spatial resolution aerial images, deep learning-based building rooftops extraction methods are efficient and accurate. Although more training data is preferred in deep learning-based tasks, the effect of data volume on building extraction models is underexplored. Therefore, the paper explores the impact of data volume on the performance of building rooftop extraction from very-high-spatial-resolution (VHSR) images using deep learning-based methods. To do so, we manually labelled 0.12m spatial resolution aerial images and perform a comparative analysis of models trained on datasets of different sizes using popular deep learning architectures for segmentation tasks, including Fully Convolutional Networks (FCN)-8s, U-Net and DeepLabv3+. The experiments showed that with more training data, algorithms converged faster and achieved higher accuracy, while better algorithms were able to better mitigate the lack of training data.



### Digital Peter: Dataset, Competition and Handwriting Recognition Methods
- **Arxiv ID**: http://arxiv.org/abs/2103.09354v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.7.5; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2103.09354v2)
- **Published**: 2021-03-16 22:37:22+00:00
- **Updated**: 2021-08-28 03:12:16+00:00
- **Authors**: Mark Potanin, Denis Dimitrov, Alex Shonenkov, Vladimir Bataev, Denis Karachev, Maxim Novopoltsev
- **Comment**: 17 pages, 7 figures, submitted to ICDAR 2021
- **Journal**: None
- **Summary**: This paper presents a new dataset of Peter the Great's manuscripts and describes a segmentation procedure that converts initial images of documents into the lines. The new dataset may be useful for researchers to train handwriting text recognition models as a benchmark for comparing different models. It consists of 9 694 images and text files corresponding to lines in historical documents. The open machine learning competition Digital Peter was held based on the considered dataset. The baseline solution for this competition as well as more advanced methods on handwritten text recognition are described in the article. Full dataset and all code are publicly available.




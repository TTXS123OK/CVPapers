# Arxiv Papers in cs.CV on 2021-03-10
### Super-Resolving Beyond Satellite Hardware Using Realistically Degraded Images
- **Arxiv ID**: http://arxiv.org/abs/2103.06270v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2103.06270v1)
- **Published**: 2021-03-10 00:20:33+00:00
- **Updated**: 2021-03-10 00:20:33+00:00
- **Authors**: Jack White, Alex Codoreanu, Ignacio Zuleta, Colm Lynch, Giovanni Marchisio, Stephen Petrie, Alan R. Duffy
- **Comment**: 6 pages, 6 figures, for supplementary results, see
  https://smpetrie.github.io/superres/
- **Journal**: None
- **Summary**: Modern deep Super-Resolution (SR) networks have established themselves as valuable techniques in image reconstruction and enhancement. However, these networks are normally trained and tested on benchmark image data that lacks the typical image degrading noise present in real images. In this paper, we test the feasibility of using deep SR in real remote sensing payloads by assessing SR performance in reconstructing realistically degraded satellite images. We demonstrate that a state-of-the-art SR technique called Enhanced Deep Super-Resolution Network (EDSR), without domain specific pre-training, can recover encoded pixel data on images with poor ground sampling distance, provided the ground resolved distance is sufficient. However, this recovery varies amongst selected geographical types. Our results indicate that custom training has potential to further improve reconstruction of overhead imagery, and that new satellite hardware should prioritise optical performance over minimising pixel size as deep SR can overcome a lack of the latter but not the former.



### Reframing Neural Networks: Deep Structure in Overcomplete Representations
- **Arxiv ID**: http://arxiv.org/abs/2103.05804v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05804v2)
- **Published**: 2021-03-10 01:15:14+00:00
- **Updated**: 2022-01-08 00:10:12+00:00
- **Authors**: Calvin Murdock, George Cazenavette, Simon Lucey
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2003.13866
- **Journal**: None
- **Summary**: In comparison to classical shallow representation learning techniques, deep neural networks have achieved superior performance in nearly every application benchmark. But despite their clear empirical advantages, it is still not well understood what makes them so effective. To approach this question, we introduce deep frame approximation: a unifying framework for constrained representation learning with structured overcomplete frames. While exact inference requires iterative optimization, it may be approximated by the operations of a feed-forward deep neural network. We indirectly analyze how model capacity relates to frame structures induced by architectural hyperparameters such as depth, width, and skip connections. We quantify these structural differences with the deep frame potential, a data-independent measure of coherence linked to representation uniqueness and stability. As a criterion for model selection, we show correlation with generalization error on a variety of common deep network architectures and datasets. We also demonstrate how recurrent networks implementing iterative optimization algorithms can achieve performance comparable to their feed-forward approximations while improving adversarial robustness. This connection to the established theory of overcomplete representations suggests promising new directions for principled deep network architecture design with less reliance on ad-hoc engineering.



### Learning to compose 6-DoF omnidirectional videos using multi-sphere images
- **Arxiv ID**: http://arxiv.org/abs/2103.05842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.05842v1)
- **Published**: 2021-03-10 03:09:55+00:00
- **Updated**: 2021-03-10 03:09:55+00:00
- **Authors**: Jisheng Li, Yuze He, Yubin Hu, Yuxing Han, Jiangtao Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Omnidirectional video is an essential component of Virtual Reality. Although various methods have been proposed to generate content that can be viewed with six degrees of freedom (6-DoF), existing systems usually involve complex depth estimation, image in-painting or stitching pre-processing. In this paper, we propose a system that uses a 3D ConvNet to generate a multi-sphere images (MSI) representation that can be experienced in 6-DoF VR. The system utilizes conventional omnidirectional VR camera footage directly without the need for a depth map or segmentation mask, thereby significantly simplifying the overall complexity of the 6-DoF omnidirectional video composition. By using a newly designed weighted sphere sweep volume (WSSV) fusing technique, our approach is compatible with most panoramic VR camera setups. A ground truth generation approach for high-quality artifact-free 6-DoF contents is proposed and can be used by the research and development community for 6-DoF content generation.



### Learning to Estimate Kernel Scale and Orientation of Defocus Blur with Asymmetric Coded Aperture
- **Arxiv ID**: http://arxiv.org/abs/2103.05843v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.05843v1)
- **Published**: 2021-03-10 03:12:15+00:00
- **Updated**: 2021-03-10 03:12:15+00:00
- **Authors**: Jisheng Li, Qi Dai, Jiangtao Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Consistent in-focus input imagery is an essential precondition for machine vision systems to perceive the dynamic environment. A defocus blur severely degrades the performance of vision systems. To tackle this problem, we propose a deep-learning-based framework estimating the kernel scale and orientation of the defocus blur to adjust lens focus rapidly. Our pipeline utilizes 3D ConvNet for a variable number of input hypotheses to select the optimal slice from the input stack. We use random shuffle and Gumbel-softmax to improve network performance. We also propose to generate synthetic defocused images with various asymmetric coded apertures to facilitate training. Experiments are conducted to demonstrate the effectiveness of our framework.



### Incorporating Orientations into End-to-end Driving Model for Steering Control
- **Arxiv ID**: http://arxiv.org/abs/2103.05846v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05846v1)
- **Published**: 2021-03-10 03:14:41+00:00
- **Updated**: 2021-03-10 03:14:41+00:00
- **Authors**: Peng Wan, Zhenbo Song, Jianfeng Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel end-to-end deep neural network model for autonomous driving that takes monocular image sequence as input, and directly generates the steering control angle. Firstly, we model the end-to-end driving problem as a local path planning process. Inspired by the environmental representation in the classical planning algorithms(i.e. the beam curvature method), pixel-wise orientations are fed into the network to learn direction-aware features. Next, to handle the imbalanced distribution of steering values in training datasets, we propose an improvement on a cost-sensitive loss function named SteeringLoss2. Besides, we also present a new end-to-end driving dataset, which provides corresponding LiDAR and image sequences, as well as standard driving behaviors. Our dataset includes multiple driving scenarios, such as urban, country, and off-road. Numerous experiments are conducted on both public available LiVi-Set and our own dataset, and the results show that the model using our proposed methods can predict steering angle accurately.



### Fusing Medical Image Features and Clinical Features with Deep Learning for Computer-Aided Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2103.05855v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05855v1)
- **Published**: 2021-03-10 03:37:21+00:00
- **Updated**: 2021-03-10 03:37:21+00:00
- **Authors**: Songxiao Yang, Xiabi Liu, Zhongshu Zheng, Wei Wang, Xiaohong Ma
- **Comment**: 10 pages, 2 figures
- **Journal**: None
- **Summary**: Current Computer-Aided Diagnosis (CAD) methods mainly depend on medical images. The clinical information, which usually needs to be considered in practical clinical diagnosis, has not been fully employed in CAD. In this paper, we propose a novel deep learning-based method for fusing Magnetic Resonance Imaging (MRI)/Computed Tomography (CT) images and clinical information for diagnostic tasks. Two paths of neural layers are performed to extract image features and clinical features, respectively, and at the same time clinical features are employed as the attention to guide the extraction of image features. Finally, these two modalities of features are concatenated to make decisions. We evaluate the proposed method on its applications to Alzheimer's disease diagnosis, mild cognitive impairment converter prediction and hepatic microvascular invasion diagnosis. The encouraging experimental results prove the values of the image feature extraction guided by clinical features and the concatenation of two modalities of features for classification, which improve the performance of diagnosis effectively and stably.



### Novel tile segmentation scheme for omnidirectional video
- **Arxiv ID**: http://arxiv.org/abs/2103.05858v1
- **DOI**: 10.1109/ICIP.2016.7532381
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.05858v1)
- **Published**: 2021-03-10 03:49:18+00:00
- **Updated**: 2021-03-10 03:49:18+00:00
- **Authors**: Jisheng Li, Ziyu Wen, Sihan Li, Yikai Zhao, Bichuan Guo, Jiangtao Wen
- **Comment**: Published in 2016 IEEE International Conference on Image Processing
  (ICIP)
- **Journal**: None
- **Summary**: Regular omnidirectional video encoding technics use map projection to flatten a scene from a spherical shape into one or several 2D shapes. Common projection methods including equirectangular and cubic projection have varying levels of interpolation that create a large number of non-information-carrying pixels that lead to wasted bitrate. In this paper, we propose a tile based omnidirectional video segmentation scheme which can save up to 28% of pixel area and 20% of BD-rate averagely compared to the traditional equirectangular projection based approach.



### Manifold Regularized Dynamic Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2103.05861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05861v1)
- **Published**: 2021-03-10 03:59:03+00:00
- **Updated**: 2021-03-10 03:59:03+00:00
- **Authors**: Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao, Chang Xu
- **Comment**: This paper is accepted by CVPR 2021. Key words: Filter pruning,
  Dynamic network, Network compression, Manifold regularization
- **Journal**: None
- **Summary**: Neural network pruning is an essential approach for reducing the computational complexity of deep models so that they can be well deployed on resource-limited devices. Compared with conventional methods, the recently developed dynamic pruning methods determine redundant filters variant to each input instance which achieves higher acceleration. Most of the existing methods discover effective sub-networks for each instance independently and do not utilize the relationship between different inputs. To maximally excavate redundancy in the given network architecture, this paper proposes a new paradigm that dynamically removes redundant filters by embedding the manifold information of all instances into the space of pruned networks (dubbed as ManiDP). We first investigate the recognition complexity and feature similarity between images in the training set. Then, the manifold relationship between instances and the pruned sub-networks will be aligned in the training procedure. The effectiveness of the proposed method is verified on several benchmarks, which shows better performance in terms of both accuracy and computational cost compared to the state-of-the-art methods. For example, our method can reduce 55.3% FLOPs of ResNet-34 with only 0.57% top-1 accuracy degradation on ImageNet.



### AutoDO: Robust AutoAugment for Biased Data with Label Noise via Scalable Probabilistic Implicit Differentiation
- **Arxiv ID**: http://arxiv.org/abs/2103.05863v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05863v2)
- **Published**: 2021-03-10 04:05:33+00:00
- **Updated**: 2021-03-11 22:15:41+00:00
- **Authors**: Denis Gudovskiy, Luca Rigazio, Shun Ishizaka, Kazuki Kozuka, Sotaro Tsukizawa
- **Comment**: Accepted to CVPR 2021. Preprint
- **Journal**: None
- **Summary**: AutoAugment has sparked an interest in automated augmentation methods for deep learning models. These methods estimate image transformation policies for train data that improve generalization to test data. While recent papers evolved in the direction of decreasing policy search complexity, we show that those methods are not robust when applied to biased and noisy data. To overcome these limitations, we reformulate AutoAugment as a generalized automated dataset optimization (AutoDO) task that minimizes the distribution shift between test data and distorted train dataset. In our AutoDO model, we explicitly estimate a set of per-point hyperparameters to flexibly change distribution of train data. In particular, we include hyperparameters for augmentation, loss weights, and soft-labels that are jointly estimated using implicit differentiation. We develop a theoretical probabilistic interpretation of this framework using Fisher information and show that its complexity scales linearly with the dataset size. Our experiments on SVHN, CIFAR-10/100, and ImageNet classification show up to 9.3% improvement for biased datasets with label noise compared to prior methods and, importantly, up to 36.6% gain for underrepresented SVHN classes.



### Tuna Nutriment Tracking using Trajectory Mapping in Application to Aquaculture Fish Tank
- **Arxiv ID**: http://arxiv.org/abs/2103.05886v1
- **DOI**: 10.1109/DICTA51227.2020.9363387
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2103.05886v1)
- **Published**: 2021-03-10 06:02:19+00:00
- **Updated**: 2021-03-10 06:02:19+00:00
- **Authors**: Hilmil Pradana, Keiichi Horio
- **Comment**: None
- **Journal**: 2020 Digital Image Computing: Techniques and Applications (DICTA)
  (2020) 1-8
- **Summary**: The cost of fish feeding is usually around 40 percent of total production cost. Estimating a state of fishes in a tank and adjusting an amount of nutriments play an important role to manage cost of fish feeding system. Our approach is based on tracking nutriments on videos collected from an active aquaculture fish farm. Tracking approach is applied to acknowledge movement of nutriment to understand more about the fish behavior. Recently, there has been increasing number of researchers focused on developing tracking algorithms to generate more accurate and faster determination of object. Unfortunately, recent studies have shown that efficient and robust tracking of multiple objects with complex relations remain unsolved. Hence, focusing to develop tracking algorithm in aquaculture is more challenging because tracked object has a lot of aquatic variant creatures. By following aforementioned problem, we develop tuna nutriment tracking based on the classical minimum cost problem which consistently performs well in real environment datasets. In evaluation, the proposed method achieved 21.32 pixels and 3.08 pixels for average error distance and standard deviation, respectively. Quantitative evaluation based on the data generated by human annotators shows that the proposed method is valuable for aquaculture fish farm and can be widely applied to real environment datasets.



### Evaluating COPY-BLEND Augmentation for Low Level Vision Tasks
- **Arxiv ID**: http://arxiv.org/abs/2103.05889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05889v1)
- **Published**: 2021-03-10 06:17:52+00:00
- **Updated**: 2021-03-10 06:17:52+00:00
- **Authors**: Pranjay Shyam, Sandeep Singh Sengar, Kuk-Jin Yoon, Kyung-Soo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Region modification-based data augmentation techniques have shown to improve performance for high level vision tasks (object detection, semantic segmentation, image classification, etc.) by encouraging underlying algorithms to focus on multiple discriminative features. However, as these techniques destroy spatial relationship with neighboring regions, performance can be deteriorated when using them to train algorithms designed for low level vision tasks (low light image enhancement, image dehazing, deblurring, etc.) where textural consistency between recovered and its neighboring regions is important to ensure effective performance. In this paper, we examine the efficacy of a simple copy-blend data augmentation technique that copies patches from noisy images and blends onto a clean image and vice versa to ensure that an underlying algorithm localizes and recovers affected regions resulting in increased perceptual quality of a recovered image. To assess performance improvement, we perform extensive experiments alongside different region modification-based augmentation techniques and report observations such as improved performance, reduced requirement for training dataset, and early convergence across tasks such as low light image enhancement, image dehazing and image deblurring without any modification to baseline algorithm.



### Limitations of Post-Hoc Feature Alignment for Robustness
- **Arxiv ID**: http://arxiv.org/abs/2103.05898v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05898v1)
- **Published**: 2021-03-10 06:55:41+00:00
- **Updated**: 2021-03-10 06:55:41+00:00
- **Authors**: Collin Burns, Jacob Steinhardt
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Feature alignment is an approach to improving robustness to distribution shift that matches the distribution of feature activations between the training distribution and test distribution. A particularly simple but effective approach to feature alignment involves aligning the batch normalization statistics between the two distributions in a trained neural network. This technique has received renewed interest lately because of its impressive performance on robustness benchmarks. However, when and why this method works is not well understood. We investigate the approach in more detail and identify several limitations. We show that it only significantly helps with a narrow set of distribution shifts and we identify several settings in which it even degrades performance. We also explain why these limitations arise by pinpointing why this approach can be so effective in the first place. Our findings call into question the utility of this approach and Unsupervised Domain Adaptation more broadly for improving robustness in practice.



### RL-CSDia: Representation Learning of Computer Science Diagrams
- **Arxiv ID**: http://arxiv.org/abs/2103.05900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.05900v1)
- **Published**: 2021-03-10 07:01:07+00:00
- **Updated**: 2021-03-10 07:01:07+00:00
- **Authors**: Shaowei Wang, LingLing Zhang, Xuan Luo, Yi Yang, Xin Hu, Jun Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies on computer vision mainly focus on natural images that express real-world scenes. They achieve outstanding performance on diverse tasks such as visual question answering. Diagram is a special form of visual expression that frequently appears in the education field and is of great significance for learners to understand multimodal knowledge. Current research on diagrams preliminarily focuses on natural disciplines such as Biology and Geography, whose expressions are still similar to natural images. Another type of diagrams such as from Computer Science is composed of graphics containing complex topologies and relations, and research on this type of diagrams is still blank. The main challenges of graphic diagrams understanding are the rarity of data and the confusion of semantics, which are mainly reflected in the diversity of expressions. In this paper, we construct a novel dataset of graphic diagrams named Computer Science Diagrams (CSDia). It contains more than 1,200 diagrams and exhaustive annotations of objects and relations. Considering the visual noises caused by the various expressions in diagrams, we introduce the topology of diagrams to parse topological structure. After that, we propose Diagram Parsing Net (DPN) to represent the diagram from three branches: topology, visual feature, and text, and apply the model to the diagram classification task to evaluate the ability of diagrams understanding. The results show the effectiveness of the proposed DPN on diagrams understanding.



### Learning a Domain-Agnostic Visual Representation for Autonomous Driving via Contrastive Loss
- **Arxiv ID**: http://arxiv.org/abs/2103.05902v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.05902v1)
- **Published**: 2021-03-10 07:06:03+00:00
- **Updated**: 2021-03-10 07:06:03+00:00
- **Authors**: Dongseok Shim, H. Jin Kim
- **Comment**: IEEE IROS 2021 Submission
- **Journal**: None
- **Summary**: Deep neural networks have been widely studied in autonomous driving applications such as semantic segmentation or depth estimation. However, training a neural network in a supervised manner requires a large amount of annotated labels which are expensive and time-consuming to collect. Recent studies leverage synthetic data collected from a virtual environment which are much easier to acquire and more accurate compared to data from the real world, but they usually suffer from poor generalization due to the inherent domain shift problem. In this paper, we propose a Domain-Agnostic Contrastive Learning (DACL) which is a two-stage unsupervised domain adaptation framework with cyclic adversarial training and contrastive loss. DACL leads the neural network to learn domain-agnostic representation to overcome performance degradation when there exists a difference between training and test data distribution. Our proposed approach achieves better performance in the monocular depth estimation task compared to previous state-of-the-art methods and also shows effectiveness in the semantic segmentation task.



### VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2103.05905v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.05905v2)
- **Published**: 2021-03-10 07:22:21+00:00
- **Updated**: 2021-03-17 02:45:50+00:00
- **Authors**: Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, Wei Liu
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: MoCo is effective for unsupervised image representation learning. In this paper, we propose VideoMoCo for unsupervised video representation learning. Given a video sequence as an input sample, we improve the temporal feature representations of MoCo from two perspectives. First, we introduce a generator to drop out several frames from this sample temporally. The discriminator is then learned to encode similar feature representations regardless of frame removals. By adaptively dropping out different frames during training iterations of adversarial learning, we augment this input sample to train a temporally robust encoder. Second, we use temporal decay to model key attenuation in the memory queue when computing the contrastive loss. As the momentum encoder updates after keys enqueue, the representation ability of these keys degrades when we use the current input sample for contrastive learning. This degradation is reflected via temporal decay to attend the input sample to recent keys in the queue. As a result, we adapt MoCo to learn video representations without empirically designing pretext tasks. By empowering the temporal robustness of the encoder and modeling the temporal decay of the keys, our VideoMoCo improves MoCo temporally based on contrastive learning. Experiments on benchmark datasets including UCF101 and HMDB51 show that VideoMoCo stands as a state-of-the-art video representation learning method.



### ES-Net: Erasing Salient Parts to Learn More in Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2103.05918v1
- **DOI**: 10.1109/TIP.2020.3046904
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05918v1)
- **Published**: 2021-03-10 08:19:46+00:00
- **Updated**: 2021-03-10 08:19:46+00:00
- **Authors**: Dong Shen, Shuai Zhao, Jinming Hu, Hao Feng, Deng Cai, Xiaofei He
- **Comment**: 11 pages, 6 figures. Accepted for publication in IEEE Transactions on
  Image Processing 2021
- **Journal**: IEEE Transactions on Image Processing, vol. 30, pp. 1676-1686,
  2021
- **Summary**: As an instance-level recognition problem, re-identification (re-ID) requires models to capture diverse features. However, with continuous training, re-ID models pay more and more attention to the salient areas. As a result, the model may only focus on few small regions with salient representations and ignore other important information. This phenomenon leads to inferior performance, especially when models are evaluated on small inter-identity variation data. In this paper, we propose a novel network, Erasing-Salient Net (ES-Net), to learn comprehensive features by erasing the salient areas in an image. ES-Net proposes a novel method to locate the salient areas by the confidence of objects and erases them efficiently in a training batch. Meanwhile, to mitigate the over-erasing problem, this paper uses a trainable pooling layer P-pooling that generalizes global max and global average pooling. Experiments are conducted on two specific re-identification tasks (i.e., Person re-ID, Vehicle re-ID). Our ES-Net outperforms state-of-the-art methods on three Person re-ID benchmarks and two Vehicle re-ID benchmarks. Specifically, mAP / Rank-1 rate: 88.6% / 95.7% on Market1501, 78.8% / 89.2% on DuckMTMC-reID, 57.3% / 80.9% on MSMT17, 81.9% / 97.0% on Veri-776, respectively. Rank-1 / Rank-5 rate: 83.6% / 96.9% on VehicleID (Small), 79.9% / 93.5% on VehicleID (Medium), 76.9% / 90.7% on VehicleID (Large), respectively. Moreover, the visualized salient areas show human-interpretable visual explanations for the ranking results.



### An Image-based Approach of Task-driven Driving Scene Categorization
- **Arxiv ID**: http://arxiv.org/abs/2103.05920v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05920v1)
- **Published**: 2021-03-10 08:23:36+00:00
- **Updated**: 2021-03-10 08:23:36+00:00
- **Authors**: Shaochi Hu, Hanwei Fan, Biao Gao, XijunZhao, Huijing Zhao
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Categorizing driving scenes via visual perception is a key technology for safe driving and the downstream tasks of autonomous vehicles.   Traditional methods infer scene category by detecting scene-related objects or using a classifier that is trained on large datasets of fine-labeled scene images.   Whereas at cluttered dynamic scenes such as campus or park, human activities are not strongly confined by rules, and the functional attributes of places are not strongly correlated with objects. So how to define, model and infer scene categories is crucial to make the technique really helpful in assisting a robot to pass through the scene.   This paper proposes a method of task-driven driving scene categorization using weakly supervised data.   Given a front-view video of a driving scene, a set of anchor points is marked by following the decision making of a human driver, where an anchor point is not a semantic label but an indicator meaning the semantic attribute of the scene is different from that of the previous one.   A measure is learned to discriminate the scenes of different semantic attributes via contrastive learning, and a driving scene profiling and categorization method is developed based on that measure.   Experiments are conducted on a front-view video that is recorded when a vehicle passed through the cluttered dynamic campus of Peking University. The scenes are categorized into straight road, turn road and alerting traffic. The results of semantic scene similarity learning and driving scene categorization are extensively studied, and positive result of scene categorization is 97.17 \% on the learning video and 85.44\% on the video of new scenes.



### Deep Sensing of Urban Waterlogging
- **Arxiv ID**: http://arxiv.org/abs/2103.05927v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2103.05927v3)
- **Published**: 2021-03-10 08:34:37+00:00
- **Updated**: 2021-08-16 01:30:30+00:00
- **Authors**: Shi-Wei Lo, Jyh-Horng Wu, Jo-Yu Chang, Chien-Hao Tseng, Meng-Wei Lin, Fang-Pang Lin
- **Comment**: 19 pages, 14 figures, under submitting and patenting
- **Journal**: None
- **Summary**: In the monsoon season, sudden flood events occur frequently in urban areas, which hamper the social and economic activities and may threaten the infrastructure and lives. The use of an efficient large-scale waterlogging sensing and information system can provide valuable real-time disaster information to facilitate disaster management and enhance awareness of the general public to alleviate losses during and after flood disasters. Therefore, in this study, a visual sensing approach driven by deep neural networks and information and communication technology was developed to provide an end-to-end mechanism to realize waterlogging sensing and event-location mapping. The use of a deep sensing system in the monsoon season in Taiwan was demonstrated, and waterlogging events were predicted on the island-wide scale. The system could sense approximately 2379 vision sources through an internet of video things framework and transmit the event-location information in 5 min. The proposed approach can sense waterlogging events at a national scale and provide an efficient and highly scalable alternative to conventional waterlogging sensing methods.



### MapFusion: A General Framework for 3D Object Detection with HDMaps
- **Arxiv ID**: http://arxiv.org/abs/2103.05929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05929v1)
- **Published**: 2021-03-10 08:36:59+00:00
- **Updated**: 2021-03-10 08:36:59+00:00
- **Authors**: Jin Fang, Dingfu Zhou, Xibin Song, Liangjun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection is a key perception component in autonomous driving. Most recent approaches are based on Lidar sensors only or fused with cameras. Maps (e.g., High Definition Maps), a basic infrastructure for intelligent vehicles, however, have not been well exploited for boosting object detection tasks. In this paper, we propose a simple but effective framework - MapFusion to integrate the map information into modern 3D object detector pipelines. In particular, we design a FeatureAgg module for HD Map feature extraction and fusion, and a MapSeg module as an auxiliary segmentation head for the detection backbone. Our proposed MapFusion is detector independent and can be easily integrated into different detectors. The experimental results of three different baselines on large public autonomous driving dataset demonstrate the superiority of the proposed framework. By fusing the map information, we can achieve 1.27 to 2.79 points improvements for mean Average Precision (mAP) on three strong 3d object detection baselines.



### AttaNet: Attention-Augmented Network for Fast and Accurate Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/2103.05930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05930v1)
- **Published**: 2021-03-10 08:38:29+00:00
- **Updated**: 2021-03-10 08:38:29+00:00
- **Authors**: Qi Song, Kangfu Mei, Rui Huang
- **Comment**: AAAI 2021
- **Journal**: None
- **Summary**: Two factors have proven to be very important to the performance of semantic segmentation models: global context and multi-level semantics. However, generating features that capture both factors always leads to high computational complexity, which is problematic in real-time scenarios. In this paper, we propose a new model, called Attention-Augmented Network (AttaNet), to capture both global context and multilevel semantics while keeping the efficiency high. AttaNet consists of two primary modules: Strip Attention Module (SAM) and Attention Fusion Module (AFM). Viewing that in challenging images with low segmentation accuracy, there are a significantly larger amount of vertical strip areas than horizontal ones, SAM utilizes a striping operation to reduce the complexity of encoding global context in the vertical direction drastically while keeping most of contextual information, compared to the non-local approaches. Moreover, AFM follows a cross-level aggregation strategy to limit the computation, and adopts an attention strategy to weight the importance of different levels of features at each pixel when fusing them, obtaining an efficient multi-level representation. We have conducted extensive experiments on two semantic segmentation benchmarks, and our network achieves different levels of speed/accuracy trade-offs on Cityscapes, e.g., 71 FPS/79.9% mIoU, 130 FPS/78.5% mIoU, and 180 FPS/70.1% mIoU, and leading performance on ADE20K as well.



### TransMed: Transformers Advance Multi-modal Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.05940v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2103.05940v1)
- **Published**: 2021-03-10 08:57:53+00:00
- **Updated**: 2021-03-10 08:57:53+00:00
- **Authors**: Yin Dai, Yifan Gao
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Over the past decade, convolutional neural networks (CNN) have shown very competitive performance in medical image analysis tasks, such as disease classification, tumor segmentation, and lesion detection. CNN has great advantages in extracting local features of images. However, due to the locality of convolution operation, it can not deal with long-range relationships well. Recently, transformers have been applied to computer vision and achieved remarkable success in large-scale datasets. Compared with natural images, multi-modal medical images have explicit and important long-range dependencies, and effective multi-modal fusion strategies can greatly improve the performance of deep models. This prompts us to study transformer-based structures and apply them to multi-modal medical images. Existing transformer-based network architectures require large-scale datasets to achieve better performance. However, medical imaging datasets are relatively small, which makes it difficult to apply pure transformers to medical image analysis. Therefore, we propose TransMed for multi-modal medical image classification. TransMed combines the advantages of CNN and transformer to efficiently extract low-level features of images and establish long-range dependencies between modalities. We evaluated our model for the challenging problem of preoperative diagnosis of parotid gland tumors, and the experimental results show the advantages of our proposed method. We argue that the combination of CNN and transformer has tremendous potential in a large number of medical image analysis tasks. To our best knowledge, this is the first work to apply transformers to medical image classification.



### Deep Convolutional Sparse Coding Network for Pansharpening with Guidance of Side Information
- **Arxiv ID**: http://arxiv.org/abs/2103.05946v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05946v1)
- **Published**: 2021-03-10 09:06:33+00:00
- **Updated**: 2021-03-10 09:06:33+00:00
- **Authors**: Shuang Xu, Jiangshe Zhang, Kai Sun, Zixiang Zhao, Lu Huang, Junmin Liu, Chunxia Zhang
- **Comment**: Accepted by ICME2021
- **Journal**: None
- **Summary**: Pansharpening is a fundamental issue in remote sensing field. This paper proposes a side information partially guided convolutional sparse coding (SCSC) model for pansharpening. The key idea is to split the low resolution multispectral image into a panchromatic image related feature map and a panchromatic image irrelated feature map, where the former one is regularized by the side information from panchromatic images. With the principle of algorithm unrolling techniques, the proposed model is generalized as a deep neural network, called as SCSC pansharpening neural network (SCSC-PNN). Compared with 13 classic and state-of-the-art methods on three satellites, the numerical experiments show that SCSC-PNN is superior to others. The codes are available at https://github.com/xsxjtu/SCSC-PNN.



### FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding
- **Arxiv ID**: http://arxiv.org/abs/2103.05950v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05950v2)
- **Published**: 2021-03-10 09:15:05+00:00
- **Updated**: 2021-03-13 16:18:01+00:00
- **Authors**: Bo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, Chi Zhang
- **Comment**: CVPR 2021 Accepted
- **Journal**: None
- **Summary**: Emerging interests have been brought to recognize previously unseen objects given very few training examples, known as few-shot object detection (FSOD). Recent researches demonstrate that good feature embedding is the key to reach favorable few-shot learning performance. We observe object proposals with different Intersection-of-Union (IoU) scores are analogous to the intra-image augmentation used in contrastive approaches. And we exploit this analogy and incorporate supervised contrastive learning to achieve more robust objects representations in FSOD. We present Few-Shot object detection via Contrastive proposals Encoding (FSCE), a simple yet effective approach to learning contrastive-aware object proposal encodings that facilitate the classification of detected objects. We notice the degradation of average precision (AP) for rare objects mainly comes from misclassifying novel instances as confusable classes. And we ease the misclassification issues by promoting instance level intra-class compactness and inter-class variance via our contrastive proposal encoding loss (CPE loss). Our design outperforms current state-of-the-art works in any shot and all data splits, with up to +8.8% on standard benchmark PASCAL VOC and +2.7% on challenging COCO benchmark. Code is available at: https: //github.com/MegviiDetection/FSCE



### Spatiotemporal Registration for Event-based Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2103.05955v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05955v2)
- **Published**: 2021-03-10 09:23:24+00:00
- **Updated**: 2021-03-19 00:50:51+00:00
- **Authors**: Daqi Liu, Alvaro Parra, Tat-Jun Chin
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: A useful application of event sensing is visual odometry, especially in settings that require high-temporal resolution. The state-of-the-art method of contrast maximisation recovers the motion from a batch of events by maximising the contrast of the image of warped events. However, the cost scales with image resolution and the temporal resolution can be limited by the need for large batch sizes to yield sufficient structure in the contrast image. In this work, we propose spatiotemporal registration as a compelling technique for event-based rotational motion estimation. We theoretcally justify the approach and establish its fundamental and practical advantages over contrast maximisation. In particular, spatiotemporal registration also produces feature tracks as a by-product, which directly supports an efficient visual odometry pipeline with graph-based optimisation for motion averaging. The simplicity of our visual odometry pipeline allows it to process more than 1 M events/second. We also contribute a new event dataset for visual odometry, where motion sequences with large velocity variations were acquired using a high-precision robot arm.



### Beyond Self-Supervision: A Simple Yet Effective Network Distillation Alternative to Improve Backbones
- **Arxiv ID**: http://arxiv.org/abs/2103.05959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05959v1)
- **Published**: 2021-03-10 09:32:44+00:00
- **Updated**: 2021-03-10 09:32:44+00:00
- **Authors**: Cheng Cui, Ruoyu Guo, Yuning Du, Dongliang He, Fu Li, Zewu Wu, Qiwen Liu, Shilei Wen, Jizhou Huang, Xiaoguang Hu, Dianhai Yu, Errui Ding, Yanjun Ma
- **Comment**: 10 pages, 3 figures, 9 tables
- **Journal**: None
- **Summary**: Recently, research efforts have been concentrated on revealing how pre-trained model makes a difference in neural network performance. Self-supervision and semi-supervised learning technologies have been extensively explored by the community and are proven to be of great potential in obtaining a powerful pre-trained model. However, these models require huge training costs (i.e., hundreds of millions of images or training iterations). In this paper, we propose to improve existing baseline networks via knowledge distillation from off-the-shelf pre-trained big powerful models. Different from existing knowledge distillation frameworks which require student model to be consistent with both soft-label generated by teacher model and hard-label annotated by humans, our solution performs distillation by only driving prediction of the student model consistent with that of the teacher model. Therefore, our distillation setting can get rid of manually labeled data and can be trained with extra unlabeled data to fully exploit capability of teacher model for better learning. We empirically find that such simple distillation settings perform extremely effective, for example, the top-1 accuracy on ImageNet-1k validation set of MobileNetV3-large and ResNet50-D can be significantly improved from 75.2% to 79% and 79.1% to 83%, respectively. We have also thoroughly analyzed what are dominant factors that affect the distillation performance and how they make a difference. Extensive downstream computer vision tasks, including transfer learning, object detection and semantic segmentation, can significantly benefit from the distilled pretrained models. All our experiments are implemented based on PaddlePaddle, codes and a series of improved pretrained models with ssld suffix are available in PaddleClas.



### COLA-Net: Collaborative Attention Network for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2103.05961v1
- **DOI**: 10.1109/TMM.2021.3063916
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05961v1)
- **Published**: 2021-03-10 09:33:17+00:00
- **Updated**: 2021-03-10 09:33:17+00:00
- **Authors**: Chong Mou, Jian Zhang, Xiaopeng Fan, Hangfan Liu, Ronggang Wang
- **Comment**: 11 pages, 6 tables, 9 figures, to be published in IEEE Transactions
  on Multimedia
- **Journal**: None
- **Summary**: Local and non-local attention-based methods have been well studied in various image restoration tasks while leading to promising performance. However, most of the existing methods solely focus on one type of attention mechanism (local or non-local). Furthermore, by exploiting the self-similarity of natural images, existing pixel-wise non-local attention operations tend to give rise to deviations in the process of characterizing long-range dependence due to image degeneration. To overcome these problems, in this paper we propose a novel collaborative attention network (COLA-Net) for image restoration, as the first attempt to combine local and non-local attention mechanisms to restore image content in the areas with complex textures and with highly repetitive details respectively. In addition, an effective and robust patch-wise non-local attention model is developed to capture long-range feature correspondences through 3D patches. Extensive experiments on synthetic image denoising, real image denoising and compression artifact reduction tasks demonstrate that our proposed COLA-Net is able to achieve state-of-the-art performance in both peak signal-to-noise ratio and visual perception, while maintaining an attractive computational complexity. The source code is available on https://github.com/MC-E/COLA-Net.



### Oversampling errors in multimodal medical imaging are due to the Gibbs effect
- **Arxiv ID**: http://arxiv.org/abs/2103.05964v2
- **DOI**: 10.13140/RG.2.2.30924.13446
- **Categories**: **math.NA**, cs.CV, cs.NA, 68U10, 65D05, 41A15
- **Links**: [PDF](http://arxiv.org/pdf/2103.05964v2)
- **Published**: 2021-03-10 09:42:13+00:00
- **Updated**: 2021-05-07 12:04:02+00:00
- **Authors**: Davide Poggiali, Diego Cecchin, Cristina Campi, Stefano De Marchi
- **Comment**: None
- **Journal**: None
- **Summary**: To analyse multimodal 3-dimensional medical images, interpolation is required for resampling which - unavoidably - introduces an interpolation error. In this work we consider three segmented 3-dimensional images resampled with three different neuroimaging software tools for comparing undersampling and oversampling strategies and to identify where the oversampling error lies. The results indicate that undersampling to the lowest image size is advantageous in terms of mean value per segment errors and that the oversampling error is larger where the gradient is steeper, showing a Gibbs effect.



### SDD-FIQA: Unsupervised Face Image Quality Assessment with Similarity Distribution Distance
- **Arxiv ID**: http://arxiv.org/abs/2103.05977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05977v1)
- **Published**: 2021-03-10 10:23:28+00:00
- **Updated**: 2021-03-10 10:23:28+00:00
- **Authors**: Fu-Zhao Ou, Xingyu Chen, Ruixin Zhang, Yuge Huang, Shaoxin Li, Jilin Li, Yong Li, Liujuan Cao, Yuan-Gen Wang
- **Comment**: None
- **Journal**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), 2021
- **Summary**: In recent years, Face Image Quality Assessment (FIQA) has become an indispensable part of the face recognition system to guarantee the stability and reliability of recognition performance in an unconstrained scenario. For this purpose, the FIQA method should consider both the intrinsic property and the recognizability of the face image. Most previous works aim to estimate the sample-wise embedding uncertainty or pair-wise similarity as the quality score, which only considers the information from partial intra-class. However, these methods ignore the valuable information from the inter-class, which is for estimating to the recognizability of face image. In this work, we argue that a high-quality face image should be similar to its intra-class samples and dissimilar to its inter-class samples. Thus, we propose a novel unsupervised FIQA method that incorporates Similarity Distribution Distance for Face Image Quality Assessment (SDD-FIQA). Our method generates quality pseudo-labels by calculating the Wasserstein Distance (WD) between the intra-class similarity distributions and inter-class similarity distributions. With these quality pseudo-labels, we are capable of training a regression network for quality prediction. Extensive experiments on benchmark datasets demonstrate that the proposed SDD-FIQA surpasses the state-of-the-arts by an impressive margin. Meanwhile, our method shows good generalization across different recognition systems.



### Reformulating HOI Detection as Adaptive Set Prediction
- **Arxiv ID**: http://arxiv.org/abs/2103.05983v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05983v2)
- **Published**: 2021-03-10 10:40:33+00:00
- **Updated**: 2021-05-06 02:31:55+00:00
- **Authors**: Mingfei Chen, Yue Liao, Si Liu, Zhiyuan Chen, Fei Wang, Chen Qian
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Determining which image regions to concentrate on is critical for Human-Object Interaction (HOI) detection. Conventional HOI detectors focus on either detected human and object pairs or pre-defined interaction locations, which limits learning of the effective features. In this paper, we reformulate HOI detection as an adaptive set prediction problem, with this novel formulation, we propose an Adaptive Set-based one-stage framework (AS-Net) with parallel instances and interaction branches. To attain this, we map a trainable interaction query set to an interaction prediction set with a transformer. Each query adaptively aggregates the interaction-relevant features from global contexts through multi-head co-attention. Besides, the training process is supervised adaptively by matching each ground truth with the interaction prediction. Furthermore, we design an effective instance-aware attention module to introduce instructive features from the instance branch into the interaction branch. Our method outperforms previous state-of-the-art methods without any extra human pose and language features on three challenging HOI detection datasets. Especially, we achieve over $31\%$ relative improvement on a large-scale HICO-DET dataset. Code is available at https://github.com/yoyomimi/AS-Net.



### Multi-Pretext Attention Network for Few-shot Learning with Self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2103.05985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05985v1)
- **Published**: 2021-03-10 10:48:37+00:00
- **Updated**: 2021-03-10 10:48:37+00:00
- **Authors**: Hainan Li, Renshuai Tao, Jun Li, Haotong Qin, Yifu Ding, Shuo Wang, Xianglong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning is an interesting and challenging study, which enables machines to learn from few samples like humans. Existing studies rarely exploit auxiliary information from large amount of unlabeled data. Self-supervised learning is emerged as an efficient method to utilize unlabeled data. Existing self-supervised learning methods always rely on the combination of geometric transformations for the single sample by augmentation, while seriously neglect the endogenous correlation information among different samples that is the same important for the task. In this work, we propose a Graph-driven Clustering (GC), a novel augmentation-free method for self-supervised learning, which does not rely on any auxiliary sample and utilizes the endogenous correlation information among input samples. Besides, we propose Multi-pretext Attention Network (MAN), which exploits a specific attention mechanism to combine the traditional augmentation-relied methods and our GC, adaptively learning their optimized weights to improve the performance and enabling the feature extractor to obtain more universal representations. We evaluate our MAN extensively on miniImageNet and tieredImageNet datasets and the results demonstrate that the proposed method outperforms the state-of-the-art (SOTA) relevant methods.



### Wide Aspect Ratio Matching for Robust Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.05993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05993v1)
- **Published**: 2021-03-10 11:05:38+00:00
- **Updated**: 2021-03-10 11:05:38+00:00
- **Authors**: Shi Luo, Xiongfei Li, Xiaoli Zhang
- **Comment**: 18 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Recently, anchor-based methods have achieved great progress in face detection. Once anchor design and anchor matching strategy determined, plenty of positive anchors will be sampled. However, faces with extreme aspect ratio always fail to be sampled according to standard anchor matching strategy. In fact, the max IoUs between anchors and extreme aspect ratio faces are still lower than fixed sampling threshold. In this paper, we firstly explore the factors that affect the max IoU of each face in theory. Then, anchor matching simulation is performed to evaluate the sampling range of face aspect ratio. Besides, we propose a Wide Aspect Ratio Matching (WARM) strategy to collect more representative positive anchors from ground-truth faces across a wide range of aspect ratio. Finally, we present a novel feature enhancement module, named Receptive Field Diversity (RFD) module, to provide diverse receptive field corresponding to different aspect ratios. Extensive experiments show that our method can help detectors better capture extreme aspect ratio faces and achieve promising detection performance on challenging face detection benchmarks, including WIDER FACE and FDDB datasets.



### Quality-Aware Network for Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/2103.05997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05997v1)
- **Published**: 2021-03-10 11:17:40+00:00
- **Updated**: 2021-03-10 11:17:40+00:00
- **Authors**: Lu Yang, Qing Song, Zhihui Wang, Zhiwei Liu, Songcen Xu, Zhihao Li
- **Comment**: None
- **Journal**: None
- **Summary**: How to estimate the quality of the network output is an important issue, and currently there is no effective solution in the field of human parsing. In order to solve this problem, this work proposes a statistical method based on the output probability map to calculate the pixel quality information, which is called pixel score. In addition, the Quality-Aware Module (QAM) is proposed to fuse the different quality information, the purpose of which is to estimate the quality of human parsing results. We combine QAM with a concise and effective network design to propose Quality-Aware Network (QANet) for human parsing. Benefiting from the superiority of QAM and QANet, we achieve the best performance on three multiple and one single human parsing benchmarks, including CIHP, MHP-v2, Pascal-Person-Part and LIP. Without increasing the training and inference time, QAM improves the AP$^\text{r}$ criterion by more than 10 points in the multiple human parsing task. QAM can be extended to other tasks with good quality estimation, e.g. instance segmentation. Specifically, QAM improves Mask R-CNN by ~1% mAP on COCO and LVISv1.0 datasets. Based on the proposed QAM and QANet, our overall system wins 1st place in CVPR2019 COCO DensePose Challenge, and 1st place in Track 1 & 2 of CVPR2020 LIP Challenge. Code and models are available at https://github.com/soeaver/QANet.



### DSEC: A Stereo Event Camera Dataset for Driving Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2103.06011v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.06011v1)
- **Published**: 2021-03-10 12:10:33+00:00
- **Updated**: 2021-03-10 12:10:33+00:00
- **Authors**: Mathias Gehrig, Willem Aarents, Daniel Gehrig, Davide Scaramuzza
- **Comment**: IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, the operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high-resolution, large-scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms.



### Principal component-based image segmentation: a new approach to outline in vitro cell colonies
- **Arxiv ID**: http://arxiv.org/abs/2103.06022v1
- **DOI**: 10.1080/21681163.2022.2035822
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06022v1)
- **Published**: 2021-03-10 12:37:51+00:00
- **Updated**: 2021-03-10 12:37:51+00:00
- **Authors**: Delmon Arous, Stefan Schrunner, Ingunn Hanson, Nina F. J. Edin, Eirik Malinen
- **Comment**: None
- **Journal**: Computer Methods in Biomechanics and Biomedical Engineering:
  Imaging & Visualization (2022)
- **Summary**: The in vitro clonogenic assay is a technique to study the ability of a cell to form a colony in a culture dish. By optical imaging, dishes with stained colonies can be scanned and assessed digitally. Identification, segmentation and counting of stained colonies play a vital part in high-throughput screening and quantitative assessment of biological assays. Image processing of such pictured/scanned assays can be affected by image/scan acquisition artifacts like background noise and spatially varying illumination, and contaminants in the suspension medium. Although existing approaches tackle these issues, the segmentation quality requires further improvement, particularly on noisy and low contrast images. In this work, we present an objective and versatile machine learning procedure to amend these issues by characterizing, extracting and segmenting inquired colonies using principal component analysis, k-means clustering and a modified watershed segmentation algorithm. The intention is to automatically identify visible colonies through spatial texture assessment and accordingly discriminate them from background in preparation for successive segmentation. The proposed segmentation algorithm yielded a similar quality as manual counting by human observers. High F1 scores (>0.9) and low root-mean-square errors (around 14%) underlined good agreement with ground truth data. Moreover, it outperformed a recent state-of-the-art method. The methodology will be an important tool in future cancer research applications.



### Model-free Vehicle Tracking and State Estimation in Point Cloud Sequences
- **Arxiv ID**: http://arxiv.org/abs/2103.06028v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.06028v2)
- **Published**: 2021-03-10 13:01:26+00:00
- **Updated**: 2021-08-05 12:24:18+00:00
- **Authors**: Ziqi Pang, Zhichao Li, Naiyan Wang
- **Comment**: Accepted by IROS2021, Camera ready version
- **Journal**: None
- **Summary**: Estimating the states of surrounding traffic participants stays at the core of autonomous driving. In this paper, we study a novel setting of this problem: model-free single-object tracking (SOT), which takes the object state in the first frame as input, and jointly solves state estimation and tracking in subsequent frames. The main purpose for this new setting is to break the strong limitation of the popular "detection and tracking" scheme in multi-object tracking. Moreover, we notice that shape completion by overlaying the point clouds, which is a by-product of our proposed task, not only improves the performance of state estimation but also has numerous applications. As no benchmark for this task is available so far, we construct a new dataset LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open dataset. We then propose an optimization-based algorithm called SOTracker involving point cloud registration, vehicle shapes, correspondence, and motion priors. Our quantitative and qualitative results prove the effectiveness of our SOTracker and reveal the challenging cases for SOT in point clouds, including the sparsity of LiDAR data, abrupt motion variation, etc. Finally, we also explore how the proposed task and algorithm may benefit other autonomous driving applications, including simulating LiDAR scans, generating motion data, and annotating optical flow. The code and protocols for our benchmark and algorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video demonstration is at https://www.youtube.com/watch?v=BpHixKs91i8.



### FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space
- **Arxiv ID**: http://arxiv.org/abs/2103.06030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06030v1)
- **Published**: 2021-03-10 13:05:23+00:00
- **Updated**: 2021-03-10 13:05:23+00:00
- **Authors**: Quande Liu, Cheng Chen, Jing Qin, Qi Dou, Pheng-Ann Heng
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Federated learning allows distributed medical institutions to collaboratively learn a shared prediction model with privacy protection. While at clinical deployment, the models trained in federated learning can still suffer from performance drop when applied to completely unseen hospitals outside the federation. In this paper, we point out and solve a novel problem setting of federated domain generalization (FedDG), which aims to learn a federated model from multiple distributed source domains such that it can directly generalize to unseen target domains. We present a novel approach, named as Episodic Learning in Continuous Frequency Space (ELCFS), for this problem by enabling each client to exploit multi-source data distributions under the challenging constraint of data decentralization. Our approach transmits the distribution information across clients in a privacy-protecting way through an effective continuous frequency space interpolation mechanism. With the transferred multi-source distributions, we further carefully design a boundary-oriented episodic learning paradigm to expose the local learning to domain distribution shifts and particularly meet the challenges of model generalization in medical image segmentation scenario. The effectiveness of our method is demonstrated with superior performance over state-of-the-arts and in-depth ablation experiments on two medical image segmentation tasks. The code is available at "https://github.com/liuquande/FedDG-ELCFS".



### Deep Superpixel Cut for Unsupervised Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.06031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06031v1)
- **Published**: 2021-03-10 13:07:41+00:00
- **Updated**: 2021-03-10 13:07:41+00:00
- **Authors**: Qinghong Lin, Weichan Zhong, Jianglin Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation, one of the most critical vision tasks, has been studied for many years. Most of the early algorithms are unsupervised methods, which use hand-crafted features to divide the image into many regions. Recently, owing to the great success of deep learning technology, CNNs based methods show superior performance in image segmentation. However, these methods rely on a large number of human annotations, which are expensive to collect. In this paper, we propose a deep unsupervised method for image segmentation, which contains the following two stages. First, a Superpixelwise Autoencoder (SuperAE) is designed to learn the deep embedding and reconstruct a smoothed image, then the smoothed image is passed to generate superpixels. Second, we present a novel clustering algorithm called Deep Superpixel Cut (DSC), which measures the deep similarity between superpixels and formulates image segmentation as a soft partitioning problem. Via backpropagation, DSC adaptively partitions the superpixels into perceptual regions. Experimental results on the BSDS500 dataset demonstrate the effectiveness of the proposed method.



### Cross-modal Image Retrieval with Deep Mutual Information Maximization
- **Arxiv ID**: http://arxiv.org/abs/2103.06032v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.06032v1)
- **Published**: 2021-03-10 13:08:09+00:00
- **Updated**: 2021-03-10 13:08:09+00:00
- **Authors**: Chunbin Gu, Jiajun Bu, Xixi Zhou, Chengwei Yao, Dongfang Ma, Zhi Yu, Xifeng Yan
- **Comment**: 35 pages,7 figures, Submitted to Neuralcomputing
- **Journal**: None
- **Summary**: In this paper, we study the cross-modal image retrieval, where the inputs contain a source image plus some text that describes certain modifications to this image and the desired image. Prior work usually uses a three-stage strategy to tackle this task: 1) extract the features of the inputs; 2) fuse the feature of the source image and its modified text to obtain fusion feature; 3) learn a similarity metric between the desired image and the source image + modified text by using deep metric learning. Since classical image/text encoders can learn the useful representation and common pair-based loss functions of distance metric learning are enough for cross-modal retrieval, people usually improve retrieval accuracy by designing new fusion networks. However, these methods do not successfully handle the modality gap caused by the inconsistent distribution and representation of the features of different modalities, which greatly influences the feature fusion and similarity learning. To alleviate this problem, we adopt the contrastive self-supervised learning method Deep InforMax (DIM) to our approach to bridge this gap by enhancing the dependence between the text, the image, and their fusion. Specifically, our method narrows the modality gap between the text modality and the image modality by maximizing mutual information between their not exactly semantically identical representation. Moreover, we seek an effective common subspace for the semantically same fusion feature and desired image's feature by utilizing Deep InforMax between the low-level layer of the image encoder and the high-level layer of the fusion network. Extensive experiments on three large-scale benchmark datasets show that we have bridged the modality gap between different modalities and achieve state-of-the-art retrieval performance.



### U-Net Transformer: Self and Cross Attention for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.06104v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06104v2)
- **Published**: 2021-03-10 14:58:31+00:00
- **Updated**: 2021-03-12 15:25:47+00:00
- **Authors**: Olivier Petit, Nicolas Thome, Clment Rambour, Luc Soler
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation remains particularly challenging for complex and low-contrast anatomical structures. In this paper, we introduce the U-Transformer network, which combines a U-shaped architecture for image segmentation with self- and cross-attention from Transformers. U-Transformer overcomes the inability of U-Nets to model long-range contextual interactions and spatial dependencies, which are arguably crucial for accurate segmentation in challenging contexts. To this end, attention mechanisms are incorporated at two main levels: a self-attention module leverages global interactions between encoder features, while cross-attention in the skip connections allows a fine spatial recovery in the U-Net decoder by filtering out non-semantic features. Experiments on two abdominal CT-image datasets show the large performance gain brought out by U-Transformer compared to U-Net and local Attention U-Nets. We also highlight the importance of using both self- and cross-attention, and the nice interpretability features brought out by U-Transformer.



### Time-Ordered Recent Event (TORE) Volumes for Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/2103.06108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06108v1)
- **Published**: 2021-03-10 15:03:38+00:00
- **Updated**: 2021-03-10 15:03:38+00:00
- **Authors**: R. Wes Baldwin, Ruixu Liu, Mohammed Almatrafi, Vijayan Asari, Keigo Hirakawa
- **Comment**: None
- **Journal**: None
- **Summary**: Event cameras are an exciting, new sensor modality enabling high-speed imaging with extremely low-latency and wide dynamic range. Unfortunately, most machine learning architectures are not designed to directly handle sparse data, like that generated from event cameras. Many state-of-the-art algorithms for event cameras rely on interpolated event representations - obscuring crucial timing information, increasing the data volume, and limiting overall network performance. This paper details an event representation called Time-Ordered Recent Event (TORE) volumes. TORE volumes are designed to compactly store raw spike timing information with minimal information loss. This bio-inspired design is memory efficient, computationally fast, avoids time-blocking (i.e. fixed and predefined frame rates), and contains "local memory" from past data. The design is evaluated on a wide range of challenging tasks (e.g. event denoising, image reconstruction, classification, and human pose estimation) and is shown to dramatically improve state-of-the-art performance. TORE volumes are an easy-to-implement replacement for any algorithm currently utilizing event representations.



### Symmetry meets AI
- **Arxiv ID**: http://arxiv.org/abs/2103.06115v2
- **DOI**: 10.21468/SciPostPhys.11.1.014
- **Categories**: **cs.LG**, cs.CV, hep-ph
- **Links**: [PDF](http://arxiv.org/pdf/2103.06115v2)
- **Published**: 2021-03-10 15:12:49+00:00
- **Updated**: 2021-06-29 11:47:38+00:00
- **Authors**: Gabriela Barenboim, Johannes Hirn, Veronica Sanz
- **Comment**: 8 pages, 8 figures
- **Journal**: SciPost Phys. 11, 014 (2021)
- **Summary**: We explore whether Neural Networks (NNs) can {\it discover} the presence of symmetries as they learn to perform a task. For this, we train hundreds of NNs on a {\it decoy task} based on well-controlled Physics templates, where no information on symmetry is provided. We use the output from the last hidden layer of all these NNs, projected to fewer dimensions, as the input for a symmetry classification task, and show that information on symmetry had indeed been identified by the original NN without guidance. As an interdisciplinary application of this procedure, we identify the presence and level of symmetry in artistic paintings from different styles such as those of Picasso, Pollock and Van Gogh.



### Spatial Attention-based Non-reference Perceptual Quality Prediction Network for Omnidirectional Images
- **Arxiv ID**: http://arxiv.org/abs/2103.06116v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.06116v1)
- **Published**: 2021-03-10 15:14:37+00:00
- **Updated**: 2021-03-10 15:14:37+00:00
- **Authors**: Li Yang, Mai Xu, Deng Xin, Bo Feng
- **Comment**: Accepted by IEEE ICME 2021
- **Journal**: None
- **Summary**: Due to the strong correlation between visual attention and perceptual quality, many methods attempt to use human saliency information for image quality assessment. Although this mechanism can get good performance, the networks require human saliency labels, which is not easily accessible for omnidirectional images (ODI). To alleviate this issue, we propose a spatial attention-based perceptual quality prediction network for non-reference quality assessment on ODIs (SAP-net). To drive our SAP-net, we establish a large-scale IQA dataset of ODIs (IQA-ODI), which is composed of subjective scores of 200 subjects on 1,080 ODIs. In IQA-ODI, there are 120 high quality ODIs as reference, and 960 ODIs with impairments in both JPEG compression and map projection. Without any human saliency labels, our network can adaptively estimate human perceptual quality on impaired ODIs through a self-attention manner, which significantly promotes the prediction performance of quality scores. Moreover, our method greatly reduces the computational complexity in quality assessment task on ODIs. Extensive experiments validate that our network outperforms 9 state-of-the-art methods for quality assessment on ODIs. The dataset and code have been available on \url{ https://github.com/yanglixiaoshen/SAP-Net}.



### Spatially Consistent Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.06122v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06122v2)
- **Published**: 2021-03-10 15:23:45+00:00
- **Updated**: 2021-04-28 09:50:07+00:00
- **Authors**: Byungseok Roh, Wuhyun Shin, Ildoo Kim, Sungwoong Kim
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Self-supervised learning has been widely used to obtain transferrable representations from unlabeled images. Especially, recent contrastive learning methods have shown impressive performances on downstream image classification tasks. While these contrastive methods mainly focus on generating invariant global representations at the image-level under semantic-preserving transformations, they are prone to overlook spatial consistency of local representations and therefore have a limitation in pretraining for localization tasks such as object detection and instance segmentation. Moreover, aggressively cropped views used in existing contrastive methods can minimize representation distances between the semantically different regions of a single image.   In this paper, we propose a spatially consistent representation learning algorithm (SCRL) for multi-object and location-specific tasks. In particular, we devise a novel self-supervised objective that tries to produce coherent spatial representations of a randomly cropped local region according to geometric translations and zooming operations. On various downstream localization tasks with benchmark datasets, the proposed SCRL shows significant performance improvements over the image-level supervised pretraining as well as the state-of-the-art self-supervised learning methods.   Code is available at https://github.com/kakaobrain/scrl



### MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks
- **Arxiv ID**: http://arxiv.org/abs/2103.06132v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06132v3)
- **Published**: 2021-03-10 15:31:02+00:00
- **Updated**: 2021-08-24 11:11:06+00:00
- **Authors**: Alexandre Rame, Remy Sun, Matthieu Cord
- **Comment**: 8 pages, 10 figures, 6 tables
- **Journal**: None
- **Summary**: Recent strategies achieved ensembling "for free" by fitting concurrently diverse subnetworks inside a single base network. The main idea during training is that each subnetwork learns to classify only one of the multiple inputs simultaneously provided. However, the question of how to best mix these multiple inputs has not been studied so far. In this paper, we introduce MixMo, a new generalized framework for learning multi-input multi-output deep subnetworks. Our key motivation is to replace the suboptimal summing operation hidden in previous approaches by a more appropriate mixing mechanism. For that purpose, we draw inspiration from successful mixed sample data augmentations. We show that binary mixing in features - particularly with rectangular patches from CutMix - enhances results by making subnetworks stronger and more diverse. We improve state of the art for image classification on CIFAR-100 and Tiny ImageNet datasets. Our easy to implement models notably outperform data augmented deep ensembles, without the inference and memory overheads. As we operate in features and simply better leverage the expressiveness of large networks, we open a new line of research complementary to previous works.



### Sim2Real 3D Object Classification using Spherical Kernel Point Convolution and a Deep Center Voting Scheme
- **Arxiv ID**: http://arxiv.org/abs/2103.06134v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.06134v1)
- **Published**: 2021-03-10 15:32:04+00:00
- **Updated**: 2021-03-10 15:32:04+00:00
- **Authors**: Jean-Baptiste Weibel, Timothy Patten, Markus Vincze
- **Comment**: None
- **Journal**: None
- **Summary**: While object semantic understanding is essential for most service robotic tasks, 3D object classification is still an open problem. Learning from artificial 3D models alleviates the cost of annotation necessary to approach this problem, but most methods still struggle with the differences existing between artificial and real 3D data. We conjecture that the cause of those issue is the fact that many methods learn directly from point coordinates, instead of the shape, as the former is hard to center and to scale under variable occlusions reliably. We introduce spherical kernel point convolutions that directly exploit the object surface, represented as a graph, and a voting scheme to limit the impact of poor segmentation on the classification results. Our proposed approach improves upon state-of-the-art methods by up to 36% when transferring from artificial objects to real objects.



### Adversarial Regression Learning for Bone Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.06149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06149v1)
- **Published**: 2021-03-10 15:58:26+00:00
- **Updated**: 2021-03-10 15:58:26+00:00
- **Authors**: Youshan Zhang, Brian D. Davison
- **Comment**: 27th Information Processing in Medical Imaging (IPMI)
- **Journal**: None
- **Summary**: Estimation of bone age from hand radiographs is essential to determine skeletal age in diagnosing endocrine disorders and depicting the growth status of children. However, existing automatic methods only apply their models to test images without considering the discrepancy between training samples and test samples, which will lead to a lower generalization ability. In this paper, we propose an adversarial regression learning network (ARLNet) for bone age estimation. Specifically, we first extract bone features from a fine-tuned Inception V3 neural network and propose regression percentage loss for training. To reduce the discrepancy between training and test data, we then propose adversarial regression loss and feature reconstruction loss to guarantee the transition from training data to test data and vice versa, preserving invariant features from both training and test data. Experimental results show that the proposed model outperforms state-of-the-art methods.



### Model-inspired Deep Learning for Light-Field Microscopy with Application to Neuron Localization
- **Arxiv ID**: http://arxiv.org/abs/2103.06164v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06164v1)
- **Published**: 2021-03-10 16:24:47+00:00
- **Updated**: 2021-03-10 16:24:47+00:00
- **Authors**: Pingfan Song, Herman Verinaz Jadan, Carmel L. Howe, Peter Quicke, Amanda J. Foust, Pier Luigi Dragotti
- **Comment**: 5 pages, 6 figures, ICASSP 2021
- **Journal**: None
- **Summary**: Light-field microscopes are able to capture spatial and angular information of incident light rays. This allows reconstructing 3D locations of neurons from a single snap-shot.In this work, we propose a model-inspired deep learning approach to perform fast and robust 3D localization of sources using light-field microscopy images. This is achieved by developing a deep network that efficiently solves a convolutional sparse coding (CSC) problem to map Epipolar Plane Images (EPI) to corresponding sparse codes. The network architecture is designed systematically by unrolling the convolutional Iterative Shrinkage and Thresholding Algorithm (ISTA) while the network parameters are learned from a training dataset. Such principled design enables the deep network to leverage both domain knowledge implied in the model, as well as new parameters learned from the data, thereby combining advantages of model-based and learning-based methods. Practical experiments on localization of mammalian neurons from light-fields show that the proposed approach simultaneously provides enhanced performance, interpretability and efficiency.



### Towards automated brain aneurysm detection in TOF-MRA: open data, weak labels, and anatomical knowledge
- **Arxiv ID**: http://arxiv.org/abs/2103.06168v6
- **DOI**: 10.1007/s12021-022-09597-0
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06168v6)
- **Published**: 2021-03-10 16:31:54+00:00
- **Updated**: 2022-08-23 15:29:03+00:00
- **Authors**: Tommaso Di Noto, Guillaume Marie, Sebastien Tourbier, Yasser Alemn-Gmez, Oscar Esteban, Guillaume Saliou, Meritxell Bach Cuadra, Patric Hagmann, Jonas Richiardi
- **Comment**: Paper accepted as Original Article in the journal Neuroinformatics
  (https://link.springer.com/article/10.1007/s12021-022-09597-0)
- **Journal**: Neuroinformatics, 2022
- **Summary**: Brain aneurysm detection in Time-Of-Flight Magnetic Resonance Angiography (TOF-MRA) has undergone drastic improvements with the advent of Deep Learning (DL). However, performances of supervised DL models heavily rely on the quantity of labeled samples, which are extremely costly to obtain. Here, we present a DL model for aneurysm detection that overcomes the issue with ''weak'' labels: oversized annotations which are considerably faster to create. Our weak labels resulted to be four times faster to generate than their voxel-wise counterparts. In addition, our model leverages prior anatomical knowledge by focusing only on plausible locations for aneurysm occurrence. We frst train and evaluate our model through cross-validation on an in-house TOF-MRA dataset comprising 284 subjects (170 females / 127 healthy controls / 157 patients with 198 aneurysms). On this dataset, our best model achieved a sensitivity of 83%, with False Positive (FP) rate of 0.8 per patient. To assess model generalizability, we then participated in a challenge for aneurysm detection with TOF-MRA data (93 patients, 20 controls, 125 aneurysms). On the public challenge, sensitivity was 68% (FP rate=2.5), ranking 4th/18 on the open leaderboard. We found no signifcant diference in sensitivity between aneurysm risk-of-rupture groups (p=0.75), locations (p=0.72), or sizes (p=0.15). Data, code and model weights are released under permissive licenses. We demonstrate that weak labels and anatomical knowledge can alleviate the necessity for prohibitively expensive voxel-wise annotations.



### Regressive Domain Adaptation for Unsupervised Keypoint Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.06175v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06175v2)
- **Published**: 2021-03-10 16:45:22+00:00
- **Updated**: 2021-06-04 07:51:21+00:00
- **Authors**: Junguang Jiang, Yifei Ji, Ximei Wang, Yufeng Liu, Jianmin Wang, Mingsheng Long
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Domain adaptation (DA) aims at transferring knowledge from a labeled source domain to an unlabeled target domain. Though many DA theories and algorithms have been proposed, most of them are tailored into classification settings and may fail in regression tasks, especially in the practical keypoint detection task. To tackle this difficult but significant task, we present a method of regressive domain adaptation (RegDA) for unsupervised keypoint detection. Inspired by the latest theoretical work, we first utilize an adversarial regressor to maximize the disparity on the target domain and train a feature generator to minimize this disparity. However, due to the high dimension of the output space, this regressor fails to detect samples that deviate from the support of the source. To overcome this problem, we propose two important ideas. First, based on our observation that the probability density of the output space is sparse, we introduce a spatial probability distribution to describe this sparsity and then use it to guide the learning of the adversarial regressor. Second, to alleviate the optimization difficulty in the high-dimensional space, we innovatively convert the minimax game in the adversarial training to the minimization of two opposite goals. Extensive experiments show that our method brings large improvement by 8% to 11% in terms of PCK on different datasets.



### Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing
- **Arxiv ID**: http://arxiv.org/abs/2103.06179v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.06179v1)
- **Published**: 2021-03-10 16:50:42+00:00
- **Updated**: 2021-03-10 16:50:42+00:00
- **Authors**: Christian Reimers, Paul Bodesheim, Jakob Runge, Joachim Denzler
- **Comment**: None
- **Journal**: None
- **Summary**: Bias in classifiers is a severe issue of modern deep learning methods, especially for their application in safety- and security-critical areas. Often, the bias of a classifier is a direct consequence of a bias in the training dataset, frequently caused by the co-occurrence of relevant features and irrelevant ones. To mitigate this issue, we require learning algorithms that prevent the propagation of bias from the dataset into the classifier. We present a novel adversarial debiasing method, which addresses a feature that is spuriously connected to the labels of training images but statistically independent of the labels for test images. Thus, the automatic identification of relevant features during training is perturbed by irrelevant features. This is the case in a wide range of bias-related problems for many computer vision tasks, such as automatic skin cancer detection or driver assistance. We argue by a mathematical proof that our approach is superior to existing techniques for the abovementioned bias. Our experiments show that our approach performs better than state-of-the-art techniques on a well-known benchmark dataset with real-world images of cats and dogs.



### Dynamical Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.06182v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, math.DS
- **Links**: [PDF](http://arxiv.org/pdf/2103.06182v3)
- **Published**: 2021-03-10 17:01:41+00:00
- **Updated**: 2021-08-12 03:08:15+00:00
- **Authors**: Heng Yang, Chris Doran, Jean-Jacques Slotine
- **Comment**: ICCV 2021 camera ready. Code: https://github.com/hankyang94/DAMP.
  Video: https://youtu.be/CDYXR1h98Q4
- **Journal**: ICCV 2021
- **Summary**: We study the problem of aligning two sets of 3D geometric primitives given known correspondences. Our first contribution is to show that this primitive alignment framework unifies five perception problems including point cloud registration, primitive (mesh) registration, category-level 3D registration, absolution pose estimation (APE), and category-level APE. Our second contribution is to propose DynAMical Pose estimation (DAMP), the first general and practical algorithm to solve primitive alignment problem by simulating rigid body dynamics arising from virtual springs and damping, where the springs span the shortest distances between corresponding primitives. We evaluate DAMP in simulated and real datasets across all five problems, and demonstrate (i) DAMP always converges to the globally optimal solution in the first three problems with 3D-3D correspondences; (ii) although DAMP sometimes converges to suboptimal solutions in the last two problems with 2D-3D correspondences, using a scheme for escaping local minima, DAMP always succeeds. Our third contribution is to demystify the surprising empirical performance of DAMP and formally prove a global convergence result in the case of point cloud registration by charactering local stability of the equilibrium points of the underlying dynamical system.



### A Study of Face Obfuscation in ImageNet
- **Arxiv ID**: http://arxiv.org/abs/2103.06191v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06191v3)
- **Published**: 2021-03-10 17:11:34+00:00
- **Updated**: 2022-06-09 17:30:55+00:00
- **Authors**: Kaiyu Yang, Jacqueline Yau, Li Fei-Fei, Jia Deng, Olga Russakovsky
- **Comment**: Accepted to ICML 2022
- **Journal**: None
- **Summary**: Face obfuscation (blurring, mosaicing, etc.) has been shown to be effective for privacy protection; nevertheless, object recognition research typically assumes access to complete, unobfuscated images. In this paper, we explore the effects of face obfuscation on the popular ImageNet challenge visual recognition benchmark. Most categories in the ImageNet challenge are not people categories; however, many incidental people appear in the images, and their privacy is a concern. We first annotate faces in the dataset. Then we demonstrate that face obfuscation has minimal impact on the accuracy of recognition models. Concretely, we benchmark multiple deep neural networks on obfuscated images and observe that the overall recognition accuracy drops only slightly (<= 1.0%). Further, we experiment with transfer learning to 4 downstream tasks (object recognition, scene recognition, face attribute classification, and object detection) and show that features learned on obfuscated images are equally transferable. Our work demonstrates the feasibility of privacy-aware visual recognition, improves the highly-used ImageNet challenge benchmark, and suggests an important path for future visual datasets. Data and code are available at https://github.com/princetonvisualai/imagenet-face-obfuscation.



### Are we using appropriate segmentation metrics? Identifying correlates of human expert perception for CNN training beyond rolling the DICE coefficient
- **Arxiv ID**: http://arxiv.org/abs/2103.06205v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06205v4)
- **Published**: 2021-03-10 17:29:11+00:00
- **Updated**: 2023-05-02 13:42:03+00:00
- **Authors**: Florian Kofler, Ivan Ezhov, Fabian Isensee, Fabian Balsiger, Christoph Berger, Maximilian Koerner, Beatrice Demiray, Julia Rackerseder, Johannes Paetzold, Hongwei Li, Suprosanna Shit, Richard McKinley, Marie Piraud, Spyridon Bakas, Claus Zimmer, Nassir Navab, Jan Kirschke, Benedikt Wiestler, Bjoern Menze
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2023:002
- **Journal**: Machine.Learning.for.Biomedical.Imaging. 2 (2023)
- **Summary**: Metrics optimized in complex machine learning tasks are often selected in an ad-hoc manner. It is unknown how they align with human expert perception. We explore the correlations between established quantitative segmentation quality metrics and qualitative evaluations by professionally trained human raters. Therefore, we conduct psychophysical experiments for two complex biomedical semantic segmentation problems. We discover that current standard metrics and loss functions correlate only moderately with the segmentation quality assessment of experts. Importantly, this effect is particularly pronounced for clinically relevant structures, such as the enhancing tumor compartment of glioma in brain magnetic resonance and grey matter in ultrasound imaging. It is often unclear how to optimize abstract metrics, such as human expert perception, in convolutional neural network (CNN) training. To cope with this challenge, we propose a novel strategy employing techniques of classical statistics to create complementary compound loss functions to better approximate human expert perception. Across all rating experiments, human experts consistently scored computer-generated segmentations better than the human-curated reference labels. Our results, therefore, strongly question many current practices in medical image segmentation and provide meaningful cues for future research.



### A Relational-learning Perspective to Multi-label Chest X-ray Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.06220v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06220v1)
- **Published**: 2021-03-10 17:44:59+00:00
- **Updated**: 2021-03-10 17:44:59+00:00
- **Authors**: Anjany Sekuboyina, Daniel Ooro-Rubio, Jens Kleesiek, Brandon Malone
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label classification of chest X-ray images is frequently performed using discriminative approaches, i.e. learning to map an image directly to its binary labels. Such approaches make it challenging to incorporate auxiliary information such as annotation uncertainty or a dependency among the labels. Building towards this, we propose a novel knowledge graph reformulation of multi-label classification, which not only readily increases predictive performance of an encoder but also serves as a general framework for introducing new domain knowledge.   Specifically, we construct a multi-modal knowledge graph out of the chest X-ray images and its labels and pose multi-label classification as a link prediction problem. Incorporating auxiliary information can then simply be achieved by adding additional nodes and relations among them. When tested on a publicly-available radiograph dataset (CheXpert), our relational-reformulation using a naive knowledge graph outperforms the state-of-art by achieving an area-under-ROC curve of 83.5%, an improvement of "sim 1" over a purely discriminative approach.



### Quantization-Guided Training for Compact TinyML Models
- **Arxiv ID**: http://arxiv.org/abs/2103.06231v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06231v1)
- **Published**: 2021-03-10 18:06:05+00:00
- **Updated**: 2021-03-10 18:06:05+00:00
- **Authors**: Sedigh Ghamari, Koray Ozcan, Thu Dinh, Andrey Melnikov, Juan Carvajal, Jan Ernst, Sek Chai
- **Comment**: TinyML Summit, March 2021
- **Journal**: None
- **Summary**: We propose a Quantization Guided Training (QGT) method to guide DNN training towards optimized low-bit-precision targets and reach extreme compression levels below 8-bit precision. Unlike standard quantization-aware training (QAT) approaches, QGT uses customized regularization to encourage weight values towards a distribution that maximizes accuracy while reducing quantization errors. One of the main benefits of this approach is the ability to identify compression bottlenecks. We validate QGT using state-of-the-art model architectures on vision datasets. We also demonstrate the effectiveness of QGT with an 81KB tiny model for person detection down to 2-bit precision (representing 17.7x size reduction), while maintaining an accuracy drop of only 3% compared to a floating-point baseline.



### Involution: Inverting the Inherence of Convolution for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.06255v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06255v2)
- **Published**: 2021-03-10 18:40:46+00:00
- **Updated**: 2021-04-11 12:30:11+00:00
- **Authors**: Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei Zhu, Tong Zhang, Qifeng Chen
- **Comment**: Accepted to CVPR 2021. Code and models are available at
  https://github.com/d-li14/involution
- **Journal**: None
- **Summary**: Convolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at https://github.com/d-li14/involution.



### A registration error estimation framework for correlative imaging
- **Arxiv ID**: http://arxiv.org/abs/2103.06256v1
- **DOI**: 10.1109/ICIP42928.2021.9506474
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06256v1)
- **Published**: 2021-03-10 18:43:18+00:00
- **Updated**: 2021-03-10 18:43:18+00:00
- **Authors**: Guillaume Potier, Frdric Lavancier, Stephan Kunne, Perrine Paul-Gilloteaux
- **Comment**: 10 pages 2 figures (made of 10 panels in total)
- **Journal**: None
- **Summary**: Correlative imaging workflows are now widely used in bioimaging and aims to image the same sample using at least two different and complementary imaging modalities. Part of the workflow relies on finding the transformation linking a source image to a target image. We are specifically interested in the estimation of registration error in point-based registration. We propose an application of multivariate linear regression to solve the registration problem allowing us to propose a framework for the estimation of the associated error in the case of rigid and affine transformations and with anisotropic noise. These developments can be used as a decision-support tool for the biologist to analyze multimodal correlative images and are available under Ec-CLEM, an open-source plugin under ICY.



### What is Multimodality?
- **Arxiv ID**: http://arxiv.org/abs/2103.06304v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, 68Txx, I.2.0; I.2.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2103.06304v3)
- **Published**: 2021-03-10 19:14:07+00:00
- **Updated**: 2021-06-10 19:32:33+00:00
- **Authors**: Letitia Parcalabescu, Nils Trost, Anette Frank
- **Comment**: Paper accepted for publication at MMSR 2021; 10 pages, 5 figures
- **Journal**: Proceedings of the 1st Workshop on Multimodal Semantic
  Representations (MMSR), 2021, Groningen, Netherlands (Online), Association
  for Computational Linguistics, p. 1--10
- **Summary**: The last years have shown rapid developments in the field of multimodal machine learning, combining e.g., vision, text or speech. In this position paper we explain how the field uses outdated definitions of multimodality that prove unfit for the machine learning era. We propose a new task-relative definition of (multi)modality in the context of multimodal machine learning that focuses on representations and information that are relevant for a given machine learning task. With our new definition of multimodality we aim to provide a missing foundation for multimodal research, an important component of language grounding and a crucial milestone towards NLU.



### Face Images as Jigsaw Puzzles: Compositional Perception of Human Faces for Machines Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.06331v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06331v1)
- **Published**: 2021-03-10 20:25:38+00:00
- **Updated**: 2021-03-10 20:25:38+00:00
- **Authors**: Mahla Abdolahnejad, Peter Xiaoping Liu
- **Comment**: None
- **Journal**: None
- **Summary**: An important goal in human-robot-interaction (HRI) is for machines to achieve a close to human level of face perception. One of the important differences between machine learning and human intelligence is the lack of compositionality. This paper introduces a new scheme to enable generative adversarial networks to learn the distribution of face images composed of smaller parts. This results in a more flexible machine face perception and easier generalization to outside training examples. We demonstrate that this model is able to produce realistic high-quality face images by generating and piecing together the parts. Additionally, we demonstrate that this model learns the relations between the facial parts and their distributions. Therefore, the specific facial parts are interchangeable between generated face images.



### Enhancing VMAF through New Feature Integration and Model Combination
- **Arxiv ID**: http://arxiv.org/abs/2103.06338v1
- **DOI**: 10.1109/PCS50896.2021.9477458
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06338v1)
- **Published**: 2021-03-10 20:43:19+00:00
- **Updated**: 2021-03-10 20:43:19+00:00
- **Authors**: Fan Zhang, Angeliki Katsenou, Christos Bampis, Lukas Krasula, Zhi Li, David Bull
- **Comment**: 5 pages, 2 figures and 4 tables
- **Journal**: None
- **Summary**: VMAF is a machine learning based video quality assessment method, originally designed for streaming applications, which combines multiple quality metrics and video features through SVM regression. It offers higher correlation with subjective opinions compared to many conventional quality assessment methods. In this paper we propose enhancements to VMAF through the integration of new video features and alternative quality metrics (selected from a diverse pool) alongside multiple model combination. The proposed combination approach enables training on multiple databases with varying content and distortion characteristics. Our enhanced VMAF method has been evaluated on eight HD video databases, and consistently outperforms the original VMAF model (0.6.1) and other benchmark quality metrics, exhibiting higher correlation with subjective ground truth data.



### PatchNet -- Short-range Template Matching for Efficient Video Processing
- **Arxiv ID**: http://arxiv.org/abs/2103.07371v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.07371v1)
- **Published**: 2021-03-10 20:56:07+00:00
- **Updated**: 2021-03-10 20:56:07+00:00
- **Authors**: Huizi Mao, Sibo Zhu, Song Han, William J. Dally
- **Comment**: None
- **Journal**: None
- **Summary**: Object recognition is a fundamental problem in many video processing tasks, accurately locating seen objects at low computation cost paves the way for on-device video recognition. We propose PatchNet, an efficient convolutional neural network to match objects in adjacent video frames. It learns the patchwise correlation features instead of pixel features. PatchNet is very compact, running at just 58MFLOPs, $5\times$ simpler than MobileNetV2. We demonstrate its application on two tasks, video object detection and visual object tracking. On ImageNet VID, PatchNet reduces the flops of R-FCN ResNet-101 by 5x and EfficientDet-D0 by 3.4x with less than 1% mAP loss. On OTB2015, PatchNet reduces SiamFC and SiamRPN by 2.5x with no accuracy loss. Experiments on Jetson Nano further demonstrate 2.8x to 4.3x speed-ups associated with flops reduction. Code is open sourced at https://github.com/RalphMao/PatchNet.



### Continual Semantic Segmentation via Repulsion-Attraction of Sparse and Disentangled Latent Representations
- **Arxiv ID**: http://arxiv.org/abs/2103.06342v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.06342v3)
- **Published**: 2021-03-10 21:02:05+00:00
- **Updated**: 2021-11-24 14:44:27+00:00
- **Authors**: Umberto Michieli, Pietro Zanuttigh
- **Comment**: CVPR 2021. 22 pages, 10 figures, 11 tables
- **Journal**: None
- **Summary**: Deep neural networks suffer from the major limitation of catastrophic forgetting old tasks when learning new ones. In this paper we focus on class incremental continual learning in semantic segmentation, where new categories are made available over time while previous training data is not retained. The proposed continual learning scheme shapes the latent space to reduce forgetting whilst improving the recognition of novel classes. Our framework is driven by three novel components which we also combine on top of existing techniques effortlessly. First, prototypes matching enforces latent space consistency on old classes, constraining the encoder to produce similar latent representation for previously seen classes in the subsequent steps. Second, features sparsification allows to make room in the latent space to accommodate novel classes. Finally, contrastive learning is employed to cluster features according to their semantics while tearing apart those of different classes. Extensive evaluation on the Pascal VOC2012 and ADE20K datasets demonstrates the effectiveness of our approach, significantly outperforming state-of-the-art methods.



### A Computed Tomography Vertebral Segmentation Dataset with Anatomical Variations and Multi-Vendor Scanner Data
- **Arxiv ID**: http://arxiv.org/abs/2103.06360v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06360v1)
- **Published**: 2021-03-10 22:07:26+00:00
- **Updated**: 2021-03-10 22:07:26+00:00
- **Authors**: Hans Liebl, David Schinz, Anjany Sekuboyina, Luca Malagutti, Maximilian T. Lffler, Amirhossein Bayat, Malek El Husseini, Giles Tetteh, Katharina Grau, Eva Niederreiter, Thomas Baum, Benedikt Wiestler, Bjoern Menze, Rickmer Braren, Claus Zimmer, Jan S. Kirschke
- **Comment**: 18 pages, 2 figures, 2 tables; Hans Liebl, David Schinz equally
  contributed to this manuscript
- **Journal**: None
- **Summary**: With the advent of deep learning algorithms, fully automated radiological image analysis is within reach. In spine imaging, several atlas- and shape-based as well as deep learning segmentation algorithms have been proposed, allowing for subsequent automated analysis of morphology and pathology. The first Large Scale Vertebrae Segmentation Challenge (VerSe 2019) showed that these perform well on normal anatomy, but fail in variants not frequently present in the training dataset. Building on that experience, we report on the largely increased VerSe 2020 dataset and results from the second iteration of the VerSe challenge (MICCAI 2020, Lima, Peru). VerSe 2020 comprises annotated spine computed tomography (CT) images from 300 subjects with 4142 fully visualized and annotated vertebrae, collected across multiple centres from four different scanner manufacturers, enriched with cases that exhibit anatomical variants such as enumeration abnormalities (n=77) and transitional vertebrae (n=161). Metadata includes vertebral labelling information, voxel-level segmentation masks obtained with a human-machine hybrid algorithm and anatomical ratings, to enable the development and benchmarking of robust and accurate segmentation algorithms.



### Structure-From-Motion and RGBD Depth Fusion
- **Arxiv ID**: http://arxiv.org/abs/2103.06366v1
- **DOI**: 10.1109/SECON.2018.8478927
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06366v1)
- **Published**: 2021-03-10 22:14:11+00:00
- **Updated**: 2021-03-10 22:14:11+00:00
- **Authors**: Akash Chandrashekar, John Papadakis, Andrew Willis, Jamie Gantert
- **Comment**: None
- **Journal**: None
- **Summary**: This article describes a technique to augment a typical RGBD sensor by integrating depth estimates obtained via Structure-from-Motion (SfM) with sensor depth measurements. Limitations in the RGBD depth sensing technology prevent capturing depth measurements in four important contexts: (1) distant surfaces (>5m), (2) dark surfaces, (3) brightly lit indoor scenes and (4) sunlit outdoor scenes. SfM technology computes depth via multi-view reconstruction from the RGB image sequence alone. As such, SfM depth estimates do not suffer the same limitations and may be computed in all four of the previously listed circumstances. This work describes a novel fusion of RGBD depth data and SfM-estimated depths to generate an improved depth stream that may be processed by one of many important downstream applications such as robotic localization and mapping, as well as object recognition and tracking.



### Automated liver tissues delineation techniques: A systematic survey on machine learning current trends and future orientations
- **Arxiv ID**: http://arxiv.org/abs/2103.06384v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06384v2)
- **Published**: 2021-03-10 23:11:16+00:00
- **Updated**: 2022-07-28 18:11:17+00:00
- **Authors**: Ayman Al-Kababji, Faycal Bensaali, Sarada Prasad Dakua, Yassine Himeur
- **Comment**: 46 pages, 11 figures, 13 equations, 4 tables. A review paper on liver
  tissues segmentation based on automated ML-based techniques
- **Journal**: None
- **Summary**: Machine learning and computer vision techniques have grown rapidly in recent years due to their automation, suitability, and ability to generate astounding results. Hence, in this paper, we survey the key studies that are published between 2014 and 2022, showcasing the different machine learning algorithms researchers have used to segment the liver, hepatic tumors, and hepatic-vasculature structures. We divide the surveyed studies based on the tissue of interest (hepatic-parenchyma, hepatic-tumors, or hepatic-vessels), highlighting the studies that tackle more than one task simultaneously. Additionally, the machine learning algorithms are classified as either supervised or unsupervised, and they are further partitioned if the amount of work that falls under a certain scheme is significant. Moreover, different datasets and challenges found in literature and websites containing masks of the aforementioned tissues are thoroughly discussed, highlighting the organizers' original contributions and those of other researchers. Also, the metrics used excessively in literature are mentioned in our review, stressing their relevance to the task at hand. Finally, critical challenges and future directions are emphasized for innovative researchers to tackle, exposing gaps that need addressing, such as the scarcity of many studies on the vessels' segmentation challenge and why their absence needs to be dealt with sooner than later.




# Arxiv Papers in cs.CV on 2021-04-30
### End-to-End Attention-based Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2104.14721v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.14721v1)
- **Published**: 2021-04-30 01:54:38+00:00
- **Updated**: 2021-04-30 01:54:38+00:00
- **Authors**: Carola Sundaramoorthy, Lin Ziwen Kelvin, Mahak Sarin, Shubham Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of image captioning specifically for molecular translation where the result would be a predicted chemical notation in InChI format for a given molecular structure. Current approaches mainly follow rule-based or CNN+RNN based methodology. However, they seem to underperform on noisy images and images with small number of distinguishable features. To overcome this, we propose an end-to-end transformer model. When compared to attention-based techniques, our proposed model outperforms on molecular datasets.



### MOOD: Multi-level Out-of-distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.14726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14726v1)
- **Published**: 2021-04-30 02:18:31+00:00
- **Updated**: 2021-04-30 02:18:31+00:00
- **Authors**: Ziqian Lin, Sreya Dutta Roy, Yixuan Li
- **Comment**: 12 pages, 8 figures, CVPR2021
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) detection is essential to prevent anomalous inputs from causing a model to fail during deployment. While improved OOD detection methods have emerged, they often rely on the final layer outputs and require a full feedforward pass for any given input. In this paper, we propose a novel framework, multi-level out-of-distribution detection MOOD, which exploits intermediate classifier outputs for dynamic and efficient OOD inference. We explore and establish a direct relationship between the OOD data complexity and optimal exit level, and show that easy OOD examples can be effectively detected early without propagating to deeper layers. At each exit, the OOD examples can be distinguished through our proposed adjusted energy score, which is both empirically and theoretically suitable for networks with multiple classifiers. We extensively evaluate MOOD across 10 OOD datasets spanning a wide range of complexities. Experiments demonstrate that MOOD achieves up to 71.05% computational reduction in inference, while maintaining competitive OOD detection performance.



### CoSformer: Detecting Co-Salient Object with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.14729v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14729v2)
- **Published**: 2021-04-30 02:39:12+00:00
- **Updated**: 2022-09-22 12:23:02+00:00
- **Authors**: Lv Tang, Bo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Co-Salient Object Detection (CoSOD) aims at simulating the human visual system to discover the common and salient objects from a group of relevant images. Recent methods typically develop sophisticated deep learning based models have greatly improved the performance of CoSOD task. But there are still two major drawbacks that need to be further addressed, 1) sub-optimal inter-image relationship modeling; 2) lacking consideration of inter-image separability. In this paper, we propose the Co-Salient Object Detection Transformer (CoSformer) network to capture both salient and common visual patterns from multiple images. By leveraging Transformer architecture, the proposed method address the influence of the input orders and greatly improve the stability of the CoSOD task. We also introduce a novel concept of inter-image separability. We construct a contrast learning scheme to modeling the inter-image separability and learn more discriminative embedding space to distinguish true common objects from noisy objects. Extensive experiments on three challenging benchmarks, i.e., CoCA, CoSOD3k, and Cosal2015, demonstrate that our CoSformer outperforms cutting-edge models and achieves the new state-of-the-art. We hope that CoSformer can motivate future research for more visual co-analysis tasks.



### Perceptual Image Quality Assessment with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.14730v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.14730v2)
- **Published**: 2021-04-30 02:45:29+00:00
- **Updated**: 2021-05-05 03:18:47+00:00
- **Authors**: Manri Cheon, Sung-Jun Yoon, Byungyeon Kang, Junwoo Lee
- **Comment**: Accepted to NTIRE workshop at CVPR 2021. 1st Place in NTIRE 2021
  perceptual IQA challenge. https://github.com/manricheon/IQT
- **Journal**: None
- **Summary**: In this paper, we propose an image quality transformer (IQT) that successfully applies a transformer architecture to a perceptual full-reference image quality assessment (IQA) task. Perceptual representation becomes more important in image quality assessment. In this context, we extract the perceptual feature representations from each of input images using a convolutional neural network (CNN) backbone. The extracted feature maps are fed into the transformer encoder and decoder in order to compare a reference and distorted images. Following an approach of the transformer-based vision models, we use extra learnable quality embedding and position embedding. The output of the transformer is passed to a prediction head in order to predict a final quality score. The experimental results show that our proposed model has an outstanding performance for the standard IQA datasets. For a large-scale IQA dataset containing output images of generative model, our model also shows the promising results. The proposed IQT was ranked first among 13 participants in the NTIRE 2021 perceptual image quality assessment challenge. Our work will be an opportunity to further expand the approach for the perceptual IQA task.



### DPR-CAE: Capsule Autoencoder with Dynamic Part Representation for Image Parsing
- **Arxiv ID**: http://arxiv.org/abs/2104.14735v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14735v2)
- **Published**: 2021-04-30 03:14:17+00:00
- **Updated**: 2021-09-07 02:25:31+00:00
- **Authors**: Canqun Xiang, Zhennan Wang, Wenbin Zou, Chen Xu
- **Comment**: The content of our article needs to be revised, and the content of
  the paper is not very accurate
- **Journal**: None
- **Summary**: Parsing an image into a hierarchy of objects, parts, and relations is important and also challenging in many computer vision tasks. This paper proposes a simple and effective capsule autoencoder to address this issue, called DPR-CAE. In our approach, the encoder parses the input into a set of part capsules, including pose, intensity, and dynamic vector. The decoder introduces a novel dynamic part representation (DPR) by combining the dynamic vector and a shared template bank. These part representations are then regulated by corresponding capsules to composite the final output in an interpretable way. Besides, an extra translation-invariant module is proposed to avoid directly learning the uncertain scene-part relationship in our DPR-CAE, which makes the resulting method achieves a promising performance gain on $rm$-MNIST and $rm$-Fashion-MNIST. % to model the scene-object relationship DPR-CAE can be easily combined with the existing stacked capsule autoencoder and experimental results show it significantly improves performance in terms of unsupervised object classification. Our code is available in the Appendix.



### Chop Chop BERT: Visual Question Answering by Chopping VisualBERT's Heads
- **Arxiv ID**: http://arxiv.org/abs/2104.14741v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2104.14741v1)
- **Published**: 2021-04-30 03:32:02+00:00
- **Updated**: 2021-04-30 03:32:02+00:00
- **Authors**: Chenyu Gao, Qi Zhu, Peng Wang, Qi Wu
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Vision-and-Language (VL) pre-training has shown great potential on many related downstream tasks, such as Visual Question Answering (VQA), one of the most popular problems in the VL field. All of these pre-trained models (such as VisualBERT, ViLBERT, LXMERT and UNITER) are built with Transformer, which extends the classical attention mechanism to multiple layers and heads. To investigate why and how these models work on VQA so well, in this paper we explore the roles of individual heads and layers in Transformer models when handling $12$ different types of questions. Specifically, we manually remove (chop) heads (or layers) from a pre-trained VisualBERT model at a time, and test it on different levels of questions to record its performance. As shown in the interesting echelon shape of the result matrices, experiments reveal different heads and layers are responsible for different question types, with higher-level layers activated by higher-level visual reasoning questions. Based on this observation, we design a dynamic chopping module that can automatically remove heads and layers of the VisualBERT at an instance level when dealing with different questions. Our dynamic chopping module can effectively reduce the parameters of the original model by 50%, while only damaging the accuracy by less than 1% on the VQA task.



### Center Prediction Loss for Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2104.14746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14746v1)
- **Published**: 2021-04-30 03:57:31+00:00
- **Updated**: 2021-04-30 03:57:31+00:00
- **Authors**: Lu Yang, Yunlong Wang, Lingqiao Liu, Peng Wang, Lu Chi, Zehuan Yuan, Changhu Wang, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The training loss function that enforces certain training sample distribution patterns plays a critical role in building a re-identification (ReID) system. Besides the basic requirement of discrimination, i.e., the features corresponding to different identities should not be mixed, additional intra-class distribution constraints, such as features from the same identities should be close to their centers, have been adopted to construct losses. Despite the advances of various new loss functions, it is still challenging to strike the balance between the need of reducing the intra-class variation and allowing certain distribution freedom. In this paper, we propose a new loss based on center predictivity, that is, a sample must be positioned in a location of the feature space such that from it we can roughly predict the location of the center of same-class samples. The prediction error is then regarded as a loss called Center Prediction Loss (CPL). We show that, without introducing additional hyper-parameters, this new loss leads to a more flexible intra-class distribution constraint while ensuring the between-class samples are well-separated. Extensive experiments on various real-world ReID datasets show that the proposed loss can achieve superior performance and can also be complementary to existing losses.



### Reproducibility of "FDA: Fourier Domain Adaptation forSemantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.14749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14749v1)
- **Published**: 2021-04-30 04:20:35+00:00
- **Updated**: 2021-04-30 04:20:35+00:00
- **Authors**: Arnesh Kumar Issar, Kirtan Mali, Aryan Mehta, Karan Uppal, Saurabh Mishra, Debashish Chakravarty
- **Comment**: 11 pages, 7 figures, 7 Tables, ML Reproducibility Challenge 2020
- **Journal**: None
- **Summary**: The following paper is a reproducibility report for "FDA: Fourier Domain Adaptation for Semantic Segmentation" published in the CVPR 2020 as part of the ML Reproducibility Challenge 2020. The original code was made available by the author. The well-commented version of the code containing all ablation studies performed derived from the original code along with WANDB integration is available at <github.com/thefatbandit/FDA> with proper instructions to execute experiments in README.



### A Refined Inertial DC Algorithm for DC Programming
- **Arxiv ID**: http://arxiv.org/abs/2104.14750v2
- **DOI**: 10.1007/s11081-022-09716-5
- **Categories**: **math.OC**, cs.CV, 90C26, 90C30, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2104.14750v2)
- **Published**: 2021-04-30 04:21:57+00:00
- **Updated**: 2022-04-25 22:40:17+00:00
- **Authors**: Yu You, Yi-Shuai Niu
- **Comment**: 24 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper we consider the difference-of-convex (DC) programming problems, whose objective function is the difference of two convex functions. The classical DC Algorithm (DCA) is well-known for solving this kind of problems, which generally returns a critical point. Recently, an inertial DC algorithm (InDCA) equipped with heavy-ball inertial-force procedure was proposed in de Oliveira et al. (Set-Valued and Variational Analysis 27(4):895--919, 2019), which potentially helps to improve both the convergence speed and the solution quality. Based on InDCA, we propose a refined inertial DC algorithm (RInDCA) equipped with enlarged inertial step-size compared with InDCA. Empirically, larger step-size accelerates the convergence. We demonstrate the subsequential convergence of our refined version to a critical point. In addition, by assuming the Kurdyka-{\L}ojasiewicz (KL) property of the objective function, we establish the sequential convergence of RInDCA. Numerical simulations on checking copositivity of matrices and image denoising problem show the benefit of larger step-size.



### Studying the Consistency and Composability of Lottery Ticket Pruning Masks
- **Arxiv ID**: http://arxiv.org/abs/2104.14753v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2104.14753v1)
- **Published**: 2021-04-30 04:38:06+00:00
- **Updated**: 2021-04-30 04:38:06+00:00
- **Authors**: Rajiv Movva, Jonathan Frankle, Michael Carbin
- **Comment**: Workshop on Science and Engineering of Deep Learning (ICLR 2021)
- **Journal**: None
- **Summary**: Magnitude pruning is a common, effective technique to identify sparse subnetworks at little cost to accuracy. In this work, we ask whether a particular architecture's accuracy-sparsity tradeoff can be improved by combining pruning information across multiple runs of training. From a shared ResNet-20 initialization, we train several network copies (\emph{siblings}) to completion using different SGD data orders on CIFAR-10. While the siblings' pruning masks are naively not much more similar than chance, starting sibling training after a few epochs of shared pretraining significantly increases pruning overlap. We then choose a subnetwork by either (1) taking all weights that survive pruning in any sibling (mask union), or (2) taking only the weights that survive pruning across all siblings (mask intersection). The resulting subnetwork is retrained. Strikingly, we find that union and intersection masks perform very similarly. Both methods match the accuracy-sparsity tradeoffs of the one-shot magnitude pruning baseline, even when we combine masks from up to $k = 10$ siblings.



### Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing
- **Arxiv ID**: http://arxiv.org/abs/2104.14754v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.14754v2)
- **Published**: 2021-04-30 04:43:24+00:00
- **Updated**: 2021-06-23 02:05:12+00:00
- **Authors**: Hyunsu Kim, Yunjey Choi, Junho Kim, Sungjoo Yoo, Youngjung Uh
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) synthesize realistic images from random latent vectors. Although manipulating the latent vectors controls the synthesized outputs, editing real images with GANs suffers from i) time-consuming optimization for projecting real images to the latent vectors, ii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the intermediate latent space has spatial dimensions, and a spatially variant modulation replaces AdaIN. It makes the embedding through an encoder more accurate than existing optimization-based methods while maintaining the properties of GANs. Experimental results demonstrate that our method significantly outperforms state-of-the-art models in various image manipulation tasks such as local editing and image interpolation. Last but not least, conventional editing methods on GANs are still valid on our StyleMapGAN. Source code is available at https://github.com/naver-ai/StyleMapGAN.



### GM-MLIC: Graph Matching based Multi-Label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2104.14762v2
- **DOI**: 10.24963/ijcai.2021/163
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.14762v2)
- **Published**: 2021-04-30 05:36:25+00:00
- **Updated**: 2021-05-07 09:20:47+00:00
- **Authors**: Yanan Wu, He Liu, Songhe Feng, Yi Jin, Gengyu Lyu, Zizhang Wu
- **Comment**: Accepted by International Joint Conferences on Artificial
  Intelligence (IJCAI-2021)
- **Journal**: the Thirtieth International Joint Conference on Artificial
  Intelligence(IJCAI), 2021: 1179--1185
- **Summary**: Multi-Label Image Classification (MLIC) aims to predict a set of labels that present in an image. The key to deal with such problem is to mine the associations between image contents and labels, and further obtain the correct assignments between images and their labels. In this paper, we treat each image as a bag of instances, and reformulate the task of MLIC as an instance-label matching selection problem. To model such problem, we propose a novel deep learning framework named Graph Matching based Multi-Label Image Classification (GM-MLIC), where Graph Matching (GM) scheme is introduced owing to its excellent capability of excavating the instance and label relationship. Specifically, we first construct an instance spatial graph and a label semantic graph respectively, and then incorporate them into a constructed assignment graph by connecting each instance to all labels. Subsequently, the graph network block is adopted to aggregate and update all nodes and edges state on the assignment graph to form structured representations for each instance and label. Our network finally derives a prediction score for each instance-label correspondence and optimizes such correspondence with a weighted cross-entropy loss. Extensive experiments conducted on various image datasets demonstrate the superiority of our proposed method.



### ICOS: Efficient and Highly Robust Rotation Search and Point Cloud Registration with Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2104.14763v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.14763v2)
- **Published**: 2021-04-30 05:41:53+00:00
- **Updated**: 2021-05-10 06:23:39+00:00
- **Authors**: Lei Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Rotation search and point cloud registration are two fundamental problems in robotics and computer vision, which aim to estimate the rotation and the transformation between the 3D vector sets and point clouds, respectively. Due to the presence of outliers, probably in very large numbers, among the putative vector or point correspondences in real-world applications, robust estimation is of great importance. In this paper, we present ICOS (Inlier searching using COmpatible Structures), a novel, efficient and highly robust solver for both the correspondence-based rotation search and point cloud registration problems. Specifically, we (i) propose and construct a series of compatible structures for the two problems where various invariants can be established, and (ii) design three time-efficient frameworks, the first for rotation search, the second for known-scale registration and the third for unknown-scale registration, to filter out outliers and seek inliers from the invariant-constrained random sampling based on the compatible structures proposed. In this manner, even with extreme outlier ratios, inliers can be sifted out and collected for solving the optimal rotation and transformation effectively, leading to our robust solver ICOS. Through plentiful experiments over standard datasets, we demonstrate that: (i) our solver ICOS is fast, accurate, robust against over 95% outliers with nearly 100% recall ratio of inliers for rotation search and both known-scale and unknown-scale registration, outperforming other state-of-the-art methods, and (ii) ICOS is practical for use in multiple real-world applications.



### CoCon: Cooperative-Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.14764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14764v1)
- **Published**: 2021-04-30 05:46:02+00:00
- **Updated**: 2021-04-30 05:46:02+00:00
- **Authors**: Nishant Rai, Ehsan Adeli, Kuan-Hui Lee, Adrien Gaidon, Juan Carlos Niebles
- **Comment**: None
- **Journal**: None
- **Summary**: Labeling videos at scale is impractical. Consequently, self-supervised visual representation learning is key for efficient video analysis. Recent success in learning image representations suggests contrastive learning is a promising framework to tackle this challenge. However, when applied to real-world videos, contrastive learning may unknowingly lead to the separation of instances that contain semantically similar events. In our work, we introduce a cooperative variant of contrastive learning to utilize complementary information across views and address this issue. We use data-driven sampling to leverage implicit relationships between multiple input video views, whether observed (e.g. RGB) or inferred (e.g. flow, segmentation masks, poses). We are one of the firsts to explore exploiting inter-instance relationships to drive learning. We experimentally evaluate our representations on the downstream task of action recognition. Our method achieves competitive performance on standard benchmarks (UCF101, HMDB51, Kinetics400). Furthermore, qualitative experiments illustrate that our models can capture higher-order class relationships.



### TREND: Truncated Generalized Normal Density Estimation of Inception Embeddings for GAN Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2104.14767v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.14767v2)
- **Published**: 2021-04-30 05:51:07+00:00
- **Updated**: 2022-07-20 04:35:14+00:00
- **Authors**: Junghyuk Lee, Jong-Seok Lee
- **Comment**: Accepted in ECCV 2022
- **Journal**: None
- **Summary**: Evaluating image generation models such as generative adversarial networks (GANs) is a challenging problem. A common approach is to compare the distributions of the set of ground truth images and the set of generated test images. The Frech\'et Inception distance is one of the most widely used metrics for evaluation of GANs, which assumes that the features from a trained Inception model for a set of images follow a normal distribution. In this paper, we argue that this is an over-simplified assumption, which may lead to unreliable evaluation results, and more accurate density estimation can be achieved using a truncated generalized normal distribution. Based on this, we propose a novel metric for accurate evaluation of GANs, named TREND (TRuncated gEneralized Normal Density estimation of inception embeddings). We demonstrate that our approach significantly reduces errors of density estimation, which consequently eliminates the risk of faulty evaluation results. Furthermore, we show that the proposed metric significantly improves robustness of evaluation results against variation of the number of image samples.



### PointLIE: Locally Invertible Embedding for Point Cloud Sampling and Recovery
- **Arxiv ID**: http://arxiv.org/abs/2104.14769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14769v1)
- **Published**: 2021-04-30 05:55:59+00:00
- **Updated**: 2021-04-30 05:55:59+00:00
- **Authors**: Weibing Zhao, Xu Yan, Jiantao Gao, Ruimao Zhang, Jiayan Zhang, Zhen Li, Song Wu, Shuguang Cui
- **Comment**: To appear in IJCAI 2021
- **Journal**: IJCAI 2021
- **Summary**: Point Cloud Sampling and Recovery (PCSR) is critical for massive real-time point cloud collection and processing since raw data usually requires large storage and computation. In this paper, we address a fundamental problem in PCSR: How to downsample the dense point cloud with arbitrary scales while preserving the local topology of discarding points in a case-agnostic manner (i.e. without additional storage for point relationship)? We propose a novel Locally Invertible Embedding for point cloud adaptive sampling and recovery (PointLIE). Instead of learning to predict the underlying geometry details in a seemingly plausible manner, PointLIE unifies point cloud sampling and upsampling to one single framework through bi-directional learning. Specifically, PointLIE recursively samples and adjusts neighboring points on each scale. Then it encodes the neighboring offsets of sampled points to a latent space and thus decouples the sampled points and the corresponding local geometric relationship. Once the latent space is determined and that the deep model is optimized, the recovery process could be conducted by passing the recover-pleasing sampled points and a randomly-drawn embedding to the same network through an invertible operation. Such a scheme could guarantee the fidelity of dense point recovery from sampled points. Extensive experiments demonstrate that the proposed PointLIE outperforms state-of-the-arts both quantitatively and qualitatively. Our code is released through https://github.com/zwb0/PointLIE.



### Cleaning Label Noise with Clusters for Minimally Supervised Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.14770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14770v1)
- **Published**: 2021-04-30 06:03:24+00:00
- **Updated**: 2021-04-30 06:03:24+00:00
- **Authors**: Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, Arif Mahmood, Seung-Ik Lee
- **Comment**: Presented in the CVPR20 Workshop Learning from Unlabeled Videos. An
  archival version of this research work, published in SPL, can be accessed at:
  https://ieeexplore.ieee.org/document/9204830. arXiv admin note: substantial
  text overlap with arXiv:2008.11887
- **Journal**: Computer Vision and Pattern Recognition Workshops (2020)
- **Summary**: Learning to detect real-world anomalous events using video-level annotations is a difficult task mainly because of the noise present in labels. An anomalous labelled video may actually contain anomaly only in a short duration while the rest of the video can be normal. In the current work, we formulate a weakly supervised anomaly detection method that is trained using only video-level labels. To this end, we propose to utilize binary clustering which helps in mitigating the noise present in the labels of anomalous videos. Our formulation encourages both the main network and the clustering to complement each other in achieving the goal of weakly supervised training. The proposed method yields 78.27% and 84.16% frame-level AUC on UCF-crime and ShanghaiTech datasets respectively, demonstrating its superiority over existing state-of-the-art algorithms.



### BiCnet-TKS: Learning Efficient Spatial-Temporal Representation for Video Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.14783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14783v1)
- **Published**: 2021-04-30 06:44:34+00:00
- **Updated**: 2021-04-30 06:44:34+00:00
- **Authors**: Ruibing Hou, Hong Chang, Bingpeng Ma, Rui Huang, Shiguang Shan
- **Comment**: Accepted by IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2021) 2021
- **Journal**: None
- **Summary**: In this paper, we present an efficient spatial-temporal representation for video person re-identification (reID). Firstly, we propose a Bilateral Complementary Network (BiCnet) for spatial complementarity modeling. Specifically, BiCnet contains two branches. Detail Branch processes frames at original resolution to preserve the detailed visual clues, and Context Branch with a down-sampling strategy is employed to capture long-range contexts. On each branch, BiCnet appends multiple parallel and diverse attention modules to discover divergent body parts for consecutive frames, so as to obtain an integral characteristic of target identity. Furthermore, a Temporal Kernel Selection (TKS) block is designed to capture short-term as well as long-term temporal relations by an adaptive mode. TKS can be inserted into BiCnet at any depth to construct BiCnetTKS for spatial-temporal modeling. Experimental results on multiple benchmarks show that BiCnet-TKS outperforms state-of-the-arts with about 50% less computations. The source code is available at https://github.com/ blue-blue272/BiCnet-TKS.



### Editable Free-viewpoint Video Using a Layered Neural Representation
- **Arxiv ID**: http://arxiv.org/abs/2104.14786v1
- **DOI**: 10.1145/3450626.3459756
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2104.14786v1)
- **Published**: 2021-04-30 06:50:45+00:00
- **Updated**: 2021-04-30 06:50:45+00:00
- **Authors**: Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Generating free-viewpoint videos is critical for immersive VR/AR experience but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper we propose the first approach for editable photo-realistic free-viewpoint video generation for large-scale dynamic scenes using only sparse 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity including the environment itself is formulated into a space-time coherent neural layered radiance representation called ST-NeRF. Such layered representation supports fully perception and realistic manipulation of the dynamic scene whilst still supporting a free viewing experience in a wide range. In our ST-NeRF, the dynamic entity/layer is represented as continuous functions, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly, and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.



### Seeing All From a Few: Nodes Selection Using Graph Pooling for Graph Clustering
- **Arxiv ID**: http://arxiv.org/abs/2105.05320v2
- **DOI**: 10.1109/TNNLS.2022.3210370
- **Categories**: **cs.SI**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.05320v2)
- **Published**: 2021-04-30 06:51:51+00:00
- **Updated**: 2021-06-08 02:51:53+00:00
- **Authors**: Yiming Wang, Dongxia Chang, Zhiqian Fu, Yao Zhao
- **Comment**: None
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems, 2022
- **Summary**: Recently, there has been considerable research interest in graph clustering aimed at data partition using the graph information. However, one limitation of the most of graph-based methods is that they assume the graph structure to operate is fixed and reliable. And there are inevitably some edges in the graph that are not conducive to graph clustering, which we call spurious edges. This paper is the first attempt to employ graph pooling technique for node clustering and we propose a novel dual graph embedding network (DGEN), which is designed as a two-step graph encoder connected by a graph pooling layer to learn the graph embedding. In our model, it is assumed that if a node and its nearest neighboring node are close to the same clustering center, this node is an informative node and this edge can be considered as a cluster-friendly edge. Based on this assumption, the neighbor cluster pooling (NCPool) is devised to select the most informative subset of nodes and the corresponding edges based on the distance of nodes and their nearest neighbors to the cluster centers. This can effectively alleviate the impact of the spurious edges on the clustering. Finally, to obtain the clustering assignment of all nodes, a classifier is trained using the clustering results of the selected nodes. Experiments on five benchmark graph datasets demonstrate the superiority of the proposed method over state-of-the-art algorithms.



### Few-Shot Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.14805v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14805v3)
- **Published**: 2021-04-30 07:38:04+00:00
- **Updated**: 2022-08-07 09:23:11+00:00
- **Authors**: Qi Fan, Chi-Keung Tang, Yu-Wing Tai
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: We introduce Few-Shot Video Object Detection (FSVOD) with three contributions to real-world visual learning challenge in our highly diverse and dynamic world: 1) a large-scale video dataset FSVOD-500 comprising of 500 classes with class-balanced videos in each category for few-shot learning; 2) a novel Tube Proposal Network (TPN) to generate high-quality video tube proposals for aggregating feature representation for the target video object which can be highly dynamic; 3) a strategically improved Temporal Matching Network (TMN+) for matching representative query tube features with better discriminative ability thus achieving higher diversity. Our TPN and TMN+ are jointly and end-to-end trained. Extensive experiments demonstrate that our method produces significantly better detection results on two few-shot video object detection datasets compared to image-based methods and other naive video-based extensions. Codes and datasets are released at \url{https://github.com/fanq15/FewX}.



### GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2104.14806v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14806v1)
- **Published**: 2021-04-30 07:40:35+00:00
- **Updated**: 2021-04-30 07:40:35+00:00
- **Authors**: Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, Nan Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Generating videos from text is a challenging task due to its high computational requirements for training and infinite possible answers for evaluation. Existing works typically experiment on simple or small datasets, where the generalization ability is quite limited. In this work, we propose GODIVA, an open-domain text-to-video pretrained model that can generate videos from text in an auto-regressive manner using a three-dimensional sparse attention mechanism. We pretrain our model on Howto100M, a large-scale text-video dataset that contains more than 136 million text-video pairs. Experiments show that GODIVA not only can be fine-tuned on downstream video generation tasks, but also has a good zero-shot capability on unseen texts. We also propose a new metric called Relative Matching (RM) to automatically evaluate the video generation quality. Several challenges are listed and discussed as future work.



### SegmentMeIfYouCan: A Benchmark for Anomaly Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.14812v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, 62-07, I.4.6; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2104.14812v2)
- **Published**: 2021-04-30 07:58:19+00:00
- **Updated**: 2021-11-09 12:11:45+00:00
- **Authors**: Robin Chan, Krzysztof Lis, Svenja Uhlemeyer, Hermann Blum, Sina Honari, Roland Siegwart, Pascal Fua, Mathieu Salzmann, Matthias Rottmann
- **Comment**: 35 pages, 18 figures, 16 tables, website
  https://segmentmeifyoucan.com/, NeurIPS 2021 Track on Datasets and Benchmarks
- **Journal**: None
- **Summary**: State-of-the-art semantic or instance segmentation deep neural networks (DNNs) are usually trained on a closed set of semantic classes. As such, they are ill-equipped to handle previously-unseen objects. However, detecting and localizing such objects is crucial for safety-critical applications such as perception for automated driving, especially if they appear on the road ahead. While some methods have tackled the tasks of anomalous or out-of-distribution object segmentation, progress remains slow, in large part due to the lack of solid benchmarks; existing datasets either consist of synthetic data, or suffer from label inconsistencies. In this paper, we bridge this gap by introducing the "SegmentMeIfYouCan" benchmark. Our benchmark addresses two tasks: Anomalous object segmentation, which considers any previously-unseen object category; and road obstacle segmentation, which focuses on any object on the road, may it be known or unknown. We provide two corresponding datasets together with a test suite performing an in-depth method analysis, considering both established pixel-wise performance metrics and recent component-wise ones, which are insensitive to object sizes. We empirically evaluate multiple state-of-the-art baseline methods, including several models specifically designed for anomaly / obstacle segmentation, on our datasets and on public ones, using our test suite. The anomaly and obstacle segmentation results show that our datasets contribute to the diversity and difficulty of both data landscapes.



### Multi Voxel-Point Neurons Convolution (MVPConv) for Fast and Accurate 3D Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.14834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14834v1)
- **Published**: 2021-04-30 08:36:03+00:00
- **Updated**: 2021-04-30 08:36:03+00:00
- **Authors**: Wei Zhou, Xin Cao, Xiaodan Zhang, Xingxing Hao, Dekui Wang, Ying He
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new convolutional neural network, called Multi Voxel-Point Neurons Convolution (MVPConv), for fast and accurate 3D deep learning. The previous works adopt either individual point-based features or local-neighboring voxel-based features to process 3D model, which limits the performance of models due to the inefficient computation. Moreover, most of the existing 3D deep learning frameworks aim at solving one specific task, and only a few of them can handle a variety of tasks. Integrating both the advantages of the voxel and point-based methods, the proposed MVPConv can effectively increase the neighboring collection between point-based features and also promote the independence among voxel-based features. Simply replacing the corresponding convolution module with MVPConv, we show that MVPConv can fit in different backbones to solve a wide range of 3D tasks. Extensive experiments on benchmark datasets such as ShapeNet Part, S3DIS and KITTI for various tasks show that MVPConv improves the accuracy of the backbone (PointNet) by up to 36%, and achieves higher accuracy than the voxel-based model with up to 34 times speedup. In addition, MVPConv also outperforms the state-of-the-art point-based models with up to 8 times speedup. Notably, our MVPConv achieves better accuracy than the newest point-voxel-based model PVCNN (a model more efficient than PointNet) with lower latency.



### RobustFusion: Robust Volumetric Performance Reconstruction under Human-object Interactions from Monocular RGBD Stream
- **Arxiv ID**: http://arxiv.org/abs/2104.14837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14837v1)
- **Published**: 2021-04-30 08:41:45+00:00
- **Updated**: 2021-04-30 08:41:45+00:00
- **Authors**: Zhuo Su, Lan Xu, Dawei Zhong, Zhong Li, Fan Deng, Shuxue Quan, Lu Fang
- **Comment**: 16 pages, 18 figures. Under review by IEEE TPAMI
- **Journal**: None
- **Summary**: High-quality 4D reconstruction of human performance with complex interactions to various objects is essential in real-world scenarios, which enables numerous immersive VR/AR applications. However, recent advances still fail to provide reliable performance reconstruction, suffering from challenging interaction patterns and severe occlusions, especially for the monocular setting. To fill this gap, in this paper, we propose RobustFusion, a robust volumetric performance reconstruction system for human-object interaction scenarios using only a single RGBD sensor, which combines various data-driven visual and interaction cues to handle the complex interaction patterns and severe occlusions. We propose a semantic-aware scene decoupling scheme to model the occlusions explicitly, with a segmentation refinement and robust object tracking to prevent disentanglement uncertainty and maintain temporal consistency. We further introduce a robust performance capture scheme with the aid of various data-driven cues, which not only enables re-initialization ability, but also models the complex human-object interaction patterns in a data-driven manner. To this end, we introduce a spatial relation prior to prevent implausible intersections, as well as data-driven interaction cues to maintain natural motions, especially for those regions under severe human-object occlusions. We also adopt an adaptive fusion scheme for temporally coherent human-object reconstruction with occlusion analysis and human parsing cue. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality 4D human performance reconstruction under complex human-object interactions whilst still maintaining the lightweight monocular setting.



### NTIRE 2021 Challenge on Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2104.14852v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14852v2)
- **Published**: 2021-04-30 09:12:19+00:00
- **Updated**: 2021-05-10 06:31:59+00:00
- **Authors**: Sanghyun Son, Suyoung Lee, Seungjun Nah, Radu Timofte, Kyoung Mu Lee
- **Comment**: An official report for NTIRE 2021 Video Super-Resolution Challenge,
  in conjunction with CVPR 2021
- **Journal**: None
- **Summary**: Super-Resolution (SR) is a fundamental computer vision task that aims to obtain a high-resolution clean image from the given low-resolution counterpart. This paper reviews the NTIRE 2021 Challenge on Video Super-Resolution. We present evaluation results from two competition tracks as well as the proposed solutions. Track 1 aims to develop conventional video SR methods focusing on the restoration quality. Track 2 assumes a more challenging environment with lower frame rates, casting spatio-temporal SR problem. In each competition, 247 and 223 participants have registered, respectively. During the final testing phase, 14 teams competed in each track to achieve state-of-the-art performance on video SR tasks.



### NTIRE 2021 Challenge on Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2104.14854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14854v1)
- **Published**: 2021-04-30 09:12:53+00:00
- **Updated**: 2021-04-30 09:12:53+00:00
- **Authors**: Seungjun Nah, Sanghyun Son, Suyoung Lee, Radu Timofte, Kyoung Mu Lee
- **Comment**: To be published in CVPR 2021 Workshop - NTIRE
- **Journal**: None
- **Summary**: Motion blur is a common photography artifact in dynamic environments that typically comes jointly with the other types of degradation. This paper reviews the NTIRE 2021 Challenge on Image Deblurring. In this challenge report, we describe the challenge specifics and the evaluation results from the 2 competition tracks with the proposed solutions. While both the tracks aim to recover a high-quality clean image from a blurry image, different artifacts are jointly involved. In track 1, the blurry images are in a low resolution while track 2 images are compressed in JPEG format. In each competition, there were 338 and 238 registered participants and in the final testing phase, 18 and 17 teams competed. The winning methods demonstrate the state-of-the-art performance on the image deblurring task with the jointly combined artifacts.



### Action in Mind: A Neural Network Approach to Action Recognition and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.14870v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.14870v1)
- **Published**: 2021-04-30 09:53:28+00:00
- **Updated**: 2021-04-30 09:53:28+00:00
- **Authors**: Zahra Gharaee
- **Comment**: Lund University Cognitive Science 2018
- **Journal**: None
- **Summary**: Recognizing and categorizing human actions is an important task with applications in various fields such as human-robot interaction, video analysis, surveillance, video retrieval, health care system and entertainment industry. This thesis presents a novel computational approach for human action recognition through different implementations of multi-layer architectures based on artificial neural networks. Each system level development is designed to solve different aspects of the action recognition problem including online real-time processing, action segmentation and the involvement of objects. The analysis of the experimental results are illustrated and described in six articles. The proposed action recognition architecture of this thesis is composed of several processing layers including a preprocessing layer, an ordered vector representation layer and three layers of neural networks. It utilizes self-organizing neural networks such as Kohonen feature maps and growing grids as the main neural network layers. Thus the architecture presents a biological plausible approach with certain features such as topographic organization of the neurons, lateral interactions, semi-supervised learning and the ability to represent high dimensional input space in lower dimensional maps. For each level of development the system is trained with the input data consisting of consecutive 3D body postures and tested with generalized input data that the system has never met before. The experimental results of different system level developments show that the system performs well with quite high accuracy for recognizing human actions.



### Robust joint registration of multiple stains and MRI for multimodal 3D histology reconstruction: Application to the Allen human brain atlas
- **Arxiv ID**: http://arxiv.org/abs/2104.14873v3
- **DOI**: 10.1016/j.media.2021.102265
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.14873v3)
- **Published**: 2021-04-30 09:57:33+00:00
- **Updated**: 2021-10-12 20:52:44+00:00
- **Authors**: Adri√† Casamitjana, Marco Lorenzi, Sebastiano Ferraris, Loc Peter, Marc Modat, Allison Stevens, Bruce Fischl, Tom Vercauteren, Juan Eugenio Iglesias
- **Comment**: Medical Image Analysis (Accepted on 07/10/2021). 22 pages, 11
  figures,
- **Journal**: None
- **Summary**: Joint registration of a stack of 2D histological sections to recover 3D structure (``3D histology reconstruction'') finds application in areas such as atlas building and validation of \emph{in vivo} imaging. Straightforward pairwise registration of neighbouring sections yields smooth reconstructions but has well-known problems such as ``banana effect'' (straightening of curved structures) and ``z-shift'' (drift). While these problems can be alleviated with an external, linearly aligned reference (e.g., Magnetic Resonance (MR) images), registration is often inaccurate due to contrast differences and the strong nonlinear distortion of the tissue, including artefacts such as folds and tears. In this paper, we present a probabilistic model of spatial deformation that yields reconstructions for multiple histological stains that that are jointly smooth, robust to outliers, and follow the reference shape. The model relies on a spanning tree of latent transforms connecting all the sections and slices of the reference volume, and assumes that the registration between any pair of images can be see as a noisy version of the composition of (possibly inverted) latent transforms connecting the two images. Bayesian inference is used to compute the most likely latent transforms given a set of pairwise registrations between image pairs within and across modalities. The framework is used for accurate 3D reconstruction of two stains (Nissl and parvalbumin) from the Allen human brain atlas, showing its benefits on real data with severe distortions. Moreover, we also provide the registration of the reconstructed volume to MNI space, bridging the gaps between two of the most widely used atlases in histology and MRI. The 3D reconstructed volumes and atlas registration can be downloaded from https://openneuro.org/datasets/ds003590. The code is freely available at https://github.com/acasamitjana/3dhirest.



### Vehicle Re-identification Method Based on Vehicle Attribute and Mutual Exclusion Between Cameras
- **Arxiv ID**: http://arxiv.org/abs/2104.14882v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.14882v1)
- **Published**: 2021-04-30 10:11:46+00:00
- **Updated**: 2021-04-30 10:11:46+00:00
- **Authors**: Junru Chen, Shiqing Geng, Yongluan Yan, Danyang Huang, Hao Liu, Yadong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle Re-identification aims to identify a specific vehicle across time and camera view. With the rapid growth of intelligent transportation systems and smart cities, vehicle Re-identification technology gets more and more attention. However, due to the difference of shooting angle and the high similarity of vehicles belonging to the same brand, vehicle re-identification becomes a great challenge for existing method. In this paper, we propose a vehicle attribute-guided method to re-rank vehicle Re-ID result. The attributes used include vehicle orientation and vehicle brand . We also focus on the camera information and introduce camera mutual exclusion theory to further fine-tune the search results. In terms of feature extraction, we combine the data augmentations of multi-resolutions with the large model ensemble to get a more robust vehicle features. Our method achieves mAP of 63.73% and rank-1 accuracy 76.61% in the CVPR 2021 AI City Challenge.



### Deep Learning Based Steel Pipe Weld Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.14907v2
- **DOI**: 10.1080/08839514.2021.1975391
- **Categories**: **cs.CV**, cs.AI, 68T07, 65D19, I.4.0; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2104.14907v2)
- **Published**: 2021-04-30 11:15:13+00:00
- **Updated**: 2021-11-20 04:39:38+00:00
- **Authors**: Dingming Yang, Yanrong Cui, Zeyu Yu, Hongqiang Yuan
- **Comment**: 17 pages,8 figures
- **Journal**: None
- **Summary**: Steel pipes are widely used in high-risk and high-pressure scenarios such as oil, chemical, natural gas, shale gas, etc. If there is some defect in steel pipes, it will lead to serious adverse consequences. Applying object detection in the field of deep learning to pipe weld defect detection and identification can effectively improve inspection efficiency and promote the development of industrial automation. Most predecessors used traditional computer vision methods applied to detect defects of steel pipe weld seams. However, traditional computer vision methods rely on prior knowledge and can only detect defects with a single feature, so it is difficult to complete the task of multi-defect classification, while deep learning is end-to-end. In this paper, the state-of-the-art single-stage object detection algorithm YOLOv5 is proposed to be applied to the field of steel pipe weld defect detection, and compared with the two-stage representative object detection algorithm Faster R-CNN. The experimental results show that applying YOLOv5 to steel pipe weld defect detection can greatly improve the accuracy, complete the multi-classification task, and meet the criteria of real-time detection.



### Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.14913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14913v1)
- **Published**: 2021-04-30 11:20:02+00:00
- **Updated**: 2021-04-30 11:20:02+00:00
- **Authors**: Yichao Yan, Jie Qin1, Jiaxin Chen, Li Liu, Fan Zhu, Ying Tai, Ling Shao
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Video-based person re-identification (re-ID) is an important research topic in computer vision. The key to tackling the challenging task is to exploit both spatial and temporal clues in video sequences. In this work, we propose a novel graph-based framework, namely Multi-Granular Hypergraph (MGH), to pursue better representational capabilities by modeling spatiotemporal dependencies in terms of multiple granularities. Specifically, hypergraphs with different spatial granularities are constructed using various levels of part-based features across the video sequence. In each hypergraph, different temporal granularities are captured by hyperedges that connect a set of graph nodes (i.e., part-based features) across different temporal ranges. Two critical issues (misalignment and occlusion) are explicitly addressed by the proposed hypergraph propagation and feature aggregation schemes. Finally, we further enhance the overall video representation by learning more diversified graph-level representations of multiple granularities based on mutual information minimization. Extensive experiments on three widely adopted benchmarks clearly demonstrate the effectiveness of the proposed framework. Notably, 90.0% top-1 accuracy on MARS is achieved using MGH, outperforming the state-of-the-arts. Code is available at https://github.com/daodaofr/hypergraph_reid.



### Certifying Emergency Landing for Safe Urban UAV
- **Arxiv ID**: http://arxiv.org/abs/2104.14928v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.14928v1)
- **Published**: 2021-04-30 11:47:46+00:00
- **Updated**: 2021-04-30 11:47:46+00:00
- **Authors**: Joris Guerin, Kevin Delmas, J√©r√©mie Guiochet
- **Comment**: 8 pages, 4 figure, 4 tables To appear in the proceedings of the 7th
  international workshop on Safety and Security of Intelligent Vehicles (SSIV
  2021) at DSN 2021
- **Journal**: None
- **Summary**: Unmanned Aerial Vehicles (UAVs) have the potential to be used for many applications in urban environments. However, allowing UAVs to fly above densely populated areas raises concerns regarding safety. One of the main safety issues is the possibility for a failure to cause the loss of navigation capabilities, which can result in the UAV falling/landing in hazardous areas such as busy roads, where it can cause fatal accidents. Current standards, such as the SORA published in 2019, do not consider applicable mitigation techniques to handle this kind of hazardous situations. Consequently, certifying UAV urban operations implies to demonstrate very high levels of integrity, which results in prohibitive development costs. To address this issue, this paper explores the concept of Emergency Landing (EL). A safety analysis is conducted on an urban UAV case study, and requirements are proposed to enable the integration of EL as an acceptable mitigation mean in the SORA. Based on these requirements, an EL implementation was developed, together with a runtime monitoring architecture to enhance confidence in the system. Preliminary qualitative results are presented and the monitor seem to be able to detect errors of the EL system effectively.



### Evaluating Contrastive Models for Instance-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2104.14939v1
- **DOI**: 10.1145/3460426.3463585
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14939v1)
- **Published**: 2021-04-30 12:05:23+00:00
- **Updated**: 2021-04-30 12:05:23+00:00
- **Authors**: Tarun Krishna, Kevin McGuinness, Noel O'Connor
- **Comment**: Accepted In Proceedings of the 2021 International Conference on
  Multimedia Retrieval (ICMR 21)
- **Journal**: None
- **Summary**: In this work, we evaluate contrastive models for the task of image retrieval. We hypothesise that models that are learned to encode semantic similarity among instances via discriminative learning should perform well on the task of image retrieval, where relevancy is defined in terms of instances of the same object. Through our extensive evaluation, we find that representations from models trained using contrastive methods perform on-par with (and outperforms) a pre-trained supervised baseline trained on the ImageNet labels in retrieval tasks under various configurations. This is remarkable given that the contrastive models require no explicit supervision. Thus, we conclude that these models can be used to bootstrap base models to build more robust image retrieval engines.



### Anomaly Detection with Prototype-Guided Discriminative Latent Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2104.14945v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14945v3)
- **Published**: 2021-04-30 12:16:52+00:00
- **Updated**: 2021-09-02 11:00:30+00:00
- **Authors**: Yuandu Lai, Yahong Han, Yaowei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent efforts towards video anomaly detection (VAD) try to learn a deep autoencoder to describe normal event patterns with small reconstruction errors. The video inputs with large reconstruction errors are regarded as anomalies at the test time. However, these methods sometimes reconstruct abnormal inputs well because of the powerful generalization ability of deep autoencoder. To address this problem, we present a novel approach for anomaly detection, which utilizes discriminative prototypes of normal data to reconstruct video frames. In this way, the model will favor the reconstruction of normal events and distort the reconstruction of abnormal events. Specifically, we use a prototype-guided memory module to perform discriminative latent embedding. We introduce a new discriminative criterion for the memory module, as well as a loss function correspondingly, which can encourage memory items to record the representative embeddings of normal data, i.e. prototypes. Besides, we design a novel two-branch autoencoder, which is composed of a future frame prediction network and an RGB difference generation network that share the same encoder. The stacked RGB difference contains motion information just like optical flow, so our model can learn temporal regularity. We evaluate the effectiveness of our method on three benchmark datasets and experimental results demonstrate the proposed method outperforms the state-of-the-art.



### SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2104.14951v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14951v2)
- **Published**: 2021-04-30 12:31:25+00:00
- **Updated**: 2021-05-18 14:41:12+00:00
- **Authors**: Haoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhihai Xu, Qi Li, Yueting Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution (SISR) aims to reconstruct high-resolution (HR) images from the given low-resolution (LR) ones, which is an ill-posed problem because one LR image corresponds to multiple HR images. Recently, learning-based SISR methods have greatly outperformed traditional ones, while suffering from over-smoothing, mode collapse or large model footprint issues for PSNR-oriented, GAN-driven and flow-based methods respectively. To solve these problems, we propose a novel single image super-resolution diffusion probabilistic model (SRDiff), which is the first diffusion-based model for SISR. SRDiff is optimized with a variant of the variational bound on the data likelihood and can provide diverse and realistic SR predictions by gradually transforming the Gaussian noise into a super-resolution (SR) image conditioned on an LR input through a Markov chain. In addition, we introduce residual prediction to the whole framework to speed up convergence. Our extensive experiments on facial and general benchmarks (CelebA and DIV2K datasets) show that 1) SRDiff can generate diverse SR results in rich details with state-of-the-art performance, given only one LR input; 2) SRDiff is easy to train with a small footprint; and 3) SRDiff can perform flexible image manipulation including latent space interpolation and content fusion.



### Determining Chess Game State From an Image
- **Arxiv ID**: http://arxiv.org/abs/2104.14963v2
- **DOI**: 10.3390/jimaging7060094
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.14963v2)
- **Published**: 2021-04-30 13:02:13+00:00
- **Updated**: 2021-06-02 14:27:50+00:00
- **Authors**: Georg W√∂lflein, Ognjen Arandjeloviƒá
- **Comment**: https://github.com/georgw777/chesscog
- **Journal**: J. Imaging 2021, 7(6), 94
- **Summary**: Identifying the configuration of chess pieces from an image of a chessboard is a problem in computer vision that has not yet been solved accurately. However, it is important for helping amateur chess players improve their games by facilitating automatic computer analysis without the overhead of manually entering the pieces. Current approaches are limited by the lack of large datasets and are not designed to adapt to unseen chess sets. This paper puts forth a new dataset synthesised from a 3D model that is an order of magnitude larger than existing ones. Trained on this dataset, a novel end-to-end chess recognition system is presented that combines traditional computer vision techniques with deep learning. It localises the chessboard using a RANSAC-based algorithm that computes a projective transformation of the board onto a regular grid. Using two convolutional neural networks, it then predicts an occupancy mask for the squares in the warped image and finally classifies the pieces. The described system achieves an error rate of 0.23% per square on the test set, 28 times better than the current state of the art. Further, a few-shot transfer learning approach is developed that is able to adapt the inference system to a previously unseen chess set using just two photos of the starting position, obtaining a per-square accuracy of 99.83% on images of that new chess set. The code, dataset, and trained models are made available online.



### Deep learning with self-supervision and uncertainty regularization to count fish in underwater images
- **Arxiv ID**: http://arxiv.org/abs/2104.14964v1
- **DOI**: 10.1371/journal.pone.0267759
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.14964v1)
- **Published**: 2021-04-30 13:02:19+00:00
- **Updated**: 2021-04-30 13:02:19+00:00
- **Authors**: Penny Tarling, Mauricio Cantor, Albert Clap√©s, Sergio Escalera
- **Comment**: 22 pages, 6 figures, submitted to indexed journal
- **Journal**: None
- **Summary**: Effective conservation actions require effective population monitoring. However, accurately counting animals in the wild to inform conservation decision-making is difficult. Monitoring populations through image sampling has made data collection cheaper, wide-reaching and less intrusive but created a need to process and analyse this data efficiently. Counting animals from such data is challenging, particularly when densely packed in noisy images. Attempting this manually is slow and expensive, while traditional computer vision methods are limited in their generalisability. Deep learning is the state-of-the-art method for many computer vision tasks, but it has yet to be properly explored to count animals. To this end, we employ deep learning, with a density-based regression approach, to count fish in low-resolution sonar images. We introduce a large dataset of sonar videos, deployed to record wild mullet schools (Mugil liza), with a subset of 500 labelled images. We utilise abundant unlabelled data in a self-supervised task to improve the supervised counting task. For the first time in this context, by introducing uncertainty quantification, we improve model training and provide an accompanying measure of prediction uncertainty for more informed biological decision-making. Finally, we demonstrate the generalisability of our proposed counting framework through testing it on a recent benchmark dataset of high-resolution annotated underwater images from varying habitats (DeepFish). From experiments on both contrasting datasets, we demonstrate our network outperforms the few other deep learning models implemented for solving this task. By providing an open-source framework along with training data, our study puts forth an efficient deep learning template for crowd counting aquatic animals thereby contributing effective methods to assess natural populations from the ever-increasing visual data.



### Unsupervised data augmentation for object detection
- **Arxiv ID**: http://arxiv.org/abs/2104.14965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14965v1)
- **Published**: 2021-04-30 13:02:42+00:00
- **Updated**: 2021-04-30 13:02:42+00:00
- **Authors**: Yichen Zhang, Zeyang Song, Wenbo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation has always been an effective way to overcome overfitting issue when the dataset is small. There are already lots of augmentation operations such as horizontal flip, random crop or even Mixup. However, unlike image classification task, we cannot simply perform these operations for object detection task because of the lack of labeled bounding boxes information for corresponding generated images. To address this challenge, we propose a framework making use of Generative Adversarial Networks(GAN) to perform unsupervised data augmentation. To be specific, based on the recently supreme performance of YOLOv4, we propose a two-step pipeline that enables us to generate an image where the object lies in a certain position. In this way, we can accomplish the goal that generating an image with bounding box label.



### Using brain inspired principles to unsupervisedly learn good representations for visual pattern recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.14970v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2104.14970v1)
- **Published**: 2021-04-30 13:08:14+00:00
- **Updated**: 2021-04-30 13:08:14+00:00
- **Authors**: Luis Sa-Couto, Andreas Wichert
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep learning has solved difficult problems in visual pattern recognition, it is mostly successful in tasks where there are lots of labeled training data available. Furthermore, the global back-propagation based training rule and the amount of employed layers represents a departure from biological inspiration. The brain is able to perform most of these tasks in a very general way from limited to no labeled data. For these reasons it is still a key research question to look into computational principles in the brain that can help guide models to unsupervisedly learn good representations which can then be used to perform tasks like classification. In this work we explore some of these principles to generate such representations for the MNIST data set. We compare the obtained results with similar recent works and verify extremely competitive results.



### CAT: Cross-Attention Transformer for One-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.14984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14984v1)
- **Published**: 2021-04-30 13:18:53+00:00
- **Updated**: 2021-04-30 13:18:53+00:00
- **Authors**: Weidong Lin, Yuyan Deng, Yang Gao, Ning Wang, Jinghao Zhou, Lingqiao Liu, Lei Zhang, Peng Wang
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Given a query patch from a novel class, one-shot object detection aims to detect all instances of that class in a target image through the semantic similarity comparison. However, due to the extremely limited guidance in the novel class as well as the unseen appearance difference between query and target instances, it is difficult to appropriately exploit their semantic similarity and generalize well. To mitigate this problem, we present a universal Cross-Attention Transformer (CAT) module for accurate and efficient semantic similarity comparison in one-shot object detection. The proposed CAT utilizes transformer mechanism to comprehensively capture bi-directional correspondence between any paired pixels from the query and the target image, which empowers us to sufficiently exploit their semantic characteristics for accurate similarity comparison. In addition, the proposed CAT enables feature dimensionality compression for inference speedup without performance loss. Extensive experiments on COCO, VOC, and FSOD under one-shot settings demonstrate the effectiveness and efficiency of our method, e.g., it surpasses CoAE, a major baseline in this task by 1.0% in AP on COCO and runs nearly 2.5 times faster. Code will be available in the future.



### Interpretable Semantic Photo Geolocation
- **Arxiv ID**: http://arxiv.org/abs/2104.14995v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14995v2)
- **Published**: 2021-04-30 13:28:18+00:00
- **Updated**: 2021-10-20 17:49:53+00:00
- **Authors**: Jonas Theiner, Eric Mueller-Budack, Ralph Ewerth
- **Comment**: Accepted for publication at WACV'22
- **Journal**: None
- **Summary**: Planet-scale photo geolocalization is the complex task of estimating the location depicted in an image solely based on its visual content. Due to the success of convolutional neural networks (CNNs), current approaches achieve super-human performance. However, previous work has exclusively focused on optimizing geolocalization accuracy. Due to the black-box property of deep learning systems, their predictions are difficult to validate for humans. State-of-the-art methods treat the task as a classification problem, where the choice of the classes, that is the partitioning of the world map, is crucial for the performance. In this paper, we present two contributions to improve the interpretability of a geolocalization model: (1) We propose a novel semantic partitioning method which intuitively leads to an improved understanding of the predictions, while achieving state-of-the-art results for geolocational accuracy on benchmark test sets; (2) We introduce a metric to assess the importance of semantic visual concepts for a certain prediction to provide additional interpretable information, which allows for a large-scale analysis of already trained models. Source code and dataset are publicly available.



### RR-Net: Injecting Interactive Semantics in Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.15015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.15015v1)
- **Published**: 2021-04-30 14:03:10+00:00
- **Updated**: 2021-04-30 14:03:10+00:00
- **Authors**: Dongming Yang, Yuexian Zou, Can Zhang, Meng Cao, Jie Chen
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) detection devotes to learn how humans interact with surrounding objects. Latest end-to-end HOI detectors are short of relation reasoning, which leads to inability to learn HOI-specific interactive semantics for predictions. In this paper, we therefore propose novel relation reasoning for HOI detection. We first present a progressive Relation-aware Frame, which brings a new structure and parameter sharing pattern for interaction inference. Upon the frame, an Interaction Intensifier Module and a Correlation Parsing Module are carefully designed, where: a) interactive semantics from humans can be exploited and passed to objects to intensify interactions, b) interactive correlations among humans, objects and interactions are integrated to promote predictions. Based on modules above, we construct an end-to-end trainable framework named Relation Reasoning Network (abbr. RR-Net). Extensive experiments show that our proposed RR-Net sets a new state-of-the-art on both V-COCO and HICO-DET benchmarks and improves the baseline about 5.5% and 9.8% relatively, validating that this first effort in exploring relation reasoning and integrating interactive semantics has brought obvious improvement for end-to-end HOI detection.



### Deep Image Destruction: Vulnerability of Deep Image-to-Image Models against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2104.15022v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.15022v2)
- **Published**: 2021-04-30 14:20:33+00:00
- **Updated**: 2022-06-28 04:13:28+00:00
- **Authors**: Jun-Ho Choi, Huan Zhang, Jun-Hyuk Kim, Cho-Jui Hsieh, Jong-Seok Lee
- **Comment**: ICPR2022
- **Journal**: None
- **Summary**: Recently, the vulnerability of deep image classification models to adversarial attacks has been investigated. However, such an issue has not been thoroughly studied for image-to-image tasks that take an input image and generate an output image (e.g., colorization, denoising, deblurring, etc.) This paper presents comprehensive investigations into the vulnerability of deep image-to-image models to adversarial attacks. For five popular image-to-image tasks, 16 deep models are analyzed from various standpoints such as output quality degradation due to attacks, transferability of adversarial examples across different tasks, and characteristics of perturbations. We show that unlike image classification tasks, the performance degradation on image-to-image tasks largely differs depending on various factors, e.g., attack methods and task objectives. In addition, we analyze the effectiveness of conventional defense methods used for classification models in improving the robustness of the image-to-image models.



### Post-training deep neural network pruning via layer-wise calibration
- **Arxiv ID**: http://arxiv.org/abs/2104.15023v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.15023v1)
- **Published**: 2021-04-30 14:20:51+00:00
- **Updated**: 2021-04-30 14:20:51+00:00
- **Authors**: Ivan Lazarevich, Alexander Kozlov, Nikita Malinin
- **Comment**: None
- **Journal**: None
- **Summary**: We present a post-training weight pruning method for deep neural networks that achieves accuracy levels tolerable for the production setting and that is sufficiently fast to be run on commodity hardware such as desktop CPUs or edge devices. We propose a data-free extension of the approach for computer vision models based on automatically-generated synthetic fractal images. We obtain state-of-the-art results for data-free neural network pruning, with ~1.5% top@1 accuracy drop for a ResNet50 on ImageNet at 50% sparsity rate. When using real data, we are able to get a ResNet50 model on ImageNet with 65% sparsity rate in 8-bit precision in a post-training setting with a ~1% top@1 accuracy drop. We release the code as a part of the OpenVINO(TM) Post-Training Optimization tool.



### Updatable Siamese Tracker with Two-stage One-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.15049v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.15049v1)
- **Published**: 2021-04-30 15:18:41+00:00
- **Updated**: 2021-04-30 15:18:41+00:00
- **Authors**: Xinglong Sun, Guangliang Han, Lihong Guo, Tingfa Xu, Jianan Li, Peixun Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Offline Siamese networks have achieved very promising tracking performance, especially in accuracy and efficiency. However, they often fail to track an object in complex scenes due to the incapacity in online update. Traditional updaters are difficult to process the irregular variations and sampling noises of objects, so it is quite risky to adopt them to update Siamese networks. In this paper, we first present a two-stage one-shot learner, which can predict the local parameters of primary classifier with object samples from diverse stages. Then, an updatable Siamese network is proposed based on the learner (SiamTOL), which is able to complement online update by itself. Concretely, we introduce an extra inputting branch to sequentially capture the latest object features, and design a residual module to update the initial exemplar using these features. Besides, an effective multi-aspect training loss is designed for our network to avoid overfit. Extensive experimental results on several popular benchmarks including OTB100, VOT2018, VOT2019, LaSOT, UAV123 and GOT10k manifest that the proposed tracker achieves the leading performance and outperforms other state-of-the-art methods



### DriveGAN: Towards a Controllable High-Quality Neural Simulation
- **Arxiv ID**: http://arxiv.org/abs/2104.15060v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.15060v1)
- **Published**: 2021-04-30 15:30:05+00:00
- **Updated**: 2021-04-30 15:30:05+00:00
- **Authors**: Seung Wook Kim, Jonah Philion, Antonio Torralba, Sanja Fidler
- **Comment**: CVPR 2021 Oral
- **Journal**: None
- **Summary**: Realistic simulators are critical for training and verifying robotics systems. While most of the contemporary simulators are hand-crafted, a scaleable way to build simulators is to use machine learning to learn how the environment behaves in response to an action, directly from data. In this work, we aim to learn to simulate a dynamic environment directly in pixel-space, by watching unannotated sequences of frames and their associated action pairs. We introduce a novel high-quality neural simulator referred to as DriveGAN that achieves controllability by disentangling different components without supervision. In addition to steering controls, it also includes controls for sampling features of a scene, such as the weather as well as the location of non-player objects. Since DriveGAN is a fully differentiable simulator, it further allows for re-simulation of a given video sequence, offering an agent to drive through a recorded scene again, possibly taking different actions. We train DriveGAN on multiple datasets, including 160 hours of real-world driving data. We showcase that our approach greatly surpasses the performance of previous data-driven simulators, and allows for new features not explored before.



### Black-box adversarial attacks using Evolution Strategies
- **Arxiv ID**: http://arxiv.org/abs/2104.15064v1
- **DOI**: 10.1145/3449726.3463137
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2104.15064v1)
- **Published**: 2021-04-30 15:33:07+00:00
- **Updated**: 2021-04-30 15:33:07+00:00
- **Authors**: Hao Qiu, Leonardo Lucio Custode, Giovanni Iacca
- **Comment**: To be published in the proceedings of ACM Genetic and Evolutionary
  Computation Conference (GECCO) Companion 2021
- **Journal**: None
- **Summary**: In the last decade, deep neural networks have proven to be very powerful in computer vision tasks, starting a revolution in the computer vision and machine learning fields. However, deep neural networks, usually, are not robust to perturbations of the input data. In fact, several studies showed that slightly changing the content of the images can cause a dramatic decrease in the accuracy of the attacked neural network. Several methods able to generate adversarial samples make use of gradients, which usually are not available to an attacker in real-world scenarios. As opposed to this class of attacks, another class of adversarial attacks, called black-box adversarial attacks, emerged, which does not make use of information on the gradients, being more suitable for real-world attack scenarios. In this work, we compare three well-known evolution strategies on the generation of black-box adversarial attacks for image classification tasks. While our results show that the attacked neural networks can be, in most cases, easily fooled by all the algorithms under comparison, they also show that some black-box optimization algorithms may be better in "harder" setups, both in terms of attack success rate and efficiency (i.e., number of queries).



### A Good Image Generator Is What You Need for High-Resolution Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2104.15069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.15069v1)
- **Published**: 2021-04-30 15:38:41+00:00
- **Updated**: 2021-04-30 15:38:41+00:00
- **Authors**: Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, Sergey Tulyakov
- **Comment**: Accepted to ICLR 2021
- **Journal**: None
- **Summary**: Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques. Code will be released at https://github.com/snap-research/MoCoGAN-HD.



### Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2104.15082v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.15082v2)
- **Published**: 2021-04-30 16:04:19+00:00
- **Updated**: 2021-05-19 01:44:41+00:00
- **Authors**: Zeqi Li, Ruowei Jiang, Parham Aarabi
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have shown significant potential in modeling high dimensional distributions of image data, especially on image-to-image translation tasks. However, due to the complexity of these tasks, state-of-the-art models often contain a tremendous amount of parameters, which results in large model size and long inference time. In this work, we propose a novel method to address this problem by applying knowledge distillation together with distillation of a semantic relation preserving matrix. This matrix, derived from the teacher's feature encoding, helps the student model learn better semantic relations. In contrast to existing compression methods designed for classification tasks, our proposed method adapts well to the image-to-image translation task on GANs. Experiments conducted on 5 different datasets and 3 different pairs of teacher and student models provide strong evidence that our methods achieve impressive results both qualitatively and quantitatively.



### Faster Meta Update Strategy for Noise-Robust Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.15092v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.15092v1)
- **Published**: 2021-04-30 16:19:07+00:00
- **Updated**: 2021-04-30 16:19:07+00:00
- **Authors**: Youjiang Xu, Linchao Zhu, Lu Jiang, Yi Yang
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: It has been shown that deep neural networks are prone to overfitting on biased training data. Towards addressing this issue, meta-learning employs a meta model for correcting the training bias. Despite the promising performances, super slow training is currently the bottleneck in the meta learning approaches. In this paper, we introduce a novel Faster Meta Update Strategy (FaMUS) to replace the most expensive step in the meta gradient computation with a faster layer-wise approximation. We empirically find that FaMUS yields not only a reasonably accurate but also a low-variance approximation of the meta gradient. We conduct extensive experiments to verify the proposed method on two tasks. We show our method is able to save two-thirds of the training time while still maintaining the comparable or achieving even better generalization performance. In particular, our method achieves the state-of-the-art performance on both synthetic and realistic noisy labels, and obtains promising performance on long-tailed recognition on standard benchmarks.



### Deep Multi-View Stereo gone wild
- **Arxiv ID**: http://arxiv.org/abs/2104.15119v2
- **DOI**: 10.1109/3DV53792.2021.00058
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.15119v2)
- **Published**: 2021-04-30 17:07:17+00:00
- **Updated**: 2021-12-02 09:56:50+00:00
- **Authors**: Fran√ßois Darmon, B√©n√©dicte Bascle, Jean-Cl√©ment Devaux, Pascal Monasse, Mathieu Aubry
- **Comment**: Accepted to 3DV2021
- **Journal**: None
- **Summary**: Deep multi-view stereo (MVS) methods have been developed and extensively compared on simple datasets, where they now outperform classical approaches. In this paper, we ask whether the conclusions reached in controlled scenarios are still valid when working with Internet photo collections. We propose a methodology for evaluation and explore the influence of three aspects of deep MVS methods: network architecture, training data, and supervision. We make several key observations, which we extensively validate quantitatively and qualitatively, both for depth prediction and complete 3D reconstructions. First, complex unsupervised approaches cannot train on data in the wild. Our new approach makes it possible with three key elements: upsampling the output, softmin based aggregation and a single reconstruction loss. Second, supervised deep depthmap-based MVS methods are state-of-the art for reconstruction of few internet images. Finally, our evaluation provides very different results than usual ones. This shows that evaluation in uncontrolled scenarios is important for new architectures.



### Differentiable Event Stream Simulator for Non-Rigid 3D Tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.15139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.15139v1)
- **Published**: 2021-04-30 17:58:07+00:00
- **Updated**: 2021-04-30 17:58:07+00:00
- **Authors**: Jalees Nehvi, Vladislav Golyanik, Franziska Mueller, Hans-Peter Seidel, Mohamed Elgharib, Christian Theobalt
- **Comment**: In CVPR 2021 Workshop on Event-based Vision. Project page:
  http://gvv.mpi-inf.mpg.de/projects/Event-based_Non-rigid_3D_Tracking
- **Journal**: None
- **Summary**: This paper introduces the first differentiable simulator of event streams, i.e., streams of asynchronous brightness change signals recorded by event cameras. Our differentiable simulator enables non-rigid 3D tracking of deformable objects (such as human hands, isometric surfaces and general watertight meshes) from event streams by leveraging an analysis-by-synthesis principle. So far, event-based tracking and reconstruction of non-rigid objects in 3D, like hands and body, has been either tackled using explicit event trajectories or large-scale datasets. In contrast, our method does not require any such processing or data, and can be readily applied to incoming event streams. We show the effectiveness of our approach for various types of non-rigid objects and compare to existing methods for non-rigid 3D tracking. In our experiments, the proposed energy-based formulations outperform competing RGB-based methods in terms of 3D errors. The source code and the new data are publicly available.



### Continuous Face Aging via Self-estimated Residual Age Embedding
- **Arxiv ID**: http://arxiv.org/abs/2105.00020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00020v1)
- **Published**: 2021-04-30 18:06:17+00:00
- **Updated**: 2021-04-30 18:06:17+00:00
- **Authors**: Zeqi Li, Ruowei Jiang, Parham Aarabi
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Face synthesis, including face aging, in particular, has been one of the major topics that witnessed a substantial improvement in image fidelity by using generative adversarial networks (GANs). Most existing face aging approaches divide the dataset into several age groups and leverage group-based training strategies, which lacks the ability to provide fine-controlled continuous aging synthesis in nature. In this work, we propose a unified network structure that embeds a linear age estimator into a GAN-based model, where the embedded age estimator is trained jointly with the encoder and decoder to estimate the age of a face image and provide a personalized target age embedding for age progression/regression. The personalized target age embedding is synthesized by incorporating both personalized residual age embedding of the current age and exemplar-face aging basis of the target age, where all preceding aging bases are derived from the learned weights of the linear age estimator. This formulation brings the unified perspective of estimating the age and generating personalized aged face, where self-estimated age embeddings can be learned for every single age. The qualitative and quantitative evaluations on different datasets further demonstrate the significant improvement in the continuous face aging aspect over the state-of-the-art.



### Submodular Mutual Information for Targeted Data Subset Selection
- **Arxiv ID**: http://arxiv.org/abs/2105.00043v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.00043v1)
- **Published**: 2021-04-30 18:53:09+00:00
- **Updated**: 2021-04-30 18:53:09+00:00
- **Authors**: Suraj Kothawade, Vishal Kaushal, Ganesh Ramakrishnan, Jeff Bilmes, Rishabh Iyer
- **Comment**: Accepted to ICLR 2021 S2D-OLAD Workshop; https://s2d-olad.github.io/.
  arXiv admin note: substantial text overlap with arXiv:2103.00128
- **Journal**: None
- **Summary**: With the rapid growth of data, it is becoming increasingly difficult to train or improve deep learning models with the right subset of data. We show that this problem can be effectively solved at an additional labeling cost by targeted data subset selection(TSS) where a subset of unlabeled data points similar to an auxiliary set are added to the training data. We do so by using a rich class of Submodular Mutual Information (SMI) functions and demonstrate its effectiveness for image classification on CIFAR-10 and MNIST datasets. Lastly, we compare the performance of SMI functions for TSS with other state-of-the-art methods for closely related problems like active learning. Using SMI functions, we observe ~20-30% gain over the model's performance before re-training with added targeted subset; ~12% more than other methods.



### Unsupervised Discriminative Embedding for Sub-Action Learning in Complex Activities
- **Arxiv ID**: http://arxiv.org/abs/2105.00067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00067v1)
- **Published**: 2021-04-30 20:07:27+00:00
- **Updated**: 2021-04-30 20:07:27+00:00
- **Authors**: Sirnam Swetha, Hilde Kuehne, Yogesh S Rawat, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition and detection in the context of long untrimmed video sequences has seen an increased attention from the research community. However, annotation of complex activities is usually time consuming and challenging in practice. Therefore, recent works started to tackle the problem of unsupervised learning of sub-actions in complex activities. This paper proposes a novel approach for unsupervised sub-action learning in complex activities. The proposed method maps both visual and temporal representations to a latent space where the sub-actions are learnt discriminatively in an end-to-end fashion. To this end, we propose to learn sub-actions as latent concepts and a novel discriminative latent concept learning (DLCL) module aids in learning sub-actions. The proposed DLCL module lends on the idea of latent concepts to learn compact representations in the latent embedding space in an unsupervised way. The result is a set of latent vectors that can be interpreted as cluster centers in the embedding space. The latent space itself is formed by a joint visual and temporal embedding capturing the visual similarity and temporal ordering of the data. Our joint learning with discriminative latent concept module is novel which eliminates the need for explicit clustering. We validate our approach on three benchmark datasets and show that the proposed combination of visual-temporal embedding and discriminative latent concepts allow to learn robust action representations in an unsupervised setting.



### Self-supervised Augmentation Consistency for Adapting Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.00097v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.00097v1)
- **Published**: 2021-04-30 21:32:40+00:00
- **Updated**: 2021-04-30 21:32:40+00:00
- **Authors**: Nikita Araslanov, Stefan Roth
- **Comment**: To appear at CVPR 2021. Code: https://github.com/visinf/da-sac
- **Journal**: None
- **Summary**: We propose an approach to domain adaptation for semantic segmentation that is both practical and highly accurate. In contrast to previous work, we abandon the use of computationally involved adversarial objectives, network ensembles and style transfer. Instead, we employ standard data augmentation techniques $-$ photometric noise, flipping and scaling $-$ and ensure consistency of the semantic predictions across these image transformations. We develop this principle in a lightweight self-supervised framework trained on co-evolving pseudo labels without the need for cumbersome extra training rounds. Simple in training from a practitioner's standpoint, our approach is remarkably effective. We achieve significant improvements of the state-of-the-art segmentation accuracy after adaptation, consistent both across different choices of the backbone architecture and adaptation scenarios.



### Embedding Semantic Hierarchy in Discrete Optimal Transport for Risk Minimization
- **Arxiv ID**: http://arxiv.org/abs/2105.00101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00101v1)
- **Published**: 2021-04-30 21:47:36+00:00
- **Updated**: 2021-04-30 21:47:36+00:00
- **Authors**: Yubin Ge, Site Li, Xuyang Li, Fangfang Fan, Wanqing Xie, Jane You, Xiaofeng Liu
- **Comment**: Accepted to IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP) 2021
- **Journal**: None
- **Summary**: The widely-used cross-entropy (CE) loss-based deep networks achieved significant progress w.r.t. the classification accuracy. However, the CE loss can essentially ignore the risk of misclassification which is usually measured by the distance between the prediction and label in a semantic hierarchical tree. In this paper, we propose to incorporate the risk-aware inter-class correlation in a discrete optimal transport (DOT) training framework by configuring its ground distance matrix. The ground distance matrix can be pre-defined following a priori of hierarchical semantic risk. Specifically, we define the tree induced error (TIE) on a hierarchical semantic tree and extend it to its increasing function from the optimization perspective. The semantic similarity in each level of a tree is integrated with the information gain. We achieve promising results on several large scale image classification tasks with a semantic tree structure in a plug and play manner.



### Classifying States of Cooking Objects Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2105.14196v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.14196v1)
- **Published**: 2021-04-30 22:26:40+00:00
- **Updated**: 2021-04-30 22:26:40+00:00
- **Authors**: Qi Zheng
- **Comment**: 6 pages,9 figures, 5 tables
- **Journal**: None
- **Summary**: Automated cooking machine is a goal for the future. The main aim is to make the cooking process easier, safer, and create human welfare. To allow robots to accurately perform the cooking activities, it is important for them to understand the cooking environment and recognize the objects, especially correctly identifying the state of the cooking objects. This will significantly improve the correctness of the following cooking recipes. In this project, several parts of the experiment were conducted to design a robust deep convolutional neural network for classifying the state of the cooking objects from scratch. The model is evaluated by using various techniques, such as adjusting architecture layers, tuning key hyperparameters, and using different optimization techniques to maximize the accuracy of state classification.



### IPatch: A Remote Adversarial Patch
- **Arxiv ID**: http://arxiv.org/abs/2105.00113v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.00113v2)
- **Published**: 2021-04-30 22:34:32+00:00
- **Updated**: 2021-06-02 15:21:53+00:00
- **Authors**: Yisroel Mirsky
- **Comment**: None
- **Journal**: None
- **Summary**: Applications such as autonomous vehicles and medical screening use deep learning models to localize and identify hundreds of objects in a single frame. In the past, it has been shown how an attacker can fool these models by placing an adversarial patch within a scene. However, these patches must be placed in the target location and do not explicitly alter the semantics elsewhere in the image.   In this paper, we introduce a new type of adversarial patch which alters a model's perception of an image's semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch. We call this new class of adversarial examples `remote adversarial patches' (RAP).   We implement our own RAP called IPatch and perform an in-depth analysis on image segmentation RAP attacks using five state-of-the-art architectures with eight different encoders on the CamVid street view dataset. Moreover, we demonstrate that the attack can be extended to object recognition models with preliminary results on the popular YOLOv3 model. We found that the patch can change the classification of a remote target region with a success rate of up to 93% on average.



### Improved Real-Time Monocular SLAM Using Semantic Segmentation on Selective Frames
- **Arxiv ID**: http://arxiv.org/abs/2105.00114v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00114v5)
- **Published**: 2021-04-30 22:34:45+00:00
- **Updated**: 2022-12-15 00:13:44+00:00
- **Authors**: Jinkyu Lee, Muhyun Back, Sung Soo Hwang, Il Yong Chun
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular simultaneous localization and mapping (SLAM) is emerging in advanced driver assistance systems and autonomous driving, because a single camera is cheap and easy to install. Conventional monocular SLAM has two major challenges leading inaccurate localization and mapping. First, it is challenging to estimate scales in localization and mapping. Second, conventional monocular SLAM uses inappropriate mapping factors such as dynamic objects and low-parallax areas in mapping. This paper proposes an improved real-time monocular SLAM that resolves the aforementioned challenges by efficiently using deep learning-based semantic segmentation. To achieve the real-time execution of the proposed method, we apply semantic segmentation only to downsampled keyframes in parallel with mapping processes. In addition, the proposed method corrects scales of camera poses and three-dimensional (3D) points, using estimated ground plane from road-labeled 3D points and the real camera height. The proposed method also removes inappropriate corner features labeled as moving objects and low parallax areas. Experiments with eight video sequences demonstrate that the proposed monocular SLAM system achieves significantly improved and comparable trajectory tracking accuracy, compared to existing state-of-the-art monocular and stereo SLAM systems, respectively. The proposed system can achieve real-time tracking on a standard CPU potentially with a standard GPU support, whereas existing segmentation-aided monocular SLAM does not.




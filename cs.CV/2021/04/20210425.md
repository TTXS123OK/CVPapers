# Arxiv Papers in cs.CV on 2021-04-25
### Distractor-Aware Fast Tracking via Dynamic Convolutions and MOT Philosophy
- **Arxiv ID**: http://arxiv.org/abs/2104.12041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12041v1)
- **Published**: 2021-04-25 00:59:53+00:00
- **Updated**: 2021-04-25 00:59:53+00:00
- **Authors**: Zikai Zhang, Bineng Zhong, Shengping Zhang, Zhenjun Tang, Xin Liu, Zhaoxiang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: A practical long-term tracker typically contains three key properties, i.e. an efficient model design, an effective global re-detection strategy and a robust distractor awareness mechanism. However, most state-of-the-art long-term trackers (e.g., Pseudo and re-detecting based ones) do not take all three key properties into account and therefore may either be time-consuming or drift to distractors. To address the issues, we propose a two-task tracking frame work (named DMTrack), which utilizes two core components (i.e., one-shot detection and re-identification (re-id) association) to achieve distractor-aware fast tracking via Dynamic convolutions (d-convs) and Multiple object tracking (MOT) philosophy. To achieve precise and fast global detection, we construct a lightweight one-shot detector using a novel dynamic convolutions generation method, which provides a unified and more flexible way for fusing target information into the search field. To distinguish the target from distractors, we resort to the philosophy of MOT to reason distractors explicitly by maintaining all potential similarities' tracklets. Benefited from the strength of high recall detection and explicit object association, our tracker achieves state-of-the-art performance on the LaSOT, OxUvA, TLP, VOT2018LT and VOT2019LT benchmarks and runs in real-time (3x faster than comparisons).



### Multi-Cycle-Consistent Adversarial Networks for Edge Denoising of Computed Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/2104.12044v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12044v1)
- **Published**: 2021-04-25 01:53:46+00:00
- **Updated**: 2021-04-25 01:53:46+00:00
- **Authors**: Xiaowe Xu, Jiawei Zhang, Jinglan Liu, Yukun Ding, Tianchen Wang, Hailong Qiu, Haiyun Yuan, Jian Zhuang, Wen Xie, Yuhao Dong, Qianjun Jia, Meiping Huang, Yiyu Shi
- **Comment**: 16 pages, 7 figures, 4 tables, accepted by the ACM Journal on
  Emerging Technologies in Computing Systems (JETC). arXiv admin note:
  substantial text overlap with arXiv:2002.12130
- **Journal**: None
- **Summary**: As one of the most commonly ordered imaging tests, computed tomography (CT) scan comes with inevitable radiation exposure that increases the cancer risk to patients. However, CT image quality is directly related to radiation dose, thus it is desirable to obtain high-quality CT images with as little dose as possible. CT image denoising tries to obtain high dose like high-quality CT images (domain X) from low dose low-quality CTimages (domain Y), which can be treated as an image-to-image translation task where the goal is to learn the transform between a source domain X (noisy images) and a target domain Y (clean images). In this paper, we propose a multi-cycle-consistent adversarial network (MCCAN) that builds intermediate domains and enforces both local and global cycle-consistency for edge denoising of CT images. The global cycle-consistency couples all generators together to model the whole denoising process, while the local cycle-consistency imposes effective supervision on the process between adjacent domains. Experiments show that both local and global cycle-consistency are important for the success of MCCAN, which outperformsCCADN in terms of denoising quality with slightly less computation resource consumption.



### Quantization of Deep Neural Networks for Accurate Edge Computing
- **Arxiv ID**: http://arxiv.org/abs/2104.12046v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12046v2)
- **Published**: 2021-04-25 02:05:12+00:00
- **Updated**: 2021-10-14 07:14:14+00:00
- **Authors**: Wentao Chen, Hailong Qiu, Jian Zhuang, Chutong Zhang, Yu Hu, Qing Lu, Tianchen Wang, Yiyu Shi, Meiping Huang, Xiaowe Xu
- **Comment**: 11 pages, 3 figures, 10 tables, accepted by the ACM Journal on
  Emerging Technologies in Computing Systems (JETC)
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have demonstrated their great potential in recent years, exceeding the per-formance of human experts in a wide range of applications. Due to their large sizes, however, compressiontechniques such as weight quantization and pruning are usually applied before they can be accommodated onthe edge. It is generally believed that quantization leads to performance degradation, and plenty of existingworks have explored quantization strategies aiming at minimum accuracy loss. In this paper, we argue thatquantization, which essentially imposes regularization on weight representations, can sometimes help toimprove accuracy. We conduct comprehensive experiments on three widely used applications: fully con-nected network (FCN) for biomedical image segmentation, convolutional neural network (CNN) for imageclassification on ImageNet, and recurrent neural network (RNN) for automatic speech recognition, and experi-mental results show that quantization can improve the accuracy by 1%, 1.95%, 4.23% on the three applicationsrespectively with 3.5x-6.4x memory reduction.



### 3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head
- **Arxiv ID**: http://arxiv.org/abs/2104.12051v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2104.12051v1)
- **Published**: 2021-04-25 02:48:19+00:00
- **Updated**: 2021-04-25 02:48:19+00:00
- **Authors**: Qianyun Wang, Zhenfeng Fan, Shihong Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Impressive progress has been made in audio-driven 3D facial animation recently, but synthesizing 3D talking-head with rich emotion is still unsolved. This is due to the lack of 3D generative models and available 3D emotional dataset with synchronized audios. To address this, we introduce 3D-TalkEmo, a deep neural network that generates 3D talking head animation with various emotions. We also create a large 3D dataset with synchronized audios and videos, rich corpus, as well as various emotion states of different persons with the sophisticated 3D face reconstruction methods. In the emotion generation network, we propose a novel 3D face representation structure - geometry map by classical multi-dimensional scaling analysis. It maps the coordinates of vertices on a 3D face to a canonical image plane, while preserving the vertex-to-vertex geodesic distance metric in a least-square sense. This maintains the adjacency relationship of each vertex and holds the effective convolutional structure for the 3D facial surface. Taking a neutral 3D mesh and a speech signal as inputs, the 3D-TalkEmo is able to generate vivid facial animations. Moreover, it provides access to change the emotion state of the animated speaker.   We present extensive quantitative and qualitative evaluation of our method, in addition to user studies, demonstrating the generated talking-heads of significantly higher quality compared to previous state-of-the-art methods.



### Swimmer Stroke Rate Estimation From Overhead Race Video
- **Arxiv ID**: http://arxiv.org/abs/2104.12056v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12056v2)
- **Published**: 2021-04-25 04:20:38+00:00
- **Updated**: 2021-05-20 20:55:46+00:00
- **Authors**: Timothy Woinoski, Ivan V. Bajić
- **Comment**: 6 pages, 4 figures, to be presented at the IEEE ICME Workshop on
  Artificial Intelligence in Sports (AI-Sports), July 2021
- **Journal**: None
- **Summary**: In this work, we propose a swimming analytics system for automatically determining swimmer stroke rates from overhead race video (ORV). General ORV is defined as any footage of swimmers in competition, taken for the purposes of viewing or analysis. Examples of this are footage from live streams, broadcasts, or specialized camera equipment, with or without camera motion. These are the most typical forms of swimming competition footage. We detail how to create a system that will automatically collect swimmer stroke rates in any competition, given the video of the competition of interest. With this information, better systems can be created and additions to our analytics system can be proposed to automatically extract other swimming metrics of interest.



### Making Generated Images Hard To Spot: A Transferable Attack On Synthetic Image Detectors
- **Arxiv ID**: http://arxiv.org/abs/2104.12069v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12069v2)
- **Published**: 2021-04-25 05:56:57+00:00
- **Updated**: 2022-06-22 17:11:40+00:00
- **Authors**: Xinwei Zhao, Matthew C. Stamm
- **Comment**: None
- **Journal**: International Conference on Pattern Recognition, August 2022,
  Montr\'eal Qu\'ebec
- **Summary**: Visually realistic GAN-generated images have recently emerged as an important misinformation threat. Research has shown that these synthetic images contain forensic traces that are readily identifiable by forensic detectors. Unfortunately, these detectors are built upon neural networks, which are vulnerable to recently developed adversarial attacks. In this paper, we propose a new anti-forensic attack capable of fooling GAN-generated image detectors. Our attack uses an adversarially trained generator to synthesize traces that these detectors associate with real images. Furthermore, we propose a technique to train our attack so that it can achieve transferability, i.e. it can fool unknown CNNs that it was not explicitly trained against. We evaluate our attack through an extensive set of experiments, where we show that our attack can fool eight state-of-the-art detection CNNs with synthetic images created using seven different GANs, and outperform other alternative attacks.



### Parallel Scale-wise Attention Network for Effective Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.12076v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.12076v1)
- **Published**: 2021-04-25 06:44:26+00:00
- **Updated**: 2021-04-25 06:44:26+00:00
- **Authors**: Usman Sajid, Michael Chow, Jin Zhang, Taejoon Kim, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The paper proposes a new text recognition network for scene-text images. Many state-of-the-art methods employ the attention mechanism either in the text encoder or decoder for the text alignment. Although the encoder-based attention yields promising results, these schemes inherit noticeable limitations. They perform the feature extraction (FE) and visual attention (VA) sequentially, which bounds the attention mechanism to rely only on the FE final single-scale output. Moreover, the utilization of the attention process is limited by only applying it directly to the single scale feature-maps. To address these issues, we propose a new multi-scale and encoder-based attention network for text recognition that performs the multi-scale FE and VA in parallel. The multi-scale channels also undergo regular fusion with each other to develop the coordinated knowledge together. Quantitative evaluation and robustness analysis on the standard benchmarks demonstrate that the proposed network outperforms the state-of-the-art in most cases.



### How Well Does Self-Supervised Pre-Training Perform with Streaming Data?
- **Arxiv ID**: http://arxiv.org/abs/2104.12081v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12081v3)
- **Published**: 2021-04-25 06:56:48+00:00
- **Updated**: 2022-07-21 02:02:22+00:00
- **Authors**: Dapeng Hu, Shipeng Yan, Qizhengqiu Lu, Lanqing Hong, Hailin Hu, Yifan Zhang, Zhenguo Li, Xinchao Wang, Jiashi Feng
- **Comment**: Accepted to ICLR 2022
- **Journal**: None
- **Summary**: Prior works on self-supervised pre-training focus on the joint training scenario, where massive unlabeled data are assumed to be given as input all at once, and only then is a learner trained. Unfortunately, such a problem setting is often impractical if not infeasible since many real-world tasks rely on sequential learning, e.g., data are decentralized or collected in a streaming fashion. In this paper, we conduct the first thorough and dedicated investigation on self-supervised pre-training with streaming data, aiming to shed light on the model behavior under this overlooked setup. Specifically, we pre-train over 500 models on four categories of pre-training streaming data from ImageNet and DomainNet and evaluate them on three types of downstream tasks and 12 different downstream datasets. Our studies show that, somehow beyond our expectation, with simple data replay or parameter regularization, sequential self-supervised pre-training turns out to be an efficient alternative for joint pre-training, as the performances of the former are mostly on par with those of the latter. Moreover, catastrophic forgetting, a common issue in sequential supervised learning, is much alleviated in sequential self-supervised learning (SSL), which is well justified through our comprehensive empirical analysis on representations and the sharpness of minima in the loss landscape. Our findings, therefore, suggest that, in practice, for SSL, the cumbersome joint training can be replaced mainly by sequential learning, which in turn enables a much broader spectrum of potential application scenarios.



### ASPCNet: A Deep Adaptive Spatial Pattern Capsule Network for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2104.12085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12085v1)
- **Published**: 2021-04-25 07:10:55+00:00
- **Updated**: 2021-04-25 07:10:55+00:00
- **Authors**: Jinping Wang, Xiaojun Tan, Jianhuang Lai, Jun Li, Canqun Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: Previous studies have shown the great potential of capsule networks for the spatial contextual feature extraction from {hyperspectral images (HSIs)}. However, the sampling locations of the convolutional kernels of capsules are fixed and cannot be adaptively changed according to the inconsistent semantic information of HSIs. Based on this observation, this paper proposes an adaptive spatial pattern capsule network (ASPCNet) architecture by developing an adaptive spatial pattern (ASP) unit, that can rotate the sampling location of convolutional kernels on the basis of an enlarged receptive field. Note that this unit can learn more discriminative representations of HSIs with fewer parameters. Specifically, two cascaded ASP-based convolution operations (ASPConvs) are applied to input images to learn relatively high-level semantic features, transmitting hierarchical structures among capsules more accurately than the use of the most fundamental features. Furthermore, the semantic features are fed into ASP-based conv-capsule operations (ASPCaps) to explore the shapes of objects among the capsules in an adaptive manner, further exploring the potential of capsule networks. Finally, the class labels of image patches centered on test samples can be determined according to the fully connected capsule layer. Experiments on three public datasets demonstrate that ASPCNet can yield competitive performance with higher accuracies than state-of-the-art methods.



### Image Inpainting with Edge-guided Learnable Bidirectional Attention Maps
- **Arxiv ID**: http://arxiv.org/abs/2104.12087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12087v1)
- **Published**: 2021-04-25 07:25:16+00:00
- **Updated**: 2021-04-25 07:25:16+00:00
- **Authors**: Dongsheng Wang, Chaohao Xie, Shaohui Liu, Zhenxing Niu, Wangmeng Zuo
- **Comment**: 16 pages,13 figures
- **Journal**: None
- **Summary**: For image inpainting, the convolutional neural networks (CNN) in previous methods often adopt standard convolutional operator, which treats valid pixels and holes indistinguishably. As a result, they are limited in handling irregular holes and tend to produce color-discrepant and blurry inpainting result. Partial convolution (PConv) copes with this issue by conducting masked convolution and feature re-normalization conditioned only on valid pixels, but the mask-updating is handcrafted and independent with image structural information. In this paper, we present an edge-guided learnable bidirectional attention map (Edge-LBAM) for improving image inpainting of irregular holes with several distinct merits. Instead of using a hard 0-1 mask, a learnable attention map module is introduced for learning feature re-normalization and mask-updating in an end-to-end manner. Learnable reverse attention maps are further proposed in the decoder for emphasizing on filling in unknown pixels instead of reconstructing all pixels. Motivated by that the filling-in order is crucial to inpainting results and largely depends on image structures in exemplar-based methods, we further suggest a multi-scale edge completion network to predict coherent edges. Our Edge-LBAM method contains dual procedures,including structure-aware mask-updating guided by predict edges and attention maps generated by masks for feature re-normalization.Extensive experiments show that our Edge-LBAM is effective in generating coherent image structures and preventing color discrepancy and blurriness, and performs favorably against the state-of-the-art methods in terms of qualitative metrics and visual quality.



### Visual Saliency Transformer
- **Arxiv ID**: http://arxiv.org/abs/2104.12099v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12099v2)
- **Published**: 2021-04-25 08:24:06+00:00
- **Updated**: 2021-08-23 11:26:29+00:00
- **Authors**: Nian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, Junwei Han
- **Comment**: None
- **Journal**: None
- **Summary**: Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.



### Multi-Scale Hourglass Hierarchical Fusion Network for Single Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/2104.12100v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12100v2)
- **Published**: 2021-04-25 08:27:01+00:00
- **Updated**: 2021-06-14 01:58:39+00:00
- **Authors**: Xiang Chen, Yufeng Huang, Lei Xu
- **Comment**: Accepted in CVPRW 2021
- **Journal**: None
- **Summary**: Rain streaks bring serious blurring and visual quality degradation, which often vary in size, direction and density. Current CNN-based methods achieve encouraging performance, while are limited to depict rain characteristics and recover image details in the poor visibility environment. To address these issues, we present a Multi-scale Hourglass Hierarchical Fusion Network (MH2F-Net) in end-to-end manner, to exactly captures rain streak features with multi-scale extraction, hierarchical distillation and information aggregation. For better extracting the features, a novel Multi-scale Hourglass Extraction Block (MHEB) is proposed to get local and global features across different scales through down- and up-sample process. Besides, a Hierarchical Attentive Distillation Block (HADB) then employs the dual attention feature responses to adaptively recalibrate the hierarchical features and eliminate the redundant ones. Further, we introduce a Residual Projected Feature Fusion (RPFF) strategy to progressively discriminate feature learning and aggregate different features instead of directly concatenating or adding. Extensive experiments on both synthetic and real rainy datasets demonstrate the effectiveness of the designed MH2F-Net by comparing with recent state-of-the-art deraining algorithms. Our source code will be available on the GitHub: https://github.com/cxtalk/MH2F-Net.



### Unsupervised Learning of Multi-level Structures for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.12102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12102v1)
- **Published**: 2021-04-25 08:38:41+00:00
- **Updated**: 2021-04-25 08:38:41+00:00
- **Authors**: Songmin Dai, Jide Li, Lu Wang, Congcong Zhu, Yifan Wu, Xiaoqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: The main difficulty in high-dimensional anomaly detection tasks is the lack of anomalous data for training. And simply collecting anomalous data from the real world, common distributions, or the boundary of normal data manifold may face the problem of missing anomaly modes. This paper first introduces a novel method to generate anomalous data by breaking up global structures while preserving local structures of normal data at multiple levels. It can efficiently expose local abnormal structures of various levels. To fully exploit the exposed multi-level abnormal structures, we propose to train multiple level-specific patch-based detectors with contrastive losses. Each detector learns to detect local abnormal structures of corresponding level at all locations and outputs patchwise anomaly scores. By aggregating the outputs of all level-specific detectors, we obtain a model that can detect all potential anomalies. The effectiveness is evaluated on MNIST, CIFAR10, and ImageNet10 dataset, where the results surpass the accuracy of state-of-the-art methods. Qualitative experiments demonstrate our model is robust that it unbiasedly detects all anomaly modes.



### Temp-Frustum Net: 3D Object Detection with Temporal Fusion
- **Arxiv ID**: http://arxiv.org/abs/2104.12106v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12106v2)
- **Published**: 2021-04-25 09:08:14+00:00
- **Updated**: 2021-05-21 10:37:01+00:00
- **Authors**: Emeç Erçelik, Ekim Yurtsever, Alois Knoll
- **Comment**: To be published in 32nd IEEE Intelligent Vehicles Symposium
- **Journal**: None
- **Summary**: 3D object detection is a core component of automated driving systems. State-of-the-art methods fuse RGB imagery and LiDAR point cloud data frame-by-frame for 3D bounding box regression. However, frame-by-frame 3D object detection suffers from noise, field-of-view obstruction, and sparsity. We propose a novel Temporal Fusion Module (TFM) to use information from previous time-steps to mitigate these problems. First, a state-of-the-art frustum network extracts point cloud features from raw RGB and LiDAR point cloud data frame-by-frame. Then, our TFM module fuses these features with a recurrent neural network. As a result, 3D object detection becomes robust against single frame failures and transient occlusions. Experiments on the KITTI object tracking dataset show the efficiency of the proposed TFM, where we obtain ~6%, ~4%, and ~6% improvements on Car, Pedestrian, and Cyclist classes, respectively, compared to frame-by-frame baselines. Furthermore, ablation studies reinforce that the subject of improvement is temporal fusion and show the effects of different placements of TFM in the object detection pipeline. Our code is open-source and available at https://github.com/emecercelik/Temp-Frustum-Net.git.



### Parallel mesh reconstruction streams for pose estimation of interacting hands
- **Arxiv ID**: http://arxiv.org/abs/2104.12123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12123v1)
- **Published**: 2021-04-25 10:14:15+00:00
- **Updated**: 2021-04-25 10:14:15+00:00
- **Authors**: Uri Wollner, Guy Ben-Yosef
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new multi-stream 3D mesh reconstruction network (MSMR-Net) for hand pose estimation from a single RGB image. Our model consists of an image encoder followed by a mesh-convolution decoder composed of connected graph convolution layers. In contrast to previous models that form a single mesh decoding path, our decoder network incorporates multiple cross-resolution trajectories that are executed in parallel. Thus, global and local information are shared to form rich decoding representations at minor additional parameter cost compared to the single trajectory network. We demonstrate the effectiveness of our method in hand-hand and hand-object interaction scenarios at various levels of interaction. To evaluate the former scenario, we propose a method to generate RGB images of closely interacting hands. Moreoever, we suggest a metric to quantify the degree of interaction and show that close hand interactions are particularly challenging. Experimental results show that the MSMR-Net outperforms existing algorithms on the hand-object FreiHAND dataset as well as on our own hand-hand dataset.



### 3D/2D regularized CNN feature hierarchy for Hyperspectral image classification
- **Arxiv ID**: http://arxiv.org/abs/2104.12136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12136v1)
- **Published**: 2021-04-25 11:26:56+00:00
- **Updated**: 2021-04-25 11:26:56+00:00
- **Authors**: Muhammad Ahmad, Manuel Mazzara, Salvatore Distefano
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have been rigorously studied for Hyperspectral Image Classification (HSIC) and are known to be effective in exploiting joint spatial-spectral information with the expense of lower generalization performance and learning speed due to the hard labels and non-uniform distribution over labels. Several regularization techniques have been used to overcome the aforesaid issues. However, sometimes models learn to predict the samples extremely confidently which is not good from a generalization point of view. Therefore, this paper proposed an idea to enhance the generalization performance of a hybrid CNN for HSIC using soft labels that are a weighted average of the hard labels and uniform distribution over ground labels. The proposed method helps to prevent CNN from becoming over-confident. We empirically show that in improving generalization performance, label smoothing also improves model calibration which significantly improves beam-search. Several publicly available Hyperspectral datasets are used to validate the experimental evaluation which reveals improved generalization performance, statistical significance, and computational complexity as compared to the state-of-the-art models. The code will be made available at https://github.com/mahmad00.



### A Novel Transformer Based Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2104.12137v6
- **DOI**: 10.1109/LGRS.2022.3143368
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12137v6)
- **Published**: 2021-04-25 11:34:22+00:00
- **Updated**: 2022-06-03 03:01:48+00:00
- **Authors**: Libo Wang, Rui Li, Chenxi Duan, Ce Zhang, Xiaoliang Meng, Shenghui Fang
- **Comment**: Accepted by
  GRSL.https://ieeexplore.ieee.org/abstract/document/9681903
- **Journal**: None
- **Summary**: The fully convolutional network (FCN) with an encoder-decoder architecture has been the standard paradigm for semantic segmentation. The encoder-decoder architecture utilizes an encoder to capture multilevel feature maps, which are incorporated into the final prediction by a decoder. As the context is crucial for precise segmentation, tremendous effort has been made to extract such information in an intelligent fashion, including employing dilated/atrous convolutions or inserting attention modules. However, these endeavors are all based on the FCN architecture with ResNet or other backbones, which cannot fully exploit the context from the theoretical concept. By contrast, we introduce the Swin Transformer as the backbone to extract the context information and design a novel decoder of densely connected feature aggregation module (DCFAM) to restore the resolution and produce the segmentation map. The experimental results on two remotely sensed semantic segmentation datasets demonstrate the effectiveness of the proposed scheme.Code is available at https://github.com/WangLibo1995/GeoSeg



### Learning to Address Intra-segment Misclassification in Retinal Imaging
- **Arxiv ID**: http://arxiv.org/abs/2104.12138v2
- **DOI**: 10.1007/978-3-030-87193-2_46
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12138v2)
- **Published**: 2021-04-25 11:57:26+00:00
- **Updated**: 2021-12-08 20:56:01+00:00
- **Authors**: Yukun Zhou, Moucheng Xu, Yipeng Hu, Hongxiang Lin, Joseph Jacob, Pearse A. Keane, Daniel C. Alexander
- **Comment**: 13 pages, 9 figures, and 2 tables
- **Journal**: None
- **Summary**: Accurate multi-class segmentation is a long-standing challenge in medical imaging, especially in scenarios where classes share strong similarity. Segmenting retinal blood vessels in retinal photographs is one such scenario, in which arteries and veins need to be identified and differentiated from each other and from the background. Intra-segment misclassification, i.e. veins classified as arteries or vice versa, frequently occurs when arteries and veins intersect, whereas in binary retinal vessel segmentation, error rates are much lower. We thus propose a new approach that decomposes multi-class segmentation into multiple binary, followed by a binary-to-multi-class fusion network. The network merges representations of artery, vein, and multi-class feature maps, each of which are supervised by expert vessel annotation in adversarial training. A skip-connection based merging process explicitly maintains class-specific gradients to avoid gradient vanishing in deep layers, to favor the discriminative features. The results show that, our model respectively improves F1-score by 4.4\%, 5.1\%, and 4.2\% compared with three state-of-the-art deep learning based methods on DRIVE-AV, LES-AV, and HRF-AV data sets. Code: https://github.com/rmaphoh/Learning-AVSegmentation



### 3D Adversarial Attacks Beyond Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2104.12146v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12146v3)
- **Published**: 2021-04-25 13:01:41+00:00
- **Updated**: 2021-11-16 00:37:21+00:00
- **Authors**: Jinlai Zhang, Lyujie Chen, Binbin Liu, Bo Ouyang, Qizhi Xie, Jihong Zhu, Weiming Li, Yanmei Meng
- **Comment**: 8 pages, 6 figs
- **Journal**: None
- **Summary**: Recently, 3D deep learning models have been shown to be susceptible to adversarial attacks like their 2D counterparts. Most of the state-of-the-art (SOTA) 3D adversarial attacks perform perturbation to 3D point clouds. To reproduce these attacks in the physical scenario, a generated adversarial 3D point cloud need to be reconstructed to mesh, which leads to a significant drop in its adversarial effect. In this paper, we propose a strong 3D adversarial attack named Mesh Attack to address this problem by directly performing perturbation on mesh of a 3D object. In order to take advantage of the most effective gradient-based attack, a differentiable sample module that back-propagate the gradient of point cloud to mesh is introduced. To further ensure the adversarial mesh examples without outlier and 3D printable, three mesh losses are adopted. Extensive experiments demonstrate that the proposed scheme outperforms SOTA 3D attacks by a significant margin. We also achieved SOTA performance under various defenses. Our code is available at: https://github.com/cuge1995/Mesh-Attack.



### MIDeepSeg: Minimally Interactive Segmentation of Unseen Objects from Medical Images Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.12166v1
- **DOI**: 10.1016/j.media.2021.102102
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12166v1)
- **Published**: 2021-04-25 14:15:17+00:00
- **Updated**: 2021-04-25 14:15:17+00:00
- **Authors**: Xiangde Luo, Guotai Wang, Tao Song, Jingyang Zhang, Michael Aertsen, Jan Deprest, Sebastien Ourselin, Tom Vercauteren, Shaoting Zhang
- **Comment**: 17 pages, 16 figures
- **Journal**: None
- **Summary**: Segmentation of organs or lesions from medical images plays an essential role in many clinical applications such as diagnosis and treatment planning. Though Convolutional Neural Networks (CNN) have achieved the state-of-the-art performance for automatic segmentation, they are often limited by the lack of clinically acceptable accuracy and robustness in complex cases. Therefore, interactive segmentation is a practical alternative to these methods. However, traditional interactive segmentation methods require a large amount of user interactions, and recently proposed CNN-based interactive segmentation methods are limited by poor performance on previously unseen objects. To solve these problems, we propose a novel deep learning-based interactive segmentation method that not only has high efficiency due to only requiring clicks as user inputs but also generalizes well to a range of previously unseen objects. Specifically, we first encode user-provided interior margin points via our proposed exponentialized geodesic distance that enables a CNN to achieve a good initial segmentation result of both previously seen and unseen objects, then we use a novel information fusion method that combines the initial segmentation with only few additional user clicks to efficiently obtain a refined segmentation. We validated our proposed framework through extensive experiments on 2D and 3D medical image segmentation tasks with a wide range of previous unseen objects that were not present in the training set. Experimental results showed that our proposed framework 1) achieves accurate results with fewer user interactions and less time compared with state-of-the-art interactive frameworks and 2) generalizes well to previously unseen objects.



### A Novel Binocular Eye-Tracking SystemWith Stereo Stimuli for 3D Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/2104.12167v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12167v3)
- **Published**: 2021-04-25 14:17:07+00:00
- **Updated**: 2021-10-28 12:37:57+00:00
- **Authors**: Jinglin Sun, Zhipeng Wu, Han Wang, Peiguang Jing, Yu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Eye-tracking technologies have been widely used in applications like psychological studies and human computer interactions (HCI). However, most current eye trackers focus on 2D point of gaze (PoG) estimation and cannot provide accurate gaze depth.Concerning future applications such as HCI with 3D displays, we propose a novel binocular eye tracking device with stereo stimuli to provide highly accurate 3D PoG estimation. In our device, the 3D stereo imaging system can provide users with a friendly and immersive 3D visual experience without wearing any accessories. The eye capturing system can directly record the users eye movements under 3D stimuli without disturbance. A regression based 3D eye tracking model is built based on collected eye movement data under stereo stimuli. Our model estimates users 2D gaze with features defined by eye region landmarks and further estimates 3D PoG with a multi source feature set constructed by comprehensive eye movement features and disparity features from stereo stimuli. Two test stereo scenes with different depths of field are designed to verify the model effectiveness. Experimental results show that the average error for 2D gaze estimation was 0.66\degree and for 3D PoG estimation, the average errors are 1.85~cm/0.15~m over the workspace volume 50~cm $\times$ 30~cm $\times$ 75~cm/2.4~m $\times$ 4.0~m $\times$ 7.9~m separately.



### Regression on Deep Visual Features using Artificial Neural Networks (ANNs) to Predict Hydraulic Blockage at Culverts
- **Arxiv ID**: http://arxiv.org/abs/2105.03233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.03233v1)
- **Published**: 2021-04-25 14:58:46+00:00
- **Updated**: 2021-04-25 14:58:46+00:00
- **Authors**: Umair Iqbal, Johan Barthelemy, Wanqing Li, Pascal Perez
- **Comment**: None
- **Journal**: None
- **Summary**: Cross drainage hydraulic structures (i.e., culverts, bridges) in urban landscapes are prone to getting blocked by transported debris which often results in causing the flash floods. In context of Australia, Wollongong City Council (WCC) blockage conduit policy is the only formal guideline to consider blockage in design process. However, many argue that this policy is based on the post floods visual inspections and hence can not be considered accurate representation of hydraulic blockage. As a result of this on-going debate, visual blockage and hydraulic blockage are considered two distinct terms with no established quantifiable relation among both. This paper attempts to relate both terms by proposing the use of deep visual features for prediction of hydraulic blockage at a given culvert. An end-to-end machine learning pipeline is propounded which takes an image of culvert as input, extract visual features using deep learning models, pre-process the visual features and feed into regression model to predict the corresponding hydraulic blockage. Dataset (i.e., Hydrology-Lab Dataset (HD), Visual Hydrology-Lab Dataset (VHD)) used in this research was collected from in-lab experiments carried out using scaled physical models of culverts where multiple blockage scenarios were replicated at scale. Performance of regression models was assessed using standard evaluation metrics. Furthermore, performance of overall machine learning pipeline was assessed in terms of processing times for relative comparison of models and hardware requirement analysis. From the results ANN used with MobileNet extracted visual features achieved the best regression performance with $R^{2}$ score of 0.7855. Positive value of $R^{2}$ score indicated the presence of correlation between visual features and hydraulic blockage and suggested that both can be interrelated with each other.



### A novel segmentation dataset for signatures on bank checks
- **Arxiv ID**: http://arxiv.org/abs/2104.12203v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12203v2)
- **Published**: 2021-04-25 16:56:09+00:00
- **Updated**: 2021-04-28 11:06:40+00:00
- **Authors**: Muhammad Saif Ullah Khan
- **Comment**: None
- **Journal**: None
- **Summary**: The dataset presented provides high-resolution images of real, filled out bank checks containing various complex backgrounds, and handwritten text and signatures in the respective fields, along with both pixel-level and patch-level segmentation masks for the signatures on the checks. The images of bank checks were obtained from different sources, including other publicly available check datasets, publicly available images on the internet, as well as scans and images of real checks. Using the GIMP graphics software, pixel-level segmentation masks for signatures on these checks were manually generated as binary images. An automated script was then used to generate patch-level masks. The dataset was created to train and test networks for extracting signatures from bank checks and other similar documents with very complex backgrounds.



### Breast Mass Detection with Faster R-CNN: On the Feasibility of Learning from Noisy Annotations
- **Arxiv ID**: http://arxiv.org/abs/2104.12218v1
- **DOI**: 10.1109/ACCESS.2021.3072997
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2104.12218v1)
- **Published**: 2021-04-25 17:43:58+00:00
- **Updated**: 2021-04-25 17:43:58+00:00
- **Authors**: Sina Famouri, Lia Morra, Leonardo Mangia, Fabrizio Lamberti
- **Comment**: None
- **Journal**: IEEE Access, 2021
- **Summary**: In this work we study the impact of noise on the training of object detection networks for the medical domain, and how it can be mitigated by improving the training procedure. Annotating large medical datasets for training data-hungry deep learning models is expensive and time consuming. Leveraging information that is already collected in clinical practice, in the form of text reports, bookmarks or lesion measurements would substantially reduce this cost. Obtaining precise lesion bounding boxes through automatic mining procedures, however, is difficult. We provide here a quantitative evaluation of the effect of bounding box coordinate noise on the performance of Faster R-CNN object detection networks for breast mass detection. Varying degrees of noise are simulated by randomly modifying the bounding boxes: in our experiments, bounding boxes could be enlarged up to six times the original size. The noise is injected in the CBIS-DDSM collection, a well curated public mammography dataset for which accurate lesion location is available. We show how, due to an imperfect matching between the ground truth and the network bounding box proposals, the noise is propagated during training and reduces the ability of the network to correctly classify lesions from background. When using the standard Intersection over Union criterion, the area under the FROC curve decreases by up to 9%. A novel matching criterion is proposed to improve tolerance to noise.



### Vector Neurons: A General Framework for SO(3)-Equivariant Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.12229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12229v1)
- **Published**: 2021-04-25 18:48:15+00:00
- **Updated**: 2021-04-25 18:48:15+00:00
- **Authors**: Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations -- including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network.



### The 5th AI City Challenge
- **Arxiv ID**: http://arxiv.org/abs/2104.12233v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.12233v2)
- **Published**: 2021-04-25 19:15:27+00:00
- **Updated**: 2021-05-24 19:26:35+00:00
- **Authors**: Milind Naphade, Shuo Wang, David C. Anastasiu, Zheng Tang, Ming-Ching Chang, Xiaodong Yang, Yue Yao, Liang Zheng, Pranamesh Chakraborty, Christian E. Lopez, Anuj Sharma, Qi Feng, Vitaly Ablavsky, Stan Sclaroff
- **Comment**: Summary of the 5th AI City Challenge Workshop in conjunction with
  CVPR 2021
- **Journal**: None
- **Summary**: The AI City Challenge was created with two goals in mind: (1) pushing the boundaries of research and development in intelligent video analysis for smarter cities use cases, and (2) assessing tasks where the level of performance is enough to cause real-world adoption. Transportation is a segment ripe for such adoption. The fifth AI City Challenge attracted 305 participating teams across 38 countries, who leveraged city-scale real traffic data and high-quality synthetic data to compete in five challenge tracks. Track 1 addressed video-based automatic vehicle counting, where the evaluation being conducted on both algorithmic effectiveness and computational efficiency. Track 2 addressed city-scale vehicle re-identification with augmented synthetic data to substantially increase the training set for the task. Track 3 addressed city-scale multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly detection. Track 5 was a new track addressing vehicle retrieval using natural language descriptions. The evaluation system shows a general leader board of all submitted results, and a public leader board of results limited to the contest participation rules, where teams are not allowed to use external data in their work. The public leader board shows results more close to real-world situations where annotated data is limited. Results show the promise of AI in Smarter Transportation. State-of-the-art performance for some tasks shows that these technologies are ready for adoption in real-world systems.



### Single Stage Class Agnostic Common Object Detection: A Simple Baseline
- **Arxiv ID**: http://arxiv.org/abs/2104.12245v1
- **DOI**: 10.5220/0010242303960407
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12245v1)
- **Published**: 2021-04-25 20:14:28+00:00
- **Updated**: 2021-04-25 20:14:28+00:00
- **Authors**: Chuong H. Nguyen, Thuy C. Nguyen, Anh H. Vo, Yamazaki Masayuki
- **Comment**: This paper is accepted to International Conference on Pattern
  Recognition Applications and Methods (ICPRAM) 2021
- **Journal**: None
- **Summary**: This paper addresses the problem of common object detection, which aims to detect objects of similar categories from a set of images. Although it shares some similarities with the standard object detection and co-segmentation, common object detection, recently promoted by \cite{Jiang2019a}, has some unique advantages and challenges. First, it is designed to work on both closed-set and open-set conditions, a.k.a. known and unknown objects. Second, it must be able to match objects of the same category but not restricted to the same instance, texture, or posture. Third, it can distinguish multiple objects. In this work, we introduce the Single Stage Common Object Detection (SSCOD) to detect class-agnostic common objects from an image set. The proposed method is built upon the standard single-stage object detector. Furthermore, an embedded branch is introduced to generate the object's representation feature, and their similarity is measured by cosine distance. Experiments are conducted on PASCAL VOC 2007 and COCO 2014 datasets. While being simple and flexible, our proposed SSCOD built upon ATSSNet performs significantly better than the baseline of the standard object detection, while still be able to match objects of unknown categories. Our source code can be found at \href{https://github.com/cybercore-co-ltd/Single-Stage-Common-Object-Detection}{(URL)}



### Customizable Reference Runtime Monitoring of Neural Networks using Resolution Boxes
- **Arxiv ID**: http://arxiv.org/abs/2104.14435v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.14435v2)
- **Published**: 2021-04-25 21:58:02+00:00
- **Updated**: 2021-07-12 12:21:53+00:00
- **Authors**: Changshun Wu, Yliès Falcone, Saddek Bensalem
- **Comment**: None
- **Journal**: None
- **Summary**: Classification neural networks fail to detect inputs that do not fall inside the classes they have been trained for. Runtime monitoring techniques on the neuron activation pattern can be used to detect such inputs. We present an approach for monitoring classification systems via data abstraction. Data abstraction relies on the notion of box with a resolution. Box-based abstraction consists in representing a set of values by its minimal and maximal values in each dimension. We augment boxes with a notion of resolution and define their clustering coverage, which is intuitively a quantitative metric that indicates the abstraction quality. This allows studying the effect of different clustering parameters on the constructed boxes and estimating an interval of sub-optimal parameters. Moreover, we automatically construct monitors that leverage both the correct and incorrect behaviors of a system. This allows checking the size of the monitor abstractions and analyzing the separability of the network. Monitors are obtained by combining the sub-monitors of each class of the system placed at some selected layers. Our experiments demonstrate the effectiveness of our clustering coverage estimation and show how to assess the effectiveness and precision of monitors according to the selected clustering parameter and monitored layers.



### Learning to Better Segment Objects from Unseen Classes with Unlabeled Videos
- **Arxiv ID**: http://arxiv.org/abs/2104.12276v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12276v2)
- **Published**: 2021-04-25 22:05:44+00:00
- **Updated**: 2021-08-20 20:28:33+00:00
- **Authors**: Yuming Du, Yang Xiao, Vincent Lepetit
- **Comment**: ICCV2021 Camera Ready. See project page
  https://dulucas.github.io/Homepage/gbopt/
- **Journal**: None
- **Summary**: The ability to localize and segment objects from unseen classes would open the door to new applications, such as autonomous object learning in active vision. Nonetheless, improving the performance on unseen classes requires additional training data, while manually annotating the objects of the unseen classes can be labor-extensive and expensive. In this paper, we explore the use of unlabeled video sequences to automatically generate training data for objects of unseen classes. It is in principle possible to apply existing video segmentation methods to unlabeled videos and automatically obtain object masks, which can then be used as a training set even for classes with no manual labels available. However, our experiments show that these methods do not perform well enough for this purpose. We therefore introduce a Bayesian method that is specifically designed to automatically create such a training set: Our method starts from a set of object proposals and relies on (non-realistic) analysis-by-synthesis to select the correct ones by performing an efficient optimization over all the frames simultaneously. Through extensive experiments, we show that our method can generate a high-quality training set which significantly boosts the performance of segmenting objects of unseen classes. We thus believe that our method could open the door for open-world instance segmentation using abundant Internet videos.



### Class Equilibrium using Coulomb's Law
- **Arxiv ID**: http://arxiv.org/abs/2104.12287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12287v1)
- **Published**: 2021-04-25 23:38:06+00:00
- **Updated**: 2021-04-25 23:38:06+00:00
- **Authors**: Saheb Chhabra, Puspita Majumdar, Mayank Vatsa, Richa Singh
- **Comment**: Accepted at IJCNN 2021
- **Journal**: None
- **Summary**: Projection algorithms learn a transformation function to project the data from input space to the feature space, with the objective of increasing the inter-class distance. However, increasing the inter-class distance can affect the intra-class distance. Maintaining an optimal inter-class separation among the classes without affecting the intra-class distance of the data distribution is a challenging task. In this paper, inspired by the Coulomb's law of Electrostatics, we propose a new algorithm to compute the equilibrium space of any data distribution where the separation among the classes is optimal. The algorithm further learns the transformation between the input space and equilibrium space to perform classification in the equilibrium space. The performance of the proposed algorithm is evaluated on four publicly available datasets at three different resolutions. It is observed that the proposed algorithm performs well for low-resolution images.



### Explainable AI For COVID-19 CT Classifiers: An Initial Comparison Study
- **Arxiv ID**: http://arxiv.org/abs/2104.14506v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, 68T01
- **Links**: [PDF](http://arxiv.org/pdf/2104.14506v1)
- **Published**: 2021-04-25 23:39:14+00:00
- **Updated**: 2021-04-25 23:39:14+00:00
- **Authors**: Qinghao Ye, Jun Xia, Guang Yang
- **Comment**: 6 pages, 4 figures, IEEE CBMS 2021
- **Journal**: None
- **Summary**: Artificial Intelligence (AI) has made leapfrogs in development across all the industrial sectors especially when deep learning has been introduced. Deep learning helps to learn the behaviour of an entity through methods of recognising and interpreting patterns. Despite its limitless potential, the mystery is how deep learning algorithms make a decision in the first place. Explainable AI (XAI) is the key to unlocking AI and the black-box for deep learning. XAI is an AI model that is programmed to explain its goals, logic, and decision making so that the end users can understand. The end users can be domain experts, regulatory agencies, managers and executive board members, data scientists, users that use AI, with or without awareness, or someone who is affected by the decisions of an AI model. Chest CT has emerged as a valuable tool for the clinical diagnostic and treatment management of the lung diseases associated with COVID-19. AI can support rapid evaluation of CT scans to differentiate COVID-19 findings from other lung diseases. However, how these AI tools or deep learning algorithms reach such a decision and which are the most influential features derived from these neural networks with typically deep layers are not clear. The aim of this study is to propose and develop XAI strategies for COVID-19 classification models with an investigation of comparison. The results demonstrate promising quantification and qualitative visualisations that can further enhance the clinician's understanding and decision making with more granular information from the results given by the learned XAI models.



### StegaPos: Preventing Unwanted Crops and Replacements with Imperceptible Positional Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2104.12290v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12290v2)
- **Published**: 2021-04-25 23:42:29+00:00
- **Updated**: 2022-12-07 07:11:09+00:00
- **Authors**: Gokhan Egri, Todd Zickler
- **Comment**: For CVPR 2022 submission, 8 pages (main)
- **Journal**: None
- **Summary**: We present a learned, spatially-varying steganography system that allows detecting when and how images have been altered by cropping, splicing or inpainting after publication. The system comprises a learned encoder that imperceptibly hides distinct positional signatures in every local image region before publication, and an accompanying learned decoder that extracts the steganographic signatures to determine, for each local image region, its 2D positional coordinates within the originally-published image. Crop and replacement edits become detectable by the inconsistencies they cause in the hidden positional signatures. Using a prototype system for small $(400 \times 400)$ images, we show experimentally that simple CNN encoder and decoder architectures can be trained jointly to achieve detection that is reliable and robust, without introducing perceptible distortion. This approach could help individuals and image-sharing platforms certify that an image was published by a trusted source, and also know which parts of such an image, if any, have been substantially altered since publication.




# Arxiv Papers in cs.CV on 2021-05-02
### Object detection for crabs in top-view seabed imagery
- **Arxiv ID**: http://arxiv.org/abs/2105.02964v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02964v1)
- **Published**: 2021-05-02 00:05:17+00:00
- **Updated**: 2021-05-02 00:05:17+00:00
- **Authors**: Vlad Velici, Adam Prügel-Bennett
- **Comment**: None
- **Journal**: None
- **Summary**: This report presents the application of object detection on a database of underwater images of different species of crabs, as well as aerial images of sea lions and finally the Pascal VOC dataset. The model is an end-to-end object detection neural network based on a convolutional network base and a Long Short-Term Memory detector.



### RADDet: Range-Azimuth-Doppler based Radar Object Detection for Dynamic Road Users
- **Arxiv ID**: http://arxiv.org/abs/2105.00363v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2105.00363v1)
- **Published**: 2021-05-02 00:25:11+00:00
- **Updated**: 2021-05-02 00:25:11+00:00
- **Authors**: Ao Zhang, Farzan Erlik Nowruzi, Robert Laganiere
- **Comment**: Accepted by 18th Conference on Robots and Vision (CRV), CRV 2021
- **Journal**: None
- **Summary**: Object detection using automotive radars has not been explored with deep learning models in comparison to the camera based approaches. This can be attributed to the lack of public radar datasets. In this paper, we collect a novel radar dataset that contains radar data in the form of Range-Azimuth-Doppler tensors along with the bounding boxes on the tensor for dynamic road users, category labels, and 2D bounding boxes on the Cartesian Bird-Eye-View range map. To build the dataset, we propose an instance-wise auto-annotation method. Furthermore, a novel Range-Azimuth-Doppler based multi-class object detection deep learning model is proposed. The algorithm is a one-stage anchor-based detector that generates both 3D bounding boxes and 2D bounding boxes on Range-Azimuth-Doppler and Cartesian domains, respectively. Our proposed algorithm achieves 56.3% AP with IOU of 0.3 on 3D bounding box predictions, and 51.6% with IOU of 0.5 on 2D bounding box prediction. Our dataset and the code can be found at https://github.com/ZhangAoCanada/RADDet.git.



### MarkerPose: Robust Real-time Planar Target Tracking for Accurate Stereo Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.00368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00368v2)
- **Published**: 2021-05-02 01:09:13+00:00
- **Updated**: 2021-05-29 22:41:12+00:00
- **Authors**: Jhacson Meza, Lenny A. Romero, Andres G. Marrugo
- **Comment**: Accepted at CVPR 2021 LXCV Workshop
- **Journal**: None
- **Summary**: Despite the attention marker-less pose estimation has attracted in recent years, marker-based approaches still provide unbeatable accuracy under controlled environmental conditions. Thus, they are used in many fields such as robotics or biomedical applications but are primarily implemented through classical approaches, which require lots of heuristics and parameter tuning for reliable performance under different environments. In this work, we propose MarkerPose, a robust, real-time pose estimation system based on a planar target of three circles and a stereo vision system. MarkerPose is meant for high-accuracy pose estimation applications. Our method consists of two deep neural networks for marker point detection. A SuperPoint-like network for pixel-level accuracy keypoint localization and classification, and we introduce EllipSegNet, a lightweight ellipse segmentation network for sub-pixel-level accuracy keypoint detection. The marker's pose is estimated through stereo triangulation. The target point detection is robust to low lighting and motion blur conditions. We compared MarkerPose with a detection method based on classical computer vision techniques using a robotic arm for validation. The results show our method provides better accuracy than the classical technique. Finally, we demonstrate the suitability of MarkerPose in a 3D freehand ultrasound system, which is an application where highly accurate pose estimation is required. Code is available in Python and C++ at https://github.com/jhacsonmeza/MarkerPose.



### Learning data association without data association: An EM approach to neural assignment prediction
- **Arxiv ID**: http://arxiv.org/abs/2105.00369v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2105.00369v1)
- **Published**: 2021-05-02 01:11:09+00:00
- **Updated**: 2021-05-02 01:11:09+00:00
- **Authors**: Michael Burke, Subramanian Ramamoorthy
- **Comment**: None
- **Journal**: None
- **Summary**: Data association is a fundamental component of effective multi-object tracking. Current approaches to data-association tend to frame this as an assignment problem relying on gating and distance-based cost matrices, or offset the challenge of data association to a problem of tracking by detection. The latter is typically formulated as a supervised learning problem, and requires labelling information about tracked object identities to train a model for object recognition. This paper introduces an expectation maximisation approach to train neural models for data association, which does not require labelling information. Here, a Sinkhorn network is trained to predict assignment matrices that maximise the marginal likelihood of trajectory observations. Importantly, networks trained using the proposed approach can be re-used in downstream tracking applications.



### Investigating the Impact of Multi-LiDAR Placement on Object Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2105.00373v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.00373v4)
- **Published**: 2021-05-02 01:52:18+00:00
- **Updated**: 2022-05-04 06:00:55+00:00
- **Authors**: Hanjiang Hu, Zuxin Liu, Sharad Chitlangia, Akhil Agnihotri, Ding Zhao
- **Comment**: CVPR 2022 camera-ready version:15 pages, 14 figures, 9 tables
- **Journal**: None
- **Summary**: The past few years have witnessed an increasing interest in improving the perception performance of LiDARs on autonomous vehicles. While most of the existing works focus on developing new deep learning algorithms or model architectures, we study the problem from the physical design perspective, i.e., how different placements of multiple LiDARs influence the learning-based perception. To this end, we introduce an easy-to-compute information-theoretic surrogate metric to quantitatively and fast evaluate LiDAR placement for 3D detection of different types of objects. We also present a new data collection, detection model training and evaluation framework in the realistic CARLA simulator to evaluate disparate multi-LiDAR configurations. Using several prevalent placements inspired by the designs of self-driving companies, we show the correlation between our surrogate metric and object detection performance of different representative algorithms on KITTI through extensive experiments, validating the effectiveness of our LiDAR placement evaluation approach. Our results show that sensor placement is non-negligible in 3D point cloud-based object detection, which will contribute up to 10% performance discrepancy in terms of average precision in challenging 3D object detection settings. We believe that this is one of the first studies to quantitatively investigate the influence of LiDAR placement on perception performance. The code is available on https://github.com/HanjiangHu/Multi-LiDAR-Placement-for-3D-Detection.



### Skin3D: Detection and Longitudinal Tracking of Pigmented Skin Lesions in 3D Total-Body Textured Meshes
- **Arxiv ID**: http://arxiv.org/abs/2105.00374v2
- **DOI**: 10.1016/j.media.2021.102329
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.00374v2)
- **Published**: 2021-05-02 01:52:28+00:00
- **Updated**: 2022-01-12 06:04:44+00:00
- **Authors**: Mengliu Zhao, Jeremy Kawahara, Kumar Abhishek, Sajjad Shamanian, Ghassan Hamarneh
- **Comment**: 11 pages, 8 figures; Zhao and Kawahara: joint first authors;
  Published in Medical Image Analysis (2021)
- **Journal**: None
- **Summary**: We present an automated approach to detect and longitudinally track skin lesions on 3D total-body skin surface scans. The acquired 3D mesh of the subject is unwrapped to a 2D texture image, where a trained objected detection model, Faster R-CNN, localizes the lesions within the 2D domain. These detected skin lesions are mapped back to the 3D surface of the subject and, for subjects imaged multiple times, we construct a graph-based matching procedure to longitudinally track lesions that considers the anatomical correspondences among pairs of meshes and the geodesic proximity of corresponding lesions and the inter-lesion geodesic distances.   We evaluated the proposed approach using 3DBodyTex, a publicly available dataset composed of 3D scans imaging the coloured skin (textured meshes) of 200 human subjects. We manually annotated locations that appeared to the human eye to contain a pigmented skin lesion as well as tracked a subset of lesions occurring on the same subject imaged in different poses. Our results, when compared to three human annotators, suggest that the trained Faster R-CNN detects lesions at a similar performance level as the human annotators. Our lesion tracking algorithm achieves an average matching accuracy of 88% on a set of detected corresponding pairs of prominent lesions of subjects imaged in different poses, and an average longitudinal accuracy of 71% when encompassing additional errors due to lesion detection. As there currently is no other large-scale publicly available dataset of 3D total-body skin lesions, we publicly release over 25,000 3DBodyTex manual annotations, which we hope will further research on total-body skin lesion analysis.



### Subspace Representation Learning for Few-shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.00379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00379v2)
- **Published**: 2021-05-02 02:29:32+00:00
- **Updated**: 2021-05-05 01:57:40+00:00
- **Authors**: Ting-Yao Hu, Zhi-Qi Cheng, Alexander G. Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a subspace representation learning (SRL) framework to tackle few-shot image classification tasks. It exploits a subspace in local CNN feature space to represent an image, and measures the similarity between two images according to a weighted subspace distance (WSD). When K images are available for each class, we develop two types of template subspaces to aggregate K-shot information: the prototypical subspace (PS) and the discriminative subspace (DS). Based on the SRL framework, we extend metric learning based techniques from vector to subspace representation. While most previous works adopted global vector representation, using subspace representation can effectively preserve the spatial structure, and diversity within an image. We demonstrate the effectiveness of the SRL framework on three public benchmark datasets: MiniImageNet, TieredImageNet and Caltech-UCSD Birds-200-2011 (CUB), and the experimental results illustrate competitive/superior performance of our method compared to the previous state-of-the-art.



### AGMB-Transformer: Anatomy-Guided Multi-Branch Transformer Network for Automated Evaluation of Root Canal Therapy
- **Arxiv ID**: http://arxiv.org/abs/2105.00381v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.00381v2)
- **Published**: 2021-05-02 02:38:31+00:00
- **Updated**: 2021-10-28 12:48:31+00:00
- **Authors**: Yunxiang Li, Guodong Zeng, Yifan Zhang, Jun Wang, Qianni Zhang, Qun Jin, Lingling Sun, Qisi Lian, Neng Xia, Ruizi Peng, Kai Tang, Yaqi Wang, Shuai Wang
- **Comment**: under review
- **Journal**: None
- **Summary**: Accurate evaluation of the treatment result on X-ray images is a significant and challenging step in root canal therapy since the incorrect interpretation of the therapy results will hamper timely follow-up which is crucial to the patients' treatment outcome. Nowadays, the evaluation is performed in a manual manner, which is time-consuming, subjective, and error-prone. In this paper, we aim to automate this process by leveraging the advances in computer vision and artificial intelligence, to provide an objective and accurate method for root canal therapy result assessment. A novel anatomy-guided multi-branch Transformer (AGMB-Transformer) network is proposed, which first extracts a set of anatomy features and then uses them to guide a multi-branch Transformer network for evaluation. Specifically, we design a polynomial curve fitting segmentation strategy with the help of landmark detection to extract the anatomy features. Moreover, a branch fusion module and a multi-branch structure including our progressive Transformer and Group Multi-Head Self-Attention (GMHSA) are designed to focus on both global and local features for an accurate diagnosis. To facilitate the research, we have collected a large-scale root canal therapy evaluation dataset with 245 root canal therapy X-ray images, and the experiment results show that our AGMB-Transformer can improve the diagnosis accuracy from 57.96% to 90.20% compared with the baseline network. The proposed AGMB-Transformer can achieve a highly accurate evaluation of root canal therapy. To our best knowledge, our work is the first to perform automatic root canal therapy evaluation and has important clinical value to reduce the workload of endodontists.



### Attention-augmented Spatio-Temporal Segmentation for Land Cover Mapping
- **Arxiv ID**: http://arxiv.org/abs/2105.02963v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02963v3)
- **Published**: 2021-05-02 05:39:42+00:00
- **Updated**: 2021-09-28 03:38:57+00:00
- **Authors**: Rahul Ghosh, Praveen Ravirathinam, Xiaowei Jia, Chenxi Lin, Zhenong Jin, Vipin Kumar
- **Comment**: 10 pages, 3rd Workshop on Data Science for Social Good at KDD2021
- **Journal**: None
- **Summary**: The availability of massive earth observing satellite data provide huge opportunities for land use and land cover mapping. However, such mapping effort is challenging due to the existence of various land cover classes, noisy data, and the lack of proper labels. Also, each land cover class typically has its own unique temporal pattern and can be identified only during certain periods. In this article, we introduce a novel architecture that incorporates the UNet structure with Bidirectional LSTM and Attention mechanism to jointly exploit the spatial and temporal nature of satellite data and to better identify the unique temporal patterns of each land cover. We evaluate this method for mapping crops in multiple regions over the world. We compare our method with other state-of-the-art methods both quantitatively and qualitatively on two real-world datasets which involve multiple land cover classes. We also visualise the attention weights to study its effectiveness in mitigating noise and identifying discriminative time period.



### Generation and frame characteristics of predefined evenly-distributed class centroids for pattern classification
- **Arxiv ID**: http://arxiv.org/abs/2105.00401v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00401v2)
- **Published**: 2021-05-02 06:35:51+00:00
- **Updated**: 2021-06-22 02:52:43+00:00
- **Authors**: Haiping Hu, Yingying Yan, Qiuyu Zhu, Guohui Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Predefined evenly-distributed class centroids (PEDCC) can be widely used in models and algorithms of pattern classification, such as CNN classifiers, classification autoencoders, clustering, and semi-supervised learning, etc. Its basic idea is to predefine the class centers, which are evenly-distributed on the unit hypersphere in feature space, to maximize the inter-class distance. The previous method of generating PEDCC uses an iterative algorithm based on a charge model, that is, the initial values of various centers (charge positions) are randomly set from the normal distribution, and the charge positions are updated iteratively with the help of the repulsive force between charges of the same polarity. The class centers generated by the algorithm will produce some errors with the theoretically evenly-distributed points, and the generation time will be longer. This paper takes advantage of regular polyhedron in high-dimensional space and the evenly distribution of points on the n dimensional hypersphere to generate PEDCC mathematically. Then, we discussed the basic and extensive characteristics of the frames formed by PEDCC. Finally, experiments show that new algorithm is not only faster than the iterative method, but also more accurate in position. The mathematical analysis and experimental results of this paper can provide a theoretical tool for using PEDCC to solve the key problems in the field of pattern recognition, such as interpretable supervised/unsupervised learning, incremental learning, uncertainty analysis and so on.



### AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.00402v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.00402v3)
- **Published**: 2021-05-02 06:36:36+00:00
- **Updated**: 2022-03-01 17:47:02+00:00
- **Authors**: Dinh Viet Sang, Tran Quang Chung, Phan Ngoc Lan, Dao Viet Hang, Dao Van Long, Nguyen Thi Thuy
- **Comment**: None
- **Journal**: None
- **Summary**: Colorectal cancer is among the most common malignancies and can develop from high-risk colon polyps. Colonoscopy is an effective screening tool to detect and remove polyps, especially in the case of precancerous lesions. However, the missing rate in clinical practice is relatively high due to many factors. The procedure could benefit greatly from using AI models for automatic polyp segmentation, which provide valuable insights for improving colon polyp detection. However, precise segmentation is still challenging due to variations of polyps in size, shape, texture, and color. This paper proposes a novel neural network architecture called AG-CUResNeSt, which enhances Coupled UNets using the robust ResNeSt backbone and attention gates. The network is capable of effectively combining multi-level features to yield accurate polyp segmentation. Experimental results on five popular benchmark datasets show that our proposed method achieves state-of-the-art accuracy compared to existing methods.



### PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text
- **Arxiv ID**: http://arxiv.org/abs/2105.00405v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00405v4)
- **Published**: 2021-05-02 07:04:30+00:00
- **Updated**: 2021-08-02 13:29:02+00:00
- **Authors**: Wenhai Wang, Enze Xie, Xiang Li, Xuebo Liu, Ding Liang, Zhibo Yang, Tong Lu, Chunhua Shen
- **Comment**: Accepted to TPAMI 2021
- **Journal**: None
- **Summary**: Scene text detection and recognition have been well explored in the past few years. Despite the progress, efficient and accurate end-to-end spotting of arbitrarily-shaped text remains challenging. In this work, we propose an end-to-end text spotting framework, termed PAN++, which can efficiently detect and recognize text of arbitrary shapes in natural scenes. PAN++ is based on the kernel representation that reformulates a text line as a text kernel (central region) surrounded by peripheral pixels. By systematically comparing with existing scene text representations, we show that our kernel representation can not only describe arbitrarily-shaped text but also well distinguish adjacent text. Moreover, as a pixel-based representation, the kernel representation can be predicted by a single fully convolutional network, which is very friendly to real-time applications. Taking the advantages of the kernel representation, we design a series of components as follows: 1) a computationally efficient feature enhancement network composed of stacked Feature Pyramid Enhancement Modules (FPEMs); 2) a lightweight detection head cooperating with Pixel Aggregation (PA); and 3) an efficient attention-based recognition head with Masked RoI. Benefiting from the kernel representation and the tailored components, our method achieves high inference speed while maintaining competitive accuracy. Extensive experiments show the superiority of our method. For example, the proposed PAN++ achieves an end-to-end text spotting F-measure of 64.9 at 29.2 FPS on the Total-Text dataset, which significantly outperforms the previous best method. Code will be available at: https://git.io/PAN.



### Data-driven Weight Initialization with Sylvester Solvers
- **Arxiv ID**: http://arxiv.org/abs/2105.10335v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.10335v1)
- **Published**: 2021-05-02 07:33:16+00:00
- **Updated**: 2021-05-02 07:33:16+00:00
- **Authors**: Debasmit Das, Yash Bhalgat, Fatih Porikli
- **Comment**: Practical Machine Learning for Developing Countries Workshop,
  International Conference on Learning Representations, 2021
- **Journal**: None
- **Summary**: In this work, we propose a data-driven scheme to initialize the parameters of a deep neural network. This is in contrast to traditional approaches which randomly initialize parameters by sampling from transformed standard distributions. Such methods do not use the training data to produce a more informed initialization. Our method uses a sequential layer-wise approach where each layer is initialized using its input activations. The initialization is cast as an optimization problem where we minimize a combination of encoding and decoding losses of the input activations, which is further constrained by a user-defined latent code. The optimization problem is then restructured into the well-known Sylvester equation, which has fast and efficient gradient-free solutions. Our data-driven method achieves a boost in performance compared to random initialization methods, both before start of training and after training is over. We show that our proposed method is especially effective in few-shot and fine-tuning settings. We conclude this paper with analyses on time complexity and the effect of different latent codes on the recognition performance.



### Feedback control of event cameras
- **Arxiv ID**: http://arxiv.org/abs/2105.00409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00409v1)
- **Published**: 2021-05-02 07:41:39+00:00
- **Updated**: 2021-05-02 07:41:39+00:00
- **Authors**: Tobi Delbruck, Rui Graca, Marcin Paluch
- **Comment**: Accepted at 2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW); Third International Workshop on Event-Based
  Vision
- **Journal**: None
- **Summary**: Dynamic vision sensor event cameras produce a variable data rate stream of brightness change events. Event production at the pixel level is controlled by threshold, bandwidth, and refractory period bias current parameter settings. Biases must be adjusted to match application requirements and the optimal settings depend on many factors. As a first step towards automatic control of biases, this paper proposes fixed-step feedback controllers that use measurements of event rate and noise. The controllers regulate the event rate within an acceptable range using threshold and refractory period control, and regulate noise using bandwidth control. Experiments demonstrate model validity and feedback control.



### A survey on VQA_Datasets and Approaches
- **Arxiv ID**: http://arxiv.org/abs/2105.00421v1
- **DOI**: 10.1109/ITCA52113.2020.00069
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.00421v1)
- **Published**: 2021-05-02 08:50:30+00:00
- **Updated**: 2021-05-02 08:50:30+00:00
- **Authors**: Yeyun Zou, Qiyu Xie
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Visual question answering (VQA) is a task that combines both the techniques of computer vision and natural language processing. It requires models to answer a text-based question according to the information contained in a visual. In recent years, the research field of VQA has been expanded. Research that focuses on the VQA, examining the reasoning ability and VQA on scientific diagrams, has also been explored more. Meanwhile, more multimodal feature fusion mechanisms have been proposed. This paper will review and analyze existing datasets, metrics, and models proposed for the VQA task.



### Brain Graph Super-Resolution Using Adversarial Graph Neural Network with Application to Functional Brain Connectivity
- **Arxiv ID**: http://arxiv.org/abs/2105.00425v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2105.00425v1)
- **Published**: 2021-05-02 09:09:56+00:00
- **Updated**: 2021-05-02 09:09:56+00:00
- **Authors**: Megi Isallari, Islem Rekik
- **Comment**: arXiv admin note: text overlap with arXiv:2009.11080
- **Journal**: None
- **Summary**: Brain image analysis has advanced substantially in recent years with the proliferation of neuroimaging datasets acquired at different resolutions. While research on brain image super-resolution has undergone a rapid development in the recent years, brain graph super-resolution is still poorly investigated because of the complex nature of non-Euclidean graph data. In this paper, we propose the first-ever deep graph super-resolution (GSR) framework that attempts to automatically generate high-resolution (HR) brain graphs with N' nodes (i.e., anatomical regions of interest (ROIs)) from low-resolution (LR) graphs with N nodes where N < N'. First, we formalize our GSR problem as a node feature embedding learning task. Once the HR nodes' embeddings are learned, the pairwise connectivity strength between brain ROIs can be derived through an aggregation rule based on a novel Graph U-Net architecture. While typically the Graph U-Net is a node-focused architecture where graph embedding depends mainly on node attributes, we propose a graph-focused architecture where the node feature embedding is based on the graph topology. Second, inspired by graph spectral theory, we break the symmetry of the U-Net architecture by super-resolving the low-resolution brain graph structure and node content with a GSR layer and two graph convolutional network layers to further learn the node embeddings in the HR graph. Third, to handle the domain shift between the ground-truth and the predicted HR brain graphs, we incorporate adversarial regularization to align their respective distributions. Our proposed AGSR-Net framework outperformed its variants for predicting high-resolution functional brain graphs from low-resolution ones. Our AGSR-Net code is available on GitHub at https://github.com/basiralab/AGSR-Net.



### Automatic Visual Inspection of Rare Defects: A Framework based on GP-WGAN and Enhanced Faster R-CNN
- **Arxiv ID**: http://arxiv.org/abs/2105.00447v1
- **DOI**: 10.1109/IAICT52856.2021.9532584
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.00447v1)
- **Published**: 2021-05-02 11:34:59+00:00
- **Updated**: 2021-05-02 11:34:59+00:00
- **Authors**: Masoud Jalayer, Reza Jalayer, Amin Kaboli, Carlotta Orsenigo, Carlo Vercellis
- **Comment**: 13 pages, submitted for THE IEEE INTERNATIONAL CONFERENCE ON INDUSTRY
  4.0, ARTIFICIAL INTELLIGENCE, AND COMMUNICATIONS TECHNOLOGY (IAICT2021)
- **Journal**: None
- **Summary**: A current trend in industries such as semiconductors and foundry is to shift their visual inspection processes to Automatic Visual Inspection (AVI) systems, to reduce their costs, mistakes, and dependency on human experts. This paper proposes a two-staged fault diagnosis framework for AVI systems. In the first stage, a generation model is designed to synthesize new samples based on real samples. The proposed augmentation algorithm extracts objects from the real samples and blends them randomly, to generate new samples and enhance the performance of the image processor. In the second stage, an improved deep learning architecture based on Faster R-CNN, Feature Pyramid Network (FPN), and a Residual Network is proposed to perform object detection on the enhanced dataset. The performance of the algorithm is validated and evaluated on two multi-class datasets. The experimental results performed over a range of imbalance severities demonstrate the superiority of the proposed framework compared to other solutions.



### Surgical Gesture Recognition Based on Bidirectional Multi-Layer Independently RNN with Explainable Spatial Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2105.00460v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.00460v1)
- **Published**: 2021-05-02 12:47:19+00:00
- **Updated**: 2021-05-02 12:47:19+00:00
- **Authors**: Dandan Zhang, Ruoxi Wang, Benny Lo
- **Comment**: 11 pages,6 figures, Accepted by ICRA2021
- **Journal**: None
- **Summary**: Minimally invasive surgery mainly consists of a series of sub-tasks, which can be decomposed into basic gestures or contexts. As a prerequisite of autonomic operation, surgical gesture recognition can assist motion planning and decision-making, and build up context-aware knowledge to improve the surgical robot control quality. In this work, we aim to develop an effective surgical gesture recognition approach with an explainable feature extraction process. A Bidirectional Multi-Layer independently RNN (BML-indRNN) model is proposed in this paper, while spatial feature extraction is implemented via fine-tuning of a Deep Convolutional Neural Network(DCNN) model constructed based on the VGG architecture. To eliminate the black-box effects of DCNN, Gradient-weighted Class Activation Mapping (Grad-CAM) is employed. It can provide explainable results by showing the regions of the surgical images that have a strong relationship with the surgical gesture classification results. The proposed method was evaluated based on the suturing task with data obtained from the public available JIGSAWS database. Comparative studies were conducted to verify the proposed framework. Results indicated that the testing accuracy for the suturing task based on our proposed method is 87.13%, which outperforms most of the state-of-the-art algorithms.



### Unsupervised Anomaly Detection in MR Images using Multi-Contrast Information
- **Arxiv ID**: http://arxiv.org/abs/2105.00463v2
- **DOI**: 10.1002/mp.15269
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.00463v2)
- **Published**: 2021-05-02 13:05:36+00:00
- **Updated**: 2021-05-19 00:21:20+00:00
- **Authors**: Byungjai Kim, Kinam Kwon, Changheun Oh, Hyunwook Park
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection in medical imaging is to distinguish the relevant biomarkers of diseases from those of normal tissues. Deep supervised learning methods have shown potentials in various detection tasks, but its performances would be limited in medical imaging fields where collecting annotated anomaly data is limited and labor-intensive. Therefore, unsupervised anomaly detection can be an effective tool for clinical practices, which uses only unlabeled normal images as training data. In this paper, we developed an unsupervised learning framework for pixel-wise anomaly detection in multi-contrast magnetic resonance imaging (MRI). The framework has two steps of feature generation and density estimation with Gaussian mixture model (GMM). A feature is derived through the learning of contrast-to-contrast translation that effectively captures the normal tissue characteristics in multi-contrast MRI. The feature is collaboratively used with another feature that is the low-dimensional representation of multi-contrast images. In density estimation using GMM, a simple but efficient way is introduced to handle the singularity problem which interrupts the joint learning process. The proposed method outperforms previous anomaly detection approaches. Quantitative and qualitative analyses demonstrate the effectiveness of the proposed method in anomaly detection for multi-contrast MRI.



### On Feature Decorrelation in Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.00470v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2105.00470v2)
- **Published**: 2021-05-02 13:28:18+00:00
- **Updated**: 2021-08-25 16:03:17+00:00
- **Authors**: Tianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, Hang Zhao
- **Comment**: ICCV 2021 Oral. The first two authors contribute equally
- **Journal**: None
- **Summary**: In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study a relatively concise framework containing the most common components from recent approaches. We verify the existence of complete collapse and discover another reachable collapse pattern that is usually overlooked, namely dimensional collapse. We connect dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation (i.e., standardizing the covariance matrix). The gains from feature decorrelation are verified empirically to highlight the importance and the potential of this insight.



### SE-Harris and eSUSAN: Asynchronous Event-Based Corner Detection Using Megapixel Resolution CeleX-V Camera
- **Arxiv ID**: http://arxiv.org/abs/2105.00480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00480v1)
- **Published**: 2021-05-02 14:06:28+00:00
- **Updated**: 2021-05-02 14:06:28+00:00
- **Authors**: Jinjian Li, Chuandong Guo, Li Su, Xiangyu Wang, Quan Hu
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Event cameras are novel neuromorphic vision sensors with ultrahigh temporal resolution and low latency, both in the order of microseconds. Instead of image frames, event cameras generate an asynchronous event stream of per-pixel intensity changes with precise timestamps. The resulting sparse data structure impedes applying many conventional computer vision techniques to event streams, and specific algorithms should be designed to leverage the information provided by event cameras. We propose a corner detection algorithm, eSUSAN, inspired by the conventional SUSAN (smallest univalue segment assimilating nucleus) algorithm for corner detection. The proposed eSUSAN extracts the univalue segment assimilating nucleus from the circle kernel based on the similarity across timestamps and distinguishes corner events by the number of pixels in the nucleus area. Moreover, eSUSAN is fast enough to be applied to CeleX-V, the event camera with the highest resolution available. Based on eSUSAN, we also propose the SE-Harris corner detector, which uses adaptive normalization based on exponential decay to quickly construct a local surface of active events and the event-based Harris detector to refine the corners identified by eSUSAN. We evaluated the proposed algorithms on a public dataset and CeleX-V data. Both eSUSAN and SE-Harris exhibit higher real-time performance than existing algorithms while maintaining high accuracy and tracking performance.



### Residual Enhanced Multi-Hypergraph Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2105.00490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00490v1)
- **Published**: 2021-05-02 14:53:32+00:00
- **Updated**: 2021-05-02 14:53:32+00:00
- **Authors**: Jing Huang, Xiaolin Huang, Jie Yang
- **Comment**: ICIP 2021 submitted
- **Journal**: None
- **Summary**: Hypergraphs are a generalized data structure of graphs to model higher-order correlations among entities, which have been successfully adopted into various research domains. Meanwhile, HyperGraph Neural Network (HGNN) is currently the de-facto method for hypergraph representation learning. However, HGNN aims at single hypergraph learning and uses a pre-concatenation approach when confronting multi-modal datasets, which leads to sub-optimal exploitation of the inter-correlations of multi-modal hypergraphs. HGNN also suffers the over-smoothing issue, that is, its performance drops significantly when layers are stacked up. To resolve these issues, we propose the Residual enhanced Multi-Hypergraph Neural Network, which can not only fuse multi-modal information from each hypergraph effectively, but also circumvent the over-smoothing issue associated with HGNN. We conduct experiments on two 3D benchmarks, the NTU and the ModelNet40 datasets, and compare against multiple state-of-the-art methods. Experimental results demonstrate that both the residual hypergraph convolutions and the multi-fusion architecture can improve the performance of the base model and the combined model achieves a new state-of-the-art. Code is available at \url{https://github.com/OneForward/ResMHGNN}.



### CDR Based Trajectories: Tentative for Filtering Ping-pong Handover
- **Arxiv ID**: http://arxiv.org/abs/2105.00526v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00526v2)
- **Published**: 2021-05-02 18:25:53+00:00
- **Updated**: 2021-05-05 19:53:47+00:00
- **Authors**: Joonas Lõmps, Artjom Lind, Amnir Hadachi
- **Comment**: None
- **Journal**: None
- **Summary**: Call Detail Records (CDRs) coupled with the coverage area locations provide the operator with an incredible amount of information on its customers' whereabouts and movement. Due to the non-static and overlapping nature of the antenna coverage area there commonly exist situations where cellphones geographically close to each other can be connected to different antennas due to handover rule - the operator hands over a certain cellphone to another antenna to spread the load between antennas. Hence, this aspect introduces a ping-pong handover phenomena in the trajectories extracted from the CDR data which can be misleading in understanding the mobility pattern. To reconstruct accurate trajectories it is a must to reduce the number of those handovers appearing in the dataset. This letter presents a novel approach for filtering ping-pong handovers from CDR based trajectories. Primarily, the approach is based on anchors model utilizing different features and parameters extracted from the coverage areas and reconstructed trajectories mined from the CDR data. Using this methodology we can significantly reduce the ping-pong handover noise in the trajectories, which gives a more accurate reconstruction of the customers' movement pattern.



### GRNN: Generative Regression Neural Network -- A Data Leakage Attack for Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.00529v3
- **DOI**: 10.1145/3510032
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.00529v3)
- **Published**: 2021-05-02 18:39:37+00:00
- **Updated**: 2022-09-12 12:42:45+00:00
- **Authors**: Hanchi Ren, Jingjing Deng, Xianghua Xie
- **Comment**: The source code can be found at: https://github.com/Rand2AI/GRNN
- **Journal**: ACM Transactions on Intelligent Systems and Technology (TIST),
  2022
- **Summary**: Data privacy has become an increasingly important issue in Machine Learning (ML), where many approaches have been developed to tackle this challenge, e.g. cryptography (Homomorphic Encryption (HE), Differential Privacy (DP), etc.) and collaborative training (Secure Multi-Party Computation (MPC), Distributed Learning and Federated Learning (FL)). These techniques have a particular focus on data encryption or secure local computation. They transfer the intermediate information to the third party to compute the final result. Gradient exchanging is commonly considered to be a secure way of training a robust model collaboratively in Deep Learning (DL). However, recent researches have demonstrated that sensitive information can be recovered from the shared gradient. Generative Adversarial Network (GAN), in particular, has shown to be effective in recovering such information. However, GAN based techniques require additional information, such as class labels which are generally unavailable for privacy-preserved learning. In this paper, we show that, in the FL system, image-based privacy data can be easily recovered in full from the shared gradient only via our proposed Generative Regression Neural Network (GRNN). We formulate the attack to be a regression problem and optimize two branches of the generative model by minimizing the distance between gradients. We evaluate our method on several image classification tasks. The results illustrate that our proposed GRNN outperforms state-of-the-art methods with better stability, stronger robustness, and higher accuracy. It also has no convergence requirement to the global FL model. Moreover, we demonstrate information leakage using face re-identification. Some defense strategies are also discussed in this work.



### An Examination of Fairness of AI Models for Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.00558v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.00558v1)
- **Published**: 2021-05-02 21:55:04+00:00
- **Updated**: 2021-05-02 21:55:04+00:00
- **Authors**: Loc Trinh, Yan Liu
- **Comment**: To appear in the 30th International Joint Conference on Artificial
  Intelligence (IJCAI-21)
- **Journal**: None
- **Summary**: Recent studies have demonstrated that deep learning models can discriminate based on protected classes like race and gender. In this work, we evaluate bias present in deepfake datasets and detection models across protected subgroups. Using facial datasets balanced by race and gender, we examine three popular deepfake detectors and find large disparities in predictive performances across races, with up to 10.7% difference in error rate between subgroups. A closer look reveals that the widely used FaceForensics++ dataset is overwhelmingly composed of Caucasian subjects, with the majority being female Caucasians. Our investigation of the racial distribution of deepfakes reveals that the methods used to create deepfakes as positive training signals tend to produce "irregular" faces - when a person's face is swapped onto another person of a different race or gender. This causes detectors to learn spurious correlations between the foreground faces and fakeness. Moreover, when detectors are trained with the Blended Image (BI) dataset from Face X-Rays, we find that those detectors develop systematic discrimination towards certain racial subgroups, primarily female Asians.



### Learning Visually Guided Latent Actions for Assistive Teleoperation
- **Arxiv ID**: http://arxiv.org/abs/2105.00580v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.HC, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2105.00580v1)
- **Published**: 2021-05-02 23:58:28+00:00
- **Updated**: 2021-05-02 23:58:28+00:00
- **Authors**: Siddharth Karamcheti, Albert J. Zhai, Dylan P. Losey, Dorsa Sadigh
- **Comment**: Accepted at Learning for Dynamics and Control (L4DC) 2021. 12 pages,
  4 figures
- **Journal**: None
- **Summary**: It is challenging for humans -- particularly those living with physical disabilities -- to control high-dimensional, dexterous robots. Prior work explores learning embedding functions that map a human's low-dimensional inputs (e.g., via a joystick) to complex, high-dimensional robot actions for assistive teleoperation; however, a central problem is that there are many more high-dimensional actions than available low-dimensional inputs. To extract the correct action and maximally assist their human controller, robots must reason over their context: for example, pressing a joystick down when interacting with a coffee cup indicates a different action than when interacting with knife. In this work, we develop assistive robots that condition their latent embeddings on visual inputs. We explore a spectrum of visual encoders and show that incorporating object detectors pretrained on small amounts of cheap, easy-to-collect structured data enables i) accurately and robustly recognizing the current context and ii) generalizing control embeddings to new objects and tasks. In user studies with a high-dimensional physical robot arm, participants leverage this approach to perform new tasks with unseen objects. Our results indicate that structured visual representations improve few-shot performance and are subjectively preferred by users.




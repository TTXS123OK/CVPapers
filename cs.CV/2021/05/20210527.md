# Arxiv Papers in cs.CV on 2021-05-27
### i3dLoc: Image-to-range Cross-domain Localization Robust to Inconsistent Environmental Conditions
- **Arxiv ID**: http://arxiv.org/abs/2105.12883v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.12883v2)
- **Published**: 2021-05-27 00:13:11+00:00
- **Updated**: 2021-06-06 14:40:42+00:00
- **Authors**: Peng Yin, Lingyun Xu, Ji Zhang, Howie Choset, Sebastian Scherer
- **Comment**: 8 Pages, 8 Figures, Accepted Robotics: Science and Systems 2021 paper
- **Journal**: Robotics: Science and Systems 2021
- **Summary**: We present a method for localizing a single camera with respect to a point cloud map in indoor and outdoor scenes. The problem is challenging because correspondences of local invariant features are inconsistent across the domains between image and 3D. The problem is even more challenging as the method must handle various environmental conditions such as illumination, weather, and seasonal changes. Our method can match equirectangular images to the 3D range projections by extracting cross-domain symmetric place descriptors. Our key insight is to retain condition-invariant 3D geometry features from limited data samples while eliminating the condition-related features by a designed Generative Adversarial Network. Based on such features, we further design a spherical convolution network to learn viewpoint-invariant symmetric place descriptors. We evaluate our method on extensive self-collected datasets, which involve \textit{Long-term} (variant appearance conditions), \textit{Large-scale} (up to $2km$ structure/unstructured environment), and \textit{Multistory} (four-floor confined space). Our method surpasses other current state-of-the-arts by achieving around $3$ times higher place retrievals to inconsistent environments, and above $3$ times accuracy on online localization. To highlight our method's generalization capabilities, we also evaluate the recognition across different datasets. With a single trained model, i3dLoc can demonstrate reliable visual localization in random conditions.



### 3D Segmentation Learning from Sparse Annotations and Hierarchical Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2105.12885v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.12885v2)
- **Published**: 2021-05-27 00:31:37+00:00
- **Updated**: 2021-06-06 14:50:33+00:00
- **Authors**: Peng Yin, Lingyun Xu, Jianmin Ji, Sebastian Scherer, Howie Choset
- **Comment**: 8 pages, 7 figures, Accepted in IEEE Robotics and Automation Letters,
  2021
- **Journal**: None
- **Summary**: One of the main obstacles to 3D semantic segmentation is the significant amount of endeavor required to generate expensive point-wise annotations for fully supervised training. To alleviate manual efforts, we propose GIDSeg, a novel approach that can simultaneously learn segmentation from sparse annotations via reasoning global-regional structures and individual-vicinal properties. GIDSeg depicts global- and individual- relation via a dynamic edge convolution network coupled with a kernelized identity descriptor. The ensemble effects are obtained by endowing a fine-grained receptive field to a low-resolution voxelized map. In our GIDSeg, an adversarial learning module is also designed to further enhance the conditional constraint of identity descriptors within the joint feature distribution. Despite the apparent simplicity, our proposed approach achieves superior performance over state-of-the-art for inferencing 3D dense segmentation with only sparse annotations. Particularly, with $5\%$ annotations of raw data, GIDSeg outperforms other 3D segmentation methods.



### Robust Navigation for Racing Drones based on Imitation Learning and Modularization
- **Arxiv ID**: http://arxiv.org/abs/2105.12923v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.12923v1)
- **Published**: 2021-05-27 03:26:40+00:00
- **Updated**: 2021-05-27 03:26:40+00:00
- **Authors**: Tianqi Wang, Dong Eui Chang
- **Comment**: Published at the 2021 International Conference on Robotics and
  Automation (ICRA 2021)
- **Journal**: None
- **Summary**: This paper presents a vision-based modularized drone racing navigation system that uses a customized convolutional neural network (CNN) for the perception module to produce high-level navigation commands and then leverages a state-of-the-art planner and controller to generate low-level control commands, thus exploiting the advantages of both data-based and model-based approaches. Unlike the state-of-the-art method which only takes the current camera image as the CNN input, we further add the latest three drone states as part of the inputs. Our method outperforms the state-of-the-art method in various track layouts and offers two switchable navigation behaviors with a single trained network. The CNN-based perception module is trained to imitate an expert policy that automatically generates ground truth navigation commands based on the pre-computed global trajectories. Owing to the extensive randomization and our modified dataset aggregation (DAgger) policy during data collection, our navigation system, which is purely trained in simulation with synthetic textures, successfully operates in environments with randomly-chosen photorealistic textures without further fine-tuning.



### Self-Ensembling Contrastive Learning for Semi-Supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.12924v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12924v2)
- **Published**: 2021-05-27 03:27:58+00:00
- **Updated**: 2021-06-10 10:47:10+00:00
- **Authors**: Jinxi Xiang, Zhuowei Li, Wenji Wang, Qing Xia, Shaoting Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has demonstrated significant improvements in medical image segmentation using a sufficiently large amount of training data with manual labels. Acquiring well-representative labels requires expert knowledge and exhaustive labors. In this paper, we aim to boost the performance of semi-supervised learning for medical image segmentation with limited labels using a self-ensembling contrastive learning technique. To this end, we propose to train an encoder-decoder network at image-level with small amounts of labeled images, and more importantly, we learn latent representations directly at feature-level by imposing contrastive loss on unlabeled images. This method strengthens intra-class compactness and inter-class separability, so as to get a better pixel classifier. Moreover, we devise a student encoder for online learning and an exponential moving average version of it, called teacher encoder, to improve the performance iteratively in a self-ensembling manner. To construct contrastive samples with unlabeled images, two sampling strategies that exploit structure similarity across medical images and utilize pseudo-labels for construction, termed region-aware and anatomical-aware contrastive sampling, are investigated. We conduct extensive experiments on an MRI and a CT segmentation dataset and demonstrate that in a limited label setting, the proposed method achieves state-of-the-art performance. Moreover, the anatomical-aware strategy that prepares contrastive samples on-the-fly using pseudo-labels realizes better contrastive regularization on feature representations.



### Image-Based Plant Wilting Estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.12926v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.12926v1)
- **Published**: 2021-05-27 03:29:21+00:00
- **Updated**: 2021-05-27 03:29:21+00:00
- **Authors**: Changye Yang, Sriram Baireddy, Enyu Cai, Valerian Meline, Denise Caldwell, Anjali S. Iyer-Pascuzzi, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: Many plants become limp or droop through heat, loss of water, or disease. This is also known as wilting. In this paper, we examine plant wilting caused by bacterial infection. In particular, we want to design a metric for wilting based on images acquired of the plant. A quantifiable wilting metric will be useful in studying bacterial wilt and identifying resistance genes. Since there is no standard way to estimate wilting, it is common to use ad hoc visual scores. This is very subjective and requires expert knowledge of the plants and the disease mechanism. Our solution consists of using various wilting metrics acquired from RGB images of the plants. We also designed several experiments to demonstrate that our metrics are effective at estimating wilting in plants.



### YOLO5Face: Why Reinventing a Face Detector
- **Arxiv ID**: http://arxiv.org/abs/2105.12931v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12931v3)
- **Published**: 2021-05-27 03:54:38+00:00
- **Updated**: 2022-01-27 16:26:17+00:00
- **Authors**: Delong Qi, Weijun Tan, Qi Yao, Jingfeng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Tremendous progress has been made on face detection in recent years using convolutional neural networks. While many face detectors use designs designated for detecting faces, we treat face detection as a generic object detection task. We implement a face detector based on the YOLOv5 object detector and call it YOLO5Face. We make a few key modifications to the YOLOv5 and optimize it for face detection. These modifications include adding a five-point landmark regression head, using a stem block at the input of the backbone, using smaller-size kernels in the SPP, and adding a P6 output in the PAN block. We design detectors of different model sizes, from an extra-large model to achieve the best performance to a super small model for real-time detection on an embedded or mobile device. Experiment results on the WiderFace dataset show that on VGA images, our face detectors can achieve state-of-the-art performance in almost all the Easy, Medium, and Hard subsets, exceeding the more complex designated face detectors. The code is available at \url{https://github.com/deepcam-cn/yolov5-face}



### Unsupervised Adaptive Semantic Segmentation with Local Lipschitz Constraint
- **Arxiv ID**: http://arxiv.org/abs/2105.12939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12939v1)
- **Published**: 2021-05-27 04:28:45+00:00
- **Updated**: 2021-05-27 04:28:45+00:00
- **Authors**: Guanyu Cai, Lianghua He
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in unsupervised domain adaptation have seen considerable progress in semantic segmentation. Existing methods either align different domains with adversarial training or involve the self-learning that utilizes pseudo labels to conduct supervised training. The former always suffers from the unstable training caused by adversarial training and only focuses on the inter-domain gap that ignores intra-domain knowledge. The latter tends to put overconfident label prediction on wrong categories, which propagates errors to more samples. To solve these problems, we propose a two-stage adaptive semantic segmentation method based on the local Lipschitz constraint that satisfies both domain alignment and domain-specific exploration under a unified principle. In the first stage, we propose the local Lipschitzness regularization as the objective function to align different domains by exploiting intra-domain knowledge, which explores a promising direction for non-adversarial adaptive semantic segmentation. In the second stage, we use the local Lipschitzness regularization to estimate the probability of satisfying Lipschitzness for each pixel, and then dynamically sets the threshold of pseudo labels to conduct self-learning. Such dynamical self-learning effectively avoids the error propagation caused by noisy labels. Optimization in both stages is based on the same principle, i.e., the local Lipschitz constraint, so that the knowledge learned in the first stage can be maintained in the second stage. Further, due to the model-agnostic property, our method can easily adapt to any CNN-based semantic segmentation networks. Experimental results demonstrate the excellent performance of our method on standard benchmarks.



### Feature Reuse and Fusion for Real-time Semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.12964v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.12964v3)
- **Published**: 2021-05-27 06:47:02+00:00
- **Updated**: 2021-06-18 07:17:09+00:00
- **Authors**: Tan Sixiang
- **Comment**: None
- **Journal**: None
- **Summary**: For real-time semantic segmentation, how to increase the speed while maintaining high resolution is a problem that has been discussed and solved. Backbone design and fusion design have always been two essential parts of real-time semantic segmentation. We hope to design a light-weight network based on previous design experience and reach the level of state-of-the-art real-time semantic segmentation without any pre-training. To achieve this goal, a encoder-decoder architectures are proposed to solve this problem by applying a decoder network onto a backbone model designed for real-time segmentation tasks and designed three different ways to fuse semantics and detailed information in the aggregation phase. We have conducted extensive experiments on two semantic segmentation benchmarks. Experiments on the Cityscapes and CamVid datasets show that the proposed FRFNet strikes a balance between speed calculation and accuracy. It achieves 72% Mean Intersection over Union (mIoU%) on the Cityscapes test dataset with the speed of 144 on a single RTX 1080Ti card. The Code is available at https://github.com/favoMJ/FRFNet.



### Joint-DetNAS: Upgrade Your Detector with NAS, Pruning and Dynamic Distillation
- **Arxiv ID**: http://arxiv.org/abs/2105.12971v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.12971v1)
- **Published**: 2021-05-27 07:25:43+00:00
- **Updated**: 2021-05-27 07:25:43+00:00
- **Authors**: Lewei Yao, Renjie Pi, Hang Xu, Wei Zhang, Zhenguo Li, Tong Zhang
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: We propose Joint-DetNAS, a unified NAS framework for object detection, which integrates 3 key components: Neural Architecture Search, pruning, and Knowledge Distillation. Instead of naively pipelining these techniques, our Joint-DetNAS optimizes them jointly. The algorithm consists of two core processes: student morphism optimizes the student's architecture and removes the redundant parameters, while dynamic distillation aims to find the optimal matching teacher. For student morphism, weight inheritance strategy is adopted, allowing the student to flexibly update its architecture while fully utilize the predecessor's weights, which considerably accelerates the search; To facilitate dynamic distillation, an elastic teacher pool is trained via integrated progressive shrinking strategy, from which teacher detectors can be sampled without additional cost in subsequent searches. Given a base detector as the input, our algorithm directly outputs the derived student detector with high performance without additional training. Experiments demonstrate that our Joint-DetNAS outperforms the naive pipelining approach by a great margin. Given a classic R101-FPN as the base detector, Joint-DetNAS is able to boost its mAP from 41.4 to 43.9 on MS COCO and reduce the latency by 47%, which is on par with the SOTA EfficientDet while requiring less search cost. We hope our proposed method can provide the community with a new way of jointly optimizing NAS, KD and pruning.



### PSRR-MaxpoolNMS: Pyramid Shifted MaxpoolNMS with Relationship Recovery
- **Arxiv ID**: http://arxiv.org/abs/2105.12990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12990v1)
- **Published**: 2021-05-27 08:24:21+00:00
- **Updated**: 2021-05-27 08:24:21+00:00
- **Authors**: Tianyi Zhang, Jie Lin, Peng Hu, Bin Zhao, Mohamed M. Sabry Aly
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Non-maximum Suppression (NMS) is an essential postprocessing step in modern convolutional neural networks for object detection. Unlike convolutions which are inherently parallel, the de-facto standard for NMS, namely GreedyNMS, cannot be easily parallelized and thus could be the performance bottleneck in convolutional object detection pipelines. MaxpoolNMS is introduced as a parallelizable alternative to GreedyNMS, which in turn enables faster speed than GreedyNMS at comparable accuracy. However, MaxpoolNMS is only capable of replacing the GreedyNMS at the first stage of two-stage detectors like Faster-RCNN. There is a significant drop in accuracy when applying MaxpoolNMS at the final detection stage, due to the fact that MaxpoolNMS fails to approximate GreedyNMS precisely in terms of bounding box selection. In this paper, we propose a general, parallelizable and configurable approach PSRR-MaxpoolNMS, to completely replace GreedyNMS at all stages in all detectors. By introducing a simple Relationship Recovery module and a Pyramid Shifted MaxpoolNMS module, our PSRR-MaxpoolNMS is able to approximate GreedyNMS more precisely than MaxpoolNMS. Comprehensive experiments show that our approach outperforms MaxpoolNMS by a large margin, and it is proven faster than GreedyNMS with comparable accuracy. For the first time, PSRR-MaxpoolNMS provides a fully parallelizable solution for customized hardware design, which can be reused for accelerating NMS everywhere.



### Passing Multi-Channel Material Textures to a 3-Channel Loss
- **Arxiv ID**: http://arxiv.org/abs/2105.13012v1
- **DOI**: 10.1145/3450623.3464685
- **Categories**: **cs.GR**, cs.CV, I.3.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2105.13012v1)
- **Published**: 2021-05-27 08:58:51+00:00
- **Updated**: 2021-05-27 08:58:51+00:00
- **Authors**: Thomas Chambon, Eric Heitz, Laurent Belcour
- **Comment**: 2 pages, 4 figures
- **Journal**: None
- **Summary**: Our objective is to compute a textural loss that can be used to train texture generators with multiple material channels typically used for physically based rendering such as albedo, normal, roughness, metalness, ambient occlusion, etc. Neural textural losses often build on top of the feature spaces of pretrained convolutional neural networks. Unfortunately, these pretrained models are only available for 3-channel RGB data and hence limit neural textural losses to this format. To overcome this limitation, we show that passing random triplets to a 3-channel loss provides a multi-channel loss that can be used to generate high-quality material textures.



### Embedded Vision for Self-Driving on Forest Roads
- **Arxiv ID**: http://arxiv.org/abs/2105.13754v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.13754v1)
- **Published**: 2021-05-27 09:05:08+00:00
- **Updated**: 2021-05-27 09:05:08+00:00
- **Authors**: Sorin Grigorescu, Mihai Zaha, Bogdan Trasnea, Cosmin Ginerica
- **Comment**: None
- **Journal**: None
- **Summary**: Forest roads in Romania are unique natural wildlife sites used for recreation by countless tourists. In order to protect and maintain these roads, we propose RovisLab AMTU (Autonomous Mobile Test Unit), which is a robotic system designed to autonomously navigate off-road terrain and inspect if any deforestation or damage occurred along tracked route. AMTU's core component is its embedded vision module, optimized for real-time environment perception. For achieving a high computation speed, we use a learning system to train a multi-task Deep Neural Network (DNN) for scene and instance segmentation of objects, while the keypoints required for simultaneous localization and mapping are calculated using a handcrafted FAST feature detector and the Lucas-Kanade tracking algorithm. Both the DNN and the handcrafted backbone are run in parallel on the GPU of an NVIDIA AGX Xavier board. We show experimental results on the test track of our research facility.



### Stylizing 3D Scene via Implicit Representation and HyperNetwork
- **Arxiv ID**: http://arxiv.org/abs/2105.13016v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13016v3)
- **Published**: 2021-05-27 09:11:30+00:00
- **Updated**: 2022-01-16 13:46:15+00:00
- **Authors**: Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-sheng Lai, Wei-Chen Chiu
- **Comment**: Accepted to WACV2022; Project page:
  https://ztex08010518.github.io/3dstyletransfer/
- **Journal**: None
- **Summary**: In this work, we aim to address the 3D scene stylization problem - generating stylized images of the scene at arbitrary novel view angles. A straightforward solution is to combine existing novel view synthesis and image/video style transfer approaches, which often leads to blurry results or inconsistent appearance. Inspired by the high-quality results of the neural radiance fields (NeRF) method, we propose a joint framework to directly render novel views with the desired style. Our framework consists of two components: an implicit representation of the 3D scene with the neural radiance fields model, and a hypernetwork to transfer the style information into the scene representation. In particular, our implicit representation model disentangles the scene into the geometry and appearance branches, and the hypernetwork learns to predict the parameters of the appearance branch from the reference style image. To alleviate the training difficulties and memory burden, we propose a two-stage training procedure and a patch sub-sampling approach to optimize the style and content losses with the neural radiance fields model. After optimization, our model is able to render consistent novel views at arbitrary view angles with arbitrary style. Both quantitative evaluation and human subject study have demonstrated that the proposed method generates faithful stylization results with consistent appearance across different views.



### BPLF: A Bi-Parallel Linear Flow Model for Facial Expression Generation from Emotion Set Images
- **Arxiv ID**: http://arxiv.org/abs/2106.07563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.07563v1)
- **Published**: 2021-05-27 09:37:09+00:00
- **Updated**: 2021-05-27 09:37:09+00:00
- **Authors**: Gao Xu, Yuanpeng Long, Siwei Liu, Lijia Yang, Shimei Xu, Xiaoming Yao, Kunxian Shu
- **Comment**: 20 pages, 10 figures
- **Journal**: None
- **Summary**: The flow-based generative model is a deep learning generative model, which obtains the ability to generate data by explicitly learning the data distribution. Theoretically its ability to restore data is stronger than other generative models. However, its implementation has many limitations, including limited model design, too many model parameters and tedious calculation. In this paper, a bi-parallel linear flow model for facial emotion generation from emotion set images is constructed, and a series of improvements have been made in terms of the expression ability of the model and the convergence speed in training. The model is mainly composed of several coupling layers superimposed to form a multi-scale structure, in which each coupling layer contains 1*1 reversible convolution and linear operation modules. Furthermore, this paper sorted out the current public data set of facial emotion images, made a new emotion data, and verified the model through this data set. The experimental results show that, under the traditional convolutional neural network, the 3-layer 3*3 convolution kernel is more conducive to extracte the features of the face images. The introduction of principal component decomposition can improve the convergence speed of the model.



### SSAN: Separable Self-Attention Network for Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.13033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13033v1)
- **Published**: 2021-05-27 10:02:04+00:00
- **Updated**: 2021-05-27 10:02:04+00:00
- **Authors**: Xudong Guo, Xun Guo, Yan Lu
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Self-attention has been successfully applied to video representation learning due to the effectiveness of modeling long range dependencies. Existing approaches build the dependencies merely by computing the pairwise correlations along spatial and temporal dimensions simultaneously. However, spatial correlations and temporal correlations represent different contextual information of scenes and temporal reasoning. Intuitively, learning spatial contextual information first will benefit temporal modeling. In this paper, we propose a separable self-attention (SSA) module, which models spatial and temporal correlations sequentially, so that spatial contexts can be efficiently used in temporal modeling. By adding SSA module into 2D CNN, we build a SSA network (SSAN) for video representation learning. On the task of video action recognition, our approach outperforms state-of-the-art methods on Something-Something and Kinetics-400 datasets. Our models often outperform counterparts with shallower network and fewer modalities. We further verify the semantic learning ability of our method in visual-language task of video retrieval, which showcases the homogeneity of video representations and text embeddings. On MSR-VTT and Youcook2 datasets, video representations learnt by SSA significantly improve the state-of-the-art performance.



### An optimized Capsule-LSTM model for facial expression recognition with video sequences
- **Arxiv ID**: http://arxiv.org/abs/2106.07564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.07564v1)
- **Published**: 2021-05-27 10:08:05+00:00
- **Updated**: 2021-05-27 10:08:05+00:00
- **Authors**: Siwei Liu, Yuanpeng Long, Gao Xu, Lijia Yang, Shimei Xu, Xiaoming Yao, Kunxian Shu
- **Comment**: 14pages,4 figurews
- **Journal**: None
- **Summary**: To overcome the limitations of convolutional neural network in the process of facial expression recognition, a facial expression recognition model Capsule-LSTM based on video frame sequence is proposed. This model is composed of three networks includingcapsule encoders, capsule decoders and LSTM network. The capsule encoder extracts the spatial information of facial expressions in video frames. Capsule decoder reconstructs the images to optimize the network. LSTM extracts the temporal information between video frames and analyzes the differences in expression changes between frames. The experimental results from the MMI dataset show that the Capsule-LSTM model proposed in this paper can effectively improve the accuracy of video expression recognition.



### The Imaginative Generative Adversarial Network: Automatic Data Augmentation for Dynamic Skeleton-Based Hand Gesture and Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.13061v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2105.13061v2)
- **Published**: 2021-05-27 11:07:09+00:00
- **Updated**: 2023-08-10 18:54:19+00:00
- **Authors**: Junxiao Shen, John Dudley, Per Ola Kristensson
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning approaches deliver state-of-the-art performance in recognition of spatiotemporal human motion data. However, one of the main challenges in these recognition tasks is limited available training data. Insufficient training data results in over-fitting and data augmentation is one approach to address this challenge. Existing data augmentation strategies based on scaling, shifting and interpolating offer limited generalizability and typically require detailed inspection of the dataset as well as hundreds of GPU hours for hyperparameter optimization. In this paper, we present a novel automatic data augmentation model, the Imaginative Generative Adversarial Network (GAN), that approximates the distribution of the input data and samples new data from this distribution. It is automatic in that it requires no data inspection and little hyperparameter tuning and therefore it is a low-cost and low-effort approach to generate synthetic data. We demonstrate our approach on small-scale skeleton-based datasets with a comprehensive experimental analysis. Our results show that the augmentation strategy is fast to train and can improve classification accuracy for both conventional neural networks and state-of-the-art methods.



### Efficient High-Resolution Image-to-Image Translation using Multi-Scale Gradient U-Net
- **Arxiv ID**: http://arxiv.org/abs/2105.13067v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13067v1)
- **Published**: 2021-05-27 11:32:35+00:00
- **Updated**: 2021-05-27 11:32:35+00:00
- **Authors**: Kumarapu Laxman, Shiv Ram Dubey, Baddam Kalyan, Satya Raj Vineel Kojjarapu
- **Comment**: 12 pages, 6 figurea
- **Journal**: None
- **Summary**: Recently, Conditional Generative Adversarial Network (Conditional GAN) have shown very promising performance in several image-to-image translation applications. However, the uses of these conditional GANs are quite limited to low-resolution images, such as 256X256.The Pix2Pix-HD is a recent attempt to utilize the conditional GAN for high-resolution image synthesis. In this paper, we propose a Multi-Scale Gradient based U-Net (MSG U-Net) model for high-resolution image-to-image translation up to 2048X1024 resolution. The proposed model is trained by allowing the flow of gradients from multiple-discriminators to a single generator at multiple scales. The proposed MSG U-Net architecture leads to photo-realistic high-resolution image-to-image translation. Moreover, the proposed model is computationally efficient as com-pared to the Pix2Pix-HD with an improvement in the inference time nearly by 2.5 times. We provide the code of MSG U-Net model at https://github.com/laxmaniron/MSG-U-Net.



### Blind Motion Deblurring Super-Resolution: When Dynamic Spatio-Temporal Learning Meets Static Image Understanding
- **Arxiv ID**: http://arxiv.org/abs/2105.13077v2
- **DOI**: 10.1109/TIP.2021.3101402
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13077v2)
- **Published**: 2021-05-27 11:52:45+00:00
- **Updated**: 2021-10-19 02:38:35+00:00
- **Authors**: Wenjia Niu, Kaihao Zhang, Wenhan Luo, Yiran Zhong
- **Comment**: To appear in IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: Single-image super-resolution (SR) and multi-frame SR are two ways to super resolve low-resolution images. Single-Image SR generally handles each image independently, but ignores the temporal information implied in continuing frames. Multi-frame SR is able to model the temporal dependency via capturing motion information. However, it relies on neighbouring frames which are not always available in the real world. Meanwhile, slight camera shake easily causes heavy motion blur on long-distance-shot low-resolution images. To address these problems, a Blind Motion Deblurring Super-Reslution Networks, BMDSRNet, is proposed to learn dynamic spatio-temporal information from single static motion-blurred images. Motion-blurred images are the accumulation over time during the exposure of cameras, while the proposed BMDSRNet learns the reverse process and uses three-streams to learn Bidirectional spatio-temporal information based on well designed reconstruction loss functions to recover clean high-resolution images. Extensive experiments demonstrate that the proposed BMDSRNet outperforms recent state-of-the-art methods, and has the ability to simultaneously deal with image deblurring and SR.



### HDRUNet: Single Image HDR Reconstruction with Denoising and Dequantization
- **Arxiv ID**: http://arxiv.org/abs/2105.13084v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13084v2)
- **Published**: 2021-05-27 12:12:34+00:00
- **Updated**: 2021-06-19 09:47:12+00:00
- **Authors**: Xiangyu Chen, Yihao Liu, Zhengwen Zhang, Yu Qiao, Chao Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Most consumer-grade digital cameras can only capture a limited range of luminance in real-world scenes due to sensor constraints. Besides, noise and quantization errors are often introduced in the imaging process. In order to obtain high dynamic range (HDR) images with excellent visual quality, the most common solution is to combine multiple images with different exposures. However, it is not always feasible to obtain multiple images of the same scene and most HDR reconstruction methods ignore the noise and quantization loss. In this work, we propose a novel learning-based approach using a spatially dynamic encoder-decoder network, HDRUNet, to learn an end-to-end mapping for single image HDR reconstruction with denoising and dequantization. The network consists of a UNet-style base network to make full use of the hierarchical multi-scale information, a condition network to perform pattern-specific modulation and a weighting network for selectively retaining information. Moreover, we propose a Tanh_L1 loss function to balance the impact of over-exposed values and well-exposed values on the network learning. Our method achieves the state-of-the-art performance in quantitative comparisons and visual quality. The proposed HDRUNet model won the second place in the single frame track of NITRE2021 High Dynamic Range Challenge.



### Video-Based Inpatient Fall Risk Assessment: A Case Study
- **Arxiv ID**: http://arxiv.org/abs/2106.07565v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07565v1)
- **Published**: 2021-05-27 13:02:29+00:00
- **Updated**: 2021-05-27 13:02:29+00:00
- **Authors**: Ziqing Wang, Mohammad Ali Armin, Simon Denman, Lars Petersson, David Ahmedt-Aristizabal
- **Comment**: None
- **Journal**: None
- **Summary**: Inpatient falls are a serious safety issue in hospitals and healthcare facilities. Recent advances in video analytics for patient monitoring provide a non-intrusive avenue to reduce this risk through continuous activity monitoring. However, in-bed fall risk assessment systems have received less attention in the literature. The majority of prior studies have focused on fall event detection, and do not consider the circumstances that may indicate an imminent inpatient fall. Here, we propose a video-based system that can monitor the risk of a patient falling, and alert staff of unsafe behaviour to help prevent falls before they occur. We propose an approach that leverages recent advances in human localisation and skeleton pose estimation to extract spatial features from video frames recorded in a simulated environment. We demonstrate that body positions can be effectively recognised and provide useful evidence for fall risk assessment. This work highlights the benefits of video-based models for analysing behaviours of interest, and demonstrates how such a system could enable sufficient lead time for healthcare professionals to respond and address patient needs, which is necessary for the development of fall intervention programs.



### Towards Interpretable Attention Networks for Cervical Cancer Analysis
- **Arxiv ID**: http://arxiv.org/abs/2106.00557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00557v1)
- **Published**: 2021-05-27 13:28:24+00:00
- **Updated**: 2021-05-27 13:28:24+00:00
- **Authors**: Ruiqi Wang, Mohammad Ali Armin, Simon Denman, Lars Petersson, David Ahmedt-Aristizabal
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep learning have enabled the development of automated frameworks for analysing medical images and signals, including analysis of cervical cancer. Many previous works focus on the analysis of isolated cervical cells, or do not offer sufficient methods to explain and understand how the proposed models reach their classification decisions on multi-cell images. Here, we evaluate various state-of-the-art deep learning models and attention-based frameworks for the classification of images of multiple cervical cells. As we aim to provide interpretable deep learning models to address this task, we also compare their explainability through the visualization of their gradients. We demonstrate the importance of using images that contain multiple cells over using isolated single-cell images. We show the effectiveness of the residual channel attention model for extracting important features from a group of cells, and demonstrate this model's efficiency for this classification task. This work highlights the benefits of channel attention mechanisms in analyzing multiple-cell images for potential relations and distributions within a group of cells. It also provides interpretable models to address the classification of cervical cells.



### Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past, Present and Future
- **Arxiv ID**: http://arxiv.org/abs/2105.13137v1
- **DOI**: 10.3390/s21144758
- **Categories**: **cs.LG**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2105.13137v1)
- **Published**: 2021-05-27 13:32:45+00:00
- **Updated**: 2021-05-27 13:32:45+00:00
- **Authors**: David Ahmedt-Aristizabal, Mohammad Ali Armin, Simon Denman, Clinton Fookes, Lars Petersson
- **Comment**: None
- **Journal**: Sensors 2021, 21, 4758
- **Summary**: With the advances of data-driven machine learning research, a wide variety of prediction problems have been tackled. It has become critical to explore how machine learning and specifically deep learning methods can be exploited to analyse healthcare data. A major limitation of existing methods has been the focus on grid-like data; however, the structure of physiological recordings are often irregular and unordered which makes it difficult to conceptualise them as a matrix. As such, graph neural networks have attracted significant attention by exploiting implicit information that resides in a biological system, with interactive nodes connected by edges whose weights can be either temporal associations or anatomical junctions. In this survey, we thoroughly review the different types of graph architectures and their applications in healthcare. We provide an overview of these methods in a systematic manner, organized by their domain of application including functional connectivity, anatomical structure and electrical-based analysis. We also outline the limitations of existing techniques and discuss potential directions for future research.



### When Liebig's Barrel Meets Facial Landmark Detection: A Practical Model
- **Arxiv ID**: http://arxiv.org/abs/2105.13150v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13150v2)
- **Published**: 2021-05-27 13:51:42+00:00
- **Updated**: 2021-06-02 10:07:37+00:00
- **Authors**: Haibo Jin, Jinpeng Li, Shengcai Liao, Ling Shao
- **Comment**: Fix minor errors
- **Journal**: None
- **Summary**: In recent years, significant progress has been made in the research of facial landmark detection. However, few prior works have thoroughly discussed about models for practical applications. Instead, they often focus on improving a couple of issues at a time while ignoring the others. To bridge this gap, we aim to explore a practical model that is accurate, robust, efficient, generalizable, and end-to-end trainable at the same time. To this end, we first propose a baseline model equipped with one transformer decoder as detection head. In order to achieve a better accuracy, we further propose two lightweight modules, namely dynamic query initialization (DQInit) and query-aware memory (QAMem). Specifically, DQInit dynamically initializes the queries of decoder from the inputs, enabling the model to achieve as good accuracy as the ones with multiple decoder layers. QAMem is designed to enhance the discriminative ability of queries on low-resolution feature maps by assigning separate memory values to each query rather than a shared one. With the help of QAMem, our model removes the dependence on high-resolution feature maps and is still able to obtain superior accuracy. Extensive experiments and analysis on three popular benchmarks show the effectiveness and practical advantages of the proposed model. Notably, our model achieves new state of the art on WFLW as well as competitive results on 300W and COFW, while still running at 50+ FPS.



### Cardiac Segmentation on CT Images through Shape-Aware Contour Attentions
- **Arxiv ID**: http://arxiv.org/abs/2105.13153v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13153v1)
- **Published**: 2021-05-27 13:54:59+00:00
- **Updated**: 2021-05-27 13:54:59+00:00
- **Authors**: Sanguk Park, Minyoung Chung
- **Comment**: None
- **Journal**: None
- **Summary**: Cardiac segmentation of atriums, ventricles, and myocardium in computed tomography (CT) images is an important first-line task for presymptomatic cardiovascular disease diagnosis. In several recent studies, deep learning models have shown significant breakthroughs in medical image segmentation tasks. Unlike other organs such as the lungs and liver, the cardiac organ consists of multiple substructures, i.e., ventricles, atriums, aortas, arteries, veins, and myocardium. These cardiac substructures are proximate to each other and have indiscernible boundaries (i.e., homogeneous intensity values), making it difficult for the segmentation network focus on the boundaries between the substructures. In this paper, to improve the segmentation accuracy between proximate organs, we introduce a novel model to exploit shape and boundary-aware features. We primarily propose a shape-aware attention module, that exploits distance regression, which can guide the model to focus on the edges between substructures so that it can outperform the conventional contour-based attention method. In the experiments, we used the Multi-Modality Whole Heart Segmentation dataset that has 20 CT cardiac images for training and validation, and 40 CT cardiac images for testing. The experimental results show that the proposed network produces more accurate results than state-of-the-art networks by improving the Dice similarity coefficient score by 4.97%. Our proposed shape-aware contour attention mechanism demonstrates that distance transformation and boundary features improve the actual attention map to strengthen the responses in the boundary area. Moreover, our proposed method significantly reduces the false-positive responses of the final output, resulting in accurate segmentation.



### An Efficient Style Virtual Try on Network for Clothing Business Industry
- **Arxiv ID**: http://arxiv.org/abs/2105.13183v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13183v2)
- **Published**: 2021-05-27 14:37:08+00:00
- **Updated**: 2021-05-30 08:34:59+00:00
- **Authors**: Shanchen Pang, Xixi Tao, Neal N. Xiong, Yukun Dong
- **Comment**: 10 pages,9 figures
- **Journal**: None
- **Summary**: With the increasing development of garment manufacturing industry, the method of combining neural network with industry to reduce product redundancy has been paid more and more attention.In order to reduce garment redundancy and achieve personalized customization, more researchers have appeared in the field of virtual trying on.They try to transfer the target clothing to the reference figure, and then stylize the clothes to meet user's requirements for fashion.But the biggest problem of virtual try on is that the shape and motion blocking distort the clothes, causing the patterns and texture on the clothes to be impossible to restore. This paper proposed a new stylized virtual try on network, which can not only retain the authenticity of clothing texture and pattern, but also obtain the undifferentiated stylized try on. The network is divided into three sub-networks, the first is the user image, the front of the target clothing image, the semantic segmentation image and the posture heat map to generate a more detailed human parsing map. Second, UV position map and dense correspondence are used to map patterns and textures to the deformed silhouettes in real time, so that they can be retained in real time, and the rationality of spatial structure can be guaranteed on the basis of improving the authenticity of images. Third,Stylize and adjust the generated virtual try on image. Through the most subtle changes, users can choose the texture, color and style of clothing to improve the user's experience.



### Pose2Drone: A Skeleton-Pose-based Framework for Human-Drone Interaction
- **Arxiv ID**: http://arxiv.org/abs/2105.13204v2
- **DOI**: 10.23919/EUSIPCO54536.2021.9616116
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.13204v2)
- **Published**: 2021-05-27 14:50:57+00:00
- **Updated**: 2021-05-28 11:15:20+00:00
- **Authors**: Zdravko Marinov, Stanka Vasileva, Qing Wang, Constantin Seibold, Jiaming Zhang, Rainer Stiefelhagen
- **Comment**: None
- **Journal**: None
- **Summary**: Drones have become a common tool, which is utilized in many tasks such as aerial photography, surveillance, and delivery. However, operating a drone requires more and more interaction with the user. A natural and safe method for Human-Drone Interaction (HDI) is using gestures. In this paper, we introduce an HDI framework building upon skeleton-based pose estimation. Our framework provides the functionality to control the movement of the drone with simple arm gestures and to follow the user while keeping a safe distance. We also propose a monocular distance estimation method, which is entirely based on image features and does not require any additional depth sensors. To perform comprehensive experiments and quantitative analysis, we create a customized testing dataset. The experiments indicate that our HDI framework can achieve an average of 93.5\% accuracy in the recognition of 11 common gestures. The code is available at: https://github.com/Zrrr1997/Pose2Drone



### A Dataset for Provident Vehicle Detection at Night
- **Arxiv ID**: http://arxiv.org/abs/2105.13236v2
- **DOI**: 10.1109/IROS51168.2021.9636162
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.13236v2)
- **Published**: 2021-05-27 15:31:33+00:00
- **Updated**: 2021-08-11 10:00:40+00:00
- **Authors**: Sascha Saralajew, Lars Ohnemus, Lukas Ewecker, Ebubekir Asan, Simon Isele, Stefan Roos
- **Comment**: to be published in the proceedings of the 2021 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2021)
- **Journal**: None
- **Summary**: In current object detection, algorithms require the object to be directly visible in order to be detected. As humans, however, we intuitively use visual cues caused by the respective object to already make assumptions about its appearance. In the context of driving, such cues can be shadows during the day and often light reflections at night. In this paper, we study the problem of how to map this intuitive human behavior to computer vision algorithms to detect oncoming vehicles at night just from the light reflections they cause by their headlights. For that, we present an extensive open-source dataset containing 59746 annotated grayscale images out of 346 different scenes in a rural environment at night. In these images, all oncoming vehicles, their corresponding light objects (e.g., headlamps), and their respective light reflections (e.g., light reflections on guardrails) are labeled. In this context, we discuss the characteristics of the dataset and the challenges in objectively describing visual cues such as light reflections. We provide different metrics for different ways to approach the task and report the results we achieved using state-of-the-art and custom object detection models as a first benchmark. With that, we want to bring attention to a new and so far neglected field in computer vision research, encourage more researchers to tackle the problem, and thereby further close the gap between human performance and computer vision systems.



### Using Early-Learning Regularization to Classify Real-World Noisy Data
- **Arxiv ID**: http://arxiv.org/abs/2105.13244v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13244v2)
- **Published**: 2021-05-27 15:41:45+00:00
- **Updated**: 2021-06-01 14:57:46+00:00
- **Authors**: Alessio Galatolo, Alfred Nilsson, Roderick Karlemstrand, Yineng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The memorization problem is well-known in the field of computer vision. Liu et al. propose a technique called Early-Learning Regularization, which improves accuracy on the CIFAR datasets when label noise is present. This project replicates their experiments and investigates the performance on a real-world dataset with intrinsic noise. Results show that their experimental results are consistent. We also explore Sharpness-Aware Minimization in addition to SGD and observed a further 14.6 percentage points improvement. Future work includes using all 6 million images and manually clean a fraction of the images to fine-tune a transfer learning model. Last but not the least, having access to clean data for testing would also improve the measurement of accuracy.



### How saccadic vision might help with theinterpretability of deep networks
- **Arxiv ID**: http://arxiv.org/abs/2105.13264v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.13264v1)
- **Published**: 2021-05-27 16:02:40+00:00
- **Updated**: 2021-05-27 16:02:40+00:00
- **Authors**: Iana Sereda, Grigory Osipov
- **Comment**: None
- **Journal**: None
- **Summary**: We describe how some problems (interpretability,lack of object-orientedness) of modern deep networks potentiallycould be solved by adapting a biologically plausible saccadicmechanism of perception. A sketch of such a saccadic visionmodel is proposed. Proof of concept experimental results areprovided to support the proposed approach.



### ICDAR 2021 Competition on Historical Map Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.13265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13265v1)
- **Published**: 2021-05-27 16:06:50+00:00
- **Updated**: 2021-05-27 16:06:50+00:00
- **Authors**: Joseph Chazalon, Edwin Carlinet, Yizi Chen, Julien Perret, Bertrand Dumnieu, Clment Mallet, Thierry Graud, Vincent Nguyen, Nam Nguyen, Josef Baloun, Ladislav Lenc, Pavel Krl
- **Comment**: Selected as one of the official competitions for the 16th
  International Conference on Document Analysis and Recognition (ICDAR 2021),
  September 5-10, 2021, Lausanne, Switzerland (https://icdar2021.org/). Extra
  material available at https://icdar21-mapseg.github.io/
- **Journal**: None
- **Summary**: This paper presents the final results of the ICDAR 2021 Competition on Historical Map Segmentation (MapSeg), encouraging research on a series of historical atlases of Paris, France, drawn at 1/5000 scale between 1894 and 1937. The competition featured three tasks, awarded separately. Task~1 consists in detecting building blocks and was won by the L3IRIS team using a DenseNet-121 network trained in a weakly supervised fashion. This task is evaluated on 3 large images containing hundreds of shapes to detect. Task~2 consists in segmenting map content from the larger map sheet, and was won by the UWB team using a U-Net-like FCN combined with a binarization method to increase detection edge accuracy. Task~3 consists in locating intersection points of geo-referencing lines, and was also won by the UWB team who used a dedicated pipeline combining binarization, line detection with Hough transform, candidate filtering, and template matching for intersection refinement. Tasks~2 and~3 are evaluated on 95 map sheets with complex content. Dataset, evaluation tools and results are available under permissive licensing at \url{https://icdar21-mapseg.github.io/}.



### Dynamic Network selection for the Object Detection task: why it matters and what we (didn't) achieve
- **Arxiv ID**: http://arxiv.org/abs/2105.13279v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2105.13279v1)
- **Published**: 2021-05-27 16:25:18+00:00
- **Updated**: 2021-05-27 16:25:18+00:00
- **Authors**: Emanuele Vitali, Anton Lokhmotov, Gianluca Palermo
- **Comment**: Paper accepted at SAMOS21 - International Conference on Embedded
  Computer Systems: Architectures, Modeling and Simulation
- **Journal**: None
- **Summary**: In this paper, we want to show the potential benefit of a dynamic auto-tuning approach for the inference process in the Deep Neural Network (DNN) context, tackling the object detection challenge. We benchmarked different neural networks to find the optimal detector for the well-known COCO 17 database, and we demonstrate that even if we only consider the quality of the prediction there is not a single optimal network. This is even more evident if we also consider the time to solution as a metric to evaluate, and then select, the most suitable network. This opens to the possibility for an adaptive methodology to switch among different object detection networks according to run-time requirements (e.g. maximum quality subject to a time-to-solution constraint).   Moreover, we demonstrated by developing an ad hoc oracle, that an additional proactive methodology could provide even greater benefits, allowing us to select the best network among the available ones given some characteristics of the processed image. To exploit this method, we need to identify some image features that can be used to steer the decision on the most promising network. Despite the optimization opportunity that has been identified, we were not able to identify a predictor function that validates this attempt neither adopting classical image features nor by using a DNN classifier.



### Drawing Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error
- **Arxiv ID**: http://arxiv.org/abs/2105.13343v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13343v2)
- **Published**: 2021-05-27 17:51:09+00:00
- **Updated**: 2022-02-24 17:49:35+00:00
- **Authors**: Stanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, Samuel L. Smith
- **Comment**: None
- **Journal**: None
- **Summary**: In computer vision, it is standard practice to draw a single sample from the data augmentation procedure for each unique image in the mini-batch. However recent work has suggested drawing multiple samples can achieve higher test accuracies. In this work, we provide a detailed empirical evaluation of how the number of augmentation samples per unique image influences model performance on held out data when training deep ResNets. We demonstrate drawing multiple samples per image consistently enhances the test accuracy achieved for both small and large batch training. Crucially, this benefit arises even if different numbers of augmentations per image perform the same number of parameter updates and gradient evaluations (requiring the same total compute). Although prior work has found variance in the gradient estimate arising from subsampling the dataset has an implicit regularization benefit, our experiments suggest variance which arises from the data augmentation process harms generalization. We apply these insights to the highly performant NFNet-F5, achieving 86.8$\%$ top-1 w/o extra data on ImageNet.



### Tracking Without Re-recognition in Humans and Machines
- **Arxiv ID**: http://arxiv.org/abs/2105.13351v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.13351v2)
- **Published**: 2021-05-27 17:56:37+00:00
- **Updated**: 2021-06-03 00:26:33+00:00
- **Authors**: Drew Linsley, Girik Malik, Junkyung Kim, Lakshmi N Govindarajan, Ennio Mingolla, Thomas Serre
- **Comment**: None
- **Journal**: None
- **Summary**: Imagine trying to track one particular fruitfly in a swarm of hundreds. Higher biological visual systems have evolved to track moving objects by relying on both appearance and motion features. We investigate if state-of-the-art deep neural networks for visual tracking are capable of the same. For this, we introduce PathTracker, a synthetic visual challenge that asks human observers and machines to track a target object in the midst of identical-looking "distractor" objects. While humans effortlessly learn PathTracker and generalize to systematic variations in task design, state-of-the-art deep networks struggle. To address this limitation, we identify and model circuit mechanisms in biological brains that are implicated in tracking objects based on motion cues. When instantiated as a recurrent network, our circuit model learns to solve PathTracker with a robust visual strategy that rivals human performance and explains a significant proportion of their decision-making on the challenge. We also show that the success of this circuit model extends to object tracking in natural videos. Adding it to a transformer-based architecture for object tracking builds tolerance to visual nuisances that affect object appearance, resulting in a new state-of-the-art performance on the large-scale TrackingNet object tracking challenge. Our work highlights the importance of building artificial vision models that can help us better understand human vision and improve computer vision.



### Unsupervised Action Segmentation by Joint Representation Learning and Online Clustering
- **Arxiv ID**: http://arxiv.org/abs/2105.13353v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13353v7)
- **Published**: 2021-05-27 17:57:37+00:00
- **Updated**: 2023-08-17 07:21:53+00:00
- **Authors**: Sateesh Kumar, Sanjay Haresh, Awais Ahmed, Andrey Konin, M. Zeeshan Zia, Quoc-Huy Tran
- **Comment**: Presented at CVPR 2022
- **Journal**: None
- **Summary**: We present a novel approach for unsupervised activity segmentation which uses video frame clustering as a pretext task and simultaneously performs representation learning and online clustering. This is in contrast with prior works where representation learning and clustering are often performed sequentially. We leverage temporal information in videos by employing temporal optimal transport. In particular, we incorporate a temporal regularization term which preserves the temporal order of the activity into the standard optimal transport module for computing pseudo-label cluster assignments. The temporal optimal transport module enables our approach to learn effective representations for unsupervised activity segmentation. Furthermore, previous methods require storing learned features for the entire dataset before clustering them in an offline manner, whereas our approach processes one mini-batch at a time in an online manner. Extensive evaluations on three public datasets, i.e. 50-Salads, YouTube Instructions, and Breakfast, and our dataset, i.e., Desktop Assembly, show that our approach performs on par with or better than previous methods, despite having significantly less memory constraints. Our code and dataset are available on our research website: https://retrocausal.ai/research/



### Fair Feature Distillation for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.04411v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.04411v2)
- **Published**: 2021-05-27 18:00:07+00:00
- **Updated**: 2021-06-10 13:55:34+00:00
- **Authors**: Sangwon Jung, Donggyu Lee, Taeeon Park, Taesup Moon
- **Comment**: None
- **Journal**: None
- **Summary**: Fairness is becoming an increasingly crucial issue for computer vision, especially in the human-related decision systems. However, achieving algorithmic fairness, which makes a model produce indiscriminative outcomes against protected groups, is still an unresolved problem. In this paper, we devise a systematic approach which reduces algorithmic biases via feature distillation for visual recognition tasks, dubbed as MMD-based Fair Distillation (MFD). While the distillation technique has been widely used in general to improve the prediction accuracy, to the best of our knowledge, there has been no explicit work that also tries to improve fairness via distillation. Furthermore, We give a theoretical justification of our MFD on the effect of knowledge distillation and fairness. Throughout the extensive experiments, we show our MFD significantly mitigates the bias against specific minorities without any loss of the accuracy on both synthetic and real-world face datasets.



### Recent advances and clinical applications of deep learning in medical image analysis
- **Arxiv ID**: http://arxiv.org/abs/2105.13381v3
- **DOI**: 10.1016/j.media.2022.102444
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13381v3)
- **Published**: 2021-05-27 18:05:12+00:00
- **Updated**: 2022-04-08 22:23:44+00:00
- **Authors**: Xuxin Chen, Ximin Wang, Ke Zhang, Kar-Ming Fung, Theresa C. Thai, Kathleen Moore, Robert S. Mannel, Hong Liu, Bin Zheng, Yuchen Qiu
- **Comment**: To appear in the journal Medical Image Analysis. The registration
  section was revised
- **Journal**: None
- **Summary**: Deep learning has received extensive research interest in developing new medical image processing algorithms, and deep learning based models have been remarkably successful in a variety of medical imaging tasks to support disease detection and diagnosis. Despite the success, the further improvement of deep learning models in medical image analysis is majorly bottlenecked by the lack of large-sized and well-annotated datasets. In the past five years, many studies have focused on addressing this challenge. In this paper, we reviewed and summarized these recent studies to provide a comprehensive overview of applying deep learning methods in various medical image analysis tasks. Especially, we emphasize the latest progress and contributions of state-of-the-art unsupervised and semi-supervised deep learning in medical image analysis, which are summarized based on different application scenarios, including classification, segmentation, detection, and image registration. We also discuss the major technical challenges and suggest the possible solutions in future research efforts.



### Type III solar radio burst detection and classification: A deep learning approach
- **Arxiv ID**: http://arxiv.org/abs/2105.13387v1
- **DOI**: None
- **Categories**: **astro-ph.SR**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13387v1)
- **Published**: 2021-05-27 18:30:39+00:00
- **Updated**: 2021-05-27 18:30:39+00:00
- **Authors**: Jeremiah Scully, Ronan Flynn, Eoin Carley, Peter Gallagher, Mark Daly
- **Comment**: 6 pages, 6 figures, Irish Signals & Systems Conference 2021
  (pre-print)
- **Journal**: None
- **Summary**: Solar Radio Bursts (SRBs) are generally observed in dynamic spectra and have five major spectral classes, labelled Type I to Type V depending on their shape and extent in frequency and time. Due to their complex characterisation, a challenge in solar radio physics is the automatic detection and classification of such radio bursts. Classification of SRBs has become fundamental in recent years due to large data rates generated by advanced radio telescopes such as the LOw-Frequency ARray, (LOFAR). Current state-of-the-art algorithms implement the Hough or Radon transform as a means of detecting predefined parametric shapes in images. These algorithms achieve up to 84% accuracy, depending on the Type of radio burst being classified. Other techniques include procedures that rely on Constant-FalseAlarm-Rate detection, which is essentially detection of radio bursts using a de-noising and adaptive threshold in dynamic spectra. It works well for a variety of different Types of radio bursts and achieves an accuracy of up to 70%. In this research, we are introducing a methodology named You Only Look Once v2 (YOLOv2) for solar radio burst classification. By using Type III simulation methods we can train the algorithm to classify real Type III solar radio bursts in real-time at an accu



### Classification and Uncertainty Quantification of Corrupted Data using Semi-Supervised Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2105.13393v2
- **DOI**: 10.3390/psf2022005012
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13393v2)
- **Published**: 2021-05-27 18:47:55+00:00
- **Updated**: 2023-04-20 20:03:19+00:00
- **Authors**: Philipp Joppich, Sebastian Dorn, Oliver De Candido, Wolfgang Utschick, Jakob Knollmller
- **Comment**: None
- **Journal**: hysical Sciences Forum. 2022; 5(1):12
- **Summary**: Parametric and non-parametric classifiers often have to deal with real-world data, where corruptions like noise, occlusions, and blur are unavoidable - posing significant challenges. We present a probabilistic approach to classify strongly corrupted data and quantify uncertainty, despite the model only having been trained with uncorrupted data. A semi-supervised autoencoder trained on uncorrupted data is the underlying architecture. We use the decoding part as a generative model for realistic data and extend it by convolutions, masking, and additive Gaussian noise to describe imperfections. This constitutes a statistical inference task in terms of the optimal latent space activations of the underlying uncorrupted datum. We solve this problem approximately with Metric Gaussian Variational Inference (MGVI). The supervision of the autoencoder's latent space allows us to classify corrupted data directly under uncertainty with the statistically inferred latent space activations. Furthermore, we demonstrate that the model uncertainty strongly depends on whether the classification is correct or wrong, setting a basis for a statistical "lie detector" of the classification. Independent of that, we show that the generative model can optimally restore the uncorrupted datum by decoding the inferred latent space activations.



### GuideMe: A Mobile Application based on Global Positioning System and Object Recognition Towards a Smart Tourist Guide
- **Arxiv ID**: http://arxiv.org/abs/2105.13426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13426v1)
- **Published**: 2021-05-27 19:58:25+00:00
- **Updated**: 2021-05-27 19:58:25+00:00
- **Authors**: Wadii Boulila, Anmar Abuhamdah, Maha Driss, Slim Kammoun, Jawad Ahmad
- **Comment**: None
- **Journal**: None
- **Summary**: Finding information about tourist places to visit is a challenging problem that people face while visiting different countries. This problem is accentuated when people are coming from different countries, speak different languages, and are from all segments of society. In this context, visitors and pilgrims face important problems to find the appropriate doaas when visiting holy places. In this paper, we propose a mobile application that helps the user find the appropriate doaas for a given holy place in an easy and intuitive manner. Three different options are developed to achieve this goal: 1) manual search, 2) GPS location to identify the holy places and therefore their corresponding doaas, and 3) deep learning (DL) based method to determine the holy place by analyzing an image taken by the visitor. Experiments show good performance of the proposed mobile application in providing the appropriate doaas for visited holy places.



### Detection of marine floating plastic using Sentinel-2 imagery and machine learning models
- **Arxiv ID**: http://arxiv.org/abs/2106.03694v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03694v2)
- **Published**: 2021-05-27 20:24:30+00:00
- **Updated**: 2021-06-08 08:59:05+00:00
- **Authors**: Srikanta Sannigrahi, Bidroha Basu, Arunima Sarkar Basu, Francesco Pilla
- **Comment**: 30 pages
- **Journal**: None
- **Summary**: The increasing level of marine plastic pollution poses severe threats to the marine ecosystem and biodiversity. The present study attempted to explore the full functionality of open Sentinel satellite data and ML models for detecting and classifying floating plastic debris in Mytilene (Greece), Limassol (Cyprus), Calabria (Italy), and Beirut (Lebanon). Two ML models, i.e. Support Vector Machine (SVM) and Random Forest (RF) were utilized to carry out the classification analysis. In-situ plastic location data was collected from the control experiment conducted in Mytilene, Greece and Limassol, Cyprus, and the same was considered for training the models. Both remote sensing bands and spectral indices were used for developing the ML models. A spectral signature profile for plastic was created for discriminating the floating plastic from other marine debris. A newly developed index, kernel Normalized Difference Vegetation Index (kNDVI), was incorporated into the modelling to examine its contribution to model performances. Both SVM and RF were performed well in five models and test case combinations. Among the two ML models, the highest performance was measured for the RF. The inclusion of kNDVI was found effective and increased the model performances, reflected by high balanced accuracy measured for model 2 (~80% to ~98 % for SVM and ~87% to ~97 % for RF). Using the best-performed model, an automated floating plastic detection system was developed and tested in Calabria and Beirut. For both sites, the trained model had detected the floating plastic with ~99% accuracy. Among the six predictors, the FDI was found the most important variable for detecting marine floating plastic. These findings collectively suggest that high-resolution remote sensing imagery and the automated ML models can be an effective alternative for the cost-effective detection of marine floating plastic.



### Training With Data Dependent Dynamic Learning Rates
- **Arxiv ID**: http://arxiv.org/abs/2105.13464v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13464v1)
- **Published**: 2021-05-27 21:52:29+00:00
- **Updated**: 2021-05-27 21:52:29+00:00
- **Authors**: Shreyas Saxena, Nidhi Vyas, Dennis DeCoste
- **Comment**: None
- **Journal**: None
- **Summary**: Recently many first and second order variants of SGD have been proposed to facilitate training of Deep Neural Networks (DNNs). A common limitation of these works stem from the fact that they use the same learning rate across all instances present in the dataset. This setting is widely adopted under the assumption that loss functions for each instance are similar in nature, and hence, a common learning rate can be used. In this work, we relax this assumption and propose an optimization framework which accounts for difference in loss function characteristics across instances. More specifically, our optimizer learns a dynamic learning rate for each instance present in the dataset. Learning a dynamic learning rate for each instance allows our optimization framework to focus on different modes of training data during optimization. When applied to an image classification task, across different CNN architectures, learning dynamic learning rates leads to consistent gains over standard optimizers. When applied to a dataset containing corrupt instances, our framework reduces the learning rates on noisy instances, and improves over the state-of-the-art. Finally, we show that our optimization framework can be used for personalization of a machine learning model towards a known targeted data distribution.



### FastRIFE: Optimization of Real-Time Intermediate Flow Estimation for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2105.13482v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13482v2)
- **Published**: 2021-05-27 22:31:40+00:00
- **Updated**: 2021-06-01 09:45:52+00:00
- **Authors**: Malwina Kubas, Grzegorz Sarwas
- **Comment**: WSCG 2021 29. International Conference in Central Europe on Computer
  Graphics, Visualization and Computer Vision
- **Journal**: None
- **Summary**: The problem of video inter-frame interpolation is an essential task in the field of image processing. Correctly increasing the number of frames in the recording while maintaining smooth movement allows to improve the quality of played video sequence, enables more effective compression and creating a slow-motion recording. This paper proposes the FastRIFE algorithm, which is some speed improvement of the RIFE (Real-Time Intermediate Flow Estimation) model. The novel method was examined and compared with other recently published algorithms. All source codes are available at https://gitlab.com/malwinq/interpolation-of-images-for-slow-motion-videos



### Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention
- **Arxiv ID**: http://arxiv.org/abs/2105.13495v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2105.13495v2)
- **Published**: 2021-05-27 23:06:50+00:00
- **Updated**: 2021-10-21 09:52:10+00:00
- **Authors**: Byung-Hoon Kim, Jong Chul Ye, Jae-Jin Kim
- **Comment**: Accepted for NeurIPS 2021
- **Journal**: None
- **Summary**: Functional connectivity (FC) between regions of the brain can be assessed by the degree of temporal correlation measured with functional neuroimaging modalities. Based on the fact that these connectivities build a network, graph-based approaches for analyzing the brain connectome have provided insights into the functions of the human brain. The development of graph neural networks (GNNs) capable of learning representation from graph structured data has led to increased interest in learning the graph representation of the brain connectome. Although recent attempts to apply GNN to the FC network have shown promising results, there is still a common limitation that they usually do not incorporate the dynamic characteristics of the FC network which fluctuates over time. In addition, a few studies that have attempted to use dynamic FC as an input for the GNN reported a reduction in performance compared to static FC methods, and did not provide temporal explainability. Here, we propose STAGIN, a method for learning dynamic graph representation of the brain connectome with spatio-temporal attention. Specifically, a temporal sequence of brain graphs is input to the STAGIN to obtain the dynamic graph representation, while novel READOUT functions and the Transformer encoder provide spatial and temporal explainability with attention, respectively. Experiments on the HCP-Rest and the HCP-Task datasets demonstrate exceptional performance of our proposed method. Analysis of the spatio-temporal attention also provide concurrent interpretation with the neuroscientific knowledge, which further validates our method. Code is available at https://github.com/egyptdj/stagin



### Unsupervised Domain Adaptation of Object Detectors: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2105.13502v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.13502v2)
- **Published**: 2021-05-27 23:34:06+00:00
- **Updated**: 2021-07-04 04:56:32+00:00
- **Authors**: Poojan Oza, Vishwanath A. Sindagi, Vibashan VS, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep learning have led to the development of accurate and efficient models for various computer vision applications such as classification, segmentation, and detection. However, learning highly accurate models relies on the availability of large-scale annotated datasets. Due to this, model performance drops drastically when evaluated on label-scarce datasets having visually distinct images, termed as domain adaptation problem. There is a plethora of works to adapt classification and segmentation models to label-scarce target datasets through unsupervised domain adaptation. Considering that detection is a fundamental task in computer vision, many recent works have focused on developing novel domain adaptive detection techniques. Here, we describe in detail the domain adaptation problem for detection and present an extensive survey of the various methods. Furthermore, we highlight strategies proposed and the associated shortcomings. Subsequently, we identify multiple aspects of the problem that are most promising for future research. We believe that this survey shall be valuable to the pattern recognition experts working in the fields of computer vision, biometrics, medical imaging, and autonomous navigation by introducing them to the problem, and familiarizing them with the current status of the progress while providing promising directions for future research.



### Learning to Stylize Novel Views
- **Arxiv ID**: http://arxiv.org/abs/2105.13509v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13509v2)
- **Published**: 2021-05-27 23:58:18+00:00
- **Updated**: 2021-09-15 17:51:58+00:00
- **Authors**: Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh Singh, Ming-Hsuan Yang
- **Comment**: Project page: https://hhsinping.github.io/3d_scene_stylization/ Code:
  https://github.com/hhsinping/stylescene
- **Journal**: None
- **Summary**: We tackle a 3D scene stylization problem - generating stylized images of a scene from arbitrary novel views given a set of images of the same scene and a reference image of the desired style as inputs. Direct solution of combining novel view synthesis and stylization approaches lead to results that are blurry or not consistent across different views. We propose a point cloud-based method for consistent 3D scene stylization. First, we construct the point cloud by back-projecting the image features to the 3D space. Second, we develop point cloud aggregation modules to gather the style information of the 3D scene, and then modulate the features in the point cloud with a linear transformation matrix. Finally, we project the transformed features to 2D space to obtain the novel views. Experimental results on two diverse datasets of real-world scenes validate that our method generates consistent stylized novel view synthesis results against other alternative approaches.




# Arxiv Papers in cs.CV on 2021-05-05
### DeepRT: A Soft Real Time Scheduler for Computer Vision Applications on the Edge
- **Arxiv ID**: http://arxiv.org/abs/2105.01803v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, C.2.4; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2105.01803v1)
- **Published**: 2021-05-05 00:08:17+00:00
- **Updated**: 2021-05-05 00:08:17+00:00
- **Authors**: Zhe Yang, Klara Nahrstedt, Hongpeng Guo, Qian Zhou
- **Comment**: Accepted by the Sixth ACM/IEEE Symposium on Edge Computing, 2021
- **Journal**: None
- **Summary**: The ubiquity of smartphone cameras and IoT cameras, together with the recent boom of deep learning and deep neural networks, proliferate various computer vision driven mobile and IoT applications deployed on the edge. This paper focuses on applications which make soft real time requests to perform inference on their data - they desire prompt responses within designated deadlines, but occasional deadline misses are acceptable. Supporting soft real time applications on a multi-tenant edge server is not easy, since the requests sharing the limited GPU computing resources of an edge server interfere with each other. In order to tackle this problem, we comprehensively evaluate how latency and throughput respond to different GPU execution plans. Based on this analysis, we propose a GPU scheduler, DeepRT, which provides latency guarantee to the requests while maintaining high overall system throughput. The key component of DeepRT, DisBatcher, batches data from different requests as much as possible while it is proven to provide latency guarantee for requests admitted by an Admission Control Module. DeepRT also includes an Adaptation Module which tackles overruns. Our evaluation results show that DeepRT outperforms state-of-the-art works in terms of the number of deadline misses and throughput.



### Soft-Attention Improves Skin Cancer Classification Performance
- **Arxiv ID**: http://arxiv.org/abs/2105.03358v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.03358v3)
- **Published**: 2021-05-05 00:13:23+00:00
- **Updated**: 2021-06-04 19:24:55+00:00
- **Authors**: Soumyya Kanti Datta, Mohammad Abuzar Shaikh, Sargur N. Srihari, Mingchen Gao
- **Comment**: 8 pages, 9 figures, 4 tables
- **Journal**: None
- **Summary**: In clinical applications, neural networks must focus on and highlight the most important parts of an input image. Soft-Attention mechanism enables a neural network toachieve this goal. This paper investigates the effectiveness of Soft-Attention in deep neural architectures. The central aim of Soft-Attention is to boost the value of important features and suppress the noise-inducing features. We compare the performance of VGG, ResNet, InceptionResNetv2 and DenseNet architectures with and without the Soft-Attention mechanism, while classifying skin lesions. The original network when coupled with Soft-Attention outperforms the baseline[16] by 4.7% while achieving a precision of 93.7% on HAM10000 dataset [25]. Additionally, Soft-Attention coupling improves the sensitivity score by 3.8% compared to baseline[31] and achieves 91.6% on ISIC-2017 dataset [2]. The code is publicly available at github.



### Real-time Face Mask Detection in Video Data
- **Arxiv ID**: http://arxiv.org/abs/2105.01816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01816v1)
- **Published**: 2021-05-05 01:03:34+00:00
- **Updated**: 2021-05-05 01:03:34+00:00
- **Authors**: Yuchen Ding, Zichen Li, David Yastremsky
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: In response to the ongoing COVID-19 pandemic, we present a robust deep learning pipeline that is capable of identifying correct and incorrect mask-wearing from real-time video streams. To accomplish this goal, we devised two separate approaches and evaluated their performance and run-time efficiency. The first approach leverages a pre-trained face detector in combination with a mask-wearing image classifier trained on a large-scale synthetic dataset. The second approach utilizes a state-of-the-art object detection network to perform localization and classification of faces in one shot, fine-tuned on a small set of labeled real-world images. The first pipeline achieved a test accuracy of 99.97% on the synthetic dataset and maintained 6 FPS running on video data. The second pipeline achieved a mAP(0.5) of 89% on real-world images while sustaining 52 FPS on video data. We have concluded that if a larger dataset with bounding-box labels can be curated, this task is best suited using object detection architectures such as YOLO and SSD due to their superior inference speed and satisfactory performance on key evaluation metrics.



### TransHash: Transformer-based Hamming Hashing for Efficient Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2105.01823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01823v1)
- **Published**: 2021-05-05 01:35:53+00:00
- **Updated**: 2021-05-05 01:35:53+00:00
- **Authors**: Yongbiao Chen, Sheng Zhang, Fangxin Liu, Zhigang Chang, Mang Ye, Zhengwei Qi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep hamming hashing has gained growing popularity in approximate nearest neighbour search for large-scale image retrieval. Until now, the deep hashing for the image retrieval community has been dominated by convolutional neural network architectures, e.g. \texttt{Resnet}\cite{he2016deep}. In this paper, inspired by the recent advancements of vision transformers, we present \textbf{Transhash}, a pure transformer-based framework for deep hashing learning. Concretely, our framework is composed of two major modules: (1) Based on \textit{Vision Transformer} (ViT), we design a siamese vision transformer backbone for image feature extraction. To learn fine-grained features, we innovate a dual-stream feature learning on top of the transformer to learn discriminative global and local features. (2) Besides, we adopt a Bayesian learning scheme with a dynamically constructed similarity matrix to learn compact binary hash codes. The entire framework is jointly trained in an end-to-end manner.~To the best of our knowledge, this is the first work to tackle deep hashing learning problems without convolutional neural networks (\textit{CNNs}). We perform comprehensive experiments on three widely-studied datasets: \textbf{CIFAR-10}, \textbf{NUSWIDE} and \textbf{IMAGENET}. The experiments have evidenced our superiority against the existing state-of-the-art deep hashing methods. Specifically, we achieve 8.2\%, 2.6\%, 12.7\% performance gains in terms of average \textit{mAP} for different hash bit lengths on three public datasets, respectively.



### Lesion Segmentation and RECIST Diameter Prediction via Click-driven Attention and Dual-path Connection
- **Arxiv ID**: http://arxiv.org/abs/2105.01828v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01828v1)
- **Published**: 2021-05-05 02:00:14+00:00
- **Updated**: 2021-05-05 02:00:14+00:00
- **Authors**: Youbao Tang, Ke Yan, Jinzheng Cai, Lingyun Huang, Guotong Xie, Jing Xiao, Jingjing Lu, Gigin Lin, Le Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Measuring lesion size is an important step to assess tumor growth and monitor disease progression and therapy response in oncology image analysis. Although it is tedious and highly time-consuming, radiologists have to work on this task by using RECIST criteria (Response Evaluation Criteria In Solid Tumors) routinely and manually. Even though lesion segmentation may be the more accurate and clinically more valuable means, physicians can not manually segment lesions as now since much more heavy laboring will be required. In this paper, we present a prior-guided dual-path network (PDNet) to segment common types of lesions throughout the whole body and predict their RECIST diameters accurately and automatically. Similar to [1], a click guidance from radiologists is the only requirement. There are two key characteristics in PDNet: 1) Learning lesion-specific attention matrices in parallel from the click prior information by the proposed prior encoder, named click-driven attention; 2) Aggregating the extracted multi-scale features comprehensively by introducing top-down and bottom-up connections in the proposed decoder, named dual-path connection. Experiments show the superiority of our proposed PDNet in lesion segmentation and RECIST diameter prediction using the DeepLesion dataset and an external test set. PDNet learns comprehensive and representative deep image features for our tasks and produces more accurate results on both lesion segmentation and RECIST diameter prediction.



### Curvatures of Stiefel manifolds with deformation metrics
- **Arxiv ID**: http://arxiv.org/abs/2105.01834v1
- **DOI**: None
- **Categories**: **math.DG**, cs.CV, cs.LG, cs.SY, eess.SY, math.OC, 65K10, 58C05, 49Q12, 53C25, 57Z20, 57Z25, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/2105.01834v1)
- **Published**: 2021-05-05 02:13:38+00:00
- **Updated**: 2021-05-05 02:13:38+00:00
- **Authors**: Du Nguyen
- **Comment**: None
- **Journal**: Journal of Lie Theory 32 (2022), No. 2, 563--600
- **Summary**: We compute curvatures of a family of tractable metrics on Stiefel manifolds, introduced recently by H{\"u}per, Markina and Silva Leite, which includes the well-known embedded and canonical metrics on Stiefel manifolds as special cases. The metrics could be identified with the Cheeger deformation metrics. We identify parameter values in the family to make a Stiefel manifold an Einstein manifold and show Stiefel manifolds always carry an Einstein metric. We analyze the sectional curvature range and identify the parameter range where the manifold has non-negative sectional curvature. We provide the exact sectional curvature range when the number of columns in a Stiefel matrix is $2$, and a conjectural range for other cases. We derive the formulas from two approaches, one from a global curvature formula derived in our recent work, another using curvature formulas for left-invariant metrics. The second approach leads to curvature formulas for Cheeger deformation metrics on normal homogeneous spaces.



### Encoder Fusion Network with Co-Attention Embedding for Referring Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.01839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01839v1)
- **Published**: 2021-05-05 02:27:25+00:00
- **Updated**: 2021-05-05 02:27:25+00:00
- **Authors**: Guang Feng, Zhiwei Hu, Lihe Zhang, Huchuan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, referring image segmentation has aroused widespread interest. Previous methods perform the multi-modal fusion between language and vision at the decoding side of the network. And, linguistic feature interacts with visual feature of each scale separately, which ignores the continuous guidance of language to multi-scale visual features. In this work, we propose an encoder fusion network (EFN), which transforms the visual encoder into a multi-modal feature learning network, and uses language to refine the multi-modal features progressively. Moreover, a co-attention mechanism is embedded in the EFN to realize the parallel update of multi-modal features, which can promote the consistent of the cross-modal information representation in the semantic space. Finally, we propose a boundary enhancement module (BEM) to make the network pay more attention to the fine structure. The experiment results on four benchmark datasets demonstrate that the proposed approach achieves the state-of-the-art performance under different evaluation metrics without any post-processing.



### CUAB: Convolutional Uncertainty Attention Block Enhanced the Chest X-ray Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2105.01840v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01840v1)
- **Published**: 2021-05-05 02:28:04+00:00
- **Updated**: 2021-05-05 02:28:04+00:00
- **Authors**: Chi-Shiang Wang, Fang-Yi Su, Tsung-Lu Michael Lee, Yi-Shan Tsai, Jung-Hsien Chiang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, convolutional neural networks (CNNs) have been successfully implemented to various image recognition applications, such as medical image analysis, object detection, and image segmentation. Many studies and applications have been working on improving the performance of CNN algorithms and models. The strategies that aim to improve the performance of CNNs can be grouped into three major approaches: (1) deeper and wider network architecture, (2) automatic architecture search, and (3) convolutional attention block. Unlike approaches (1) and (2), the convolutional attention block approach is more flexible with lower cost. It enhances the CNN performance by extracting more efficient features. However, the existing attention blocks focus on enhancing the significant features, which lose some potential features in the uncertainty information. Inspired by the test time augmentation and test-time dropout approaches, we developed a novel convolutional uncertainty attention block (CUAB) that can leverage the uncertainty information to improve CNN-based models. The proposed module discovers potential information from the uncertain regions on feature maps in computer vision tasks. It is a flexible functional attention block that can be applied to any position in the convolutional block in CNN models. We evaluated the CUAB with notable backbone models, ResNet and ResNeXt, on a medical image segmentation task. The CUAB achieved a dice score of 73% and 84% in pneumonia and pneumothorax segmentation, respectively, thereby outperforming the original model and other notable attention approaches. The results demonstrated that the CUAB can efficiently utilize the uncertainty information to improve the model performance.



### Joint Registration and Segmentation via Multi-Task Learning for Adaptive Radiotherapy of Prostate Cancer
- **Arxiv ID**: http://arxiv.org/abs/2105.01844v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01844v1)
- **Published**: 2021-05-05 02:45:49+00:00
- **Updated**: 2021-05-05 02:45:49+00:00
- **Authors**: Mohamed S. Elmahdy, Laurens Beljaards, Sahar Yousefi, Hessam Sokooti, Fons Verbeek, U. A. van der Heide, Marius Staring
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image registration and segmentation are two of the most frequent tasks in medical image analysis. As these tasks are complementary and correlated, it would be beneficial to apply them simultaneously in a joint manner. In this paper, we formulate registration and segmentation as a joint problem via a Multi-Task Learning (MTL) setting, allowing these tasks to leverage their strengths and mitigate their weaknesses through the sharing of beneficial information. We propose to merge these tasks not only on the loss level, but on the architectural level as well. We studied this approach in the context of adaptive image-guided radiotherapy for prostate cancer, where planning and follow-up CT images as well as their corresponding contours are available for training. The study involves two datasets from different manufacturers and institutes. The first dataset was divided into training (12 patients) and validation (6 patients), and was used to optimize and validate the methodology, while the second dataset (14 patients) was used as an independent test set. We carried out an extensive quantitative comparison between the quality of the automatically generated contours from different network architectures as well as loss weighting methods. Moreover, we evaluated the quality of the generated deformation vector field (DVF). We show that MTL algorithms outperform their Single-Task Learning (STL) counterparts and achieve better generalization on the independent test set. The best algorithm achieved a mean surface distance of $1.06 \pm 0.3$ mm, $1.27 \pm 0.4$ mm, $0.91 \pm 0.4$ mm, and $1.76 \pm 0.8$ mm on the validation set for the prostate, seminal vesicles, bladder, and rectum, respectively. The high accuracy of the proposed method combined with the fast inference speed, makes it a promising method for automatic re-contouring of follow-up scans for adaptive radiotherapy.



### PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific Table Image Recognition to Latex
- **Arxiv ID**: http://arxiv.org/abs/2105.01846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.01846v1)
- **Published**: 2021-05-05 03:15:48+00:00
- **Updated**: 2021-05-05 03:15:48+00:00
- **Authors**: Yelin He, Xianbiao Qi, Jiaquan Ye, Peng Gao, Yihao Chen, Bingcong Li, Xin Tang, Rong Xiao
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: This paper presents our solution for the ICDAR 2021 Competition on Scientific Table Image Recognition to LaTeX. This competition has two sub-tasks: Table Structure Reconstruction (TSR) and Table Content Reconstruction (TCR). We treat both sub-tasks as two individual image-to-sequence recognition problems. We leverage our previously proposed algorithm MASTER \cite{lu2019master}, which is originally proposed for scene text recognition. We optimize the MASTER model from several perspectives: network structure, optimizer, normalization method, pre-trained model, resolution of input image, data augmentation, and model ensemble. Our method achieves 0.7444 Exact Match and 0.8765 Exact Match @95\% on the TSR task, and obtains 0.5586 Exact Match and 0.7386 Exact Match 95\% on the TCR task.



### PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific Literature Parsing Task B: Table Recognition to HTML
- **Arxiv ID**: http://arxiv.org/abs/2105.01848v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.01848v1)
- **Published**: 2021-05-05 03:20:26+00:00
- **Updated**: 2021-05-05 03:20:26+00:00
- **Authors**: Jiaquan Ye, Xianbiao Qi, Yelin He, Yihao Chen, Dengyi Gu, Peng Gao, Rong Xiao
- **Comment**: 8 Pages, 7 Figures
- **Journal**: None
- **Summary**: This paper presents our solution for ICDAR 2021 competition on scientific literature parsing taskB: table recognition to HTML. In our method, we divide the table content recognition task into foursub-tasks: table structure recognition, text line detection, text line recognition, and box assignment.Our table structure recognition algorithm is customized based on MASTER [1], a robust image textrecognition algorithm. PSENet [2] is used to detect each text line in the table image. For text linerecognition, our model is also built on MASTER. Finally, in the box assignment phase, we associatedthe text boxes detected by PSENet with the structure item reconstructed by table structure prediction,and fill the recognized content of the text line into the corresponding item. Our proposed methodachieves a 96.84% TEDS score on 9,115 validation samples in the development phase, and a 96.32%TEDS score on 9,064 samples in the final evaluation phase.



### Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer RGBD Sensors
- **Arxiv ID**: http://arxiv.org/abs/2105.01859v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01859v2)
- **Published**: 2021-05-05 04:12:38+00:00
- **Updated**: 2021-05-06 07:38:27+00:00
- **Authors**: Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, Yebin Liu
- **Comment**: CVPR 2021 Oral Paper, Project Page:
  http://www.liuyebin.com/Function4D/Function4D.html, THuman2.0 dataset
  available. Youtube: https://www.youtube.com/watch?v=-rWUn4fEQNU&t=126s
- **Journal**: None
- **Summary**: Human volumetric capture is a long-standing topic in computer vision and computer graphics. Although high-quality results can be achieved using sophisticated off-line systems, real-time human volumetric capture of complex scenarios, especially using light-weight setups, remains challenging. In this paper, we propose a human volumetric capture method that combines temporal volumetric fusion and deep implicit functions. To achieve high-quality and temporal-continuous reconstruction, we propose dynamic sliding fusion to fuse neighboring depth observations together with topology consistency. Moreover, for detailed and complete surface generation, we propose detail-preserving deep implicit functions for RGBD input which can not only preserve the geometric details on the depth inputs but also generate more plausible texturing results. Results and experiments show that our method outperforms existing methods in terms of view sparsity, generalization capacity, reconstruction quality, and run-time efficiency.



### MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space
- **Arxiv ID**: http://arxiv.org/abs/2105.01879v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01879v1)
- **Published**: 2021-05-05 05:58:29+00:00
- **Updated**: 2021-05-05 05:58:29+00:00
- **Authors**: Rui Huang, Yixuan Li
- **Comment**: Paper accepted as an oral presentation in CVPR'21
- **Journal**: None
- **Summary**: Detecting out-of-distribution (OOD) inputs is a central challenge for safely deploying machine learning models in the real world. Existing solutions are mainly driven by small datasets, with low resolution and very few class labels (e.g., CIFAR). As a result, OOD detection for large-scale image classification tasks remains largely unexplored. In this paper, we bridge this critical gap by proposing a group-based OOD detection framework, along with a novel OOD scoring function termed MOS. Our key idea is to decompose the large semantic space into smaller groups with similar concepts, which allows simplifying the decision boundaries between in- vs. out-of-distribution data for effective OOD detection. Our method scales substantially better for high-dimensional class space than previous approaches. We evaluate models trained on ImageNet against four carefully curated OOD datasets, spanning diverse semantics. MOS establishes state-of-the-art performance, reducing the average FPR95 by 14.33% while achieving 6x speedup in inference compared to the previous best method.



### A Robotic Approach towards Quantifying Epipelagic Bound Plastic Using Deep Visual Models
- **Arxiv ID**: http://arxiv.org/abs/2105.01882v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.01882v4)
- **Published**: 2021-05-05 06:04:26+00:00
- **Updated**: 2021-10-19 21:38:32+00:00
- **Authors**: Gautam Tata, Sarah-Jeanne Royer, Olivier Poirion, Jay Lowe
- **Comment**: 8 Pages, 6 Figures, 2 Tables - Added Paragraph for Code Availability
  - Submitted preprint to Elsevier
- **Journal**: None
- **Summary**: The quantification of positively buoyant marine plastic debris is critical to understanding how plastic litter accumulates across the world's oceans and is also crucial to identifying hotspots for targeted cleanup efforts. Currently, the most common method to quantify marine plastic is using manta trawls for manual sampling. However, this method is cost-intensive and requires human labor. This study removes the need for manual sampling by using an autonomous method using neural networks and computer vision models, which trained on images captured from various layers of the ocean column to perform real-time plastic quantification. The best performing model has a Mean Average Precision of 85% and an F1-Score of 0.89 while maintaining near real-time processing speeds ~2 ms/img.



### RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.01883v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01883v3)
- **Published**: 2021-05-05 06:17:40+00:00
- **Updated**: 2022-03-30 13:36:31+00:00
- **Authors**: Xiaohan Ding, Chunlong Xia, Xiangyu Zhang, Xiaojie Chu, Jungong Han, Guiguang Ding
- **Comment**: This was a work in progress. Latest version is arXiv:2112.11081
  (accepted by CVPR 2022)
- **Journal**: None
- **Summary**: We propose RepMLP, a multi-layer-perceptron-style neural network building block for image recognition, which is composed of a series of fully-connected (FC) layers. Compared to convolutional layers, FC layers are more efficient, better at modeling the long-range dependencies and positional patterns, but worse at capturing the local structures, hence usually less favored for image recognition. We propose a structural re-parameterization technique that adds local prior into an FC to make it powerful for image recognition. Specifically, we construct convolutional layers inside a RepMLP during training and merge them into the FC for inference. On CIFAR, a simple pure-MLP model shows performance very close to CNN. By inserting RepMLP in traditional CNN, we improve ResNets by 1.8% accuracy on ImageNet, 2.9% for face recognition, and 2.3% mIoU on Cityscapes with lower FLOPs. Our intriguing findings highlight that combining the global representational capacity and positional perception of FC with the local prior of convolution can improve the performance of neural network with faster speed on both the tasks with translation invariance (e.g., semantic segmentation) and those with aligned images and positional patterns (e.g., face recognition). The code and models are available at https://github.com/DingXiaoH/RepMLP.



### MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/2105.01899v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01899v1)
- **Published**: 2021-05-05 07:17:57+00:00
- **Updated**: 2021-05-05 07:17:57+00:00
- **Authors**: Tsung Wei Tsai, Chongxuan Li, Jun Zhu
- **Comment**: International Conference on Learning Representations (ICLR) 2021
- **Journal**: None
- **Summary**: We present Mixture of Contrastive Experts (MiCE), a unified probabilistic clustering framework that simultaneously exploits the discriminative representations learned by contrastive learning and the semantic structures captured by a latent mixture model. Motivated by the mixture of experts, MiCE employs a gating function to partition an unlabeled dataset into subsets according to the latent semantics and multiple experts to discriminate distinct subsets of instances assigned to them in a contrastive learning manner. To solve the nontrivial inference and learning problems caused by the latent variables, we further develop a scalable variant of the Expectation-Maximization (EM) algorithm for MiCE and provide proof of the convergence. Empirically, we evaluate the clustering performance of MiCE on four widely adopted natural image datasets. MiCE achieves significantly better results than various previous methods and a strong contrastive learning baseline.



### 4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface
- **Arxiv ID**: http://arxiv.org/abs/2105.01905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01905v1)
- **Published**: 2021-05-05 07:39:12+00:00
- **Updated**: 2021-05-05 07:39:12+00:00
- **Authors**: Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng, Matthias Nie√üner
- **Comment**: Data: https://github.com/rabbityl/DeformingThings4D
- **Journal**: None
- **Summary**: Tracking non-rigidly deforming scenes using range sensors has numerous applications including computer vision, AR/VR, and robotics. However, due to occlusions and physical limitations of range sensors, existing methods only handle the visible surface, thus causing discontinuities and incompleteness in the motion field. To this end, we introduce 4DComplete, a novel data-driven approach that estimates the non-rigid motion for the unobserved geometry. 4DComplete takes as input a partial shape and motion observation, extracts 4D time-space embedding, and jointly infers the missing geometry and motion field using a sparse fully-convolutional network. For network training, we constructed a large-scale synthetic dataset called DeformingThings4D, which consists of 1972 animation sequences spanning 31 different animals or humanoid categories with dense 4D annotation. Experiments show that 4DComplete 1) reconstructs high-resolution volumetric shape and motion field from a partial observation, 2) learns an entangled 4D feature representation that benefits both shape and motion estimation, 3) yields more accurate and natural deformation than classic non-rigid priors such as As-Rigid-As-Possible (ARAP) deformation, and 4) generalizes well to unseen objects in real-world sequences.



### Weakly Supervised Pseudo-Label assisted Learning for ALS Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.01919v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01919v1)
- **Published**: 2021-05-05 08:07:21+00:00
- **Updated**: 2021-05-05 08:07:21+00:00
- **Authors**: Puzuo Wang, Wei Yao
- **Comment**: accepted for publication in the ISPRS Annals of the Photogrammetry,
  Remote Sensing and Spatial Information Sciences (online from July 2021)
- **Journal**: None
- **Summary**: Competitive point cloud semantic segmentation results usually rely on a large amount of labeled data. However, data annotation is a time-consuming and labor-intensive task, particularly for three-dimensional point cloud data. Thus, obtaining accurate results with limited ground truth as training data is considerably important. As a simple and effective method, pseudo labels can use information from unlabeled data for training neural networks. In this study, we propose a pseudo-label-assisted point cloud segmentation method with very few sparsely sampled labels that are normally randomly selected for each class. An adaptive thresholding strategy was proposed to generate a pseudo-label based on the prediction probability. Pseudo-label learning is an iterative process, and pseudo labels were updated solely on ground-truth weak labels as the model converged to improve the training efficiency. Experiments using the ISPRS 3D sematic labeling benchmark dataset indicated that our proposed method achieved an equally competitive result compared to that using a full supervision scheme with only up to 2$\unicode{x2030}$ of labeled points from the original training set, with an overall accuracy of 83.7% and an average F1 score of 70.2%.



### SeaDronesSee: A Maritime Benchmark for Detecting Humans in Open Water
- **Arxiv ID**: http://arxiv.org/abs/2105.01922v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01922v2)
- **Published**: 2021-05-05 08:18:36+00:00
- **Updated**: 2021-10-19 07:23:09+00:00
- **Authors**: Leon Amadeus Varga, Benjamin Kiefer, Martin Messmer, Andreas Zell
- **Comment**: Leon Amadeus Varga, Benjamin Kiefer, Martin Messmer contributed
  equally to this work. The order of names is determined by coin flipping.
  Accepted WACV 2022
- **Journal**: None
- **Summary**: Unmanned Aerial Vehicles (UAVs) are of crucial importance in search and rescue missions in maritime environments due to their flexible and fast operation capabilities. Modern computer vision algorithms are of great interest in aiding such missions. However, they are dependent on large amounts of real-case training data from UAVs, which is only available for traffic scenarios on land. Moreover, current object detection and tracking data sets only provide limited environmental information or none at all, neglecting a valuable source of information. Therefore, this paper introduces a large-scaled visual object detection and tracking benchmark (SeaDronesSee) aiming to bridge the gap from land-based vision systems to sea-based ones. We collect and annotate over 54,000 frames with 400,000 instances captured from various altitudes and viewing angles ranging from 5 to 260 meters and 0 to 90 degrees while providing the respective meta information for altitude, viewing angle and other meta data. We evaluate multiple state-of-the-art computer vision algorithms on this newly established benchmark serving as baselines. We provide an evaluation server where researchers can upload their prediction and compare their results on a central leaderboard



### Novelty Detection and Analysis of Traffic Scenario Infrastructures in the Latent Space of a Vision Transformer-Based Triplet Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2105.01924v2
- **DOI**: 10.1109/IV48863.2021.9575730
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01924v2)
- **Published**: 2021-05-05 08:24:03+00:00
- **Updated**: 2021-11-02 07:18:36+00:00
- **Authors**: Jonas Wurst, Lakshman Balasubramanian, Michael Botsch, Wolfgang Utschick
- **Comment**: Copyright 2021 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: 2021 IEEE Intelligent Vehicles Symposium (IV)
- **Summary**: Detecting unknown and untested scenarios is crucial for scenario-based testing. Scenario-based testing is considered to be a possible approach to validate autonomous vehicles. A traffic scenario consists of multiple components, with infrastructure being one of it. In this work, a method to detect novel traffic scenarios based on their infrastructure images is presented. An autoencoder triplet network provides latent representations for infrastructure images which are used for outlier detection. The triplet training of the network is based on the connectivity graphs of the infrastructure. By using the proposed architecture, expert-knowledge is used to shape the latent space such that it incorporates a pre-defined similarity in the neighborhood relationships of an autoencoder. An ablation study on the architecture is highlighting the importance of the triplet autoencoder combination. The best performing architecture is based on vision transformers, a convolution-free attention-based network. The presented method outperforms other state-of-the-art outlier detection approaches.



### Instances as Queries
- **Arxiv ID**: http://arxiv.org/abs/2105.01928v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01928v3)
- **Published**: 2021-05-05 08:38:25+00:00
- **Updated**: 2021-05-23 16:36:54+00:00
- **Authors**: Yuxin Fang, Shusheng Yang, Xinggang Wang, Yu Li, Chen Fang, Ying Shan, Bin Feng, Wenyu Liu
- **Comment**: 14 pages, 8 figures, including the Appendix
- **Journal**: None
- **Summary**: Recently, query based object detection frameworks achieve comparable performance with previous state-of-the-art object detectors. However, how to fully leverage such frameworks to perform instance segmentation remains an open problem. In this paper, we present QueryInst (Instances as Queries), a query based instance segmentation method driven by parallel supervision on dynamic mask heads. The key insight of QueryInst is to leverage the intrinsic one-to-one correspondence in object queries across different stages, as well as one-to-one correspondence between mask RoI features and object queries in the same stage. This approach eliminates the explicit multi-stage mask head connection and the proposal distribution inconsistency issues inherent in non-query based multi-stage instance segmentation methods. We conduct extensive experiments on three challenging benchmarks, i.e., COCO, CityScapes, and YouTube-VIS to evaluate the effectiveness of QueryInst in instance segmentation and video instance segmentation (VIS) task. Specifically, using ResNet-101-FPN backbone, QueryInst obtains 48.1 box AP and 42.8 mask AP on COCO test-dev, which is 2 points higher than HTC in terms of both box AP and mask AP, while runs 2.4 times faster. For video instance segmentation, QueryInst achieves the best performance among all online VIS approaches and strikes a decent speed-accuracy trade-off. Code is available at \url{https://github.com/hustvl/QueryInst}.



### FLEX: Extrinsic Parameters-free Multi-view 3D Human Motion Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2105.01937v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2105.01937v4)
- **Published**: 2021-05-05 09:08:12+00:00
- **Updated**: 2022-10-21 14:56:49+00:00
- **Authors**: Brian Gordon, Sigal Raab, Guy Azov, Raja Giryes, Daniel Cohen-Or
- **Comment**: Accepted to ECCV22 Project page: https://briang13.github.io/FLEX/
  Video: https://www.youtube.com/watch?v=hXc97BDJJTg
- **Journal**: None
- **Summary**: The increasing availability of video recordings made by multiple cameras has offered new means for mitigating occlusion and depth ambiguities in pose and motion reconstruction methods. Yet, multi-view algorithms strongly depend on camera parameters; particularly, the relative transformations between the cameras. Such a dependency becomes a hurdle once shifting to dynamic capture in uncontrolled settings. We introduce FLEX (Free muLti-view rEconstruXion), an end-to-end extrinsic parameter-free multi-view model. FLEX is extrinsic parameter-free (dubbed ep-free) in the sense that it does not require extrinsic camera parameters. Our key idea is that the 3D angles between skeletal parts, as well as bone lengths, are invariant to the camera position. Hence, learning 3D rotations and bone lengths rather than locations allows predicting common values for all camera views. Our network takes multiple video streams, learns fused deep features through a novel multi-view fusion layer, and reconstructs a single consistent skeleton with temporally coherent joint rotations. We demonstrate quantitative and qualitative results on three public datasets, and on synthetic multi-person video streams captured by dynamic cameras. We compare our model to state-of-the-art methods that are not ep-free and show that in the absence of camera parameters, we outperform them by a large margin while obtaining comparable results when camera parameters are available. Code, trained models, and other materials are available on our project page.



### Towards Self-Supervision for Video Identification of Individual Holstein-Friesian Cattle: The Cows2021 Dataset
- **Arxiv ID**: http://arxiv.org/abs/2105.01938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01938v1)
- **Published**: 2021-05-05 09:08:19+00:00
- **Updated**: 2021-05-05 09:08:19+00:00
- **Authors**: Jing Gao, Tilo Burghardt, William Andrew, Andrew W. Dowsey, Neill W. Campbell
- **Comment**: 6 pages, 8 figures, 1 table, dataset will be available, code will be
  available
- **Journal**: None
- **Summary**: In this paper we publish the largest identity-annotated Holstein-Friesian cattle dataset Cows2021 and a first self-supervision framework for video identification of individual animals. The dataset contains 10,402 RGB images with labels for localisation and identity as well as 301 videos from the same herd. The data shows top-down in-barn imagery, which captures the breed's individually distinctive black and white coat pattern. Motivated by the labelling burden involved in constructing visual cattle identification systems, we propose exploiting the temporal coat pattern appearance across videos as a self-supervision signal for animal identity learning. Using an individual-agnostic cattle detector that yields oriented bounding-boxes, rotation-normalised tracklets of individuals are formed via tracking-by-detection and enriched via augmentations. This produces a `positive' sample set per tracklet, which is paired against a `negative' set sampled from random cattle of other videos. Frame-triplet contrastive learning is then employed to construct a metric latent space. The fitting of a Gaussian Mixture Model to this space yields a cattle identity classifier. Results show an accuracy of Top-1 57.0% and Top-4: 76.9% and an Adjusted Rand Index: 0.53 compared to the ground truth. Whilst supervised training surpasses this benchmark by a large margin, we conclude that self-supervision can nevertheless play a highly effective role in speeding up labelling efforts when initially constructing supervision information. We provide all data and full source code alongside an analysis and evaluation of the system.



### Continual Learning on the Edge with TensorFlow Lite
- **Arxiv ID**: http://arxiv.org/abs/2105.01946v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01946v1)
- **Published**: 2021-05-05 09:32:06+00:00
- **Updated**: 2021-05-05 09:32:06+00:00
- **Authors**: Giorgos Demosthenous, Vassilis Vassiliades
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: Deploying sophisticated deep learning models on embedded devices with the purpose of solving real-world problems is a struggle using today's technology. Privacy and data limitations, network connection issues, and the need for fast model adaptation are some of the challenges that constitute today's approaches unfit for many applications on the edge and make real-time on-device training a necessity. Google is currently working on tackling these challenges by embedding an experimental transfer learning API to their TensorFlow Lite, machine learning library. In this paper, we show that although transfer learning is a good first step for on-device model training, it suffers from catastrophic forgetting when faced with more realistic scenarios. We present this issue by testing a simple transfer learning model on the CORe50 benchmark as well as by demonstrating its limitations directly on an Android application we developed. In addition, we expand the TensorFlow Lite library to include continual learning capabilities, by integrating a simple replay approach into the head of the current transfer learning model. We test our continual learning model on the CORe50 benchmark to show that it tackles catastrophic forgetting, and we demonstrate its ability to continually learn, even under non-ideal conditions, using the application we developed. Finally, we open-source the code of our Android application to enable developers to integrate continual learning to their own smartphone applications, as well as to facilitate further development of continual learning functionality into the TensorFlow Lite environment.



### Multi-scale Image Decomposition using a Local Statistical Edge Model
- **Arxiv ID**: http://arxiv.org/abs/2105.01951v1
- **DOI**: 10.1109/ICVR51878.2021.9483837
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01951v1)
- **Published**: 2021-05-05 09:38:07+00:00
- **Updated**: 2021-05-05 09:38:07+00:00
- **Authors**: Kin-Ming Wong
- **Comment**: 9 pages and 13 figures. To be published in the Proceedings of IEEE
  7th International Conference on Virtual Reality, 2021
- **Journal**: None
- **Summary**: We present a progressive image decomposition method based on a novel non-linear filter named Sub-window Variance filter. Our method is specifically designed for image detail enhancement purpose; this application requires extraction of image details which are small in terms of both spatial and variation scales. We propose a local statistical edge model which develops its edge awareness using spatially defined image statistics. Our decomposition method is controlled by two intuitive parameters which allow the users to define what image details to suppress or enhance. By using the summed-area table acceleration method, our decomposition pipeline is highly parallel. The proposed filter is gradient preserving and this allows our enhancement results free from the gradient-reversal artefact. In our evaluations, we compare our method in various multi-scale image detail manipulation applications with other mainstream solutions.



### Perceptual Gradient Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.01957v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01957v1)
- **Published**: 2021-05-05 09:58:22+00:00
- **Updated**: 2021-05-05 09:58:22+00:00
- **Authors**: Dmitry Nikulin, Roman Suvorov, Aleksei Ivakhnenko, Victor Lempitsky
- **Comment**: 28 pages, 15 figures, 8 tables
- **Journal**: None
- **Summary**: Many applications of deep learning for image generation use perceptual losses for either training or fine-tuning of the generator networks. The use of perceptual loss however incurs repeated forward-backward passes in a large image classification network as well as a considerable memory overhead required to store the activations of this network. It is therefore desirable or sometimes even critical to get rid of these overheads.   In this work, we propose a way to train generator networks using approximations of perceptual loss that are computed without forward-backward passes. Instead, we use a simpler perceptual gradient network that directly synthesizes the gradient field of a perceptual loss. We introduce the concept of proxy targets, which stabilize the predicted gradient, meaning that learning with it does not lead to divergence or oscillations. In addition, our method allows interpretation of the predicted gradient, providing insight into the internals of perceptual loss and suggesting potential ways to improve it in future work.



### AdaVQA: Overcoming Language Priors with Adapted Margin Cosine Loss
- **Arxiv ID**: http://arxiv.org/abs/2105.01993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01993v1)
- **Published**: 2021-05-05 11:41:38+00:00
- **Updated**: 2021-05-05 11:41:38+00:00
- **Authors**: Yangyang Guo, Liqiang Nie, Zhiyong Cheng, Feng Ji, Ji Zhang, Alberto Del Bimbo
- **Comment**: None
- **Journal**: None
- **Summary**: A number of studies point out that current Visual Question Answering (VQA) models are severely affected by the language prior problem, which refers to blindly making predictions based on the language shortcut. Some efforts have been devoted to overcoming this issue with delicate models. However, there is no research to address it from the angle of the answer feature space learning, despite of the fact that existing VQA methods all cast VQA as a classification task. Inspired by this, in this work, we attempt to tackle the language prior problem from the viewpoint of the feature space learning. To this end, an adapted margin cosine loss is designed to discriminate the frequent and the sparse answer feature space under each question type properly. As a result, the limited patterns within the language modality are largely reduced, thereby less language priors would be introduced by our method. We apply this loss function to several baseline models and evaluate its effectiveness on two VQA-CP benchmarks. Experimental results demonstrate that our adapted margin cosine loss can greatly enhance the baseline models with an absolute performance gain of 15\% on average, strongly verifying the potential of tackling the language prior problem in VQA from the angle of the answer feature space learning.



### Instance segmentation of fallen trees in aerial color infrared imagery using active multi-contour evolution with fully convolutional network-based intensity priors
- **Arxiv ID**: http://arxiv.org/abs/2105.01998v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01998v1)
- **Published**: 2021-05-05 11:54:05+00:00
- **Updated**: 2021-05-05 11:54:05+00:00
- **Authors**: Przemyslaw Polewski, Jacquelyn Shelton, Wei Yao, Marco Heurich
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a framework for segmenting instances of a common object class by multiple active contour evolution over semantic segmentation maps of images obtained through fully convolutional networks. The contour evolution is cast as an energy minimization problem, where the aggregate energy functional incorporates a data fit term, an explicit shape model, and accounts for object overlap. Efficient solution neighborhood operators are proposed, enabling optimization through metaheuristics such as simulated annealing. We instantiate the proposed framework in the context of segmenting individual fallen stems from high-resolution aerial multispectral imagery. We validated our approach on 3 real-world scenes of varying complexity. The test plots were situated in regions of the Bavarian Forest National Park, Germany, which sustained a heavy bark beetle infestation. Evaluations were performed on both the polygon and line segment level, showing that the multi-contour segmentation can achieve up to 0.93 precision and 0.82 recall. An improvement of up to 7 percentage points (pp) in recall and 6 in precision compared to an iterative sample consensus line segment detection was achieved. Despite the simplicity of the applied shape parametrization, an explicit shape model incorporated into the energy function improved the results by up to 4 pp of recall. Finally, we show the importance of using a deep learning based semantic segmentation method as the basis for individual stem detection. Our method is a step towards increased accessibility of automatic fallen tree mapping, due to higher cost efficiency of aerial imagery acquisition compared to laser scanning. The precise fallen tree maps could be further used as a basis for plant and animal habitat modeling, studies on carbon sequestration as well as soil quality in forest ecosystems.



### Contrastive Learning and Self-Training for Unsupervised Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.02001v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.02001v1)
- **Published**: 2021-05-05 11:55:53+00:00
- **Updated**: 2021-05-05 11:55:53+00:00
- **Authors**: Robert A. Marsden, Alexander Bartler, Mario D√∂bler, Bin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks have considerably improved state-of-the-art results for semantic segmentation. Nevertheless, even modern architectures lack the ability to generalize well to a test dataset that originates from a different domain. To avoid the costly annotation of training data for unseen domains, unsupervised domain adaptation (UDA) attempts to provide efficient knowledge transfer from a labeled source domain to an unlabeled target domain. Previous work has mainly focused on minimizing the discrepancy between the two domains by using adversarial training or self-training. While adversarial training may fail to align the correct semantic categories as it minimizes the discrepancy between the global distributions, self-training raises the question of how to provide reliable pseudo-labels. To align the correct semantic categories across domains, we propose a contrastive learning approach that adapts category-wise centroids across domains. Furthermore, we extend our method with self-training, where we use a memory-efficient temporal ensemble to generate consistent and reliable pseudo-labels. Although both contrastive learning and self-training (CLST) through temporal ensembling enable knowledge transfer between two domains, it is their combination that leads to a symbiotic structure. We validate our approach on two domain adaptation benchmarks: GTA5 $\rightarrow$ Cityscapes and SYNTHIA $\rightarrow$ Cityscapes. Our method achieves better or comparable results than the state-of-the-art. We will make the code publicly available.



### Real-time Multi-Adaptive-Resolution-Surfel 6D LiDAR Odometry using Continuous-time Trajectory Optimization
- **Arxiv ID**: http://arxiv.org/abs/2105.02010v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.02010v2)
- **Published**: 2021-05-05 12:14:39+00:00
- **Updated**: 2021-09-29 14:53:57+00:00
- **Authors**: Jan Quenzel, Sven Behnke
- **Comment**: In Proceedings of IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS), Prague, Czech Republic, September 2021
- **Journal**: None
- **Summary**: Simultaneous Localization and Mapping (SLAM) is an essential capability for autonomous robots, but due to high data rates of 3D LiDARs real-time SLAM is challenging. We propose a real-time method for 6D LiDAR odometry. Our approach combines a continuous-time B-Spline trajectory representation with a Gaussian Mixture Model (GMM) formulation to jointly align local multi-resolution surfel maps. Sparse voxel grids and permutohedral lattices ensure fast access to map surfels, and an adaptive resolution selection scheme effectively speeds up registration. A thorough experimental evaluation shows the performance of our approach on multiple datasets and during real-robot experiments.



### This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.02968v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02968v4)
- **Published**: 2021-05-05 12:28:34+00:00
- **Updated**: 2021-06-23 12:28:10+00:00
- **Authors**: Adrian Hoffmann, Claudio Fanconi, Rahul Rade, Jonas Kohler
- **Comment**: None
- **Journal**: ICML 2021 Workshop on Theoretic Foundation, Criticism, and
  Application Trend of Explainable AI
- **Summary**: Deep neural networks that yield human interpretable decisions by architectural design have lately become an increasingly popular alternative to post hoc interpretation of traditional black-box models. Among these networks, the arguably most widespread approach is so-called prototype learning, where similarities to learned latent prototypes serve as the basis of classifying an unseen data point. In this work, we point to an important shortcoming of such approaches. Namely, there is a semantic gap between similarity in latent space and similarity in input space, which can corrupt interpretability. We design two experiments that exemplify this issue on the so-called ProtoPNet. Specifically, we find that this network's interpretability mechanism can be led astray by intentionally crafted or even JPEG compression artefacts, which can produce incomprehensible decisions. We argue that practitioners ought to have this shortcoming in mind when deploying prototype-based models in practice.



### Towards an efficient framework for Data Extraction from Chart Images
- **Arxiv ID**: http://arxiv.org/abs/2105.02039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02039v1)
- **Published**: 2021-05-05 13:18:53+00:00
- **Updated**: 2021-05-05 13:18:53+00:00
- **Authors**: Weihong Ma, Hesuo Zhang, Shuang Yan, Guangshun Yao, Yichao Huang, Hui Li, Yaqiang Wu, Lianwen Jin
- **Comment**: accepted by ICDAR2021
- **Journal**: None
- **Summary**: In this paper, we fill the research gap by adopting state-of-the-art computer vision techniques for the data extraction stage in a data mining system. As shown in Fig.1, this stage contains two subtasks, namely, plot element detection and data conversion. For building a robust box detector, we comprehensively compare different deep learning-based methods and find a suitable method to detect box with high precision. For building a robust point detector, a fully convolutional network with feature fusion module is adopted, which can distinguish close points compared to traditional methods. The proposed system can effectively handle various chart data without making heuristic assumptions. For data conversion, we translate the detected element into data with semantic value. A network is proposed to measure feature similarities between legends and detected elements in the legend matching phase. Furthermore, we provide a baseline on the competition of Harvesting raw tables from Infographics. Some key factors have been found to improve the performance of each stage. Experimental results demonstrate the effectiveness of the proposed system.



### Bayesian Logistic Shape Model Inference: application to cochlea image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.02045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.02045v1)
- **Published**: 2021-05-05 13:21:42+00:00
- **Updated**: 2021-05-05 13:21:42+00:00
- **Authors**: Wang Zihao, Demarcy Thomas, Vandersteen Clair, Gnansia Dan, Raffaelli Charles, Guevara Nicolas, Delingette Herv√©
- **Comment**: None
- **Journal**: None
- **Summary**: Incorporating shape information is essential for the delineation of many organs and anatomical structures in medical images. While previous work has mainly focused on parametric spatial transformations applied on reference template shapes, in this paper, we address the Bayesian inference of parametric shape models for segmenting medical images with the objective to provide interpretable results. The proposed framework defines a likelihood appearance probability and a prior label probability based on a generic shape function through a logistic function. A reference length parameter defined in the sigmoid controls the trade-off between shape and appearance information. The inference of shape parameters is performed within an Expectation-Maximisation approach where a Gauss-Newton optimization stage allows to provide an approximation of the posterior probability of shape parameters. This framework is applied to the segmentation of cochlea structures from clinical CT images constrained by a 10 parameter shape model. It is evaluated on three different datasets, one of which includes more than 200 patient images. The results show performances comparable to supervised methods and better than previously proposed unsupervised ones. It also enables an analysis of parameter distributions and the quantification of segmentation uncertainty including the effect of the shape model.



### Few-shot Partial Multi-view Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.02046v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02046v4)
- **Published**: 2021-05-05 13:34:43+00:00
- **Updated**: 2023-05-18 13:25:25+00:00
- **Authors**: Yuan Zhou, Yanrong Guo, Shijie Hao, Richang Hong, Jiebo Luo
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: It is often the case that data are with multiple views in real-world applications. Fully exploring the information of each view is significant for making data more representative. However, due to various limitations and failures in data collection and pre-processing, it is inevitable for real data to suffer from view missing and data scarcity. The coexistence of these two issues makes it more challenging to achieve the pattern classification task. Currently, to our best knowledge, few appropriate methods can well-handle these two issues simultaneously. Aiming to draw more attention from the community to this challenge, we propose a new task in this paper, called few-shot partial multi-view learning, which focuses on overcoming the negative impact of the view-missing issue in the low-data regime. The challenges of this task are twofold: (i) it is difficult to overcome the impact of data scarcity under the interference of missing views; (ii) the limited number of data exacerbates information scarcity, thus making it harder to address the view-missing issue in turn. To address these challenges, we propose a new unified Gaussian dense-anchoring method. The unified dense anchors are learned for the limited partial multi-view data, thereby anchoring them into a unified dense representation space where the influence of data scarcity and view missing can be alleviated. We conduct extensive experiments to evaluate our method. The results on Cub-googlenet-doc2vec, Handwritten, Caltech102, Scene15, Animal, ORL, tieredImagenet, and Birds-200-2011 datasets validate its effectiveness.



### Cuboids Revisited: Learning Robust 3D Shape Fitting to Single RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2105.02047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02047v1)
- **Published**: 2021-05-05 13:36:00+00:00
- **Updated**: 2021-05-05 13:36:00+00:00
- **Authors**: Florian Kluger, Hanno Ackermann, Eric Brachmann, Michael Ying Yang, Bodo Rosenhahn
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Humans perceive and construct the surrounding world as an arrangement of simple parametric models. In particular, man-made environments commonly consist of volumetric primitives such as cuboids or cylinders. Inferring these primitives is an important step to attain high-level, abstract scene descriptions. Previous approaches directly estimate shape parameters from a 2D or 3D input, and are only able to reproduce simple objects, yet unable to accurately parse more complex 3D scenes. In contrast, we propose a robust estimator for primitive fitting, which can meaningfully abstract real-world environments using cuboids. A RANSAC estimator guided by a neural network fits these primitives to 3D features, such as a depth map. We condition the network on previously detected parts of the scene, thus parsing it one-by-one. To obtain 3D features from a single RGB image, we additionally optimise a feature extraction CNN in an end-to-end manner. However, naively minimising point-to-primitive distances leads to large or spurious cuboids occluding parts of the scene behind. We thus propose an occlusion-aware distance metric correctly handling opaque scenes. The proposed algorithm does not require labour-intensive labels, such as cuboid annotations, for training. Results on the challenging NYU Depth v2 dataset demonstrate that the proposed algorithm successfully abstracts cluttered real-world 3D scene layouts.



### Proposal-free One-stage Referring Expression via Grid-Word Cross-Attention
- **Arxiv ID**: http://arxiv.org/abs/2105.02061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02061v1)
- **Published**: 2021-05-05 13:53:53+00:00
- **Updated**: 2021-05-05 13:53:53+00:00
- **Authors**: Wei Suo, Mengyang Sun, Peng Wang, Qi Wu
- **Comment**: To be published in the 30th International Joint Conference on
  Artificial Intelligence (IJCAI-2021)
- **Journal**: None
- **Summary**: Referring Expression Comprehension (REC) has become one of the most important tasks in visual reasoning, since it is an essential step for many vision-and-language tasks such as visual question answering. However, it has not been widely used in many downstream tasks because it suffers 1) two-stage methods exist heavy computation cost and inevitable error accumulation, and 2) one-stage methods have to depend on lots of hyper-parameters (such as anchors) to generate bounding box. In this paper, we present a proposal-free one-stage (PFOS) model that is able to regress the region-of-interest from the image, based on a textual query, in an end-to-end manner. Instead of using the dominant anchor proposal fashion, we directly take the dense-grid of an image as input for a cross-attention transformer that learns grid-word correspondences. The final bounding box is predicted directly from the image without the time-consuming anchor selection process that previous methods suffer. Our model achieves the state-of-the-art performance on four referring expression datasets with higher efficiency, comparing to previous best one-stage and two-stage methods.



### Deep Spherical Manifold Gaussian Kernel for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2105.02089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02089v1)
- **Published**: 2021-05-05 14:39:20+00:00
- **Updated**: 2021-05-05 14:39:20+00:00
- **Authors**: Youshan Zhang, Brian D. Davison
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised Domain adaptation is an effective method in addressing the domain shift issue when transferring knowledge from an existing richly labeled domain to a new domain. Existing manifold-based methods either are based on traditional models or largely rely on Grassmannian manifold via minimizing differences of single covariance matrices of two domains. In addition, existing pseudo-labeling algorithms inadequately consider the quality of pseudo labels in aligning the conditional distribution between two domains. In this work, a deep spherical manifold Gaussian kernel (DSGK) framework is proposed to map the source and target subspaces into a spherical manifold and reduce the discrepancy between them by embedding both extracted features and a Gaussian kernel. To align the conditional distributions, we further develop an easy-to-hard pseudo label refinement process to improve the quality of the pseudo labels and then reduce categorical spherical manifold Gaussian kernel geodesic loss. Extensive experimental results show that DSGK outperforms state-of-the-art methods, especially on challenging cross-domain learning tasks.



### Image Embedding and Model Ensembling for Automated Chest X-Ray Interpretation
- **Arxiv ID**: http://arxiv.org/abs/2105.02966v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02966v1)
- **Published**: 2021-05-05 14:48:59+00:00
- **Updated**: 2021-05-05 14:48:59+00:00
- **Authors**: Edoardo Giacomello, Pier Luca Lanzi, Daniele Loiacono, Luca Nassano
- **Comment**: Accepted at IJCNN 2021
- **Journal**: None
- **Summary**: Chest X-ray (CXR) is perhaps the most frequently-performed radiological investigation globally. In this work, we present and study several machine learning approaches to develop automated CXR diagnostic models. In particular, we trained several Convolutional Neural Networks (CNN) on the CheXpert dataset, a large collection of more than 200k CXR labeled images. Then, we used the trained CNNs to compute embeddings of the CXR images, in order to train two sets of tree-based classifiers from them. Finally, we described and compared three ensembling strategies to combine together the classifiers trained. Rather than expecting some performance-wise benefits, our goal in this work is showing that the above two methodologies, i.e., the extraction of image embeddings and models ensembling, can be effective and viable to solve tasks that require medical imaging understanding. Our results in that perspective are encouraging and worthy of further investigation.



### Prototype Memory for Large-scale Face Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.02103v2
- **DOI**: 10.1109/ACCESS.2022.3146059
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02103v2)
- **Published**: 2021-05-05 15:08:34+00:00
- **Updated**: 2022-02-03 17:46:54+00:00
- **Authors**: Evgeny Smirnov, Nikita Garaev, Vasiliy Galyuk, Evgeny Lukyanets
- **Comment**: None
- **Journal**: IEEE Access, vol. 10, pp. 12031-12046, 2022
- **Summary**: Face representation learning using datasets with a massive number of identities requires appropriate training methods. Softmax-based approach, currently the state-of-the-art in face recognition, in its usual "full softmax" form is not suitable for datasets with millions of persons. Several methods, based on the "sampled softmax" approach, were proposed to remove this limitation. These methods, however, have a set of disadvantages. One of them is a problem of "prototype obsolescence": classifier weights (prototypes) of the rarely sampled classes receive too scarce gradients and become outdated and detached from the current encoder state, resulting in incorrect training signals. This problem is especially serious in ultra-large-scale datasets. In this paper, we propose a novel face representation learning model called Prototype Memory, which alleviates this problem and allows training on a dataset of any size. Prototype Memory consists of the limited-size memory module for storing recent class prototypes and employs a set of algorithms to update it in appropriate way. New class prototypes are generated on the fly using exemplar embeddings in the current mini-batch. These prototypes are enqueued to the memory and used in a role of classifier weights for softmax classification-based training. To prevent obsolescence and keep the memory in close connection with the encoder, prototypes are regularly refreshed, and oldest ones are dequeued and disposed of. Prototype Memory is computationally efficient and independent of dataset size. It can be used with various loss functions, hard example mining algorithms and encoder architectures. We prove the effectiveness of the proposed model by extensive experiments on popular face recognition benchmarks.



### Conditional Invertible Neural Networks for Diverse Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2105.02104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T01
- **Links**: [PDF](http://arxiv.org/pdf/2105.02104v1)
- **Published**: 2021-05-05 15:10:37+00:00
- **Updated**: 2021-05-05 15:10:37+00:00
- **Authors**: Lynton Ardizzone, Jakob Kruse, Carsten L√ºth, Niels Bracher, Carsten Rother, Ullrich K√∂the
- **Comment**: arXiv admin note: text overlap with arXiv:1907.02392
- **Journal**: None
- **Summary**: We introduce a new architecture called a conditional invertible neural network (cINN), and use it to address the task of diverse image-to-image translation for natural images. This is not easily possible with existing INN models due to some fundamental limitations. The cINN combines the purely generative INN model with an unconstrained feed-forward network, which efficiently preprocesses the conditioning image into maximally informative features. All parameters of a cINN are jointly optimized with a stable, maximum likelihood-based training procedure. Even though INN-based models have received far less attention in the literature than GANs, they have been shown to have some remarkable properties absent in GANs, e.g. apparent immunity to mode collapse. We find that our cINNs leverage these properties for image-to-image translation, demonstrated on day to night translation and image colorization. Furthermore, we take advantage of our bidirectional cINN architecture to explore and manipulate emergent properties of the latent space, such as changing the image style in an intuitive way.



### Pairwise Point Cloud Registration using Graph Matching and Rotation-invariant Features
- **Arxiv ID**: http://arxiv.org/abs/2105.02151v1
- **DOI**: 10.1109/LGRS.2021.3109470
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02151v1)
- **Published**: 2021-05-05 16:03:05+00:00
- **Updated**: 2021-05-05 16:03:05+00:00
- **Authors**: Rong Huang, Wei Yao, Yusheng Xu, Zhen Ye, Uwe Stilla
- **Comment**: None
- **Journal**: None
- **Summary**: Registration is a fundamental but critical task in point cloud processing, which usually depends on finding element correspondence from two point clouds. However, the finding of reliable correspondence relies on establishing a robust and discriminative description of elements and the correct matching of corresponding elements. In this letter, we develop a coarse-to-fine registration strategy, which utilizes rotation-invariant features and a new weighted graph matching method for iteratively finding correspondence. In the graph matching method, the similarity of nodes and edges in Euclidean and feature space are formulated to construct the optimization function. The proposed strategy is evaluated using two benchmark datasets and compared with several state-of-the-art methods. Regarding the experimental results, our proposed method can achieve a fine registration with rotation errors of less than 0.2 degrees and translation errors of less than 0.1m.



### VoxelContext-Net: An Octree based Framework for Point Cloud Compression
- **Arxiv ID**: http://arxiv.org/abs/2105.02158v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.02158v1)
- **Published**: 2021-05-05 16:12:48+00:00
- **Updated**: 2021-05-05 16:12:48+00:00
- **Authors**: Zizheng Que, Guo Lu, Dong Xu
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: In this paper, we propose a two-stage deep learning framework called VoxelContext-Net for both static and dynamic point cloud compression. Taking advantages of both octree based methods and voxel based schemes, our approach employs the voxel context to compress the octree structured data. Specifically, we first extract the local voxel representation that encodes the spatial neighbouring context information for each node in the constructed octree. Then, in the entropy coding stage, we propose a voxel context based deep entropy model to compress the symbols of non-leaf nodes in a lossless way. Furthermore, for dynamic point cloud compression, we additionally introduce the local voxel representations from the temporal neighbouring point clouds to exploit temporal dependency. More importantly, to alleviate the distortion from the octree construction procedure, we propose a voxel context based 3D coordinate refinement method to produce more accurate reconstructed point cloud at the decoder side, which is applicable to both static and dynamic point cloud compression. The comprehensive experiments on both static and dynamic point cloud benchmark datasets(e.g., ScanNet and Semantic KITTI) clearly demonstrate the effectiveness of our newly proposed method VoxelContext-Net for 3D point cloud geometry compression.



### Visual Relationship Detection Using Part-and-Sum Transformers with Composite Queries
- **Arxiv ID**: http://arxiv.org/abs/2105.02170v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02170v2)
- **Published**: 2021-05-05 16:31:32+00:00
- **Updated**: 2021-08-19 21:26:08+00:00
- **Authors**: Qi Dong, Zhuowen Tu, Haofu Liao, Yuting Zhang, Vijay Mahadevan, Stefano Soatto
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: Computer vision applications such as visual relationship detection and human object interaction can be formulated as a composite (structured) set detection problem in which both the parts (subject, object, and predicate) and the sum (triplet as a whole) are to be detected in a hierarchical fashion. In this paper, we present a new approach, denoted Part-and-Sum detection Transformer (PST), to perform end-to-end visual composite set detection. Different from existing Transformers in which queries are at a single level, we simultaneously model the joint part and sum hypotheses/interactions with composite queries and attention modules. We explicitly incorporate sum queries to enable better modeling of the part-and-sum relations that are absent in the standard Transformers. Our approach also uses novel tensor-based part queries and vector-based sum queries, and models their joint interaction. We report experiments on two vision tasks, visual relationship detection and human object interaction and demonstrate that PST achieves state of the art results among single-stage models, while nearly matching the results of custom designed two-stage models.



### Learning Feature Aggregation for Deep 3D Morphable Models
- **Arxiv ID**: http://arxiv.org/abs/2105.02173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02173v1)
- **Published**: 2021-05-05 16:41:00+00:00
- **Updated**: 2021-05-05 16:41:00+00:00
- **Authors**: Zhixiang Chen, Tae-Kyun Kim
- **Comment**: Published in CVPR 2021
- **Journal**: None
- **Summary**: 3D morphable models are widely used for the shape representation of an object class in computer vision and graphics applications. In this work, we focus on deep 3D morphable models that directly apply deep learning on 3D mesh data with a hierarchical structure to capture information at multiple scales. While great efforts have been made to design the convolution operator, how to best aggregate vertex features across hierarchical levels deserves further attention. In contrast to resorting to mesh decimation, we propose an attention based module to learn mapping matrices for better feature aggregation across hierarchical levels. Specifically, the mapping matrices are generated by a compatibility function of the keys and queries. The keys and queries are trainable variables, learned by optimizing the target objective, and shared by all data samples of the same object class. Our proposed module can be used as a train-only drop-in replacement for the feature aggregation in existing architectures for both downsampling and upsampling. Our experiments show that through the end-to-end training of the mapping matrices, we achieve state-of-the-art results on a variety of 3D shape datasets in comparison to existing morphable models.



### PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2105.02184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02184v1)
- **Published**: 2021-05-05 16:55:53+00:00
- **Updated**: 2021-05-05 16:55:53+00:00
- **Authors**: Enze Xie, Wenhai Wang, Mingyu Ding, Ruimao Zhang, Ping Luo
- **Comment**: TPAMI 2021 Accepted
- **Journal**: None
- **Summary**: Reducing the complexity of the pipeline of instance segmentation is crucial for real-world applications. This work addresses this issue by introducing an anchor-box free and single-shot instance segmentation framework, termed PolarMask, which reformulates the instance segmentation problem as predicting the contours of objects in the polar coordinate, with several appealing benefits. (1) The polar representation unifies instance segmentation (masks) and object detection (bounding boxes) into a single framework, reducing the design and computational complexity. (2) Two modules are carefully designed (i.e. soft polar centerness and polar IoU loss) to sample high-quality center examples and optimize polar contour regression, making the performance of PolarMask does not depend on the bounding box prediction results and thus becomes more efficient in training. (3) PolarMask is fully convolutional and can be easily embedded into most off-the-shelf detection methods. To further improve the accuracy of the framework, a Refined Feature Pyramid is introduced to further improve the feature representation at different scales, termed PolarMask++. Extensive experiments demonstrate the effectiveness of both PolarMask and PolarMask++, which achieve competitive results on instance segmentation in the challenging COCO dataset with single-model and single-scale training and testing, as well as new state-of-the-art results on rotate text detection and cell segmentation. We hope the proposed polar representation can provide a new perspective for designing algorithms to solve single-shot instance segmentation. The codes and models are available at: github.com/xieenze/PolarMask.



### RandCrowns: A Quantitative Metric for Imprecisely Labeled Tree Crown Delineation
- **Arxiv ID**: http://arxiv.org/abs/2105.02186v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02186v3)
- **Published**: 2021-05-05 16:57:23+00:00
- **Updated**: 2021-10-20 17:11:02+00:00
- **Authors**: Dylan Stewart, Alina Zare, Sergio Marconi, Ben G. Weinstein, Ethan P. White, Sarah J. Graves, Stephanie A. Bohlman, Aditya Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised methods for object delineation in remote sensing require labeled ground-truth data. Gathering sufficient high quality ground-truth data is difficult, especially when targets are of irregular shape or difficult to distinguish from background or neighboring objects. Tree crown delineation provides key information from remote sensing images for forestry, ecology, and management. However, tree crowns in remote sensing imagery are often difficult to label and annotate due to irregular shape, overlapping canopies, shadowing, and indistinct edges. There are also multiple approaches to annotation in this field (e.g., rectangular boxes vs. convex polygons) that further contribute to annotation imprecision. However, current evaluation methods do not account for this uncertainty in annotations, and quantitative metrics for evaluation can vary across multiple annotators. In this paper, we address these limitations by developing an adaptation of the Rand index for weakly-labeled crown delineation that we call RandCrowns. Our new RandCrowns evaluation metric provides a method to appropriately evaluate delineated tree crowns while taking into account imprecision in the ground-truth delineations. The RandCrowns metric reformulates the Rand index by adjusting the areas over which each term of the index is computed to account for uncertain and imprecise object delineation labels. Quantitative comparisons to the commonly used intersection over union method shows a decrease in the variance generated by differences among multiple annotators. Combined with qualitative examples, our results suggest that the RandCrowns metric is more robust for scoring target delineations in the presence of uncertainty and imprecision in annotations that are inherent to tree crown delineation.



### Rethinking Ultrasound Augmentation: A Physics-Inspired Approach
- **Arxiv ID**: http://arxiv.org/abs/2105.02188v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.02188v1)
- **Published**: 2021-05-05 16:59:38+00:00
- **Updated**: 2021-05-05 16:59:38+00:00
- **Authors**: Maria Tirindelli, Christine Eilers, Walter Simson, Magdalini Paschali, Mohammad Farid Azampour, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Medical Ultrasound (US), despite its wide use, is characterized by artifacts and operator dependency. Those attributes hinder the gathering and utilization of US datasets for the training of Deep Neural Networks used for Computer-Assisted Intervention Systems. Data augmentation is commonly used to enhance model generalization and performance. However, common data augmentation techniques, such as affine transformations do not align with the physics of US and, when used carelessly can lead to unrealistic US images. To this end, we propose a set of physics-inspired transformations, including deformation, reverb and Signal-to-Noise Ratio, that we apply on US B-mode images for data augmentation. We evaluate our method on a new spine US dataset for the tasks of bone segmentation and classification.



### Moving SLAM: Fully Unsupervised Deep Learning in Non-Rigid Scenes
- **Arxiv ID**: http://arxiv.org/abs/2105.02195v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02195v2)
- **Published**: 2021-05-05 17:08:10+00:00
- **Updated**: 2021-06-01 06:49:35+00:00
- **Authors**: Dan Xu, Andrea Vedaldi, Joao F. Henriques
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to train deep networks to decompose videos into 3D geometry (camera and depth), moving objects, and their motions, with no supervision. We build on the idea of view synthesis, which uses classical camera geometry to re-render a source image from a different point-of-view, specified by a predicted relative pose and depth map. By minimizing the error between the synthetic image and the corresponding real image in a video, the deep network that predicts pose and depth can be trained completely unsupervised. However, the view synthesis equations rely on a strong assumption: that objects do not move. This rigid-world assumption limits the predictive power, and rules out learning about objects automatically. We propose a simple solution: minimize the error on small regions of the image instead. While the scene as a whole may be non-rigid, it is always possible to find small regions that are approximately rigid, such as inside a moving object. Our network can then predict different poses for each region, in a sliding window from a learned dense pose map. This represents a significantly richer model, including 6D object motions, with little additional complexity. We achieve very competitive performance on unsupervised odometry and depth prediction on KITTI. We also demonstrate new capabilities on EPIC-Kitchens, a challenging dataset of indoor videos, where there is no ground truth information for depth, odometry, object segmentation or motion. Yet all are recovered automatically by our method.



### Impact of individual rater style on deep learning uncertainty in medical imaging segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.02197v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.02197v1)
- **Published**: 2021-05-05 17:11:18+00:00
- **Updated**: 2021-05-05 17:11:18+00:00
- **Authors**: Olivier Vincent, Charley Gros, Julien Cohen-Adad
- **Comment**: 17 pages, 8 figures, in submission at MELBA journal
- **Journal**: None
- **Summary**: While multiple studies have explored the relation between inter-rater variability and deep learning model uncertainty in medical segmentation tasks, little is known about the impact of individual rater style. This study quantifies rater style in the form of bias and consistency and explores their impacts when used to train deep learning models. Two multi-rater public datasets were used, consisting of brain multiple sclerosis lesion and spinal cord grey matter segmentation. On both datasets, results show a correlation ($R^2 = 0.60$ and $0.93$) between rater bias and deep learning uncertainty. The impact of label fusion between raters' annotations on this relationship is also explored, and we show that multi-center consensuses are more effective than single-center consensuses to reduce uncertainty, since rater style is mostly center-specific.



### PD-GAN: Probabilistic Diverse GAN for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2105.02201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02201v1)
- **Published**: 2021-05-05 17:20:48+00:00
- **Updated**: 2021-05-05 17:20:48+00:00
- **Authors**: Hongyu Liu, Ziyu Wan, Wei Huang, Yibing Song, Xintong Han, Jing Liao
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: We propose PD-GAN, a probabilistic diverse GAN for image inpainting. Given an input image with arbitrary hole regions, PD-GAN produces multiple inpainting results with diverse and visually realistic content. Our PD-GAN is built upon a vanilla GAN which generates images based on random noise. During image generation, we modulate deep features of input random noise from coarse-to-fine by injecting an initially restored image and the hole regions in multiple scales. We argue that during hole filling, the pixels near the hole boundary should be more deterministic (i.e., with higher probability trusting the context and initially restored image to create natural inpainting boundary), while those pixels lie in the center of the hole should enjoy more degrees of freedom (i.e., more likely to depend on the random noise for enhancing diversity). To this end, we propose spatially probabilistic diversity normalization (SPDNorm) inside the modulation to model the probability of generating a pixel conditioned on the context information. SPDNorm dynamically balances the realism and diversity inside the hole region, making the generated content more diverse towards the hole center and resemble neighboring image content more towards the hole boundary. Meanwhile, we propose a perceptual diversity loss to further empower PD-GAN for diverse content generation. Experiments on benchmark datasets including CelebA-HQ, Places2 and Paris Street View indicate that PD-GAN is effective for diverse and visually realistic image restoration.



### Physically Inspired Dense Fusion Networks for Relighting
- **Arxiv ID**: http://arxiv.org/abs/2105.02209v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.02209v1)
- **Published**: 2021-05-05 17:33:45+00:00
- **Updated**: 2021-05-05 17:33:45+00:00
- **Authors**: Amirsaeed Yazdani, Tiantong Guo, Vishal Monga
- **Comment**: Rank second in NTIRE 2021 One-to-one depth guided image relighting
  challenge, accepted by CVPRW 2021
- **Journal**: None
- **Summary**: Image relighting has emerged as a problem of significant research interest inspired by augmented reality applications. Physics-based traditional methods, as well as black box deep learning models, have been developed. The existing deep networks have exploited training to achieve a new state of the art; however, they may perform poorly when training is limited or does not represent problem phenomenology, such as the addition or removal of dense shadows. We propose a model which enriches neural networks with physical insight. More precisely, our method generates the relighted image with new illumination settings via two different strategies and subsequently fuses them using a weight map (w). In the first strategy, our model predicts the material reflectance parameters (albedo) and illumination/geometry parameters of the scene (shading) for the relit image (we refer to this strategy as intrinsic image decomposition (IID)). The second strategy is solely based on the black box approach, where the model optimizes its weights based on the ground-truth images and the loss terms in the training stage and generates the relit output directly (we refer to this strategy as direct). While our proposed method applies to both one-to-one and any-to-any relighting problems, for each case we introduce problem-specific components that enrich the model performance: 1) For one-to-one relighting we incorporate normal vectors of the surfaces in the scene to adjust gloss and shadows accordingly in the image. 2) For any-to-any relighting, we propose an additional multiscale block to the architecture to enhance feature extraction. Experimental results on the VIDIT 2020 and the VIDIT 2021 dataset (used in the NTIRE 2021 relighting challenge) reveals that our proposal can outperform many state-of-the-art methods in terms of well-known fidelity metrics and perceptual loss.



### Self-Supervised Multi-Frame Monocular Scene Flow
- **Arxiv ID**: http://arxiv.org/abs/2105.02216v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.02216v1)
- **Published**: 2021-05-05 17:49:55+00:00
- **Updated**: 2021-05-05 17:49:55+00:00
- **Authors**: Junhwa Hur, Stefan Roth
- **Comment**: To appear at CVPR 2021. Code available:
  https://github.com/visinf/multi-mono-sf
- **Journal**: None
- **Summary**: Estimating 3D scene flow from a sequence of monocular images has been gaining increased attention due to the simple, economical capture setup. Owing to the severe ill-posedness of the problem, the accuracy of current methods has been limited, especially that of efficient, real-time approaches. In this paper, we introduce a multi-frame monocular scene flow network based on self-supervised learning, improving the accuracy over previous networks while retaining real-time efficiency. Based on an advanced two-frame baseline with a split-decoder design, we propose (i) a multi-frame model using a triple frame input and convolutional LSTM connections, (ii) an occlusion-aware census loss for better accuracy, and (iii) a gradient detaching strategy to improve training stability. On the KITTI dataset, we observe state-of-the-art accuracy among monocular scene flow methods based on self-supervised learning.



### Attention for Image Registration (AiR): an unsupervised Transformer approach
- **Arxiv ID**: http://arxiv.org/abs/2105.02282v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.02282v2)
- **Published**: 2021-05-05 18:49:32+00:00
- **Updated**: 2023-03-24 19:39:50+00:00
- **Authors**: Zihao Wang, Herv√© Delingette
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration is a crucial task in signal processing, but it often encounters issues with stability and efficiency. Non-learning registration approaches rely on optimizing similarity metrics between fixed and moving images, which can be expensive in terms of time and space complexity. This problem can be exacerbated when the images are large or there are significant deformations between them. Recently, deep learning, specifically convolutional neural network (CNN)-based methods, have been explored as an effective solution to the weaknesses of non-learning approaches. To further advance learning approaches in image registration, we introduce an attention mechanism in the deformable image registration problem. Our proposed approach is based on a Transformer framework called AiR, which can be efficiently trained on GPGPU devices. We treat the image registration problem as a language translation task and use the Transformer to learn the deformation field. The method learns an unsupervised generated deformation map and is tested on two benchmark datasets. In summary, our approach shows promising effectiveness in addressing stability and efficiency issues in image registration tasks. The source code of AiR is available on Github.



### R2U3D: Recurrent Residual 3D U-Net for Lung Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.02290v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.02290v1)
- **Published**: 2021-05-05 19:17:14+00:00
- **Updated**: 2021-05-05 19:17:14+00:00
- **Authors**: Dhaval D. Kadia, Md Zahangir Alom, Ranga Burada, Tam V. Nguyen, Vijayan K. Asari
- **Comment**: The paper is under review in a journal
- **Journal**: None
- **Summary**: 3D lung segmentation is essential since it processes the volumetric information of the lungs, removes the unnecessary areas of the scan, and segments the actual area of the lungs in a 3D volume. Recently, the deep learning model, such as U-Net outperforms other network architectures for biomedical image segmentation. In this paper, we propose a novel model, namely, Recurrent Residual 3D U-Net (R2U3D), for the 3D lung segmentation task. In particular, the proposed model integrates 3D convolution into the Recurrent Residual Neural Network based on U-Net. It helps learn spatial dependencies in 3D and increases the propagation of 3D volumetric information. The proposed R2U3D network is trained on the publicly available dataset LUNA16 and it achieves state-of-the-art performance on both LUNA16 (testing set) and VESSEL12 dataset. In addition, we show that training the R2U3D model with a smaller number of CT scans, i.e., 100 scans, without applying data augmentation achieves an outstanding result in terms of Soft Dice Similarity Coefficient (Soft-DSC) of 0.9920.



### A Step Toward More Inclusive People Annotations for Fairness
- **Arxiv ID**: http://arxiv.org/abs/2105.02317v1
- **DOI**: 10.1145/3461702.3462594
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02317v1)
- **Published**: 2021-05-05 20:44:56+00:00
- **Updated**: 2021-05-05 20:44:56+00:00
- **Authors**: Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari, Caroline Pantofaru
- **Comment**: None
- **Journal**: AIES (2021)
- **Summary**: The Open Images Dataset contains approximately 9 million images and is a widely accepted dataset for computer vision research. As is common practice for large datasets, the annotations are not exhaustive, with bounding boxes and attribute labels for only a subset of the classes in each image. In this paper, we present a new set of annotations on a subset of the Open Images dataset called the MIAP (More Inclusive Annotations for People) subset, containing bounding boxes and attributes for all of the people visible in those images. The attributes and labeling methodology for the MIAP subset were designed to enable research into model fairness. In addition, we analyze the original annotation methodology for the person class and its subclasses, discussing the resulting patterns in order to inform future annotation efforts. By considering both the original and exhaustive annotation sets, researchers can also now study how systematic patterns in training annotations affect modeling.



### Magnifying Subtle Facial Motions for Effective 4D Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.02319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02319v1)
- **Published**: 2021-05-05 20:47:43+00:00
- **Updated**: 2021-05-05 20:47:43+00:00
- **Authors**: Qingkai Zhen, Di Huang, Yunhong Wang, Hassen Drira, Boulbaba Ben Amor, Mohamed Daoudi
- **Comment**: International Conference On Pattern Recognition 2016
- **Journal**: None
- **Summary**: In this paper, an effective pipeline to automatic 4D Facial Expression Recognition (4D FER) is proposed. It combines two growing but disparate ideas in Computer Vision -- computing the spatial facial deformations using tools from Riemannian geometry and magnifying them using temporal filtering. The flow of 3D faces is first analyzed to capture the spatial deformations based on the recently-developed Riemannian approach, where registration and comparison of neighboring 3D faces are led jointly. Then, the obtained temporal evolution of these deformations are fed into a magnification method in order to amplify the facial activities over the time. The latter, main contribution of this paper, allows revealing subtle (hidden) deformations which enhance the emotion classification performance. We evaluated our approach on BU-4DFE dataset, the state-of-art 94.18% average performance and an improvement that exceeds 10% in classification accuracy, after magnifying extracted geometric features (deformations), are achieved.



### Iterative Human and Automated Identification of Wildlife Images
- **Arxiv ID**: http://arxiv.org/abs/2105.02320v3
- **DOI**: 10.1038/s42256-021-00393-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02320v3)
- **Published**: 2021-05-05 20:51:30+00:00
- **Updated**: 2021-10-19 03:26:10+00:00
- **Authors**: Zhongqi Miao, Ziwei Liu, Kaitlyn M. Gaynor, Meredith S. Palmer, Stella X. Yu, Wayne M. Getz
- **Comment**: This preprint has not undergone peer review (when applicable) or any
  post-submission improvements or corrections. It is published in Nature
  Machine Intelligence: https://www.nature.com/articles/s42256-021-00393-0
- **Journal**: Nat Mach Intell 3, 885-895 (2021)
- **Summary**: Camera trapping is increasingly used to monitor wildlife, but this technology typically requires extensive data annotation. Recently, deep learning has significantly advanced automatic wildlife recognition. However, current methods are hampered by a dependence on large static data sets when wildlife data is intrinsically dynamic and involves long-tailed distributions. These two drawbacks can be overcome through a hybrid combination of machine learning and humans in the loop. Our proposed iterative human and automated identification approach is capable of learning from wildlife imagery data with a long-tailed distribution. Additionally, it includes self-updating learning that facilitates capturing the community dynamics of rapidly changing natural systems. Extensive experiments show that our approach can achieve a ~90% accuracy employing only ~20% of the human annotations of existing approaches. Our synergistic collaboration of humans and machines transforms deep learning from a relatively inefficient post-annotation tool to a collaborative on-going annotation tool that vastly relieves the burden of human annotation and enables efficient and constant model updates.



### DeepSMOTE: Fusing Deep Learning and SMOTE for Imbalanced Data
- **Arxiv ID**: http://arxiv.org/abs/2105.02340v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02340v1)
- **Published**: 2021-05-05 21:49:37+00:00
- **Updated**: 2021-05-05 21:49:37+00:00
- **Authors**: Damien Dablain, Bartosz Krawczyk, Nitesh V. Chawla
- **Comment**: 14 pages, 9 figures
- **Journal**: None
- **Summary**: Despite over two decades of progress, imbalanced data is still considered a significant challenge for contemporary machine learning models. Modern advances in deep learning have magnified the importance of the imbalanced data problem. The two main approaches to address this issue are based on loss function modifications and instance resampling. Instance sampling is typically based on Generative Adversarial Networks (GANs), which may suffer from mode collapse. Therefore, there is a need for an oversampling method that is specifically tailored to deep learning models, can work on raw images while preserving their properties, and is capable of generating high quality, artificial images that can enhance minority classes and balance the training set. We propose DeepSMOTE - a novel oversampling algorithm for deep learning models. It is simple, yet effective in its design. It consists of three major components: (i) an encoder/decoder framework; (ii) SMOTE-based oversampling; and (iii) a dedicated loss function that is enhanced with a penalty term. An important advantage of DeepSMOTE over GAN-based oversampling is that DeepSMOTE does not require a discriminator, and it generates high-quality artificial images that are both information-rich and suitable for visual inspection. DeepSMOTE code is publicly available at: https://github.com/dd1github/DeepSMOTE



### Content4All Open Research Sign Language Translation Datasets
- **Arxiv ID**: http://arxiv.org/abs/2105.02351v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2105.02351v1)
- **Published**: 2021-05-05 22:14:53+00:00
- **Updated**: 2021-05-05 22:14:53+00:00
- **Authors**: Necati Cihan Camgoz, Ben Saunders, Guillaume Rochette, Marco Giovanelli, Giacomo Inches, Robin Nachtrab-Ribback, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: Computational sign language research lacks the large-scale datasets that enables the creation of useful reallife applications. To date, most research has been limited to prototype systems on small domains of discourse, e.g. weather forecasts. To address this issue and to push the field forward, we release six datasets comprised of 190 hours of footage on the larger domain of news. From this, 20 hours of footage have been annotated by Deaf experts and interpreters and is made publicly available for research purposes. In this paper, we share the dataset collection process and tools developed to enable the alignment of sign language video and subtitles, as well as baseline translation results to underpin future research.



### Explainable Artificial Intelligence for Human Decision-Support System in Medical Domain
- **Arxiv ID**: http://arxiv.org/abs/2105.02357v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02357v1)
- **Published**: 2021-05-05 22:29:28+00:00
- **Updated**: 2021-05-05 22:29:28+00:00
- **Authors**: Samanta Knapiƒç, Avleen Malhi, Rohit Saluja, Kary Fr√§mling
- **Comment**: None
- **Journal**: None
- **Summary**: In the present paper we present the potential of Explainable Artificial Intelligence methods for decision-support in medical image analysis scenarios. With three types of explainable methods applied to the same medical image data set our aim was to improve the comprehensibility of the decisions provided by the Convolutional Neural Network (CNN). The visual explanations were provided on in-vivo gastral images obtained from a Video capsule endoscopy (VCE), with the goal of increasing the health professionals' trust in the black box predictions. We implemented two post-hoc interpretable machine learning methods LIME and SHAP and the alternative explanation approach CIU, centered on the Contextual Value and Utility (CIU). The produced explanations were evaluated using human evaluation. We conducted three user studies based on the explanations provided by LIME, SHAP and CIU. Users from different non-medical backgrounds carried out a series of tests in the web-based survey setting and stated their experience and understanding of the given explanations. Three user groups (n=20, 20, 20) with three distinct forms of explanations were quantitatively analyzed. We have found that, as hypothesized, the CIU explainable method performed better than both LIME and SHAP methods in terms of increasing support for human decision-making as well as being more transparent and thus understandable to users. Additionally, CIU outperformed LIME and SHAP by generating explanations more rapidly. Our findings suggest that there are notable differences in human decision-making between various explanation support settings. In line with that, we present three potential explainable methods that can with future improvements in implementation be generalized on different medical data sets and can provide great decision-support for medical experts.



### Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks
- **Arxiv ID**: http://arxiv.org/abs/2105.02358v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.02358v2)
- **Published**: 2021-05-05 22:29:52+00:00
- **Updated**: 2021-05-31 14:49:59+00:00
- **Authors**: Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, Shi-Min Hu
- **Comment**: 11 pages, 6 figures. external attention and EAMLP
- **Journal**: None
- **Summary**: Attention mechanisms, especially self-attention, have played an increasingly important role in deep feature representation for visual tasks. Self-attention updates the feature at each position by computing a weighted sum of features using pair-wise affinities across all positions to capture the long-range dependency within a single sample. However, self-attention has quadratic complexity and ignores potential correlation between different samples. This paper proposes a novel attention mechanism which we call external attention, based on two external, small, learnable, shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers; it conveniently replaces self-attention in existing popular architectures. External attention has linear complexity and implicitly considers the correlations between all data samples. We further incorporate the multi-head mechanism into external attention to provide an all-MLP architecture, external attention MLP (EAMLP), for image classification. Extensive experiments on image classification, object detection, semantic segmentation, instance segmentation, image generation, and point cloud analysis reveal that our method provides results comparable or superior to the self-attention mechanism and some of its variants, with much lower computational and memory costs.



### MODS -- A USV-oriented object detection and obstacle segmentation benchmark
- **Arxiv ID**: http://arxiv.org/abs/2105.02359v2
- **DOI**: 10.1109/TITS.2021.3124192
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02359v2)
- **Published**: 2021-05-05 22:40:27+00:00
- **Updated**: 2022-02-09 15:00:09+00:00
- **Authors**: Borja Bovcon, Jon Muhoviƒç, Du≈°ko Vranac, Dean Mozetiƒç, Janez Per≈°, Matej Kristan
- **Comment**: 16 pages, 15 figures. The dataset, as well as the proposed evaluation
  protocols, are published on our website: https://www.vicos.si/resources/
- **Journal**: None
- **Summary**: Small-sized unmanned surface vehicles (USV) are coastal water devices with a broad range of applications such as environmental control and surveillance. A crucial capability for autonomous operation is obstacle detection for timely reaction and collision avoidance, which has been recently explored in the context of camera-based visual scene interpretation. Owing to curated datasets, substantial advances in scene interpretation have been made in a related field of unmanned ground vehicles. However, the current maritime datasets do not adequately capture the complexity of real-world USV scenes and the evaluation protocols are not standardised, which makes cross-paper comparison of different methods difficult and hinders the progress. To address these issues, we introduce a new obstacle detection benchmark MODS, which considers two major perception tasks: maritime object detection and the more general maritime obstacle segmentation. We present a new diverse maritime evaluation dataset containing approximately 81k stereo images synchronized with an on-board IMU, with over 60k objects annotated. We propose a new obstacle segmentation performance evaluation protocol that reflects the detection accuracy in a way meaningful for practical USV navigation. Nineteen recent state-of-the-art object detection and obstacle segmentation methods are evaluated using the proposed protocol, creating a benchmark to facilitate development of the field. The proposed dataset, as well as evaluation routines, are made publicly available at vicos.si/resources.




# Arxiv Papers in cs.CV on 2021-05-19
### Learning optimally separated class-specific subspace representations using convolutional autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2105.08865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08865v1)
- **Published**: 2021-05-19 00:45:34+00:00
- **Updated**: 2021-05-19 00:45:34+00:00
- **Authors**: Krishan Sharma, Shikha Gupta, Renu Rameshan
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a novel convolutional autoencoder based architecture to generate subspace specific feature representations that are best suited for classification task. The class-specific data is assumed to lie in low dimensional linear subspaces, which could be noisy and not well separated, i.e., subspace distance (principal angle) between two classes is very low. The proposed network uses a novel class-specific self expressiveness (CSSE) layer sandwiched between encoder and decoder networks to generate class-wise subspace representations which are well separated. The CSSE layer along with encoder/ decoder are trained in such a way that data still lies in subspaces in the feature space with minimum principal angle much higher than that of the input space. To demonstrate the effectiveness of the proposed approach, several experiments have been carried out on state-of-the-art machine learning datasets and a significant improvement in classification performance is observed over existing subspace based transformation learning methods.



### A Lightweight Privacy-Preserving Scheme Using Label-based Pixel Block Mixing for Image Classification in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.08876v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68T07, I.2.6; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2105.08876v1)
- **Published**: 2021-05-19 01:50:50+00:00
- **Updated**: 2021-05-19 01:50:50+00:00
- **Authors**: Yuexin Xiang, Tiantian Li, Wei Ren, Tianqing Zhu, Kim-Kwang Raymond Choo
- **Comment**: 11 pages, 16 figures
- **Journal**: None
- **Summary**: To ensure the privacy of sensitive data used in the training of deep learning models, a number of privacy-preserving methods have been designed by the research community. However, existing schemes are generally designed to work with textual data, or are not efficient when a large number of images is used for training. Hence, in this paper we propose a lightweight and efficient approach to preserve image privacy while maintaining the availability of the training set. Specifically, we design the pixel block mixing algorithm for image classification privacy preservation in deep learning. To evaluate its utility, we use the mixed training set to train the ResNet50, VGG16, InceptionV3 and DenseNet121 models on the WIKI dataset and the CNBC face dataset. Experimental findings on the testing set show that our scheme preserves image privacy while maintaining the availability of the training set in the deep learning models. Additionally, the experimental results demonstrate that we achieve good performance for the VGG16 model on the WIKI dataset and both ResNet50 and DenseNet121 on the CNBC dataset. The pixel block algorithm achieves fairly high efficiency in the mixing of the images, and it is computationally challenging for the attackers to restore the mixed training set to the original training set. Moreover, data augmentation can be applied to the mixed training set to improve the training's effectiveness.



### Font Style that Fits an Image -- Font Generation Based on Image Context
- **Arxiv ID**: http://arxiv.org/abs/2105.08879v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.08879v1)
- **Published**: 2021-05-19 01:53:04+00:00
- **Updated**: 2021-05-19 01:53:04+00:00
- **Authors**: Taiga Miyazono, Brian Kenji Iwana, Daichi Haraguchi, Seiichi Uchida
- **Comment**: Accepted to ICDAR 2021
- **Journal**: None
- **Summary**: When fonts are used on documents, they are intentionally selected by designers. For example, when designing a book cover, the typography of the text is an important factor in the overall feel of the book. In addition, it needs to be an appropriate font for the rest of the book cover. Thus, we propose a method of generating a book title image based on its context within a book cover. We propose an end-to-end neural network that inputs the book cover, a target location mask, and a desired book title and outputs stylized text suitable for the cover. The proposed network uses a combination of a multi-input encoder-decoder, a text skeleton prediction network, a perception network, and an adversarial discriminator. We demonstrate that the proposed method can effectively produce desirable and appropriate book cover text through quantitative and qualitative results.



### Multiple Meta-model Quantifying for Medical Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2105.08913v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08913v2)
- **Published**: 2021-05-19 04:06:05+00:00
- **Updated**: 2021-06-26 10:49:54+00:00
- **Authors**: Tuong Do, Binh X. Nguyen, Erman Tjiputra, Minh Tran, Quang D. Tran, Anh Nguyen
- **Comment**: Provisional accepted in MICCAI 2021
- **Journal**: None
- **Summary**: Transfer learning is an important step to extract meaningful features and overcome the data limitation in the medical Visual Question Answering (VQA) task. However, most of the existing medical VQA methods rely on external data for transfer learning, while the meta-data within the dataset is not fully utilized. In this paper, we present a new multiple meta-model quantifying method that effectively learns meta-annotation and leverages meaningful features to the medical VQA task. Our proposed method is designed to increase meta-data by auto-annotation, deal with noisy labels, and output meta-models which provide robust features for medical VQA tasks. Extensively experimental results on two public medical VQA datasets show that our approach achieves superior accuracy in comparison with other state-of-the-art methods, while does not require external data to train meta-models.



### Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2105.08919v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08919v1)
- **Published**: 2021-05-19 04:40:53+00:00
- **Updated**: 2021-05-19 04:40:53+00:00
- **Authors**: Taehyeon Kim, Jaehoon Oh, NakYil Kim, Sangwook Cho, Se-Young Yun
- **Comment**: Proceedings of International Joint Conference on Artificial
  Intelligence (IJCAI), 2021
- **Journal**: None
- **Summary**: Knowledge distillation (KD), transferring knowledge from a cumbersome teacher model to a lightweight student model, has been investigated to design efficient neural architectures. Generally, the objective function of KD is the Kullback-Leibler (KL) divergence loss between the softened probability distributions of the teacher model and the student model with the temperature scaling hyperparameter tau. Despite its widespread use, few studies have discussed the influence of such softening on generalization. Here, we theoretically show that the KL divergence loss focuses on the logit matching when tau increases and the label matching when tau goes to 0 and empirically show that the logit matching is positively correlated to performance improvement in general. From this observation, we consider an intuitive KD loss function, the mean squared error (MSE) between the logit vectors, so that the student model can directly learn the logit of the teacher model. The MSE loss outperforms the KL divergence loss, explained by the difference in the penultimate layer representations between the two losses. Furthermore, we show that sequential distillation can improve performance and that KD, particularly when using the KL divergence loss with small tau, mitigates the label noise. The code to reproduce the experiments is publicly available online at https://github.com/jhoon-oh/kd_data/.



### Large-scale Localization Datasets in Crowded Indoor Spaces
- **Arxiv ID**: http://arxiv.org/abs/2105.08941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08941v1)
- **Published**: 2021-05-19 06:20:49+00:00
- **Updated**: 2021-05-19 06:20:49+00:00
- **Authors**: Donghwan Lee, Soohyun Ryu, Suyong Yeon, Yonghan Lee, Deokhwa Kim, Cheolho Han, Yohann Cabon, Philippe Weinzaepfel, Nicolas Guérin, Gabriela Csurka, Martin Humenberger
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the precise location of a camera using visual localization enables interesting applications such as augmented reality or robot navigation. This is particularly useful in indoor environments where other localization technologies, such as GNSS, fail. Indoor spaces impose interesting challenges on visual localization algorithms: occlusions due to people, textureless surfaces, large viewpoint changes, low light, repetitive textures, etc. Existing indoor datasets are either comparably small or do only cover a subset of the mentioned challenges. In this paper, we introduce 5 new indoor datasets for visual localization in challenging real-world environments. They were captured in a large shopping mall and a large metro station in Seoul, South Korea, using a dedicated mapping platform consisting of 10 cameras and 2 laser scanners. In order to obtain accurate ground truth camera poses, we developed a robust LiDAR SLAM which provides initial poses that are then refined using a novel structure-from-motion based optimization. We present a benchmark of modern visual localization algorithms on these challenging datasets showing superior performance of structure-based methods using robust image features. The datasets are available at: https://naverlabs.com/datasets



### Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network
- **Arxiv ID**: http://arxiv.org/abs/2105.08949v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08949v3)
- **Published**: 2021-05-19 06:47:31+00:00
- **Updated**: 2021-07-05 19:40:52+00:00
- **Authors**: Chun-Mei Feng, Huazhu Fu, Shuhao Yuan, Yong Xu
- **Comment**: 10 pages, 3 figures
- **Journal**: International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI2021)
- **Summary**: Super-resolution (SR) plays a crucial role in improving the image quality of magnetic resonance imaging (MRI). MRI produces multi-contrast images and can provide a clear display of soft tissues. However, current super-resolution methods only employ a single contrast, or use a simple multi-contrast fusion mechanism, ignoring the rich relations among different contrasts, which are valuable for improving SR. In this work, we propose a multi-stage integration network (i.e., MINet) for multi-contrast MRI SR, which explicitly models the dependencies between multi-contrast images at different stages to guide image SR. In particular, our MINet first learns a hierarchical feature representation from multiple convolutional stages for each of different-contrast image. Subsequently, we introduce a multi-stage integration module to mine the comprehensive relations between the representations of the multi-contrast images. Specifically, the module matches each representation with all other features, which are integrated in terms of their similarities to obtain an enriched representation. Extensive experiments on fastMRI and real-world clinical datasets demonstrate that 1) our MINet outperforms state-of-the-art multi-contrast SR methods in terms of various metrics and 2) our multi-stage integration module is able to excavate complex interactions among multi-contrast features at different stages, leading to improved target-image quality.



### BatchQuant: Quantized-for-all Architecture Search with Robust Quantizer
- **Arxiv ID**: http://arxiv.org/abs/2105.08952v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.08952v1)
- **Published**: 2021-05-19 06:56:43+00:00
- **Updated**: 2021-05-19 06:56:43+00:00
- **Authors**: Haoping Bai, Meng Cao, Ping Huang, Jiulong Shan
- **Comment**: None
- **Journal**: None
- **Summary**: As the applications of deep learning models on edge devices increase at an accelerating pace, fast adaptation to various scenarios with varying resource constraints has become a crucial aspect of model deployment. As a result, model optimization strategies with adaptive configuration are becoming increasingly popular. While single-shot quantized neural architecture search enjoys flexibility in both model architecture and quantization policy, the combined search space comes with many challenges, including instability when training the weight-sharing supernet and difficulty in navigating the exponentially growing search space. Existing methods tend to either limit the architecture search space to a small set of options or limit the quantization policy search space to fixed precision policies. To this end, we propose BatchQuant, a robust quantizer formulation that allows fast and stable training of a compact, single-shot, mixed-precision, weight-sharing supernet. We employ BatchQuant to train a compact supernet (offering over $10^{76}$ quantized subnets) within substantially fewer GPU hours than previous methods. Our approach, Quantized-for-all (QFA), is the first to seamlessly extend one-shot weight-sharing NAS supernet to support subnets with arbitrary ultra-low bitwidth mixed-precision quantization policies without retraining. QFA opens up new possibilities in joint hardware-aware neural architecture search and quantization. We demonstrate the effectiveness of our method on ImageNet and achieve SOTA Top-1 accuracy under a low complexity constraint ($<20$ MFLOPs). The code and models will be made publicly available at https://github.com/bhpfelix/QFA.



### VSGM -- Enhance robot task understanding ability through visual semantic graph
- **Arxiv ID**: http://arxiv.org/abs/2105.08959v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08959v2)
- **Published**: 2021-05-19 07:22:31+00:00
- **Updated**: 2021-05-25 09:36:36+00:00
- **Authors**: Cheng Yu Tsai, Mu-Chun Su
- **Comment**: 16 pages, 7 figures
- **Journal**: None
- **Summary**: In recent years, developing AI for robotics has raised much attention. The interaction of vision and language of robots is particularly difficult. We consider that giving robots an understanding of visual semantics and language semantics will improve inference ability. In this paper, we propose a novel method-VSGM (Visual Semantic Graph Memory), which uses the semantic graph to obtain better visual image features, improve the robot's visual understanding ability. By providing prior knowledge of the robot and detecting the objects in the image, it predicts the correlation between the attributes of the object and the objects and converts them into a graph-based representation; and mapping the object in the image to be a top-down egocentric map. Finally, the important object features of the current task are extracted by Graph Neural Networks. The method proposed in this paper is verified in the ALFRED (Action Learning From Realistic Environments and Directives) dataset. In this dataset, the robot needs to perform daily indoor household tasks following the required language instructions. After the model is added to the VSGM, the task success rate can be improved by 6~10%.



### Railroad is not a Train: Saliency as Pseudo-pixel Supervision for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.08965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08965v1)
- **Published**: 2021-05-19 07:31:11+00:00
- **Updated**: 2021-05-19 07:31:11+00:00
- **Authors**: Seungho Lee, Minhyun Lee, Jongwuk Lee, Hyunjung Shim
- **Comment**: CVPR 2021 accepted
- **Journal**: None
- **Summary**: Existing studies in weakly-supervised semantic segmentation (WSSS) using image-level weak supervision have several limitations: sparse object coverage, inaccurate object boundaries, and co-occurring pixels from non-target objects. To overcome these challenges, we propose a novel framework, namely Explicit Pseudo-pixel Supervision (EPS), which learns from pixel-level feedback by combining two weak supervisions; the image-level label provides the object identity via the localization map and the saliency map from the off-the-shelf saliency detection model offers rich boundaries. We devise a joint training strategy to fully utilize the complementary relationship between both information. Our method can obtain accurate object boundaries and discard co-occurring pixels, thereby significantly improving the quality of pseudo-masks. Experimental results show that the proposed method remarkably outperforms existing methods by resolving key challenges of WSSS and achieves the new state-of-the-art performance on both PASCAL VOC 2012 and MS COCO 2014 datasets.



### Prototype Guided Federated Learning of Visual Feature Representations
- **Arxiv ID**: http://arxiv.org/abs/2105.08982v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08982v1)
- **Published**: 2021-05-19 08:29:12+00:00
- **Updated**: 2021-05-19 08:29:12+00:00
- **Authors**: Umberto Michieli, Mete Ozay
- **Comment**: 11 pages manuscript, 6 pages supplemental material
- **Journal**: None
- **Summary**: Federated Learning (FL) is a framework which enables distributed model training using a large corpus of decentralized training data. Existing methods aggregate models disregarding their internal representations, which are crucial for training models in vision tasks. System and statistical heterogeneity (e.g., highly imbalanced and non-i.i.d. data) further harm model training. To this end, we introduce a method, called FedProto, which computes client deviations using margins of prototypical representations learned on distributed data, and applies them to drive federated optimization via an attention mechanism. In addition, we propose three methods to analyse statistical properties of feature representations learned in FL, in order to elucidate the relationship between accuracy, margins and feature discrepancy of FL models. In experimental analyses, FedProto demonstrates state-of-the-art accuracy and convergence rate across image classification and semantic segmentation benchmarks by enabling maximum margin training of FL models. Moreover, FedProto reduces uncertainty of predictions of FL models compared to the baseline. To our knowledge, this is the first work evaluating FL models in dense prediction tasks, such as semantic segmentation.



### TarGAN: Target-Aware Generative Adversarial Networks for Multi-modality Medical Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2105.08993v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08993v1)
- **Published**: 2021-05-19 08:45:33+00:00
- **Updated**: 2021-05-19 08:45:33+00:00
- **Authors**: Junxiao Chen, Jia Wei, Rui Li
- **Comment**: 10 pages, 3 figures. It has been provisionally accepted for MICCAI
  2021
- **Journal**: None
- **Summary**: Paired multi-modality medical images, can provide complementary information to help physicians make more reasonable decisions than single modality medical images. But they are difficult to generate due to multiple factors in practice (e.g., time, cost, radiation dose). To address these problems, multi-modality medical image translation has aroused increasing research interest recently. However, the existing works mainly focus on translation effect of a whole image instead of a critical target area or Region of Interest (ROI), e.g., organ and so on. This leads to poor-quality translation of the localized target area which becomes blurry, deformed or even with extra unreasonable textures. In this paper, we propose a novel target-aware generative adversarial network called TarGAN, which is a generic multi-modality medical image translation model capable of (1) learning multi-modality medical image translation without relying on paired data, (2) enhancing quality of target area generation with the help of target area labels. The generator of TarGAN jointly learns mapping at two levels simultaneously - whole image translation mapping and target area translation mapping. These two mappings are interrelated through a proposed crossing loss. The experiments on both quantitative measures and qualitative evaluations demonstrate that TarGAN outperforms the state-of-the-art methods in all cases. Subsequent segmentation task is conducted to demonstrate effectiveness of synthetic images generated by TarGAN in a real-world application. Our code is available at https://github.com/2165998/TarGAN.



### Efficient Transfer Learning via Joint Adaptation of Network Architecture and Weight
- **Arxiv ID**: http://arxiv.org/abs/2105.08994v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08994v1)
- **Published**: 2021-05-19 08:58:04+00:00
- **Updated**: 2021-05-19 08:58:04+00:00
- **Authors**: Ming Sun, Haoxuan Dou, Junjie Yan
- **Comment**: NAS is one part of transfer learning
- **Journal**: ECCV 2020
- **Summary**: Transfer learning can boost the performance on the targettask by leveraging the knowledge of the source domain. Recent worksin neural architecture search (NAS), especially one-shot NAS, can aidtransfer learning by establishing sufficient network search space. How-ever, existing NAS methods tend to approximate huge search spaces byexplicitly building giant super-networks with multiple sub-paths, anddiscard super-network weights after a child structure is found. Both thecharacteristics of existing approaches causes repetitive network trainingon source tasks in transfer learning. To remedy the above issues, we re-duce the super-network size by randomly dropping connection betweennetwork blocks while embedding a larger search space. Moreover, wereuse super-network weights to avoid redundant training by proposinga novel framework consisting of two modules, the neural architecturesearch module for architecture transfer and the neural weight searchmodule for weight transfer. These two modules conduct search on thetarget task based on a reduced super-networks, so we only need to trainonce on the source task. We experiment our framework on both MS-COCO and CUB-200 for the object detection and fine-grained imageclassification tasks, and show promising improvements with onlyO(CN)super-network complexity.



### When Deep Classifiers Agree: Analyzing Correlations between Learning Order and Image Statistics
- **Arxiv ID**: http://arxiv.org/abs/2105.08997v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08997v2)
- **Published**: 2021-05-19 09:03:02+00:00
- **Updated**: 2022-07-19 14:08:42+00:00
- **Authors**: Iuliia Pliushch, Martin Mundt, Nicolas Lupp, Visvanathan Ramesh
- **Comment**: Accepted for publication at ECCV 2022. Version includes supplementary
  material
- **Journal**: None
- **Summary**: Although a plethora of architectural variants for deep classification has been introduced over time, recent works have found empirical evidence towards similarities in their training process. It has been hypothesized that neural networks converge not only to similar representations, but also exhibit a notion of empirical agreement on which data instances are learned first. Following in the latter works$'$ footsteps, we define a metric to quantify the relationship between such classification agreement over time, and posit that the agreement phenomenon can be mapped to core statistics of the investigated dataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal, ImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to be independent of specific architectures, training hyper-parameters or labels, albeit follows an ordering according to image statistics.



### A Novel lightweight Convolutional Neural Network, ExquisiteNetV2
- **Arxiv ID**: http://arxiv.org/abs/2105.09008v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.09008v5)
- **Published**: 2021-05-19 09:21:30+00:00
- **Updated**: 2022-03-27 13:19:34+00:00
- **Authors**: Shi-Yao Zhou, Chung-Yen Su
- **Comment**: 6 pages, 6 figures, 29 tables
- **Journal**: None
- **Summary**: In the paper of ExquisiteNetV1, the ability of classification of ExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and better model ExquisiteNetV2. We conduct many experiments to evaluate its performance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known models on 15 credible datasets under the same condition. According to the experimental results, ExquisiteNetV2 gets the highest classification accuracy over half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts of parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing speed.



### Guided Facial Skin Color Correction
- **Arxiv ID**: http://arxiv.org/abs/2105.09034v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09034v1)
- **Published**: 2021-05-19 09:59:55+00:00
- **Updated**: 2021-05-19 09:59:55+00:00
- **Authors**: Keiichiro Shirai, Tatsuya Baba, Shunsuke Ono, Masahiro Okuda, Yusuke Tatesumi, Paul Perrotin
- **Comment**: 12 pages, 16 figures
- **Journal**: None
- **Summary**: This paper proposes an automatic image correction method for portrait photographs, which promotes consistency of facial skin color by suppressing skin color changes due to background colors. In portrait photographs, skin color is often distorted due to the lighting environment (e.g., light reflected from a colored background wall and over-exposure by a camera strobe), and if the photo is artificially combined with another background color, this color change is emphasized, resulting in an unnatural synthesized result. In our framework, after roughly extracting the face region and rectifying the skin color distribution in a color space, we perform color and brightness correction around the face in the original image to achieve a proper color balance of the facial image, which is not affected by luminance and background colors. Unlike conventional algorithms for color correction, our final result is attained by a color correction process with a guide image. In particular, our guided image filtering for the color correction does not require a perfectly-aligned guide image required in the original guide image filtering method proposed by He et al. Experimental results show that our method generates more natural results than conventional methods on not only headshot photographs but also natural scene photographs. We also show automatic yearbook style photo generation as an another application.



### Deep Learning Radio Frequency Signal Classification with Hybrid Images
- **Arxiv ID**: http://arxiv.org/abs/2105.09063v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2105.09063v1)
- **Published**: 2021-05-19 11:12:09+00:00
- **Updated**: 2021-05-19 11:12:09+00:00
- **Authors**: Hilal Elyousseph, Majid L Altamimi
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, Deep Learning (DL) has been successfully applied to detect and classify Radio Frequency (RF) Signals. A DL approach is especially useful since it identifies the presence of a signal without needing full protocol information, and can also detect and/or classify non-communication waveforms, such as radar signals. In this work, we focus on the different pre-processing steps that can be used on the input training data, and test the results on a fixed DL architecture. While previous works have mostly focused exclusively on either time-domain or frequency domain approaches, we propose a hybrid image that takes advantage of both time and frequency domain information, and tackles the classification as a Computer Vision problem. Our initial results point out limitations to classical pre-processing approaches while also showing that it's possible to build a classifier that can leverage the strengths of multiple signal representations.



### Dynamic region proposal networks for semantic segmentation in automated glaucoma screening
- **Arxiv ID**: http://arxiv.org/abs/2105.11364v1
- **DOI**: 10.1109/ISBI.2019.8759171
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.11364v1)
- **Published**: 2021-05-19 11:19:14+00:00
- **Updated**: 2021-05-19 11:19:14+00:00
- **Authors**: Shivam Shah, Nikhil Kasukurthi, Harshit Pande
- **Comment**: None
- **Journal**: None
- **Summary**: Screening for the diagnosis of glaucoma through a fundus image can be determined by the optic cup to disc diameter ratio (CDR), which requires the segmentation of the cup and disc regions. In this paper, we propose two novel approaches, namely Parameter-Shared Branched Network (PSBN) andWeak Region of Interest Model-based segmentation (WRoIM) to identify disc and cup boundaries. Unlike the previous approaches, the proposed methods are trained end-to-end through a single neural network architecture and use dynamic cropping instead of manual or traditional computer vision-based cropping. We are able to achieve similar performance as that of state-of-the-art approaches with less number of network parameters. Our experiments include comparison with different best known methods on publicly available Drishti-GS1 and RIM-ONE v3 datasets. With $7.8 \times 10^6$ parameters our approach achieves a Dice score of 0.96/0.89 for disc/cup segmentation on Drishti-GS1 data whereas the existing state-of-the-art approach uses $19.8\times 10^6$ parameters to achieve a dice score of 0.97/0.89.



### Localization and Tracking of User-Defined Points on Deformable Objects for Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2105.09067v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.09067v1)
- **Published**: 2021-05-19 11:25:33+00:00
- **Updated**: 2021-05-19 11:25:33+00:00
- **Authors**: Sven Dittus, Benjamin Alt, Andreas Hermann, Darko Katic, Rainer Jäkel, Jürgen Fleischer
- **Comment**: 4 pages, 4 figures, accepted at the ICRA 2021 Workshop on
  Representing and Manipulating Deformable Objects
- **Journal**: None
- **Summary**: This paper introduces an efficient procedure to localize user-defined points on the surface of deformable objects and track their positions in 3D space over time. To cope with a deformable object's infinite number of DOF, we propose a discretized deformation field, which is estimated during runtime using a multi-step non-linear solver pipeline. The resulting high-dimensional energy minimization problem describes the deviation between an offline-defined reference model and a pre-processed camera image. An additional regularization term allows for assumptions about the object's hidden areas and increases the solver's numerical stability. Our approach is capable of solving the localization problem online in a data-parallel manner, making it ideally suitable for the perception of non-rigid objects in industrial manufacturing processes.



### Light-weight Document Image Cleanup using Perceptual Loss
- **Arxiv ID**: http://arxiv.org/abs/2105.09076v1
- **DOI**: 10.1007/978-3-030-86334-0_16
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09076v1)
- **Published**: 2021-05-19 11:54:28+00:00
- **Updated**: 2021-05-19 11:54:28+00:00
- **Authors**: Soumyadeep Dey, Pratik Jawanpuria
- **Comment**: Accepted in 16th International Conference on Document Analysis and
  Recognition 2021 (ICDAR 21)
- **Journal**: None
- **Summary**: Smartphones have enabled effortless capturing and sharing of documents in digital form. The documents, however, often undergo various types of degradation due to aging, stains, or shortcoming of capturing environment such as shadow, non-uniform lighting, etc., which reduces the comprehensibility of the document images. In this work, we consider the problem of document image cleanup on embedded applications such as smartphone apps, which usually have memory, energy, and latency limitations due to the device and/or for best human user experience. We propose a light-weight encoder decoder based convolutional neural network architecture for removing the noisy elements from document images. To compensate for generalization performance with a low network capacity, we incorporate the perceptual loss for knowledge transfer from pre-trained deep CNN network in our loss function. In terms of the number of parameters and product-sum operations, our models are 65-1030 and 3-27 times, respectively, smaller than existing state-of-the-art document enhancement models. Overall, the proposed models offer a favorable resource versus accuracy trade-off and we empirically illustrate the efficacy of our approach on several real-world benchmark datasets.



### Local Aggressive Adversarial Attacks on 3D Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2105.09090v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.09090v2)
- **Published**: 2021-05-19 12:22:56+00:00
- **Updated**: 2021-10-11 02:18:07+00:00
- **Authors**: Yiming Sun, Feng Chen, Zhiyu Chen, Mingjie Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are found to be prone to adversarial examples which could deliberately fool the model to make mistakes. Recently, a few of works expand this task from 2D image to 3D point cloud by using global point cloud optimization. However, the perturbations of global point are not effective for misleading the victim model. First, not all points are important in optimization toward misleading. Abundant points account considerable distortion budget but contribute trivially to attack. Second, the multi-label optimization is suboptimal for adversarial attack, since it consumes extra energy in finding multi-label victim model collapse and causes instance transformation to be dissimilar to any particular instance. Third, the independent adversarial and perceptibility losses, caring misclassification and dissimilarity separately, treat the updating of each point equally without a focus. Therefore, once perceptibility loss approaches its budget threshold, all points would be stock in the surface of hypersphere and attack would be locked in local optimality. Therefore, we propose a local aggressive adversarial attacks (L3A) to solve above issues. Technically, we select a bunch of salient points, the high-score subset of point cloud according to gradient, to perturb. Then a flow of aggressive optimization strategies are developed to reinforce the unperceptive generation of adversarial examples toward misleading victim models. Extensive experiments on PointNet, PointNet++ and DGCNN demonstrate the state-of-the-art performance of our method against existing adversarial attack methods.



### Recursive-NeRF: An Efficient and Dynamically Growing NeRF
- **Arxiv ID**: http://arxiv.org/abs/2105.09103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09103v1)
- **Published**: 2021-05-19 12:51:54+00:00
- **Updated**: 2021-05-19 12:51:54+00:00
- **Authors**: Guo-Wei Yang, Wen-Yang Zhou, Hao-Yang Peng, Dun Liang, Tai-Jiang Mu, Shi-Min Hu
- **Comment**: 11 pages, 12 figures
- **Journal**: None
- **Summary**: View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on three public datasets shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at https://github.com/Gword/Recursive-NeRF.



### An Orthogonal Classifier for Improving the Adversarial Robustness of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.09109v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09109v2)
- **Published**: 2021-05-19 13:12:14+00:00
- **Updated**: 2021-09-24 07:33:42+00:00
- **Authors**: Cong Xu, Xiang Li, Min Yang
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: Neural networks are susceptible to artificially designed adversarial perturbations. Recent efforts have shown that imposing certain modifications on classification layer can improve the robustness of the neural networks. In this paper, we explicitly construct a dense orthogonal weight matrix whose entries have the same magnitude, thereby leading to a novel robust classifier. The proposed classifier avoids the undesired structural redundancy issue in previous work. Applying this classifier in standard training on clean data is sufficient to ensure the high accuracy and good robustness of the model. Moreover, when extra adversarial samples are used, better robustness can be further obtained with the help of a special worst-case loss. Experimental results show that our method is efficient and competitive to many state-of-the-art defensive approaches. Our code is available at \url{https://github.com/MTandHJ/roboc}.



### Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead
- **Arxiv ID**: http://arxiv.org/abs/2105.09121v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09121v3)
- **Published**: 2021-05-19 13:30:34+00:00
- **Updated**: 2022-06-29 08:13:57+00:00
- **Authors**: Arian Bakhtiarnia, Qi Zhang, Alexandros Iosifidis
- **Comment**: Accepted by Neural Networks journal
- **Journal**: None
- **Summary**: Deploying deep learning models in time-critical applications with limited computational resources, for instance in edge computing systems and IoT networks, is a challenging task that often relies on dynamic inference methods such as early exiting. In this paper, we introduce a novel architecture for early exiting based on the vision transformer architecture, as well as a fine-tuning strategy that significantly increase the accuracy of early exit branches compared to conventional approaches while introducing less overhead. Through extensive experiments on image and audio classification as well as audiovisual crowd counting, we show that our method works for both classification and regression problems, and in both single- and multi-modal settings. Additionally, we introduce a novel method for integrating audio and visual modalities within early exits in audiovisual data analysis, that can lead to a more fine-grained dynamic inference.



### Learn Fine-grained Adaptive Loss for Multiple Anatomical Landmark Detection in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2105.09124v1
- **DOI**: 10.1109/JBHI.2021.3080703
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09124v1)
- **Published**: 2021-05-19 13:39:18+00:00
- **Updated**: 2021-05-19 13:39:18+00:00
- **Authors**: Guang-Quan Zhou, Juzheng Miao, Xin Yang, Rui Li, En-Ze Huo, Wenlong Shi, Yuhao Huang, Jikuan Qian, Chaoyu Chen, Dong Ni
- **Comment**: 12 pages, 10 figures, accepted by IEEE Journal of Biomedical and
  Health Informatics
- **Journal**: None
- **Summary**: Automatic and accurate detection of anatomical landmarks is an essential operation in medical image analysis with a multitude of applications. Recent deep learning methods have improved results by directly encoding the appearance of the captured anatomy with the likelihood maps (i.e., heatmaps). However, most current solutions overlook another essence of heatmap regression, the objective metric for regressing target heatmaps and rely on hand-crafted heuristics to set the target precision, thus being usually cumbersome and task-specific. In this paper, we propose a novel learning-to-learn framework for landmark detection to optimize the neural network and the target precision simultaneously. The pivot of this work is to leverage the reinforcement learning (RL) framework to search objective metrics for regressing multiple heatmaps dynamically during the training process, thus avoiding setting problem-specific target precision. We also introduce an early-stop strategy for active termination of the RL agent's interaction that adapts the optimal precision for separate targets considering exploration-exploitation tradeoffs. This approach shows better stability in training and improved localization accuracy in inference. Extensive experimental results on two different applications of landmark localization: 1) our in-house prenatal ultrasound (US) dataset and 2) the publicly available dataset of cephalometric X-Ray landmark detection, demonstrate the effectiveness of our proposed method. Our proposed framework is general and shows the potential to improve the efficiency of anatomical landmark detection.



### XCycles Backprojection Acoustic Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2105.09128v1
- **DOI**: 10.3390/s21103453
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2105.09128v1)
- **Published**: 2021-05-19 13:43:15+00:00
- **Updated**: 2021-05-19 13:43:15+00:00
- **Authors**: Feras Almasri, Jurgen Vandendriessche, Laurent Segers, Bruno da Silva, An Braeken, Kris Steenhaut, Abdellah Touhafi, Olivier Debeir
- **Comment**: None
- **Journal**: Sensors 2021, 21, 3453
- **Summary**: The computer vision community has paid much attention to the development of visible image super-resolution (SR) using deep neural networks (DNNs) and has achieved impressive results. The advancement of non-visible light sensors, such as acoustic imaging sensors, has attracted much attention, as they allow people to visualize the intensity of sound waves beyond the visible spectrum. However, because of the limitations imposed on acquiring acoustic data, new methods for improving the resolution of the acoustic images are necessary. At this time, there is no acoustic imaging dataset designed for the SR problem. This work proposed a novel backprojection model architecture for the acoustic image super-resolution problem, together with Acoustic Map Imaging VUB-ULB Dataset (AMIVU). The dataset provides large simulated and real captured images at different resolutions. The proposed XCycles BackProjection model (XCBP), in contrast to the feedforward model approach, fully uses the iterative correction procedure in each cycle to reconstruct the residual error correction for the encoded features in both low- and high-resolution space. The proposed approach was evaluated on the dataset and showed high outperformance compared to the classical interpolation operators and to the recent feedforward state-of-the-art models. It also contributed to a drastically reduced sub-sampling error produced during the data acquisition.



### TableZa -- A classical Computer Vision approach to Tabular Extraction
- **Arxiv ID**: http://arxiv.org/abs/2105.09137v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR, I.5.1; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2105.09137v1)
- **Published**: 2021-05-19 13:55:33+00:00
- **Updated**: 2021-05-19 13:55:33+00:00
- **Authors**: Saumya Banthia, Anantha Sharma, Ravi Mangipudi
- **Comment**: 14 pages, 16 figures, 1 table
- **Journal**: None
- **Summary**: Computer aided Tabular Data Extraction has always been a very challenging and error prone task because it demands both Spectral and Spatial Sanity of data. In this paper we discuss an approach for Tabular Data Extraction in the realm of document comprehension. Given the different kinds of the Tabular formats that are often found across various documents, we discuss a novel approach using Computer Vision for extraction of tabular data from images or vector pdf(s) converted to image(s).



### Adaptive Hypergraph Convolutional Network for No-Reference 360-degree Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2105.09143v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09143v1)
- **Published**: 2021-05-19 14:02:48+00:00
- **Updated**: 2021-05-19 14:02:48+00:00
- **Authors**: Jun Fu, Chen Hou, Wei Zhou, Jiahua Xu, Zhibo Chen
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In no-reference 360-degree image quality assessment (NR 360IQA), graph convolutional networks (GCNs), which model interactions between viewports through graphs, have achieved impressive performance. However, prevailing GCN-based NR 360IQA methods suffer from three main limitations. First, they only use high-level features of the distorted image to regress the quality score, while the human visual system (HVS) scores the image based on hierarchical features. Second, they simplify complex high-order interactions between viewports in a pairwise fashion through graphs. Third, in the graph construction, they only consider spatial locations of viewports, ignoring its content characteristics. Accordingly, to address these issues, we propose an adaptive hypergraph convolutional network for NR 360IQA, denoted as AHGCN. Specifically, we first design a multi-level viewport descriptor for extracting hierarchical representations from viewports. Then, we model interactions between viewports through hypergraphs, where each hyperedge connects two or more viewports. In the hypergraph construction, we build a location-based hyperedge and a content-based hyperedge for each viewport. Experimental results on two public 360IQA databases demonstrate that our proposed approach has a clear advantage over state-of-the-art full-reference and no-reference IQA models.



### Generalizable Person Re-identification with Relevance-aware Mixture of Experts
- **Arxiv ID**: http://arxiv.org/abs/2105.09156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09156v1)
- **Published**: 2021-05-19 14:19:34+00:00
- **Updated**: 2021-05-19 14:19:34+00:00
- **Authors**: Yongxing Dai, Xiaotong Li, Jun Liu, Zekun Tong, Ling-Yu Duan
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Domain generalizable (DG) person re-identification (ReID) is a challenging problem because we cannot access any unseen target domain data during training. Almost all the existing DG ReID methods follow the same pipeline where they use a hybrid dataset from multiple source domains for training, and then directly apply the trained model to the unseen target domains for testing. These methods often neglect individual source domains' discriminative characteristics and their relevances w.r.t. the unseen target domains, though both of which can be leveraged to help the model's generalization. To handle the above two issues, we propose a novel method called the relevance-aware mixture of experts (RaMoE), using an effective voting-based mixture mechanism to dynamically leverage source domains' diverse characteristics to improve the model's generalization. Specifically, we propose a decorrelation loss to make the source domain networks (experts) keep the diversity and discriminability of individual domains' characteristics. Besides, we design a voting network to adaptively integrate all the experts' features into the more generalizable aggregated features with domain relevance. Considering the target domains' invisibility during training, we propose a novel learning-to-learn algorithm combined with our relation alignment loss to update the voting network. Extensive experiments demonstrate that our proposed RaMoE outperforms the state-of-the-art methods.



### PPR10K: A Large-Scale Portrait Photo Retouching Dataset with Human-Region Mask and Group-Level Consistency
- **Arxiv ID**: http://arxiv.org/abs/2105.09180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09180v1)
- **Published**: 2021-05-19 14:55:56+00:00
- **Updated**: 2021-05-19 14:55:56+00:00
- **Authors**: Jie Liang, Hui Zeng, Miaomiao Cui, Xuansong Xie, Lei Zhang
- **Comment**: To appear at CVPR 2021
- **Journal**: None
- **Summary**: Different from general photo retouching tasks, portrait photo retouching (PPR), which aims to enhance the visual quality of a collection of flat-looking portrait photos, has its special and practical requirements such as human-region priority (HRP) and group-level consistency (GLC). HRP requires that more attention should be paid to human regions, while GLC requires that a group of portrait photos should be retouched to a consistent tone. Models trained on existing general photo retouching datasets, however, can hardly meet these requirements of PPR. To facilitate the research on this high-frequency task, we construct a large-scale PPR dataset, namely PPR10K, which is the first of its kind to our best knowledge. PPR10K contains $1, 681$ groups and $11, 161$ high-quality raw portrait photos in total. High-resolution segmentation masks of human regions are provided. Each raw photo is retouched by three experts, while they elaborately adjust each group of photos to have consistent tones. We define a set of objective measures to evaluate the performance of PPR and propose strategies to learn PPR models with good HRP and GLC performance. The constructed PPR10K dataset provides a good benchmark for studying automatic PPR methods, and experiments demonstrate that the proposed learning strategies are effective to improve the retouching performance. Datasets and codes are available: https://github.com/csjliang/PPR10K.



### High-Resolution Photorealistic Image Translation in Real-Time: A Laplacian Pyramid Translation Network
- **Arxiv ID**: http://arxiv.org/abs/2105.09188v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09188v1)
- **Published**: 2021-05-19 15:05:22+00:00
- **Updated**: 2021-05-19 15:05:22+00:00
- **Authors**: Jie Liang, Hui Zeng, Lei Zhang
- **Comment**: To appear at CVPR 2021
- **Journal**: None
- **Summary**: Existing image-to-image translation (I2IT) methods are either constrained to low-resolution images or long inference time due to their heavy computational burden on the convolution of high-resolution feature maps. In this paper, we focus on speeding-up the high-resolution photorealistic I2IT tasks based on closed-form Laplacian pyramid decomposition and reconstruction. Specifically, we reveal that the attribute transformations, such as illumination and color manipulation, relate more to the low-frequency component, while the content details can be adaptively refined on high-frequency components. We consequently propose a Laplacian Pyramid Translation Network (LPTN) to simultaneously perform these two tasks, where we design a lightweight network for translating the low-frequency component with reduced resolution and a progressive masking strategy to efficiently refine the high-frequency ones. Our model avoids most of the heavy computation consumed by processing high-resolution feature maps and faithfully preserves the image details. Extensive experimental results on various tasks demonstrate that the proposed method can translate 4K images in real-time using one normal GPU while achieving comparable transformation performance against existing methods. Datasets and codes are available: https://github.com/csjliang/LPTN.



### Tool- and Domain-Agnostic Parameterization of Style Transfer Effects Leveraging Pretrained Perceptual Metrics
- **Arxiv ID**: http://arxiv.org/abs/2105.09207v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2105.09207v1)
- **Published**: 2021-05-19 15:39:10+00:00
- **Updated**: 2021-05-19 15:39:10+00:00
- **Authors**: Hiromu Yakura, Yuki Koyama, Masataka Goto
- **Comment**: To appear in Proceedings of the 30th International Joint Conference
  on Artificial Intelligence (IJCAI 2021); Project page available at
  https://yumetaro.info/projects/parametric-transcription/
- **Journal**: None
- **Summary**: Current deep learning techniques for style transfer would not be optimal for design support since their "one-shot" transfer does not fit exploratory design processes. To overcome this gap, we propose parametric transcription, which transcribes an end-to-end style transfer effect into parameter values of specific transformations available in an existing content editing tool. With this approach, users can imitate the style of a reference sample in the tool that they are familiar with and thus can easily continue further exploration by manipulating the parameters. To enable this, we introduce a framework that utilizes an existing pretrained model for style transfer to calculate a perceptual style distance to the reference sample and uses black-box optimization to find the parameters that minimize this distance. Our experiments with various third-party tools, such as Instagram and Blender, show that our framework can effectively leverage deep learning techniques for computational design support.



### Joint Calibrationless Reconstruction and Segmentation of Parallel MRI
- **Arxiv ID**: http://arxiv.org/abs/2105.09220v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09220v1)
- **Published**: 2021-05-19 16:04:20+00:00
- **Updated**: 2021-05-19 16:04:20+00:00
- **Authors**: Aniket Pramanik, Xiaodong Wu, Mathews Jacob
- **Comment**: None
- **Journal**: None
- **Summary**: The volume estimation of brain regions from MRI data is a key problem in many clinical applications, where the acquisition of data at high spatial resolution is desirable. While parallel MRI and constrained image reconstruction algorithms can accelerate the scans, image reconstruction artifacts are inevitable, especially at high acceleration factors. We introduce a novel image domain deep-learning framework for calibrationless parallel MRI reconstruction, coupled with a segmentation network to improve image quality and to reduce the vulnerability of current segmentation algorithms to image artifacts resulting from acceleration. The combination of the proposed image domain deep calibrationless approach with the segmentation algorithm offers improved image quality, while increasing the accuracy of the segmentations. The novel architecture with an encoder shared between the reconstruction and segmentation tasks is seen to reduce the need for segmented training datasets. In particular, the proposed few-shot training strategy requires only 10% of segmented datasets to offer good performance.



### Image to Image Translation : Generating maps from satellite images
- **Arxiv ID**: http://arxiv.org/abs/2105.09253v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09253v1)
- **Published**: 2021-05-19 16:58:04+00:00
- **Updated**: 2021-05-19 16:58:04+00:00
- **Authors**: Vaishali Ingale, Rishabh Singh, Pragati Patwal
- **Comment**: None
- **Journal**: None
- **Summary**: Generation of maps from satellite images is conventionally done by a range of tools. Maps became an important part of life whose conversion from satellite images may be a bit expensive but Generative models can pander to this challenge. These models aims at finding the patterns between the input and output image. Image to image translation is employed to convert satellite image to corresponding map. Different techniques for image to image translations like Generative adversarial network, Conditional adversarial networks and Co-Variational Auto encoders are used to generate the corresponding human-readable maps for that region, which takes a satellite image at a given zoom level as its input. We are training our model on Conditional Generative Adversarial Network which comprises of Generator model which which generates fake images while the discriminator tries to classify the image as real or fake and both these models are trained synchronously in adversarial manner where both try to fool each other and result in enhancing model performance.



### Do We Really Need to Learn Representations from In-domain Data for Outlier Detection?
- **Arxiv ID**: http://arxiv.org/abs/2105.09270v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09270v1)
- **Published**: 2021-05-19 17:30:28+00:00
- **Updated**: 2021-05-19 17:30:28+00:00
- **Authors**: Zhisheng Xiao, Qing Yan, Yali Amit
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised outlier detection, which predicts if a test sample is an outlier or not using only the information from unlabelled inlier data, is an important but challenging task. Recently, methods based on the two-stage framework achieve state-of-the-art performance on this task. The framework leverages self-supervised representation learning algorithms to train a feature extractor on inlier data, and applies a simple outlier detector in the feature space. In this paper, we explore the possibility of avoiding the high cost of training a distinct representation for each outlier detection task, and instead using a single pre-trained network as the universal feature extractor regardless of the source of in-domain data. In particular, we replace the task-specific feature extractor by one network pre-trained on ImageNet with a self-supervised loss. In experiments, we demonstrate competitive or better performance on a variety of outlier detection benchmarks compared with previous two-stage methods, suggesting that learning representations from in-domain data may be unnecessary for outlier detection.



### Correlated Input-Dependent Label Noise in Large-Scale Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.10305v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2105.10305v1)
- **Published**: 2021-05-19 17:30:59+00:00
- **Updated**: 2021-05-19 17:30:59+00:00
- **Authors**: Mark Collier, Basil Mustafa, Efi Kokiopoulou, Rodolphe Jenatton, Jesse Berent
- **Comment**: Accepted as Oral at CVPR 2021
- **Journal**: None
- **Summary**: Large scale image classification datasets often contain noisy labels. We take a principled probabilistic approach to modelling input-dependent, also known as heteroscedastic, label noise in these datasets. We place a multivariate Normal distributed latent variable on the final hidden layer of a neural network classifier. The covariance matrix of this latent variable, models the aleatoric uncertainty due to label noise. We demonstrate that the learned covariance structure captures known sources of label noise between semantically similar and co-occurring classes. Compared to standard neural network training and other baselines, we show significantly improved accuracy on Imagenet ILSVRC 2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a new state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These datasets range from over 1M to over 300M training examples and from 1k classes to more than 21k classes. Our method is simple to use, and we provide an implementation that is a drop-in replacement for the final fully-connected layer in a deep classifier.



### Unsupervised Discriminative Learning of Sounds for Audio Event Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.09279v2
- **DOI**: 10.1109/ICASSP39728.2021.9413482
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2105.09279v2)
- **Published**: 2021-05-19 17:42:03+00:00
- **Updated**: 2021-05-20 10:51:57+00:00
- **Authors**: Sascha Hornauer, Ke Li, Stella X. Yu, Shabnam Ghaffarzadegan, Liu Ren
- **Comment**: ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP) | 978-1-7281-7605-5/20/$31.00 (c) 2021 IEEE |
  DOI: 10.1109/ICASSP39728.2021.9413482
- **Journal**: None
- **Summary**: Recent progress in network-based audio event classification has shown the benefit of pre-training models on visual data such as ImageNet. While this process allows knowledge transfer across different domains, training a model on large-scale visual datasets is time consuming. On several audio event classification benchmarks, we show a fast and effective alternative that pre-trains the model unsupervised, only on audio data and yet delivers on-par performance with ImageNet pre-training. Furthermore, we show that our discriminative audio learning can be used to transfer knowledge across audio datasets and optionally include ImageNet pre-training.



### Generative Adversarial Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2105.09356v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09356v3)
- **Published**: 2021-05-19 18:54:44+00:00
- **Updated**: 2021-06-23 04:49:43+00:00
- **Authors**: Seyed Saeed Changiz Rezaei, Fred X. Han, Di Niu, Mohammad Salameh, Keith Mills, Shuo Lian, Wei Lu, Shangling Jui
- **Comment**: 17 pages, 9 figures, 13 Tables
- **Journal**: None
- **Summary**: Despite the empirical success of neural architecture search (NAS) in deep learning applications, the optimality, reproducibility and cost of NAS schemes remain hard to assess. In this paper, we propose Generative Adversarial NAS (GA-NAS) with theoretically provable convergence guarantees, promoting stability and reproducibility in neural architecture search. Inspired by importance sampling, GA-NAS iteratively fits a generator to previously discovered top architectures, thus increasingly focusing on important parts of a large search space. Furthermore, we propose an efficient adversarial learning approach, where the generator is trained by reinforcement learning based on rewards provided by a discriminator, thus being able to explore the search space without evaluating a large number of architectures. Extensive experiments show that GA-NAS beats the best published results under several cases on three public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search constraints and search spaces. We show that GA-NAS can be used to improve already optimized baselines found by other NAS methods, including EfficientNet and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in their original search space.



### Exploring The Limits Of Data Augmentation For Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.09365v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09365v2)
- **Published**: 2021-05-19 19:15:31+00:00
- **Updated**: 2021-05-30 13:04:05+00:00
- **Authors**: Enes Sadi Uysal, M. Şafak Bilici, B. Selin Zaza, M. Yiğit Özgenç, Onur Boyar
- **Comment**: 7 pages, 2 figures
- **Journal**: None
- **Summary**: Retinal Vessel Segmentation is important for the diagnosis of various diseases. The research on retinal vessel segmentation focuses mainly on the improvement of the segmentation model which is usually based on U-Net architecture. In our study, we use the U-Net architecture and we rely on heavy data augmentation in order to achieve better performance. The success of the data augmentation relies on successfully addressing the problem of input images. By analyzing input images and performing the augmentation accordingly we show that the performance of the U-Net model can be increased dramatically. Results are reported using the most widely used retina dataset, DRIVE.



### VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation
- **Arxiv ID**: http://arxiv.org/abs/2105.09371v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09371v2)
- **Published**: 2021-05-19 19:25:23+00:00
- **Updated**: 2021-10-08 14:30:36+00:00
- **Authors**: Haresh Karnan, Garrett Warnell, Xuesu Xiao, Peter Stone
- **Comment**: Under Submission to ICRA+RAL 2022
- **Journal**: ICRA 2022
- **Summary**: While imitation learning for vision based autonomous mobile robot navigation has recently received a great deal of attention in the research community, existing approaches typically require state action demonstrations that were gathered using the deployment platform. However, what if one cannot easily outfit their platform to record these demonstration signals or worse yet the demonstrator does not have access to the platform at all? Is imitation learning for vision based autonomous navigation even possible in such scenarios? In this work, we hypothesize that the answer is yes and that recent ideas from the Imitation from Observation (IfO) literature can be brought to bear such that a robot can learn to navigate using only ego centric video collected by a demonstrator, even in the presence of viewpoint mismatch. To this end, we introduce a new algorithm, Visual Observation only Imitation Learning for Autonomous navigation (VOILA), that can successfully learn navigation policies from a single video demonstration collected from a physically different agent. We evaluate VOILA in the photorealistic AirSim simulator and show that VOILA not only successfully imitates the expert, but that it also learns navigation policies that can generalize to novel environments. Further, we demonstrate the effectiveness of VOILA in a real world setting by showing that it allows a wheeled Jackal robot to successfully imitate a human walking in an environment using a video recorded using a mobile phone camera.



### Endless Loops: Detecting and Animating Periodic Patterns in Still Images
- **Arxiv ID**: http://arxiv.org/abs/2105.09374v1
- **DOI**: 10.1145/3450626.3459935
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09374v1)
- **Published**: 2021-05-19 19:39:58+00:00
- **Updated**: 2021-05-19 19:39:58+00:00
- **Authors**: Tavi Halperin, Hanit Hakim, Orestis Vantzos, Gershon Hochman, Netai Benaim, Lior Sassy, Michael Kupchik, Ofir Bibi, Ohad Fried
- **Comment**: SIGGRAPH 2021. Project page:
  https://pub.res.lightricks.com/endless-loops/ . Video:
  https://youtu.be/8ZYUvxWuD2Y
- **Journal**: ACM Trans. Graph., Vol. 40, No. 4, Article 142. Publication date:
  August 2021
- **Summary**: We present an algorithm for producing a seamless animated loop from a single image. The algorithm detects periodic structures, such as the windows of a building or the steps of a staircase, and generates a non-trivial displacement vector field that maps each segment of the structure onto a neighboring segment along a user- or auto-selected main direction of motion. This displacement field is used, together with suitable temporal and spatial smoothing, to warp the image and produce the frames of a continuous animation loop. Our cinemagraphs are created in under a second on a mobile device. Over 140,000 users downloaded our app and exported over 350,000 cinemagraphs. Moreover, we conducted two user studies that show that users prefer our method for creating surreal and structured cinemagraphs compared to more manual approaches and compared to previous methods.



### Robust partial Fourier reconstruction for diffusion-weighted imaging using a recurrent convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2105.09378v2
- **DOI**: 10.1002/mrm.29100
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09378v2)
- **Published**: 2021-05-19 20:00:04+00:00
- **Updated**: 2022-01-09 17:05:15+00:00
- **Authors**: Fasil Gadjimuradov, Thomas Benkert, Marcel Dominik Nickel, Andreas Maier
- **Comment**: Revisions made as required for appearance in Magnetic Resonance in
  Medicine
- **Journal**: None
- **Summary**: Purpose: To develop an algorithm for robust partial Fourier (PF) reconstruction applicable to diffusion-weighted (DW) images with non-smooth phase variations.   Methods: Based on an unrolled proximal splitting algorithm, a neural network architecture is derived which alternates between data consistency operations and regularization implemented by recurrent convolutions. In order to exploit correlations, multiple repetitions of the same slice are jointly reconstructed under consideration of permutation-equivariance. The algorithm is trained on DW liver data of 60 volunteers and evaluated on retrospectively and prospectively sub-sampled data of different anatomies and resolutions.   Results: The proposed method is able to significantly outperform conventional PF techniques on retrospectively sub-sampled data in terms of quantitative measures as well as perceptual image quality. In this context, joint reconstruction of repetitions as well as the particular type of recurrent network unrolling are found to be beneficial with respect to reconstruction quality. On prospectively PF-sampled data, the proposed method enables DW imaging with higher signal without sacrificing image resolution or introducing additional artifacts. Alternatively, it can be used to counter the TE increase in acquisitions with higher resolution. Further, generalizability can be shown to prospective brain data exhibiting anatomies and contrasts not present in the training set.   Conclusion: This work demonstrates that robust PF reconstruction of DW data is feasible even at strong PF factors in anatomies prone to phase variations. Since the proposed method does not rely on smoothness priors of the phase but uses learned recurrent convolutions instead, artifacts of conventional PF methods can be avoided.



### Birds of a Feather: Capturing Avian Shape Models from Images
- **Arxiv ID**: http://arxiv.org/abs/2105.09396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09396v1)
- **Published**: 2021-05-19 20:53:48+00:00
- **Updated**: 2021-05-19 20:53:48+00:00
- **Authors**: Yufu Wang, Nikos Kolotouros, Kostas Daniilidis, Marc Badger
- **Comment**: CVPR 2021. Project website: https://yufu-wang.github.io/aves/
- **Journal**: None
- **Summary**: Animals are diverse in shape, but building a deformable shape model for a new species is not always possible due to the lack of 3D data. We present a method to capture new species using an articulated template and images of that species. In this work, we focus mainly on birds. Although birds represent almost twice the number of species as mammals, no accurate shape model is available. To capture a novel species, we first fit the articulated template to each training sample. By disentangling pose and shape, we learn a shape space that captures variation both among species and within each species from image evidence. We learn models of multiple species from the CUB dataset, and contribute new species-specific and multi-species shape models that are useful for downstream reconstruction tasks. Using a low-dimensional embedding, we show that our learned 3D shape space better reflects the phylogenetic relationships among birds than learned perceptual features.



### Heterogeneous Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.09401v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09401v3)
- **Published**: 2021-05-19 21:01:41+00:00
- **Updated**: 2022-07-26 00:11:14+00:00
- **Authors**: Lecheng Zheng, Jinjun Xiong, Yada Zhu, Jingrui He
- **Comment**: Accepted by KDD22
- **Journal**: None
- **Summary**: With the advent of big data across multiple high-impact applications, we are often facing the challenge of complex heterogeneity. The newly collected data usually consist of multiple modalities and are characterized with multiple labels, thus exhibiting the co-existence of multiple types of heterogeneity. Although state-of-the-art techniques are good at modeling complex heterogeneity with sufficient label information, such label information can be quite expensive to obtain in real applications. Recently, researchers pay great attention to contrastive learning due to its prominent performance by utilizing rich unlabeled data. However, existing work on contrastive learning is not able to address the problem of false negative pairs, i.e., some `negative' pairs may have similar representations if they have the same label. To overcome the issues, in this paper, we propose a unified heterogeneous learning framework, which combines both the weighted unsupervised contrastive loss and the weighted supervised contrastive loss to model multiple types of heterogeneity. We first provide a theoretical analysis showing that the vanilla contrastive learning loss easily leads to the sub-optimal solution in the presence of false negative pairs, whereas the proposed weighted loss could automatically adjust the weight based on the similarity of the learned representations to mitigate this issue. Experimental results on real-world data sets demonstrate the effectiveness and the efficiency of the proposed framework modeling multiple types of heterogeneity.



### Unsupervised learning of text line segmentation by differentiating coarse patterns
- **Arxiv ID**: http://arxiv.org/abs/2105.09405v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09405v2)
- **Published**: 2021-05-19 21:21:30+00:00
- **Updated**: 2021-05-21 00:39:27+00:00
- **Authors**: Berat Kurar Barakat, Ahmad Droby, Raid Saabni, Jihad El-Sana
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent advances in the field of supervised deep learning for text line segmentation, unsupervised deep learning solutions are beginning to gain popularity. In this paper, we present an unsupervised deep learning method that embeds document image patches to a compact Euclidean space where distances correspond to a coarse text line pattern similarity. Once this space has been produced, text line segmentation can be easily implemented using standard techniques with the embedded feature vectors. To train the model, we extract random pairs of document image patches with the assumption that neighbour patches contain a similar coarse trend of text lines, whereas if one of them is rotated, they contain different coarse trends of text lines. Doing well on this task requires the model to learn to recognize the text lines and their salient parts. The benefit of our approach is zero manual labelling effort. We evaluate the method qualitatively and quantitatively on several variants of text line segmentation datasets to demonstrate its effectivity.



### Classifying concepts via visual properties
- **Arxiv ID**: http://arxiv.org/abs/2105.09422v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09422v1)
- **Published**: 2021-05-19 22:24:30+00:00
- **Updated**: 2021-05-19 22:24:30+00:00
- **Authors**: Fausto Giunchiglia, Mayukh Bagchi
- **Comment**: None
- **Journal**: None
- **Summary**: We assume that substances in the world are represented by two types of concepts, namely substance concepts and classification concepts, the former instrumental to (visual) perception, the latter to (language based) classification. Based on this distinction, we introduce a general methodology for building lexico-semantic hierarchies of substance concepts, where nodes are annotated with the media, e.g.,videos or photos, from which substance concepts are extracted, and are associated with the corresponding classification concepts. The methodology is based on Ranganathan's original faceted approach, contextualized to the problem of classifying substance concepts. The key novelty is that the hierarchy is built exploiting the visual properties of substance concepts, while the linguistically defined properties of classification concepts are only used to describe substance concepts. The validity of the approach is exemplified by providing some highlights of an ongoing project whose goal is to build a large scale multimedia multilingual concept hierarchy.



### End-to-End Unsupervised Document Image Blind Denoising
- **Arxiv ID**: http://arxiv.org/abs/2105.09437v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09437v2)
- **Published**: 2021-05-19 23:55:15+00:00
- **Updated**: 2021-10-07 21:57:51+00:00
- **Authors**: Mehrdad J Gangeh, Marcin Plata, Hamid Motahari, Nigel P Duffy
- **Comment**: 10 pages main & 10 pages supplementary, the paper is accepted at ICCV
  2021
- **Journal**: None
- **Summary**: Removing noise from scanned pages is a vital step before their submission to the optical character recognition (OCR) system. Most available image denoising methods are supervised where the pairs of noisy/clean pages are required. However, this assumption is rarely met in real settings. Besides, there is no single model that can remove various noise types from documents. Here, we propose a unified end-to-end unsupervised deep learning model, for the first time, that can effectively remove multiple types of noise, including salt \& pepper noise, blurred and/or faded text, as well as watermarks from documents at various levels of intensity. We demonstrate that the proposed model significantly improves the quality of scanned images and the OCR of the pages on several test datasets.




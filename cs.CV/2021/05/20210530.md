# Arxiv Papers in cs.CV on 2021-05-30
### Enhanced Isotropy Maximization Loss: Seamless and High-Performance Out-of-Distribution Detection Simply Replacing the SoftMax Loss
- **Arxiv ID**: http://arxiv.org/abs/2105.14399v10
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2105.14399v10)
- **Published**: 2021-05-30 00:55:03+00:00
- **Updated**: 2022-04-05 12:43:01+00:00
- **Authors**: David Macêdo, Teresa Ludermir
- **Comment**: None
- **Journal**: None
- **Summary**: Current out-of-distribution detection approaches usually present special requirements (e.g., collecting outlier data and hyperparameter validation) and produce side effects (e.g., classification accuracy drop and slow/inefficient inferences). Recently, entropic out-of-distribution detection has been proposed as a seamless approach (i.e., a solution that avoids all previously mentioned drawbacks). The entropic out-of-distribution detection solution uses the IsoMax loss for training and the entropic score for out-of-distribution detection. The IsoMax loss works as a drop-in replacement of the SoftMax loss (i.e., the combination of the output linear layer, the SoftMax activation, and the cross-entropy loss) because swapping the SoftMax loss with the IsoMax loss requires no changes in the model's architecture or training procedures/hyperparameters. In this paper, we perform what we call an isometrization of the distances used in the IsoMax loss. Additionally, we propose replacing the entropic score with the minimum distance score. Experiments showed that these modifications significantly increase out-of-distribution detection performance while keeping the solution seamless. Besides being competitive with or outperforming all major current approaches, the proposed solution avoids all their current limitations, in addition to being much easier to use because only a simple loss replacement for training the neural network is required. The code to replace the SoftMax loss with the IsoMax+ loss and reproduce the results is available at https://github.com/dlmacedo/entropic-out-of-distribution-detection.



### An improved LogNNet classifier for IoT application
- **Arxiv ID**: http://arxiv.org/abs/2105.14412v2
- **DOI**: 10.1088/1742-6596/2094/3/032015
- **Categories**: **cs.LG**, cs.CV, cs.NE, eess.IV, nlin.CD
- **Links**: [PDF](http://arxiv.org/pdf/2105.14412v2)
- **Published**: 2021-05-30 02:12:45+00:00
- **Updated**: 2022-01-13 01:04:57+00:00
- **Authors**: Hanif Heidari, Andrei Velichko
- **Comment**: 9 pages, 7 figures
- **Journal**: J. Phys.: Conf. Ser. 2094 032015 (2021)
- **Summary**: In the age of neural networks and Internet of Things (IoT), the search for new neural network architectures capable of operating on devices with limited computing power and small memory size is becoming an urgent agenda. Designing suitable algorithms for IoT applications is an important task. The paper proposes a feed forward LogNNet neural network, which uses a semi-linear Henon type discrete chaotic map to classify MNIST-10 dataset. The model is composed of reservoir part and trainable classifier. The aim of the reservoir part is transforming the inputs to maximize the classification accuracy using a special matrix filing method and a time series generated by the chaotic map. The parameters of the chaotic map are optimized using particle swarm optimization with random immigrants. As a result, the proposed LogNNet/Henon classifier has higher accuracy and the same RAM usage, compared to the original version of LogNNet, and offers promising opportunities for implementation in IoT devices. In addition, a direct relation between the value of entropy and accuracy of the classification is demonstrated.



### VersatileGait: A Large-Scale Synthetic Gait Dataset Towards in-the-Wild Simulation
- **Arxiv ID**: http://arxiv.org/abs/2105.14421v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14421v2)
- **Published**: 2021-05-30 03:39:53+00:00
- **Updated**: 2021-06-01 02:53:54+00:00
- **Authors**: Pengyi Zhang, Huanzhang Dou, Wenhu Zhang, Yuhan Zhao, Songyuan Li, Zequn Qin, Xi Li
- **Comment**: We should have updated 2101.01394 but we did a new submission
- **Journal**: None
- **Summary**: Gait recognition has a rapid development in recent years. However, gait recognition in the wild is not well explored yet. An obvious reason could be ascribed to the lack of diverse training data from the perspective of intrinsic and extrinsic factors. To remedy this problem, we propose to construct a large-scale gait dataset with the help of controllable computer simulation. In detail, to diversify the intrinsic factors of gait, we generate numerous characters with diverse attributes and empower them with various types of walking styles. To diversify the extrinsic factors of gait, we build a complicated scene with a dense camera layout. Finally, we design an automated generation toolkit under Unity3D for simulating the walking scenario and capturing the gait data automatically. As a result, we obtain an in-the-wild gait dataset, called VersatileGait, which has more than one million silhouette sequences of 10,000 subjects with diverse scenarios. VersatileGait possesses several nice properties, including huge dataset size, diverse pedestrian attributes, complicated camera layout, high-quality annotations, small domain gap with the real one, good scalability for new demands, and no privacy issues. Based on VersatileGait, we propose series of experiments and applications for both research exploration of gait in the wild and practical applications. Our dataset and its corresponding generation toolkit will be publicly available for further studies.



### Gaze Estimation using Transformer
- **Arxiv ID**: http://arxiv.org/abs/2105.14424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14424v1)
- **Published**: 2021-05-30 04:06:29+00:00
- **Updated**: 2021-05-30 04:06:29+00:00
- **Authors**: Yihua Cheng, Feng Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has proven the effectiveness of transformers in many computer vision tasks. However, the performance of transformers in gaze estimation is still unexplored. In this paper, we employ transformers and assess their effectiveness for gaze estimation. We consider two forms of vision transformer which are pure transformers and hybrid transformers. We first follow the popular ViT and employ a pure transformer to estimate gaze from images. On the other hand, we preserve the convolutional layers and integrate CNNs as well as transformers. The transformer serves as a component to complement CNNs. We compare the performance of the two transformers in gaze estimation. The Hybrid transformer significantly outperforms the pure transformer in all evaluation datasets with less parameters. We further conduct experiments to assess the effectiveness of the hybrid transformer and explore the advantage of self-attention mechanism. Experiments show the hybrid transformer can achieve state-of-the-art performance in all benchmarks with pre-training.To facilitate further research, we release codes and models in https://github.com/yihuacheng/GazeTR.



### ICDAR 2021 Competition on Scientific Table Image Recognition to LaTeX
- **Arxiv ID**: http://arxiv.org/abs/2105.14426v2
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.14426v2)
- **Published**: 2021-05-30 04:17:55+00:00
- **Updated**: 2021-11-11 11:21:08+00:00
- **Authors**: Pratik Kayal, Mrinal Anand, Harsh Desai, Mayank Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Tables present important information concisely in many scientific documents. Visual features like mathematical symbols, equations, and spanning cells make structure and content extraction from tables embedded in research documents difficult. This paper discusses the dataset, tasks, participants' methods, and results of the ICDAR 2021 Competition on Scientific Table Image Recognition to LaTeX. Specifically, the task of the competition is to convert a tabular image to its corresponding LaTeX source code. We proposed two subtasks. In Subtask 1, we ask the participants to reconstruct the LaTeX structure code from an image. In Subtask 2, we ask the participants to reconstruct the LaTeX content code from an image. This report describes the datasets and ground truth specification, details the performance evaluation metrics used, presents the final results, and summarizes the participating methods. Submission by team VCGroup got the highest Exact Match accuracy score of 74% for Subtask 1 and 55% for Subtask 2, beating previous baselines by 5% and 12%, respectively. Although improvements can still be made to the recognition capabilities of models, this competition contributes to the development of fully automated table recognition systems by challenging practitioners to solve problems under specific constraints and sharing their approaches; the platform will remain available for post-challenge submissions at https://competitions.codalab.org/competitions/26979 .



### Rethinking the constraints of multimodal fusion: case study in Weakly-Supervised Audio-Visual Video Parsing
- **Arxiv ID**: http://arxiv.org/abs/2105.14430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14430v1)
- **Published**: 2021-05-30 05:13:30+00:00
- **Updated**: 2021-05-30 05:13:30+00:00
- **Authors**: Jianning Wu, Zhuqing Jiang, Shiping Wen, Aidong Men, Haiying Wang
- **Comment**: None
- **Journal**: None
- **Summary**: For multimodal tasks, a good feature extraction network should extract information as much as possible and ensure that the extracted feature embedding and other modal feature embedding have an excellent mutual understanding. The latter is often more critical in feature fusion than the former. Therefore, selecting the optimal feature extraction network collocation is a very important subproblem in multimodal tasks. Most of the existing studies ignore this problem or adopt an ergodic approach. This problem is modeled as an optimization problem in this paper. A novel method is proposed to convert the optimization problem into an issue of comparative upper bounds by referring to the general practice of extreme value conversion in mathematics. Compared with the traditional method, it reduces the time cost.   Meanwhile, aiming at the common problem that the feature similarity and the feature semantic similarity are not aligned in the multimodal time-series problem, we refer to the idea of contrast learning and propose a multimodal time-series contrastive loss(MTSC).   Based on the above issues, We demonstrated the feasibility of our approach in the audio-visual video parsing task. Substantial analyses verify that our methods promote the fusion of different modal features.



### TransMatcher: Deep Image Matching Through Transformers for Generalizable Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2105.14432v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14432v2)
- **Published**: 2021-05-30 05:38:33+00:00
- **Updated**: 2021-12-07 07:09:23+00:00
- **Authors**: Shengcai Liao, Ling Shao
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: Transformers have recently gained increasing attention in computer vision. However, existing studies mostly use Transformers for feature representation learning, e.g. for image classification and dense predictions, and the generalizability of Transformers is unknown. In this work, we further investigate the possibility of applying Transformers for image matching and metric learning given pairs of images. We find that the Vision Transformer (ViT) and the vanilla Transformer with decoders are not adequate for image matching due to their lack of image-to-image attention. Thus, we further design two naive solutions, i.e. query-gallery concatenation in ViT, and query-gallery cross-attention in the vanilla Transformer. The latter improves the performance, but it is still limited. This implies that the attention mechanism in Transformers is primarily designed for global feature aggregation, which is not naturally suitable for image matching. Accordingly, we propose a new simplified decoder, which drops the full attention implementation with the softmax weighting, keeping only the query-key similarity computation. Additionally, global max pooling and a multilayer perceptron (MLP) head are applied to decode the matching result. This way, the simplified decoder is computationally more efficient, while at the same time more effective for image matching. The proposed method, called TransMatcher, achieves state-of-the-art performance in generalizable person re-identification, with up to 6.1% and 5.7% performance gains in Rank-1 and mAP, respectively, on several popular datasets. Code is available at https://github.com/ShengcaiLiao/QAConv.



### EPSANet: An Efficient Pyramid Squeeze Attention Block on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2105.14447v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14447v2)
- **Published**: 2021-05-30 07:26:41+00:00
- **Updated**: 2021-07-22 13:08:44+00:00
- **Authors**: Hu Zhang, Keke Zu, Jian Lu, Yuru Zou, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, it has been demonstrated that the performance of a deep convolutional neural network can be effectively improved by embedding an attention module into it. In this work, a novel lightweight and effective attention method named Pyramid Squeeze Attention (PSA) module is proposed. By replacing the 3x3 convolution with the PSA module in the bottleneck blocks of the ResNet, a novel representational block named Efficient Pyramid Squeeze Attention (EPSA) is obtained. The EPSA block can be easily added as a plug-and-play component into a well-established backbone network, and significant improvements on model performance can be achieved. Hence, a simple and efficient backbone architecture named EPSANet is developed in this work by stacking these ResNet-style EPSA blocks. Correspondingly, a stronger multi-scale representation ability can be offered by the proposed EPSANet for various computer vision tasks including but not limited to, image classification, object detection, instance segmentation, etc. Without bells and whistles, the performance of the proposed EPSANet outperforms most of the state-of-the-art channel attention methods. As compared to the SENet-50, the Top-1 accuracy is improved by 1.93% on ImageNet dataset, a larger margin of +2.7 box AP for object detection and an improvement of +1.7 mask AP for instance segmentation by using the Mask-RCNN on MS-COCO dataset are obtained. Our source code is available at:https://github.com/murufeng/EPSANet.



### Learning Personal Style from Few Examples
- **Arxiv ID**: http://arxiv.org/abs/2105.14457v2
- **DOI**: 10.1145/3461778.3462115
- **Categories**: **cs.CV**, cs.HC, H.5.0; I.5.4; J.5
- **Links**: [PDF](http://arxiv.org/pdf/2105.14457v2)
- **Published**: 2021-05-30 08:04:42+00:00
- **Updated**: 2021-06-17 12:30:13+00:00
- **Authors**: David Chuan-En Lin, Nikolas Martelaro
- **Comment**: None
- **Journal**: None
- **Summary**: A key task in design work is grasping the client's implicit tastes. Designers often do this based on a set of examples from the client. However, recognizing a common pattern among many intertwining variables such as color, texture, and layout and synthesizing them into a composite preference can be challenging. In this paper, we leverage the pattern recognition capability of computational models to aid in this task. We offer a set of principles for computationally learning personal style. The principles are manifested in PseudoClient, a deep learning framework that learns a computational model for personal graphic design style from only a handful of examples. In several experiments, we found that PseudoClient achieves a 79.40% accuracy with only five positive and negative examples, outperforming several alternative methods. Finally, we discuss how PseudoClient can be utilized as a building block to support the development of future design applications.



### High Performance Hyperspectral Image Classification using Graphics Processing Units
- **Arxiv ID**: http://arxiv.org/abs/2106.12942v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.LG, 68W10, 68W15
- **Links**: [PDF](http://arxiv.org/pdf/2106.12942v1)
- **Published**: 2021-05-30 09:26:03+00:00
- **Updated**: 2021-05-30 09:26:03+00:00
- **Authors**: Mahmoud Hossam
- **Comment**: Master Thesis, Ain Shams University
- **Journal**: None
- **Summary**: Real-time remote sensing applications like search and rescue missions, military target detection, environmental monitoring, hazard prevention and other time-critical applications require onboard real time processing capabilities or autonomous decision making. Some unmanned remote systems like satellites are physically remote from their operators, and all control of the spacecraft and data returned by the spacecraft must be transmitted over a wireless radio link. This link may not be available for extended periods when the satellite is out of line of sight of its ground station. Therefore, lightweight, small size and low power consumption hardware is essential for onboard real time processing systems. With increasing dimensionality, size and resolution of recent hyperspectral imaging sensors, additional challenges are posed upon remote sensing processing systems and more capable computing architectures are needed. Graphical Processing Units (GPUs) emerged as promising architecture for light weight high performance computing that can address these computational requirements for onboard systems. The goal of this study is to build high performance methods for onboard hyperspectral analysis. We propose accelerated methods for the well-known recursive hierarchical segmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a GPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the National Aeronautics and Space Administration (NASA), which is designed to provide rich classification information with several output levels. The achieved speedups by parallel solutions compared to CPU sequential implementations are 21x for parallel single GPU and 240x for hybrid multi-node computer clusters with 16 computing nodes. The energy consumption is reduced to 74% using a single GPU compared to the equivalent parallel CPU cluster.



### Towards Diverse Paragraph Captioning for Untrimmed Videos
- **Arxiv ID**: http://arxiv.org/abs/2105.14477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14477v1)
- **Published**: 2021-05-30 09:28:43+00:00
- **Updated**: 2021-05-30 09:28:43+00:00
- **Authors**: Yuqing Song, Shizhe Chen, Qin Jin
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Video paragraph captioning aims to describe multiple events in untrimmed videos with descriptive paragraphs. Existing approaches mainly solve the problem in two steps: event detection and then event captioning. Such two-step manner makes the quality of generated paragraphs highly dependent on the accuracy of event proposal detection which is already a challenging task. In this paper, we propose a paragraph captioning model which eschews the problematic event detection stage and directly generates paragraphs for untrimmed videos. To describe coherent and diverse events, we propose to enhance the conventional temporal attention with dynamic video memories, which progressively exposes new video features and suppresses over-accessed video contents to control visual focuses of the model. In addition, a diversity-driven training strategy is proposed to improve diversity of paragraph on the language perspective. Considering that untrimmed videos generally contain massive but redundant frames, we further augment the video encoder with keyframe awareness to improve efficiency. Experimental results on the ActivityNet and Charades datasets show that our proposed model significantly outperforms the state-of-the-art performance on both accuracy and diversity metrics without using any event boundary annotations. Code will be released at https://github.com/syuqings/video-paragraph.



### Knowledge Transfer for Few-shot Segmentation of Novel White Matter Tracts
- **Arxiv ID**: http://arxiv.org/abs/2105.14513v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.14513v2)
- **Published**: 2021-05-30 11:57:41+00:00
- **Updated**: 2021-06-01 04:54:59+00:00
- **Authors**: Qi Lu, Chuyang Ye
- **Comment**: accepted by IPMI 2021
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved stateof-the-art performance for white matter (WM) tract segmentation based on diffusion magnetic resonance imaging (dMRI). These CNNs require a large number of manual delineations of the WM tracts of interest for training, which are generally labor-intensive and costly. The expensive manual delineation can be a particular disadvantage when novel WM tracts, i.e., tracts that have not been included in existing manual delineations, are to be analyzed. To accurately segment novel WM tracts, it is desirable to transfer the knowledge learned about existing WM tracts, so that even with only a few delineations of the novel WM tracts, CNNs can learn adequately for the segmentation. In this paper, we explore the transfer of such knowledge to the segmentation of novel WM tracts in the few-shot setting. Although a classic fine-tuning strategy can be used for the purpose, the information in the last task-specific layer for segmenting existing WM tracts is completely discarded. We hypothesize that the weights of this last layer can bear valuable information for segmenting the novel WM tracts and thus completely discarding the information is not optimal. In particular, we assume that the novel WM tracts can correlate with existing WM tracts and the segmentation of novel WM tracts can be predicted with the logits of existing WM tracts. In this way, better initialization of the last layer than random initialization can be achieved for fine-tuning. Further, we show that a more adaptive use of the knowledge in the last layer for segmenting existing WM tracts can be conveniently achieved by simply inserting a warmup stage before classic fine-tuning. The proposed method was evaluated on a publicly available dMRI dataset, where we demonstrate the benefit of our method for few-shot segmentation of novel WM tracts.



### Unsupervised Joint Learning of Depth, Optical Flow, Ego-motion from Video
- **Arxiv ID**: http://arxiv.org/abs/2105.14520v1
- **DOI**: None
- **Categories**: **cs.CV**, 65Dxx
- **Links**: [PDF](http://arxiv.org/pdf/2105.14520v1)
- **Published**: 2021-05-30 12:39:48+00:00
- **Updated**: 2021-05-30 12:39:48+00:00
- **Authors**: Jianfeng Li, Junqiao Zhao, Shuangfu Song, Tiantian Feng
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Estimating geometric elements such as depth, camera motion, and optical flow from images is an important part of the robot's visual perception. We use a joint self-supervised method to estimate the three geometric elements. Depth network, optical flow network and camera motion network are independent of each other but are jointly optimized during training phase. Compared with independent training, joint training can make full use of the geometric relationship between geometric elements and provide dynamic and static information of the scene. In this paper, we improve the joint self-supervision method from three aspects: network structure, dynamic object segmentation, and geometric constraints. In terms of network structure, we apply the attention mechanism to the camera motion network, which helps to take advantage of the similarity of camera movement between frames. And according to attention mechanism in Transformer, we propose a plug-and-play convolutional attention module. In terms of dynamic object, according to the different influences of dynamic objects in the optical flow self-supervised framework and the depth-pose self-supervised framework, we propose a threshold algorithm to detect dynamic regions, and mask that in the loss function respectively. In terms of geometric constraints, we use traditional methods to estimate the fundamental matrix from the corresponding points to constrain the camera motion network. We demonstrate the effectiveness of our method on the KITTI dataset. Compared with other joint self-supervised methods, our method achieves state-of-the-art performance in the estimation of pose and optical flow, and the depth estimation has also achieved competitive results. Code will be available https://github.com/jianfenglihg/Unsupervised_geometry.



### Longer Version for "Deep Context-Encoding Network for Retinal Image Captioning"
- **Arxiv ID**: http://arxiv.org/abs/2105.14538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2105.14538v1)
- **Published**: 2021-05-30 13:37:03+00:00
- **Updated**: 2021-05-30 13:37:03+00:00
- **Authors**: Jia-Hong Huang, Ting-Wei Wu, Chao-Han Huck Yang, Marcel Worring
- **Comment**: This paper is a longer version of "Deep Context-Encoding Network for
  Retinal Image Captioning" which is accepted by IEEE International Conference
  on Image Processing (ICIP), 2021
- **Journal**: None
- **Summary**: Automatically generating medical reports for retinal images is one of the promising ways to help ophthalmologists reduce their workload and improve work efficiency. In this work, we propose a new context-driven encoding network to automatically generate medical reports for retinal images. The proposed model is mainly composed of a multi-modal input encoder and a fused-feature decoder. Our experimental results show that our proposed method is capable of effectively leveraging the interactive information between the input image and context, i.e., keywords in our case. The proposed method creates more accurate and meaningful reports for retinal images than baseline models and achieves state-of-the-art performance. This performance is shown in several commonly used metrics for the medical report generation task: BLEU-avg (+16%), CIDEr (+10.2%), and ROUGE (+8.6%).



### Attention Based Semantic Segmentation on UAV Dataset for Natural Disaster Damage Assessment
- **Arxiv ID**: http://arxiv.org/abs/2105.14540v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2105.14540v2)
- **Published**: 2021-05-30 13:39:03+00:00
- **Updated**: 2021-06-01 18:11:43+00:00
- **Authors**: Tashnim Chowdhury, Maryam Rahnemoonfar
- **Comment**: arXiv admin note: text overlap with arXiv:2009.01193
- **Journal**: None
- **Summary**: The detrimental impacts of climate change include stronger and more destructive hurricanes happening all over the world. Identifying different damaged structures of an area including buildings and roads are vital since it helps the rescue team to plan their efforts to minimize the damage caused by a natural disaster. Semantic segmentation helps to identify different parts of an image. We implement a novel self-attention based semantic segmentation model on a high resolution UAV dataset and attain Mean IoU score of around 88% on the test set. The result inspires to use self-attention schemes in natural disaster damage assessment which will save human lives and reduce economic losses.



### Xihe: A 3D Vision-based Lighting Estimation Framework for Mobile Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2106.15280v1
- **DOI**: 10.1145/3458864.3467886
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2106.15280v1)
- **Published**: 2021-05-30 13:48:29+00:00
- **Updated**: 2021-05-30 13:48:29+00:00
- **Authors**: Yiqin Zhao, Tian Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Omnidirectional lighting provides the foundation for achieving spatially-variant photorealistic 3D rendering, a desirable property for mobile augmented reality applications. However, in practice, estimating omnidirectional lighting can be challenging due to limitations such as partial panoramas of the rendering positions, and the inherent environment lighting and mobile user dynamics. A new opportunity arises recently with the advancements in mobile 3D vision, including built-in high-accuracy depth sensors and deep learning-powered algorithms, which provide the means to better sense and understand the physical surroundings. Centering the key idea of 3D vision, in this work, we design an edge-assisted framework called Xihe to provide mobile AR applications the ability to obtain accurate omnidirectional lighting estimation in real time. Specifically, we develop a novel sampling technique that efficiently compresses the raw point cloud input generated at the mobile device. This technique is derived based on our empirical analysis of a recent 3D indoor dataset and plays a key role in our 3D vision-based lighting estimator pipeline design. To achieve the real-time goal, we develop a tailored GPU pipeline for on-device point cloud processing and use an encoding technique that reduces network transmitted bytes. Finally, we present an adaptive triggering strategy that allows Xihe to skip unnecessary lighting estimations and a practical way to provide temporal coherent rendering integration with the mobile AR ecosystem. We evaluate both the lighting estimation accuracy and time of Xihe using a reference mobile application developed with Xihe's APIs. Our results show that Xihe takes as fast as 20.67ms per lighting estimation and achieves 9.4% better estimation accuracy than a state-of-the-art neural network.



### Z2P: Instant Visualization of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2105.14548v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.14548v2)
- **Published**: 2021-05-30 13:58:24+00:00
- **Updated**: 2022-02-21 10:59:02+00:00
- **Authors**: Gal Metzer, Rana Hanocka, Raja Giryes, Niloy J. Mitra, Daniel Cohen-Or
- **Comment**: Eurographics 2022
- **Journal**: None
- **Summary**: We present a technique for visualizing point clouds using a neural network. Our technique allows for an instant preview of any point cloud, and bypasses the notoriously difficult surface reconstruction problem or the need to estimate oriented normals for splat-based rendering. We cast the preview problem as a conditional image-to-image translation task, and design a neural network that translates point depth-map directly into an image, where the point cloud is visualized as though a surface was reconstructed from it. Furthermore, the resulting appearance of the visualized point cloud can be, optionally, conditioned on simple control variables (e.g., color and light). We demonstrate that our technique instantly produces plausible images, and can, on-the-fly effectively handle noise, non-uniform sampling, and thin surfaces sheets.



### Multiscale IoU: A Metric for Evaluation of Salient Object Detection with Fine Structures
- **Arxiv ID**: http://arxiv.org/abs/2105.14572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14572v1)
- **Published**: 2021-05-30 15:31:42+00:00
- **Updated**: 2021-05-30 15:31:42+00:00
- **Authors**: Azim Ahmadzadeh, Dustin J. Kempton, Yang Chen, Rafal A. Angryk
- **Comment**: 5 pages, 3 figures, ICIP 2021
- **Journal**: None
- **Summary**: General-purpose object-detection algorithms often dismiss the fine structure of detected objects. This can be traced back to how their proposed regions are evaluated. Our goal is to renegotiate the trade-off between the generality of these algorithms and their coarse detections. In this work, we present a new metric that is a marriage of a popular evaluation metric, namely Intersection over Union (IoU), and a geometrical concept, called fractal dimension. We propose Multiscale IoU (MIoU) which allows comparison between the detected and ground-truth regions at multiple resolution levels. Through several reproducible examples, we show that MIoU is indeed sensitive to the fine boundary structures which are completely overlooked by IoU and f1-score. We further examine the overall reliability of MIoU by comparing its distribution with that of IoU on synthetic and real-world datasets of objects. We intend this work to re-initiate exploration of new evaluation methods for object-detection algorithms.



### StyTr$^2$: Image Style Transfer with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2105.14576v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.14576v3)
- **Published**: 2021-05-30 15:57:09+00:00
- **Updated**: 2022-04-01 09:05:25+00:00
- **Authors**: Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, Changsheng Xu
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: The goal of image style transfer is to render an image with artistic features guided by a style reference while maintaining the original content. Owing to the locality in convolutional neural networks (CNNs), extracting and maintaining the global information of input images is difficult. Therefore, traditional neural style transfer methods face biased content representation. To address this critical issue, we take long-range dependencies of input images into account for image style transfer by proposing a transformer-based approach called StyTr$^2$. In contrast with visual transformers for other vision tasks, StyTr$^2$ contains two different transformer encoders to generate domain-specific sequences for content and style, respectively. Following the encoders, a multi-layer transformer decoder is adopted to stylize the content sequence according to the style sequence. We also analyze the deficiency of existing positional encoding methods and propose the content-aware positional encoding (CAPE), which is scale-invariant and more suitable for image style transfer tasks. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed StyTr$^2$ compared with state-of-the-art CNN-based and flow-based approaches. Code and models are available at https://github.com/diyiiyiii/StyTR-2.



### Polygonal Point Set Tracking
- **Arxiv ID**: http://arxiv.org/abs/2105.14584v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.14584v1)
- **Published**: 2021-05-30 17:12:36+00:00
- **Updated**: 2021-05-30 17:12:36+00:00
- **Authors**: Gunhee Nam, Miran Heo, Seoung Wug Oh, Joon-Young Lee, Seon Joo Kim
- **Comment**: 14 pages, 10 figures, 6 tables
- **Journal**: None
- **Summary**: In this paper, we propose a novel learning-based polygonal point set tracking method. Compared to existing video object segmentation~(VOS) methods that propagate pixel-wise object mask information, we propagate a polygonal point set over frames.   Specifically, the set is defined as a subset of points in the target contour, and our goal is to track corresponding points on the target contour. Those outputs enable us to apply various visual effects such as motion tracking, part deformation, and texture mapping. To this end, we propose a new method to track the corresponding points between frames by the global-local alignment with delicately designed losses and regularization terms. We also introduce a novel learning strategy using synthetic and VOS datasets that makes it possible to tackle the problem without developing the point correspondence dataset. Since the existing datasets are not suitable to validate our method, we build a new polygonal point set tracking dataset and demonstrate the superior performance of our method over the baselines and existing contour-based VOS methods. In addition, we present visual-effects applications of our method on part distortion and text mapping.



### Cascaded Diffusion Models for High Fidelity Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2106.15282v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15282v3)
- **Published**: 2021-05-30 17:14:52+00:00
- **Updated**: 2021-12-17 17:21:04+00:00
- **Authors**: Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, Tim Salimans
- **Comment**: None
- **Journal**: None
- **Summary**: We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256, outperforming VQ-VAE-2.



### Deep Hierarchical Super Resolution for Scientific Data
- **Arxiv ID**: http://arxiv.org/abs/2107.00462v3
- **DOI**: 10.1109/TVCG.2022.3214420
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00462v3)
- **Published**: 2021-05-30 18:32:11+00:00
- **Updated**: 2022-10-14 13:48:57+00:00
- **Authors**: Skylar W. Wurster, Hanqi Guo, Han-Wei Shen, Thomas Peterka, Jiayi Xu
- **Comment**: Accepted by IEEE Transactions on Visualization and Computer Graphics
  (TVCG) 2022
- **Journal**: None
- **Summary**: We present a novel technique for hierarchical super resolution (SR) with neural networks (NNs), which upscales volumetric data represented with an octree data structure to a high-resolution uniform grid with minimal seam artifacts on octree node boundaries. Our method uses existing state-of-the-art SR models and adds flexibility to upscale input data with varying levels of detail across the domain, instead of only uniform grid data that are supported in previous approaches. The key is to use a hierarchy of SR NNs, each trained to perform 2x SR between two levels of detail, with a hierarchical SR algorithm that minimizes seam artifacts by starting from the coarsest level of detail and working up. We show that our hierarchical approach outperforms baseline interpolation and hierarchical upscaling methods, and demonstrate the usefulness of our proposed approach across three use cases including data reduction using hierarchical downsampling+SR instead of uniform downsampling+SR, computation savings for hierarchical finite-time Lyapunov exponent field calculation, and super-resolving low-resolution simulation results for a high-resolution approximation visualization.



### Identity and Attribute Preserving Thumbnail Upscaling
- **Arxiv ID**: http://arxiv.org/abs/2105.14609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14609v1)
- **Published**: 2021-05-30 19:32:27+00:00
- **Updated**: 2021-05-30 19:32:27+00:00
- **Authors**: Noam Gat, Sagie Benaim, Lior Wolf
- **Comment**: ICIP 2021
- **Journal**: None
- **Summary**: We consider the task of upscaling a low resolution thumbnail image of a person, to a higher resolution image, which preserves the person's identity and other attributes. Since the thumbnail image is of low resolution, many higher resolution versions exist. Previous approaches produce solutions where the person's identity is not preserved, or biased solutions, such as predominantly Caucasian faces. We address the existing ambiguity by first augmenting the feature extractor to better capture facial identity, facial attributes (such as smiling or not) and race, and second, use this feature extractor to generate high-resolution images which are identity preserving as well as conditioned on race and facial attributes. Our results indicate an improvement in face similarity recognition and lookalike generation as well as in the ability to generate higher resolution images which preserve an input thumbnail identity and whose race and attributes are maintained.



### Patch Tracking-based Streaming Tensor Ring Completion for Visual Data Recovery
- **Arxiv ID**: http://arxiv.org/abs/2105.14620v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.14620v3)
- **Published**: 2021-05-30 20:33:36+00:00
- **Updated**: 2022-08-12 04:02:45+00:00
- **Authors**: Yicong He, George K. Atia
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor completion aims to recover the missing entries of a partially observed tensor by exploiting its low-rank structure, and has been applied to visual data recovery. In applications where the data arrives sequentially such as streaming video completion, the missing entries of the tensor need to be dynamically recovered in a streaming fashion. Traditional streaming tensor completion algorithms treat the entire visual data as a tensor, which may not work satisfactorily when there is a big change in the tensor subspace along the temporal dimension, such as due to strong motion across the video frames. In this paper, we develop a novel patch tracking-based streaming tensor ring completion framework for visual data recovery. Given a newly incoming frame, small patches are tracked from the previous frame. Meanwhile, for each tracked patch, a patch tensor is constructed by stacking similar patches from the new frame. Patch tensors are then completed using a streaming tensor ring completion algorithm, and the incoming frame is recovered using the completed patch tensors. We propose a new patch tracking strategy that can accurately and efficiently track the patches with missing data. Further, a new streaming tensor ring completion algorithm is proposed which can efficiently and accurately update the latent core tensors and complete the missing entries of the patch tensors. Extensive experimental results demonstrate the superior performance of the proposed algorithms compared with both batch and streaming state-of-the-art tensor completion methods.



### DAAIN: Detection of Anomalous and Adversarial Input using Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2105.14638v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.14638v1)
- **Published**: 2021-05-30 22:07:13+00:00
- **Updated**: 2021-05-30 22:07:13+00:00
- **Authors**: Samuel von Baußnern, Johannes Otterbach, Adrian Loy, Mathieu Salzmann, Thomas Wollmann
- **Comment**: 14 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Despite much recent work, detecting out-of-distribution (OOD) inputs and adversarial attacks (AA) for computer vision models remains a challenge. In this work, we introduce a novel technique, DAAIN, to detect OOD inputs and AA for image segmentation in a unified setting. Our approach monitors the inner workings of a neural network and learns a density estimator of the activation distribution. We equip the density estimator with a classification head to discriminate between regular and anomalous inputs. To deal with the high-dimensional activation-space of typical segmentation networks, we subsample them to obtain a homogeneous spatial and layer-wise coverage. The subsampling pattern is chosen once per monitored model and kept fixed for all inputs. Since the attacker has access to neither the detection model nor the sampling key, it becomes harder for them to attack the segmentation network, as the attack cannot be backpropagated through the detector. We demonstrate the effectiveness of our approach using an ESPNet trained on the Cityscapes dataset as segmentation model, an affine Normalizing Flow as density estimator and use blue noise to ensure homogeneous sampling. Our model can be trained on a single GPU making it compute efficient and deployable without requiring specialized accelerators.




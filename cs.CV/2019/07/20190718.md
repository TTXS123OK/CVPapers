# Arxiv Papers in cs.CV on 2019-07-18
### EEG-Based Emotion Recognition Using Regularized Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.07835v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.07835v4)
- **Published**: 2019-07-18 01:44:44+00:00
- **Updated**: 2020-05-13 03:19:26+00:00
- **Authors**: Peixiang Zhong, Di Wang, Chunyan Miao
- **Comment**: None
- **Journal**: None
- **Summary**: Electroencephalography (EEG) measures the neuronal activities in different brain regions via electrodes. Many existing studies on EEG-based emotion recognition do not fully exploit the topology of EEG channels. In this paper, we propose a regularized graph neural network (RGNN) for EEG-based emotion recognition. RGNN considers the biological topology among different brain regions to capture both local and global relations among different EEG channels. Specifically, we model the inter-channel relations in EEG signals via an adjacency matrix in a graph neural network where the connection and sparseness of the adjacency matrix are inspired by neuroscience theories of human brain organization. In addition, we propose two regularizers, namely node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to better handle cross-subject EEG variations and noisy labels, respectively. Extensive experiments on two public datasets, SEED and SEED-IV, demonstrate the superior performance of our model than state-of-the-art models in most experimental settings. Moreover, ablation studies show that the proposed adjacency matrix and two regularizers contribute consistent and significant gain to the performance of our RGNN model. Finally, investigations on the neuronal activities reveal important brain regions and inter-channel relations for EEG-based emotion recognition.



### Growing a Brain: Fine-Tuning by Increasing Model Capacity
- **Arxiv ID**: http://arxiv.org/abs/1907.07844v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.07844v1)
- **Published**: 2019-07-18 02:20:18+00:00
- **Updated**: 2019-07-18 02:20:18+00:00
- **Authors**: Yu-Xiong Wang, Deva Ramanan, Martial Hebert
- **Comment**: CVPR
- **Journal**: None
- **Summary**: CNNs have made an undeniable impact on computer vision through the ability to learn high-capacity models with large annotated training sets. One of their remarkable properties is the ability to transfer knowledge from a large source dataset to a (typically smaller) target dataset. This is usually accomplished through fine-tuning a fixed-size network on new target data. Indeed, virtually every contemporary visual recognition system makes use of fine-tuning to transfer knowledge from ImageNet. In this work, we analyze what components and parameters change during fine-tuning, and discover that increasing model capacity allows for more natural model adaptation through fine-tuning. By making an analogy to developmental learning, we demonstrate that "growing" a CNN with additional units, either by widening existing layers or deepening the overall network, significantly outperforms classic fine-tuning approaches. But in order to properly grow a network, we show that newly-added units must be appropriately normalized to allow for a pace of learning that is consistent with existing units. We empirically validate our approach on several benchmark datasets, producing state-of-the-art results.



### Event-based Feature Extraction Using Adaptive Selection Thresholds
- **Arxiv ID**: http://arxiv.org/abs/1907.07853v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07853v2)
- **Published**: 2019-07-18 03:15:09+00:00
- **Updated**: 2019-07-30 04:41:30+00:00
- **Authors**: Saeed Afshar, Ying Xu, Jonathan Tapson, André van Schaik, Gregory Cohen
- **Comment**: 15 Pages. 9 Figures
- **Journal**: None
- **Summary**: Unsupervised feature extraction algorithms form one of the most important building blocks in machine learning systems. These algorithms are often adapted to the event-based domain to perform online learning in neuromorphic hardware. However, not designed for the purpose, such algorithms typically require significant simplification during implementation to meet hardware constraints, creating trade offs with performance. Furthermore, conventional feature extraction algorithms are not designed to generate useful intermediary signals which are valuable only in the context of neuromorphic hardware limitations. In this work a novel event-based feature extraction method is proposed that focuses on these issues. The algorithm operates via simple adaptive selection thresholds which allow a simpler implementation of network homeostasis than previous works by trading off a small amount of information loss in the form of missed events that fall outside the selection thresholds. The behavior of the selection thresholds and the output of the network as a whole are shown to provide uniquely useful signals indicating network weight convergence without the need to access network weights. A novel heuristic method for network size selection is proposed which makes use of noise events and their feature representations. The use of selection thresholds is shown to produce network activation patterns that predict classification accuracy allowing rapid evaluation and optimization of system parameters without the need to run back-end classifiers. The feature extraction method is tested on both the N-MNIST benchmarking dataset and a dataset of airplanes passing through the field of view. Multiple configurations with different classifiers are tested with the results quantifying the resultant performance gains at each processing stage.



### Understanding Video Content: Efficient Hero Detection and Recognition for the Game "Honor of Kings"
- **Arxiv ID**: http://arxiv.org/abs/1907.07854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07854v1)
- **Published**: 2019-07-18 03:23:53+00:00
- **Updated**: 2019-07-18 03:23:53+00:00
- **Authors**: Wentao Yao, Zixun Sun, Xiao Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In order to understand content and automatically extract labels for videos of the game "Honor of Kings", it is necessary to detect and recognize characters (called "hero") together with their camps in the game video. In this paper, we propose an efficient two-stage algorithm to detect and recognize heros in game videos. First, we detect all heros in a video frame based on blood bar template-matching method, and classify them according to their camps (self/ friend/ enemy). Then we recognize the name of each hero using one or more deep convolution neural networks. Our method needs almost no work for labelling training and testing samples in the recognition stage. Experiments show its efficiency and accuracy in the task of hero detection and recognition in game videos.



### Post-Earthquake Assessment of Buildings Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.07877v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07877v1)
- **Published**: 2019-07-18 05:22:25+00:00
- **Updated**: 2019-07-18 05:22:25+00:00
- **Authors**: Dhananjay Nahata, Harish Kumar Mulchandani, Suraj Bansal, G Muthukumar
- **Comment**: 8 pages 4 figures
- **Journal**: None
- **Summary**: Classification of the extent of damage suffered by a building in a seismic event is crucial from the safety perspective and repairing work. In this study, authors have proposed a CNN based autonomous damage detection model. Over 1200 images of different types of buildings-1000 for training and 200 for testing classified into 4 categories according to the extent of damage suffered. Categories are namely, no damage, minor damage, major damage, and collapse. Trained network tested by the application of various algorithms with different learning rates. The most optimum results were obtained on the application of VGG16 transfer learning model with a learning rate of 1e-5 as it gave a training accuracy of 97.85% and validation accuracy of up to 89.38%. The model developed has real-time application in the event of an earthquake.



### A Strong Feature Representation for Siamese Network Tracker
- **Arxiv ID**: http://arxiv.org/abs/1907.07880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07880v1)
- **Published**: 2019-07-18 05:26:08+00:00
- **Updated**: 2019-07-18 05:26:08+00:00
- **Authors**: Zhipeng Zhou, Rui Zhang, Dong Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Object tracking has important application in assistive technologies for personalized monitoring. Recent trackers choosing AlexNet as their backbone to extract features have gained great success. However, AlexNet is too shallow to form a strong feature representation, the tracker based on the Siamese network have an accuracy gap compared with state-of-the-art algorithms. To solve this problem, this paper proposes a tracker called SiamPF. Firstly, the modified pre-trained VGG16 network is fine-tuned as the backbone. Secondly, an AlexNet-like branch is added after the third convolutional layer and merged with the response map of the backbone network to form a preliminary strong feature representation. And then, a channel attention block is designed to adaptively select the contribution features. Finally, the APCE is modified to process the response map to reduce interference and focus the tracker on the target. Our SiamPF only used ILSVRC2015-VID for training, but it achieved excellent performance on OTB-2013 / OTB-2015 / VOT2015 / VOT2017, while maintaining the real-time performance of 41FPS on the GTX 1080Ti.



### A feasibility study of deep neural networks for the recognition of banknotes regarding central bank requirements
- **Arxiv ID**: http://arxiv.org/abs/1907.07890v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML, I.7.5; I.5.1; I.2.6; G.1.6; G.3
- **Links**: [PDF](http://arxiv.org/pdf/1907.07890v2)
- **Published**: 2019-07-18 06:29:31+00:00
- **Updated**: 2019-10-07 08:21:15+00:00
- **Authors**: Julia Schulte, Daniel Staps, Alexander Lampe
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: This paper contains a feasibility study of deep neural networks for the classification of Euro banknotes with respect to requirements of central banks on the ATM and high speed sorting industry. Instead of concentrating on the accuracy for a large number of classes as in the famous ImageNet Challenge we focus thus on conditions with few classes and the requirement of rejection of images belonging clearly to neither of the trained classes (i.e. classification in a so-called 0-class). These special requirements are part of frameworks defined by central banks as the European Central Bank and are met by current ATMs and high speed sorting machines. We also consider training and classification time on state of the art GPU hardware. The study concentrates on the banknote recognition whereas banknote class dependent authenticity and fitness checks are a topic of its own which is not considered in this work.



### Incorporating Temporal Prior from Motion Flow for Instrument Segmentation in Minimally Invasive Surgery Video
- **Arxiv ID**: http://arxiv.org/abs/1907.07899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07899v1)
- **Published**: 2019-07-18 06:51:05+00:00
- **Updated**: 2019-07-18 06:51:05+00:00
- **Authors**: Yueming Jin, Keyun Cheng, Qi Dou, Pheng-Ann Heng
- **Comment**: Accepted by MICCAI 2019; Code is available in
  https://github.com/keyuncheng/MF-TAPNet
- **Journal**: None
- **Summary**: Automatic instrument segmentation in video is an essentially fundamental yet challenging problem for robot-assisted minimally invasive surgery. In this paper, we propose a novel framework to leverage instrument motion information, by incorporating a derived temporal prior to an attention pyramid network for accurate segmentation. Our inferred prior can provide reliable indication of the instrument location and shape, which is propagated from the previous frame to the current frame according to inter-frame motion flow. This prior is injected to the middle of an encoder-decoder segmentation network as an initialization of a pyramid of attention modules, to explicitly guide segmentation output from coarse to fine. In this way, the temporal dynamics and the attention network can effectively complement and benefit each other. As additional usage, our temporal prior enables semi-supervised learning with periodically unlabeled video frames, simply by reverse execution. We extensively validate our method on the public 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge dataset with three different tasks. Our method consistently exceeds the state-of-the-art results across all three tasks by a large margin. Our semi-supervised variant also demonstrates a promising potential for reducing annotation cost in the clinical practice.



### A Computer Vision Application for Assessing Facial Acne Severity from Selfie Images
- **Arxiv ID**: http://arxiv.org/abs/1907.07901v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07901v3)
- **Published**: 2019-07-18 06:51:33+00:00
- **Updated**: 2019-07-31 16:34:46+00:00
- **Authors**: Tingting Zhao, Hang Zhang, Jacob Spoelstra
- **Comment**: 4 pages, 6 figures. To be presented at the 2019 KDD workshop on
  Applied data science in Healthcare: bridging the gap between data and
  knowledge
- **Journal**: None
- **Summary**: We worked with Nestle SHIELD (Skin Health, Innovation, Education, and Longevity Development, NSH) to develop a deep learning model that is able to assess acne severity from selfie images as accurate as dermatologists. The model was deployed as a mobile application, providing patients an easy way to assess and track the progress of their acne treatment. NSH acquired 4,700 selfie images for this study and recruited 11 internal dermatologists to label them in five categories: 1-Clear, 2- Almost Clear, 3-Mild, 4-Moderate, 5-Severe. Using OpenCV to detect facial landmarks we cut specific skin patches from the selfie images in order to minimize irrelevant background. We then applied a transfer learning approach by extracting features from the patches using a ResNet 152 pre-trained model, followed by a fully connected layer trained to approximate the desired severity rating. To address the problem of spatial sensitivity of CNN models, we introduce a new image rolling data augmentation approach, effectively causing acne lesions appeared in more locations in the training images. Our results demonstrate that this approach improved the generalization of the CNN model, outperforming more than half of the panel of human dermatologists on test images. To our knowledge, this is the first deep learning-based solution for acne assessment using selfie images.



### Locality-constrained Spatial Transformer Network for Video Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1907.07911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.07911v1)
- **Published**: 2019-07-18 07:25:26+00:00
- **Updated**: 2019-07-18 07:25:26+00:00
- **Authors**: Yanyan Fang, Biyun Zhan, Wandi Cai, Shenghua Gao, Bo Hu
- **Comment**: Accepted by ICME2019(Oral)
- **Journal**: None
- **Summary**: Compared with single image based crowd counting, video provides the spatial-temporal information of the crowd that would help improve the robustness of crowd counting. But translation, rotation and scaling of people lead to the change of density map of heads between neighbouring frames. Meanwhile, people walking in/out or being occluded in dynamic scenes leads to the change of head counts. To alleviate these issues in video crowd counting, a Locality-constrained Spatial Transformer Network (LSTN) is proposed. Specifically, we first leverage a Convolutional Neural Networks to estimate the density map for each frame. Then to relate the density maps between neighbouring frames, a Locality-constrained Spatial Transformer (LST) module is introduced to estimate the density map of next frame with that of current frame. To facilitate the performance evaluation, a large-scale video crowd counting dataset is collected, which contains 15K frames with about 394K annotated heads captured from 13 different scenes. As far as we know, it is the largest video crowd counting dataset. Extensive experiments on our dataset and other crowd counting datasets validate the effectiveness of our LSTN for crowd counting.



### MintNet: Building Invertible Neural Networks with Masked Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1907.07945v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.07945v2)
- **Published**: 2019-07-18 09:24:55+00:00
- **Updated**: 2019-10-29 07:20:45+00:00
- **Authors**: Yang Song, Chenlin Meng, Stefano Ermon
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: We propose a new way of constructing invertible neural networks by combining simple building blocks with a novel set of composition rules. This leads to a rich set of invertible architectures, including those similar to ResNets. Inversion is achieved with a locally convergent iterative procedure that is parallelizable and very fast in practice. Additionally, the determinant of the Jacobian can be computed analytically and efficiently, enabling their generative use as flow models. To demonstrate their flexibility, we show that our invertible neural networks are competitive with ResNets on MNIST and CIFAR-10 classification. When trained as generative models, our invertible networks achieve competitive likelihoods on MNIST, CIFAR-10 and ImageNet 32x32, with bits per dimension of 0.98, 3.32 and 4.06 respectively.



### Automatic vocal tract landmark localization from midsagittal MRI data
- **Arxiv ID**: http://arxiv.org/abs/1907.07951v2
- **DOI**: 10.1038/s41598-020-58103-6
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1907.07951v2)
- **Published**: 2019-07-18 09:38:09+00:00
- **Updated**: 2020-01-09 16:37:46+00:00
- **Authors**: Mohammad Eslami, Christiane Neuschaefer-Rube, Antoine Serrurier
- **Comment**: None
- **Journal**: None
- **Summary**: The various speech sounds of a language are obtained by varying the shape and position of the articulators surrounding the vocal tract. Analyzing their variations is crucial for understanding speech production, diagnosing speech disorders and planning therapy. Identifying key anatomical landmarks of these structures on medical images is a pre-requisite for any quantitative analysis and the rising amount of data generated in the field calls for an automatic solution. The challenge lies in the high inter- and intra-speaker variability, the mutual interaction between the articulators and the moderate quality of the images. This study addresses this issue for the first time and tackles it by means by means of Deep Learning. It proposes a dedicated network architecture named Flat-net and its performance are evaluated and compared with eleven state-of-the-art methods from the literature. The dataset contains midsagittal anatomical Magnetic Resonance Images for 9 speakers sustaining 62 articulations with 21 annotated anatomical landmarks per image. Results show that the Flat-net approach outperforms the former methods, leading to an overall Root Mean Square Error of 3.6 pixels/0.36 cm obtained in a leave-one-out procedure over the speakers. The implementation codes are also shared publicly on GitHub.



### Automated Gleason Grading of Prostate Biopsies using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.07980v1
- **DOI**: 10.1016/S1470-2045(19)30739-9
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.07980v1)
- **Published**: 2019-07-18 10:47:26+00:00
- **Updated**: 2019-07-18 10:47:26+00:00
- **Authors**: Wouter Bulten, Hans Pinckaers, Hester van Boven, Robert Vink, Thomas de Bel, Bram van Ginneken, Jeroen van der Laak, Christina Hulsbergen-van de Kaa, Geert Litjens
- **Comment**: 13 pages, 6 figures
- **Journal**: The Lancet Oncology, Available online 8 January 2020
- **Summary**: The Gleason score is the most important prognostic marker for prostate cancer patients but suffers from significant inter-observer variability. We developed a fully automated deep learning system to grade prostate biopsies. The system was developed using 5834 biopsies from 1243 patients. A semi-automatic labeling technique was used to circumvent the need for full manual annotation by pathologists. The developed system achieved a high agreement with the reference standard. In a separate observer experiment, the deep learning system outperformed 10 out of 15 pathologists. The system has the potential to improve prostate cancer prognostics by acting as a first or second reader.



### Deep Learning in Video Multi-Object Tracking: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1907.12740v4
- **DOI**: 10.1016/j.neucom.2019.11.023
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.12740v4)
- **Published**: 2019-07-18 11:51:26+00:00
- **Updated**: 2019-11-19 11:26:20+00:00
- **Authors**: Gioele Ciaparrone, Francisco Luque Sánchez, Siham Tabik, Luigi Troiano, Roberto Tagliaferri, Francisco Herrera
- **Comment**: Accepted in Neurocomputing, 2019. New in v4: updated license in
  compliance with Elsevier policy. Main text: 29 pages, 10 figures, 7 tables.
  Summary table in appendix at the end of the paper
- **Journal**: None
- **Summary**: The problem of Multiple Object Tracking (MOT) consists in following the trajectory of different objects in a sequence, usually a video. In recent years, with the rise of Deep Learning, the algorithms that provide a solution to this problem have benefited from the representational power of deep models. This paper provides a comprehensive survey on works that employ Deep Learning models to solve the task of MOT on single-camera videos. Four main steps in MOT algorithms are identified, and an in-depth review of how Deep Learning was employed in each one of these stages is presented. A complete experimental comparison of the presented works on the three MOTChallenge datasets is also provided, identifying a number of similarities among the top-performing methods and presenting some possible future research directions.



### Real-Time Driver State Monitoring Using a CNN Based Spatio-Temporal Approach
- **Arxiv ID**: http://arxiv.org/abs/1907.08009v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08009v1)
- **Published**: 2019-07-18 12:03:12+00:00
- **Updated**: 2019-07-18 12:03:12+00:00
- **Authors**: Neslihan Kose, Okan Kopuklu, Alexander Unnervik, Gerhard Rigoll
- **Comment**: Accepted for publication by the IEEE Intelligent Transportation
  Systems Conference (ITSC 2019)
- **Journal**: None
- **Summary**: Many road accidents occur due to distracted drivers. Today, driver monitoring is essential even for the latest autonomous vehicles to alert distracted drivers in order to take over control of the vehicle in case of emergency. In this paper, a spatio-temporal approach is applied to classify drivers' distraction level and movement decisions using convolutional neural networks (CNNs). We approach this problem as action recognition to benefit from temporal information in addition to spatial information. Our approach relies on features extracted from sparsely selected frames of an action using a pre-trained BN-Inception network. Experiments show that our approach outperforms the state-of-the art results on the Distracted Driver Dataset (96.31%), with an accuracy of 99.10% for 10-class classification while providing real-time performance. We also analyzed the impact of fusion using RGB and optical flow modalities with a very recent data level fusion strategy. The results on the Distracted Driver and Brain4Cars datasets show that fusion of these modalities further increases the accuracy.



### Temporally Coherent General Dynamic Scene Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1907.08195v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08195v2)
- **Published**: 2019-07-18 12:33:25+00:00
- **Updated**: 2020-08-03 13:34:35+00:00
- **Authors**: Armin Mustafa, Marco Volino, Hansung Kim, Jean-Yves Guillemaut, Adrian Hilton
- **Comment**: Submitted to IJCV 2019. arXiv admin note: substantial text overlap
  with arXiv:1603.03381
- **Journal**: None
- **Summary**: Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: An automatic method for initial coarse reconstruction to initialize joint estimation; Sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to free-viewpoint rendering and virtual reality.



### Automatic Grading of Individual Knee Osteoarthritis Features in Plain Radiographs using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.08020v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.08020v1)
- **Published**: 2019-07-18 12:52:32+00:00
- **Updated**: 2019-07-18 12:52:32+00:00
- **Authors**: Aleksei Tiulpin, Simo Saarakkala
- **Comment**: None
- **Journal**: None
- **Summary**: Knee osteoarthritis (OA) is the most common musculoskeletal disease in the world. In primary healthcare, knee OA is diagnosed using clinical examination and radiographic assessment. Osteoarthritis Research Society International (OARSI) atlas of OA radiographic features allows to perform independent assessment of knee osteophytes, joint space narrowing and other knee features. This provides a fine-grained OA severity assessment of the knee, compared to the gold standard and most commonly used Kellgren-Lawrence (KL) composite score. However, both OARSI and KL grading systems suffer from moderate inter-rater agreement, and therefore, the use of computer-aided methods could help to improve the reliability of the process. In this study, we developed a robust, automatic method to simultaneously predict KL and OARSI grades in knee radiographs. Our method is based on Deep Learning and leverages an ensemble of deep residual networks with 50 layers, squeeze-excitation and ResNeXt blocks. Here, we used transfer learning from ImageNet with a fine-tuning on the whole Osteoarthritis Initiative (OAI) dataset. An independent testing of our model was performed on the whole Multicenter Osteoarthritis Study (MOST) dataset. Our multi-task method yielded Cohen's kappa coefficients of 0.82 for KL-grade and 0.79, 0.84, 0.94, 0.83, 0.84, 0.90 for femoral osteophytes, tibial osteophytes and joint space narrowing for lateral and medial compartments respectively. Furthermore, our method yielded area under the ROC curve of 0.98 and average precision of 0.98 for detecting the presence of radiographic OA (KL $\geq 2$), which is better than the current state-of-the-art.



### Exploiting bilateral symmetry in brain lesion segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.08196v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1907.08196v1)
- **Published**: 2019-07-18 13:42:22+00:00
- **Updated**: 2019-07-18 13:42:22+00:00
- **Authors**: Kevin Raina, Uladzimir Yahorau, Tanya Schmah
- **Comment**: None
- **Journal**: None
- **Summary**: Brain lesions, including stroke and tumours, have a high degree of variability in terms of location, size, intensity and form, making automatic segmentation difficult. We propose an improvement to existing segmentation methods by exploiting the bilateral quasi-symmetry of healthy brains, which breaks down when lesions are present. Specifically, we use nonlinear registration of a neuroimage to a reflected version of itself ("reflective registration") to determine for each voxel its homologous (corresponding) voxel in the other hemisphere. A patch around the homologous voxel is added as a set of new features to the segmentation algorithm. To evaluate this method, we implemented two different CNN-based multimodal MRI stroke lesion segmentation algorithms, and then augmented them by adding extra symmetry features using the reflective registration method described above. For each architecture, we compared the performance with and without symmetry augmentation, on the SISS Training dataset of the Ischemic Stroke Lesion Segmentation Challenge (ISLES) 2015 challenge. Using affine reflective registration improves performance over baseline, but nonlinear reflective registration gives significantly better results: an improvement in Dice coefficient of 13 percentage points over baseline for one architecture and 9 points for the other. We argue for the broad applicability of adding symmetric features to existing segmentation algorithms, specifically using nonlinear, template-free methods.



### Self-supervised Training of Proposal-based Segmentation via Background Prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.08051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08051v1)
- **Published**: 2019-07-18 13:52:06+00:00
- **Updated**: 2019-07-18 13:52:06+00:00
- **Authors**: Isinsu Katircioglu, Helge Rhodin, Victor Constantin, Jörg Spörri, Mathieu Salzmann, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: While supervised object detection methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this in scenarios where annotating data is prohibitively expensive, we introduce a self-supervised approach to object detection and segmentation, able to work with monocular images captured with a moving camera. At the heart of our approach lies the observation that segmentation and background reconstruction are linked tasks, and the idea that, because we observe a structured scene, background regions can be re-synthesized from their surroundings, whereas regions depicting the object cannot. We therefore encode this intuition as a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of object proposals, we develop a Monte Carlo-based training strategy that allows us to explore the large space of object proposals. Our experiments demonstrate that our approach yields accurate detections and segmentations in images that visually depart from those of standard benchmarks, outperforming existing self-supervised methods and approaching weakly supervised ones that exploit large annotated datasets.



### Analysis of "User-Specific Effect" and Impact of Operator Skills on Fingerprint PAD Systems
- **Arxiv ID**: http://arxiv.org/abs/1907.08068v1
- **DOI**: 10.1007/978-3-030-30754-7_6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08068v1)
- **Published**: 2019-07-18 14:19:06+00:00
- **Updated**: 2019-07-18 14:19:06+00:00
- **Authors**: Giulia Orrù, Pierluigi Tuveri, Luca Ghiani, Gian Luca Marcialis
- **Comment**: Preprint version of a paper accepted at BioFor 2019
- **Journal**: New Trends in Image Analysis and Processing - ICIAP 2019. Lecture
  Notes in Computer Science, vol 11808. Springer, Cham
- **Summary**: Fingerprint Liveness detection, or presentation attacks detection (PAD), that is, the ability of detecting if a fingerprint submitted to an electronic capture device is authentic or made up of some artificial materials, boosted the attention of the scientific community and recently machine learning approaches based on deep networks opened novel scenarios. A significant step ahead was due thanks to the public availability of large sets of data; in particular, the ones released during the International Fingerprint Liveness Detection Competition (LivDet). Among others, the fifth edition carried on in 2017, challenged the participants in two more challenges which were not detailed in the official report. In this paper, we want to extend that report by focusing on them: the first one was aimed at exploring the case in which the PAD is integrated into a fingerprint verification systems, where templates of users are available too and the designer is not constrained to refer only to a generic users population for the PAD settings. The second one faces with the exploitation ability of attackers of the provided fakes, and how this ability impacts on the final performance. These two challenges together may set at which extent the fingerprint presentation attacks are an actual threat and how to exploit additional information to make the PAD more effective.



### Precipitation Nowcasting with Star-Bridge Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.08069v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08069v2)
- **Published**: 2019-07-18 14:19:28+00:00
- **Updated**: 2019-11-23 05:06:50+00:00
- **Authors**: Yuan Cao, Qiuying Li, Hongming Shan, Zhizhong Huang, Lei Chen, Leiming Ma, Junping Zhang
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Precipitation nowcasting, which aims to precisely predict the short-term rainfall intensity of a local region, is gaining increasing attention in the artificial intelligence community. Existing deep learning-based algorithms use a single network to process various rainfall intensities together, compromising the predictive accuracy. Therefore, this paper proposes a novel recurrent neural network (RNN) based star-bridge network (StarBriNet) for precipitation nowcasting. The novelty of this work lies in the following three aspects. First, the proposed network comprises multiple sub-networks to deal with different rainfall intensities and duration separately, which can significantly improve the model performance. Second, we propose a star-shaped information bridge to enhance the information flow across RNN layers. Third, we introduce a multi-sigmoid loss function to take the precipitation nowcasting criterion into account. Experimental results demonstrate superior performance for precipitation nowcasting over existing algorithms, including the state-of-the-art one, on a natural radar echo dataset.



### Discriminative Embedding Autoencoder with a Regressor Feedback for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.08070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.08070v1)
- **Published**: 2019-07-18 14:19:49+00:00
- **Updated**: 2019-07-18 14:19:49+00:00
- **Authors**: Ying Shi, Wei Wei, Zhiming Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize the novel object categories using the semantic representation of categories, and the key idea is to explore the knowledge of how the novel class is semantically related to the familiar classes. Some typical models are to learn the proper embedding between the image feature space and the semantic space, whilst it is important to learn discriminative features and comprise the coarse-to-fine image feature and semantic information. In this paper, we propose a discriminative embedding autoencoder with a regressor feedback model for ZSL. The encoder learns a mapping from the image feature space to the discriminative embedding space, which regulates both inter-class and intra-class distances between the learned features by a margin, making the learned features be discriminative for object recognition. The regressor feedback learns to map the reconstructed samples back to the the discriminative embedding and the semantic embedding, assisting the decoder to improve the quality of the samples and provide a generalization to the unseen classes. The proposed model is validated extensively on four benchmark datasets: SUN, CUB, AWA1, AWA2, the experiment results show that our proposed model outperforms the state-of-the-art models, and especially in the generalized zero-shot learning (GZSL), significant improvements are achieved.



### Robust and fast generation of top and side grasps for unknown objects
- **Arxiv ID**: http://arxiv.org/abs/1907.08088v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08088v1)
- **Published**: 2019-07-18 14:44:18+00:00
- **Updated**: 2019-07-18 14:44:18+00:00
- **Authors**: Brice Denoun, Beatriz Leon, Claudio Zito, Rustam Stolkin, Lorenzo Jamone, Miles Hansard
- **Comment**: Extended abstract
- **Journal**: Workshop on Task-Informed Grasping (TIG-II): From Perception to
  Physical Interaction, Robotics: Science and Systems (RSS), 2019
- **Summary**: In this work, we present a geometry-based grasping algorithm that is capable of efficiently generating both top and side grasps for unknown objects, using a single view RGB-D camera, and of selecting the most promising one. We demonstrate the effectiveness of our approach on a picking scenario on a real robot platform. Our approach has shown to be more reliable than another recent geometry-based method considered as baseline [7] in terms of grasp stability, by increasing the successful grasp attempts by a factor of six.



### Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery
- **Arxiv ID**: http://arxiv.org/abs/1907.08225v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.08225v4)
- **Published**: 2019-07-18 18:07:47+00:00
- **Updated**: 2020-02-14 10:16:54+00:00
- **Authors**: Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, Sergey Levine
- **Comment**: 11+6 pages, 6+2 figures, last two authors (Tuomas Haarnoja, Sergey
  Levine) advised equally
- **Journal**: None
- **Summary**: Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new goals, making it possible to learn complex tasks efficiently. We show that dynamical distances can be used in a semi-supervised regime, where unsupervised interaction with the environment is used to learn the dynamical distances, while a small amount of preference supervision is used to determine the task goal, without any manually engineered reward function or goal examples. We evaluate our method both on a real-world robot and in simulation. We show that our method can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and just ten preference labels, without any other supervision. Videos of the learned skills can be found on the project website: https://sites.google.com/view/dynamical-distance-learning.



### Fully-automated deep learning-powered system for DCE-MRI analysis of brain tumors
- **Arxiv ID**: http://arxiv.org/abs/1907.08303v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08303v1)
- **Published**: 2019-07-18 21:57:02+00:00
- **Updated**: 2019-07-18 21:57:02+00:00
- **Authors**: Jakub Nalepa, Pablo Ribalta Lorenzo, Michal Marcinkiewicz, Barbara Bobek-Billewicz, Pawel Wawrzyniak, Maksym Walczak, Michal Kawulok, Wojciech Dudzik, Grzegorz Mrukwa, Pawel Ulrych, Michael P. Hayball
- **Comment**: Submitted for publication in Artificial Intelligence in Medicine
- **Journal**: None
- **Summary**: Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an important role in diagnosis and grading of brain tumor. Although manual DCE biomarker extraction algorithms boost the diagnostic yield of DCE-MRI by providing quantitative information on tumor prognosis and prediction, they are time-consuming and prone to human error. In this paper, we propose a fully-automated, end-to-end system for DCE-MRI analysis of brain tumors. Our deep learning-powered technique does not require any user interaction, it yields reproducible results, and it is rigorously validated against benchmark (BraTS'17 for tumor segmentation, and a test dataset released by the Quantitative Imaging Biomarkers Alliance for the contrast-concentration fitting) and clinical (44 low-grade glioma patients) data. Also, we introduce a cubic model of the vascular input function used for pharmacokinetic modeling which significantly decreases the fitting error when compared with the state of the art, alongside a real-time algorithm for determination of the vascular input region. An extensive experimental study, backed up with statistical tests, showed that our system delivers state-of-the-art results (in terms of segmentation accuracy and contrast-concentration fitting) while requiring less than 3 minutes to process an entire input DCE-MRI study using a single GPU.



### XferNAS: Transfer Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1907.08307v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.08307v1)
- **Published**: 2019-07-18 22:05:49+00:00
- **Updated**: 2019-07-18 22:05:49+00:00
- **Authors**: Martin Wistuba
- **Comment**: None
- **Journal**: None
- **Summary**: The term Neural Architecture Search (NAS) refers to the automatic optimization of network architectures for a new, previously unknown task. Since testing an architecture is computationally very expensive, many optimizers need days or even weeks to find suitable architectures. However, this search time can be significantly reduced if knowledge from previous searches on different tasks is reused. In this work, we propose a generally applicable framework that introduces only minor changes to existing optimizers to leverage this feature. As an example, we select an existing optimizer and demonstrate the complexity of the integration of the framework as well as its impact. In experiments on CIFAR-10 and CIFAR-100, we observe a reduction in the search time from 200 to only 6 GPU days, a speed up by a factor of 33. In addition, we observe new records of 1.99 and 14.06 for NAS optimizers on the CIFAR benchmarks, respectively. In a separate study, we analyze the impact of the amount of source and target data. Empirically, we demonstrate that the proposed framework generally gives better results and, in the worst case, is just as good as the unmodified optimizer.



### Statistical Descriptors-based Automatic Fingerprint Identification: Machine Learning Approaches
- **Arxiv ID**: http://arxiv.org/abs/1907.12741v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12741v1)
- **Published**: 2019-07-18 22:17:13+00:00
- **Updated**: 2019-07-18 22:17:13+00:00
- **Authors**: Hamid Jan, Amjad Ali, Shahid Mahmood, Gautam Srivastava
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Identification of a person from fingerprints of good quality has been used by commercial applications and law enforcement agencies for many years, however identification of a person from latent fingerprints is very difficult and challenging. A latent fingerprint is a fingerprint left on a surface by deposits of oils and/or perspiration from the finger. It is not usually visible to the naked eye but may be detected with special techniques such as dusting with fine powder and then lifting the pattern of powder with transparent tape. We have evaluated the quality of machine learning techniques that has been implemented in automatic fingerprint identification. In this paper, we use fingerprints of low quality from database DB1 of Fingerprint Verification Competition (FVC 2002) to conduct our experiments. Fingerprints are processed to find its core point using Poincare index and carry out enhancement using Diffusion coherence filter whose performance is known to be good in the high curvature regions of fingerprints. Grey-level Co-Occurrence Matrix (GLCM) based seven statistical descriptors with four different inter pixel distances are then extracted as features and put forward to train and test REPTree, RandomTree, J48, Decision Stump and Random Forest Machine Learning techniques for personal identification. Experiments are conducted on 80 instances and 28 attributes. Our experiments proved that Random Forests and J48 give good results for latent fingerprints as compared to other machine learning techniques and can help improve the identification accuracy.



### Deep Perceptual Compression
- **Arxiv ID**: http://arxiv.org/abs/1907.08310v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.08310v2)
- **Published**: 2019-07-18 22:17:52+00:00
- **Updated**: 2019-07-31 21:17:27+00:00
- **Authors**: Yash Patel, Srikar Appalaraju, R. Manmatha
- **Comment**: None
- **Journal**: None
- **Summary**: Several deep learned lossy compression techniques have been proposed in the recent literature. Most of these are optimized by using either MS-SSIM (multi-scale structural similarity) or MSE (mean squared error) as a loss function. Unfortunately, neither of these correlate well with human perception and this is clearly visible from the resulting compressed images. In several cases, the MS-SSIM for deep learned techniques is higher than say a conventional, non-deep learned codec such as JPEG-2000 or BPG. However, the images produced by these deep learned techniques are in many cases clearly worse to human eyes than those produced by JPEG-2000 or BPG.   We propose the use of an alternative, deep perceptual metric, which has been shown to align better with human perceptual similarity. We then propose Deep Perceptual Compression (DPC) which makes use of an encoder-decoder based image compression model to jointly optimize on the deep perceptual metric and MS-SSIM. Via extensive human evaluations, we show that the proposed method generates visually better results than previous learning based compression methods and JPEG-2000, and is comparable to BPG. Furthermore, we demonstrate that for tasks like object-detection, images compressed with DPC give better accuracy.



### Multi-Task Regression-based Learning for Autonomous Unmanned Aerial Vehicle Flight Control within Unstructured Outdoor Environments
- **Arxiv ID**: http://arxiv.org/abs/1907.08320v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1907.08320v1)
- **Published**: 2019-07-18 23:45:05+00:00
- **Updated**: 2019-07-18 23:45:05+00:00
- **Authors**: Bruna G. Maciel-Pearson, Samet Akcay, Amir Atapour-Abarghouei, Christopher Holder, Toby P. Breckon
- **Comment**: None
- **Journal**: None
- **Summary**: Increased growth in the global Unmanned Aerial Vehicles (UAV) (drone) industry has expanded possibilities for fully autonomous UAV applications. A particular application which has in part motivated this research is the use of UAV in wide area search and surveillance operations in unstructured outdoor environments. The critical issue with such environments is the lack of structured features that could aid in autonomous flight, such as road lines or paths. In this paper, we propose an End-to-End Multi-Task Regression-based Learning approach capable of defining flight commands for navigation and exploration under the forest canopy, regardless of the presence of trails or additional sensors (i.e. GPS). Training and testing are performed using a software in the loop pipeline which allows for a detailed evaluation against state-of-the-art pose estimation techniques. Our extensive experiments demonstrate that our approach excels in performing dense exploration within the required search perimeter, is capable of covering wider search regions, generalises to previously unseen and unexplored environments and outperforms contemporary state-of-the-art techniques.




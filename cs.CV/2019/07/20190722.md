# Arxiv Papers in cs.CV on 2019-07-22
### Covering up bias in CelebA-like datasets with Markov blankets: A post-hoc cure for attribute prior avoidance
- **Arxiv ID**: http://arxiv.org/abs/1907.12917v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.12917v1)
- **Published**: 2019-07-22 00:18:34+00:00
- **Updated**: 2019-07-22 00:18:34+00:00
- **Authors**: Vinay Uday Prabhu, Dian Ang Yap, Alexander Wang, John Whaley
- **Comment**: Accepted for presentation at the first workshop on Invertible Neural
  Networks and Normalizing Flows (ICML 2019), Long Beach, CA, USA
- **Journal**: None
- **Summary**: Attribute prior avoidance entails subconscious or willful non-modeling of (meta)attributes that datasets are oft born with, such as the 40 semantic facial attributes associated with the CelebA and CelebA-HQ datasets. The consequences of this infirmity, we discover, are especially stark in state-of-the-art deep generative models learned on these datasets that just model the pixel-space measurements, resulting in an inter-attribute bias-laden latent space. This viscerally manifests itself when we perform face manipulation experiments based on latent vector interpolations. In this paper, we address this and propose a post-hoc solution that utilizes an Ising attribute prior learned in the attribute space and showcase its efficacy via qualitative experiments.



### Class-specific Anchoring Proposal for 3D Object Recognition in LIDAR and RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1907.09081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09081v1)
- **Published**: 2019-07-22 02:02:06+00:00
- **Updated**: 2019-07-22 02:02:06+00:00
- **Authors**: Amir Hossein Raffiee, Humayun Irshad
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Detecting objects in a two-dimensional setting is often insufficient in the context of real-life applications where the surrounding environment needs to be accurately recognized and oriented in three-dimension (3D), such as in the case of autonomous driving vehicles. Therefore, accurately and efficiently detecting objects in the three-dimensional setting is becoming increasingly relevant to a wide range of industrial applications, and thus is progressively attracting the attention of researchers. Building systems to detect objects in 3D is a challenging task though, because it relies on the multi-modal fusion of data derived from different sources. In this paper, we study the effects of anchoring using the current state-of-the-art 3D object detector and propose Class-specific Anchoring Proposal (CAP) strategy based on object sizes and aspect ratios based clustering of anchors. The proposed anchoring strategy significantly increased detection accuracy's by 7.19%, 8.13% and 8.8% on Easy, Moderate and Hard setting of the pedestrian class, 2.19%, 2.17% and 1.27% on Easy, Moderate and Hard setting of the car class and 12.1% on Easy setting of cyclist class. We also show that the clustering in anchoring process also enhances the performance of the regional proposal network in proposing regions of interests significantly. Finally, we propose the best cluster numbers for each class of objects in KITTI dataset that improves the performance of detection model significantly.



### Automatic Radiology Report Generation based on Multi-view Image Fusion and Medical Concept Enrichment
- **Arxiv ID**: http://arxiv.org/abs/1907.09085v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.09085v2)
- **Published**: 2019-07-22 02:25:33+00:00
- **Updated**: 2019-07-23 00:45:21+00:00
- **Authors**: Jianbo Yuan, Haofu Liao, Rui Luo, Jiebo Luo
- **Comment**: None
- **Journal**: MICCAI 2019
- **Summary**: Generating radiology reports is time-consuming and requires extensive expertise in practice. Therefore, reliable automatic radiology report generation is highly desired to alleviate the workload. Although deep learning techniques have been successfully applied to image classification and image captioning tasks, radiology report generation remains challenging in regards to understanding and linking complicated medical visual contents with accurate natural language descriptions. In addition, the data scales of open-access datasets that contain paired medical images and reports remain very limited. To cope with these practical challenges, we propose a generative encoder-decoder model and focus on chest x-ray images and reports with the following improvements. First, we pretrain the encoder with a large number of chest x-ray images to accurately recognize 14 common radiographic observations, while taking advantage of the multi-view images by enforcing the cross-view consistency. Second, we synthesize multi-view visual features based on a sentence-level attention mechanism in a late fusion fashion. In addition, in order to enrich the decoder with descriptive semantics and enforce the correctness of the deterministic medical-related contents such as mentions of organs or diagnoses, we extract medical concepts based on the radiology reports in the training data and fine-tune the encoder to extract the most frequent medical concepts from the x-ray images. Such concepts are fused with each decoding step by a word-level attention model. The experimental results conducted on the Indiana University Chest X-Ray dataset demonstrate that the proposed model achieves the state-of-the-art performance compared with other baseline approaches.



### DetectFusion: Detecting and Segmenting Both Known and Unknown Dynamic Objects in Real-time SLAM
- **Arxiv ID**: http://arxiv.org/abs/1907.09127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09127v1)
- **Published**: 2019-07-22 04:29:32+00:00
- **Updated**: 2019-07-22 04:29:32+00:00
- **Authors**: Ryo Hachiuma, Christian Pirchheim, Dieter Schmalstieg, Hideo Saito
- **Comment**: 12 pages, 4 figures, 4 tables, accepted by BMVC 2019 spotlight
  session
- **Journal**: None
- **Summary**: We present DetectFusion, an RGB-D SLAM system that runs in real-time and can robustly handle semantically known and unknown objects that can move dynamically in the scene. Our system detects, segments and assigns semantic class labels to known objects in the scene, while tracking and reconstructing them even when they move independently in front of the monocular camera. In contrast to related work, we achieve real-time computational performance on semantic instance segmentation with a novel method combining 2D object detection and 3D geometric segmentation. In addition, we propose a method for detecting and segmenting the motion of semantically unknown objects, thus further improving the accuracy of camera tracking and map reconstruction. We show that our method performs on par or better than previous work in terms of localization and object reconstruction accuracy, while achieving about 20 FPS even if the objects are segmented in each frame.



### Real-time Background-aware 3D Textureless Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1907.09128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09128v1)
- **Published**: 2019-07-22 04:31:45+00:00
- **Updated**: 2019-07-22 04:31:45+00:00
- **Authors**: Mang Shao, Danhang Tang, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a modified fuzzy decision forest for real-time 3D object pose estimation based on typical template representation. We employ an extra preemptive background rejector node in the decision forest framework to terminate the examination of background locations as early as possible, result in a significantly improvement on efficiency. Our approach is also scalable to large dataset since the tree structure naturally provides a logarithm time complexity to the number of objects. Finally we further reduce the validation stage with a fast breadth-first scheme. The results show that our approach outperform the state-of-the-arts on the efficiency while maintaining a comparable accuracy.



### Feature Graph Learning for 3D Point Cloud Denoising
- **Arxiv ID**: http://arxiv.org/abs/1907.09138v2
- **DOI**: 10.1109/TSP.2020.2978617
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1907.09138v2)
- **Published**: 2019-07-22 05:02:12+00:00
- **Updated**: 2020-01-15 02:11:27+00:00
- **Authors**: Wei Hu, Xiang Gao, Gene Cheung, Zongming Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Identifying an appropriate underlying graph kernel that reflects pairwise similarities is critical in many recent graph spectral signal restoration schemes, including image denoising, dequantization, and contrast enhancement. Existing graph learning algorithms compute the most likely entries of a properly defined graph Laplacian matrix $\mathbf{L}$, but require a large number of signal observations $\mathbf{z}$'s for a stable estimate. In this work, we assume instead the availability of a relevant feature vector $\mathbf{f}_i$ per node $i$, from which we compute an optimal feature graph via optimization of a feature metric. Specifically, we alternately optimize the diagonal and off-diagonal entries of a Mahalanobis distance matrix $\mathbf{M}$ by minimizing the graph Laplacian regularizer (GLR) $\mathbf{z}^{\top} \mathbf{L} \mathbf{z}$, where edge weight is $w_{i,j} = \exp\{-(\mathbf{f}_i - \mathbf{f}_j)^{\top} \mathbf{M} (\mathbf{f}_i - \mathbf{f}_j) \}$, given a single observation $\mathbf{z}$. We optimize diagonal entries via proximal gradient (PG), where we constrain $\mathbf{M}$ to be positive definite (PD) via linear inequalities derived from the Gershgorin circle theorem. To optimize off-diagonal entries, we design a block descent algorithm that iteratively optimizes one row and column of $\mathbf{M}$. To keep $\mathbf{M}$ PD, we constrain the Schur complement of sub-matrix $\mathbf{M}_{2,2}$ of $\mathbf{M}$ to be PD when optimizing via PG. Our algorithm mitigates full eigen-decomposition of $\mathbf{M}$, thus ensuring fast computation speed even when feature vector $\mathbf{f}_i$ has high dimension. To validate its usefulness, we apply our feature graph learning algorithm to the problem of 3D point cloud denoising, resulting in state-of-the-art performance compared to competing schemes in extensive experiments.



### Multi-scale Cell Instance Segmentation with Keypoint Graph based Bounding Boxes
- **Arxiv ID**: http://arxiv.org/abs/1907.09140v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09140v2)
- **Published**: 2019-07-22 05:25:01+00:00
- **Updated**: 2019-07-25 18:01:59+00:00
- **Authors**: Jingru Yi, Pengxiang Wu, Qiaoying Huang, Hui Qu, Bo Liu, Daniel J. Hoeppner, Dimitris N. Metaxas
- **Comment**: accepted by MICCAI 2019
- **Journal**: None
- **Summary**: Most existing methods handle cell instance segmentation problems directly without relying on additional detection boxes. These methods generally fails to separate touching cells due to the lack of global understanding of the objects. In contrast, box-based instance segmentation solves this problem by combining object detection with segmentation. However, existing methods typically utilize anchor box-based detectors, which would lead to inferior instance segmentation performance due to the class imbalance issue. In this paper, we propose a new box-based cell instance segmentation method. In particular, we first detect the five pre-defined points of a cell via keypoints detection. Then we group these points according to a keypoint graph and subsequently extract the bounding box for each cell. Finally, cell segmentation is performed on feature maps within the bounding boxes. We validate our method on two cell datasets with distinct object shapes, and empirically demonstrate the superiority of our method compared to other instance segmentation techniques. Code is available at: https://github.com/yijingru/KG_Instance_Segmentation.



### Learned Image Downscaling for Upscaling using Content Adaptive Resampler
- **Arxiv ID**: http://arxiv.org/abs/1907.12904v2
- **DOI**: 10.1109/TIP.2020.2970248
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.12904v2)
- **Published**: 2019-07-22 05:35:27+00:00
- **Updated**: 2019-11-05 13:00:42+00:00
- **Authors**: Wanjie Sun, Zhenzhong Chen
- **Comment**: 15 pages; not the final version
- **Journal**: None
- **Summary**: Deep convolutional neural network based image super-resolution (SR) models have shown superior performance in recovering the underlying high resolution (HR) images from low resolution (LR) images obtained from the predefined downscaling methods. In this paper we propose a learned image downscaling method based on content adaptive resampler (CAR) with consideration on the upscaling process. The proposed resampler network generates content adaptive image resampling kernels that are applied to the original HR input to generate pixels on the downscaled image. Moreover, a differentiable upscaling (SR) module is employed to upscale the LR result into its underlying HR counterpart. By back-propagating the reconstruction error down to the original HR input across the entire framework to adjust model parameters, the proposed framework achieves a new state-of-the-art SR performance through upscaling guided image resamplers which adaptively preserve detailed information that is essential to the upscaling. Experimental results indicate that the quality of the generated LR image is comparable to that of the traditional interpolation based method, but the significant SR performance gain is achieved by deep SR models trained jointly with the CAR model. The code is publicly available on: URL https://github.com/sunwj/CAR.



### Extended Local Binary Patterns for Efficient and Robust Spontaneous Facial Micro-Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.09160v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09160v2)
- **Published**: 2019-07-22 07:15:53+00:00
- **Updated**: 2019-09-17 12:34:48+00:00
- **Authors**: Chengyu Guo, Jingyun Liang, Geng Zhan, Zhong Liu, Matti PietikÃ¤inen, Li Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Facial Micro-Expressions (MEs) are spontaneous, involuntary facial movements when a person experiences an emotion but deliberately or unconsciously attempts to conceal his or her genuine emotions. Recently, ME recognition has attracted increasing attention due to its potential applications such as clinical diagnosis, business negotiation, interrogations, and security. However, it is expensive to build large scale ME datasets, mainly due to the difficulty of inducing spontaneous MEs. This limits the application of deep learning techniques which require lots of training data. In this paper, we propose a simple, efficient yet robust descriptor called Extended Local Binary Patterns on Three Orthogonal Planes (ELBPTOP) for ME recognition. ELBPTOP consists of three complementary binary descriptors: LBPTOP and two novel ones Radial Difference LBPTOP (RDLBPTOP) and Angular Difference LBPTOP (ADLBPTOP), which explore the local second order information along the radial and angular directions contained in ME video sequences. ELBPTOP is a novel ME descriptor inspired by unique and subtle facial movements. It is computationally efficient and only marginally increases the cost of computing LBPTOP, yet is extremely effective for ME recognition. In addition, by firstly introducing Whitened Principal Component Analysis (WPCA) to ME recognition, we can further obtain more compact and discriminative feature representations, then achieve significantly computational savings. Extensive experimental evaluation on three popular spontaneous ME datasets SMIC, CASME II and SAMM show that our proposed ELBPTOP approach significantly outperforms the previous state-of-the-art on all three single evaluated datasets and achieves promising results on cross-database recognition.Our code will be made available.



### Sensor Aware Lidar Odometry
- **Arxiv ID**: http://arxiv.org/abs/1907.09167v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.09167v3)
- **Published**: 2019-07-22 07:39:12+00:00
- **Updated**: 2020-01-20 10:26:52+00:00
- **Authors**: Dmitri Kovalenko, Mikhail Korobkin, Andrey Minin
- **Comment**: to appear in European Conference on Mobile Robots 2019
- **Journal**: None
- **Summary**: A lidar odometry method, integrating into the computation the knowledge about the physics of the sensor, is proposed. A model of measurement error enables higher precision in estimation of the point normal covariance. Adjacent laser beams are used in an outlier correspondence rejection scheme. The method is ranked in the KITTI's leaderboard with 1.37% positioning error. 3.67% is achieved in comparison with the LOAM method on the internal dataset.



### Polyp Detection and Segmentation using Mask R-CNN: Does a Deeper Feature Extractor CNN Always Perform Better?
- **Arxiv ID**: http://arxiv.org/abs/1907.09180v1
- **DOI**: 10.1109/ISMICT.2019.8743694
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09180v1)
- **Published**: 2019-07-22 08:34:47+00:00
- **Updated**: 2019-07-22 08:34:47+00:00
- **Authors**: Hemin Ali Qadir, Younghak Shin, Johannes Solhusvik, Jacob Bergsland, Lars Aabakken, Ilangko Balasingham
- **Comment**: 6
- **Journal**: 2019 13th International Symposium on Medical Information and
  Communication Technology (ISMICT)
- **Summary**: Automatic polyp detection and segmentation are highly desirable for colon screening due to polyp miss rate by physicians during colonoscopy, which is about 25%. However, this computerization is still an unsolved problem due to various polyp-like structures in the colon and high interclass polyp variations in terms of size, color, shape, and texture. In this paper, we adapt Mask R-CNN and evaluate its performance with different modern convolutional neural networks (CNN) as its feature extractor for polyp detection and segmentation. We investigate the performance improvement of each feature extractor by adding extra polyp images to the training dataset to answer whether we need deeper and more complex CNNs or better dataset for training in automatic polyp detection and segmentation. Finally, we propose an ensemble method for further performance improvement. We evaluate the performance on the 2015 MICCAI polyp detection dataset. The best results achieved are 72.59% recall, 80% precision, 70.42% dice, and 61.24% Jaccard. The model achieved state-of-the-art segmentation performance.



### FD-FCN: 3D Fully Dense and Fully Convolutional Network for Semantic Segmentation of Brain Anatomy
- **Arxiv ID**: http://arxiv.org/abs/1907.09194v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09194v2)
- **Published**: 2019-07-22 09:19:05+00:00
- **Updated**: 2020-04-30 10:39:17+00:00
- **Authors**: Binbin Yang, Weiwei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a 3D patch-based fully dense and fully convolutional network (FD-FCN) is proposed for fast and accurate segmentation of subcortical structures in T1-weighted magnetic resonance images. Developed from the seminal FCN with an end-to-end learning-based approach and constructed by newly designed dense blocks including a dense fully-connected layer, the proposed FD-FCN is different from other FCN-based methods and leads to an outperformance in the perspective of both efficiency and accuracy. Compared with the U-shaped architecture, FD-FCN discards the upsampling path for model fitness. To alleviate the problem of parameter explosion, the inputs of dense blocks are no longer directly passed to subsequent layers. This architecture of FD-FCN brings a great reduction on both memory and time consumption in training process. Although FD-FCN is slimmed down, in model competence it gains better capability of dense inference than other conventional networks. This benefits from the construction of network architecture and the incorporation of redesigned dense blocks. The multi-scale FD-FCN models both local and global context by embedding intermediate-layer outputs in the final prediction, which encourages consistency between features extracted at different scales and embeds fine-grained information directly in the segmentation process. In addition, dense blocks are rebuilt to enlarge the receptive fields without significantly increasing parameters, and spectral coordinates are exploited for spatial context of the original input patch. The experiments were performed over the IBSR dataset, and FD-FCN produced an accurate segmentation result of overall Dice overlap value of 89.81% for 11 brain structures in 53 seconds, with at least 3.66% absolute improvement of dice accuracy than state-of-the-art 3D FCN-based methods.



### Image-and-Spatial Transformer Networks for Structure-Guided Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1907.09200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09200v1)
- **Published**: 2019-07-22 09:39:53+00:00
- **Updated**: 2019-07-22 09:39:53+00:00
- **Authors**: Matthew C. H. Lee, Ozan Oktay, Andreas Schuh, Michiel Schaap, Ben Glocker
- **Comment**: Accepted at MICCAI 2019. Code available on
  https://github.com/biomedia-mira/istn
- **Journal**: None
- **Summary**: Image registration with deep neural networks has become an active field of research and exciting avenue for a long standing problem in medical imaging. The goal is to learn a complex function that maps the appearance of input image pairs to parameters of a spatial transformation in order to align corresponding anatomical structures. We argue and show that the current direct, non-iterative approaches are sub-optimal, in particular if we seek accurate alignment of Structures-of-Interest (SoI). Information about SoI is often available at training time, for example, in form of segmentations or landmarks. We introduce a novel, generic framework, Image-and-Spatial Transformer Networks (ISTNs), to leverage SoI information allowing us to learn new image representations that are optimised for the downstream registration task. Thanks to these representations we can employ a test-specific, iterative refinement over the transformation parameters which yields highly accurate registration even with very limited training data. Performance is demonstrated on pairwise 3D brain registration and illustrative synthetic data.



### Reg R-CNN: Lesion Detection and Grading under Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1907.12915v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.12915v3)
- **Published**: 2019-07-22 10:16:28+00:00
- **Updated**: 2019-08-26 10:16:05+00:00
- **Authors**: Gregor N. Ramien, Paul F. Jaeger, Simon A. A. Kohl, Klaus H. Maier-Hein
- **Comment**: 9 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: For the task of concurrently detecting and categorizing objects, the medical imaging community commonly adopts methods developed on natural images. Current state-of-the-art object detectors are comprised of two stages: the first stage generates region proposals, the second stage subsequently categorizes them. Unlike in natural images, however, for anatomical structures of interest such as tumors, the appearance in the image (e.g., scale or intensity) links to a malignancy grade that lies on a continuous ordinal scale. While classification models discard this ordinal relation between grades by discretizing the continuous scale to an unordered bag of categories, regression models are trained with distance metrics, which preserve the relation. This advantage becomes all the more important in the setting of label confusions on ambiguous data sets, which is the usual case with medical images. To this end, we propose Reg R-CNN, which replaces the second-stage classification model of a current object detector with a regression model. We show the superiority of our approach on a public data set with 1026 patients and a series of toy experiments. Code will be available at github.com/MIC-DKFZ/RegRCNN.



### Single Image based Head Pose Estimation with Spherical Parameterization and 3D Morphing
- **Arxiv ID**: http://arxiv.org/abs/1907.09217v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09217v3)
- **Published**: 2019-07-22 10:16:30+00:00
- **Updated**: 2020-01-03 14:44:18+00:00
- **Authors**: Hui Yuan, Mengyu Li, Junhui Hou, Jimin Xiao
- **Comment**: 34pages, 5figures, Journal
- **Journal**: None
- **Summary**: Head pose estimation plays a vital role in various applications, e.g., driverassistance systems, human-computer interaction, virtual reality technology, and so on. We propose a novel geometry based algorithm for accurately estimating the head pose from a single 2D face image at a very low computational cost. Specifically, the rectangular coordinates of only four non-coplanar feature points from a predefined 3D facial model as well as the corresponding ones automatically/ manually extracted from a 2D face image are first normalized to exclude the effect of external factors (i.e., scale factor and translation parameters). Then, the four normalized 3D feature points are represented in spherical coordinates with reference to the uniquely determined sphere by themselves. Due to the spherical parameterization, the coordinates of feature points can then be morphed along all the three directions in the rectangular coordinates effectively. Finally, the rotation matrix indicating the head pose is obtained by minimizing the Euclidean distance between the normalized 2D feature points and the 2D re-projections of morphed 3D feature points. Comprehensive experimental results over two popular databases, i.e., Pointing'04 and Biwi Kinect, demonstrate that the proposed algorithm can estimate head poses with higher accuracy and lower run time than state-of-the-art geometry based methods. Even compared with start-of-the-art learning based methods or geometry based methods with additional depth information, our algorithm still produces comparable performance.



### Adapting Computer Vision Algorithms for Omnidirectional Video
- **Arxiv ID**: http://arxiv.org/abs/1907.09233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09233v1)
- **Published**: 2019-07-22 11:12:35+00:00
- **Updated**: 2019-07-22 11:12:35+00:00
- **Authors**: Hannes Fassold
- **Comment**: Accepted for 27th ACM International Conference on Multimedia (ACMM MM
  2019, Nice, France)
- **Journal**: None
- **Summary**: Omnidirectional (360{\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video.



### RGB-D image-based Object Detection: from Traditional Methods to Deep Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/1907.09236v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09236v1)
- **Published**: 2019-07-22 11:18:01+00:00
- **Updated**: 2019-07-22 11:18:01+00:00
- **Authors**: Isaac Ronald Ward, Hamid Laga, Mohammed Bennamoun
- **Comment**: Chapter in the book 'RGB-D Image Analysis and Processing' (Paul
  Rosin)
- **Journal**: None
- **Summary**: Object detection from RGB images is a long-standing problem in image processing and computer vision. It has applications in various domains including robotics, surveillance, human-computer interaction, and medical diagnosis. With the availability of low cost 3D scanners, a large number of RGB-D object detection approaches have been proposed in the past years. This chapter provides a comprehensive survey of the recent developments in this field. We structure the chapter into two parts; the focus of the first part is on techniques that are based on hand-crafted features combined with machine learning algorithms. The focus of the second part is on the more recent work, which is based on deep learning. Deep learning techniques, coupled with the availability of large training datasets, have now revolutionized the field of computer vision, including RGB-D object detection, achieving an unprecedented level of performance. We survey the key contributions, summarize the most commonly used pipelines, discuss their benefits and limitations, and highlight some important directions for future research.



### Quadruplet Selection Methods for Deep Embedding Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.09245v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09245v1)
- **Published**: 2019-07-22 11:39:15+00:00
- **Updated**: 2019-07-22 11:39:15+00:00
- **Authors**: Kaan Karaman, Erhan Gundogdu, Aykut Koc, A. Aydin Alatan
- **Comment**: 6 pages, 2 figures, accepted by IEEE ICIP 2019
- **Journal**: None
- **Summary**: Recognition of objects with subtle differences has been used in many practical applications, such as car model recognition and maritime vessel identification. For discrimination of the objects in fine-grained detail, we focus on deep embedding learning by using a multi-task learning framework, in which the hierarchical labels (coarse and fine labels) of the samples are utilized both for classification and a quadruplet-based loss function. In order to improve the recognition strength of the learned features, we present a novel feature selection method specifically designed for four training samples of a quadruplet. By experiments, it is observed that the selection of very hard negative samples with relatively easy positive ones from the same coarse and fine classes significantly increases some performance metrics in a fine-grained dataset when compared to selecting the quadruplet samples randomly. The feature embedding learned by the proposed method achieves favorable performance against its state-of-the-art counterparts.



### Probabilistic Point Cloud Reconstructions for Vertebral Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/1907.09254v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09254v2)
- **Published**: 2019-07-22 12:08:19+00:00
- **Updated**: 2019-08-02 19:23:18+00:00
- **Authors**: Anjany Sekuboyina, Markus Rempfler, Alexander Valentinitsch, Maximilian Loeffler, Jan S. Kirschke, Bjoern H. Menze
- **Comment**: Accepted at Medical Image Computing and Computer-Assisted
  Intervention (MICCAI), 2019; JSK and BHM are joint supervising authors
- **Journal**: None
- **Summary**: We propose an auto-encoding network architecture for point clouds (PC) capable of extracting shape signatures without supervision. Building on this, we (i) design a loss function capable of modelling data variance on PCs which are unstructured, and (ii) regularise the latent space as in a variational auto-encoder, both of which increase the auto-encoders' descriptive capacity while making them probabilistic. Evaluating the reconstruction quality of our architectures, we employ them for detecting vertebral fractures without any supervision. By learning to efficiently reconstruct only healthy vertebrae, fractures are detected as anomalous reconstructions. Evaluating on a dataset containing $\sim$1500 vertebrae, we achieve area-under-ROC curve of $>$75%, without using intensity-based features.



### A-Phase classification using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1907.09296v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09296v1)
- **Published**: 2019-07-22 13:10:38+00:00
- **Updated**: 2019-07-22 13:10:38+00:00
- **Authors**: Edgar R. Arce-Santana, Alfonso Alba, Martin O. Mendez, Valdemar Arce-Guevara
- **Comment**: 19 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: A series of short events, called A-phases, can be observed in the human electroencephalogram during NREM sleep. These events can be classified in three groups (A1, A2 and A3) according to their spectral contents, and are thought to play a role in the transitions between the different sleep stages. A-phase detection and classification is usually performed manually by a trained expert, but it is a tedious and time-consuming task. In the past two decades, various researchers have designed algorithms to automatically detect and classify the A-phases with varying degrees of success, but the problem remains open. In this paper, a different approach is proposed: instead of attempting to design a general classifier for all subjects, we propose to train ad-hoc classifiers for each subject using as little data as possible, in order to drastically reduce the amount of time required from the expert. The proposed classifiers are based on deep convolutional neural networks using the log-spectrogram of the EEG signal as input data. Results are encouraging, achieving average accuracies of 80.31% when discriminating between A-phases and non A-phases, and 71.87% when classifying among A-phase sub-types, with only 25% of the total A-phases used for training. When additional expert-validated data is considered, the sub-type classification accuracy increases to 78.92%. These results show that a semi-automatic annotation system with assistance from an expert could provide a better alternative to fully automatic classifiers.



### An Efficient Target Detection and Recognition Method in Aerial Remote-sensing Images Based on Multiangle Regions-of-Interest
- **Arxiv ID**: http://arxiv.org/abs/1907.09320v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09320v2)
- **Published**: 2019-07-22 13:48:05+00:00
- **Updated**: 2022-06-08 02:21:54+00:00
- **Authors**: Guangcun Shan, Hongyu Wang, Wei Liang, Congcong Liu, Qizi Ma, Quan Quan
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Recently, deep learning technology have been extensively used in the field of image recognition. However, its main application is the recognition and detection of ordinary pictures and common scenes. It is challenging to effectively and expediently analyze remote-sensing images obtained by the image acquisition systems on unmanned aerial vehicles (UAVs), which includes the identification of the target and calculation of its position. Aerial remote sensing images have different shooting angles and methods compared with ordinary pictures or images, which makes remote-sensing images play an irreplaceable role in some areas. In this study, a new target detection and recognition method in remote-sensing images is proposed based on deep convolution neural network (CNN) for the provision of multilevel information of images in combination with a region proposal network used to generate multiangle regions-of-interest. The proposed method generated results that were much more accurate and precise than those obtained with traditional ways. This demonstrated that the model proposed herein displays tremendous applicability potential in remote-sensing image recognition.



### VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1907.09340v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09340v1)
- **Published**: 2019-07-22 14:33:43+00:00
- **Updated**: 2019-07-22 14:33:43+00:00
- **Authors**: Pranava Madhyastha, Josiah Wang, Lucia Specia
- **Comment**: Accepted for publication at ACL 2019
- **Journal**: None
- **Summary**: We address the task of evaluating image description generation systems. We propose a novel image-aware metric for this task: VIFIDEL. It estimates the faithfulness of a generated caption with respect to the content of the actual image, based on the semantic similarity between labels of objects depicted in images and words in the description. The metric is also able to take into account the relative importance of objects mentioned in human reference descriptions during evaluation. Even if these human reference descriptions are not available, VIFIDEL can still reliably evaluate system descriptions. The metric achieves high correlation with human judgments on two well-known datasets and is competitive with metrics that depend on human references



### Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods
- **Arxiv ID**: http://arxiv.org/abs/1907.09358v3
- **DOI**: 10.1613/jair.1.11688
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09358v3)
- **Published**: 2019-07-22 14:53:48+00:00
- **Updated**: 2021-12-31 20:40:20+00:00
- **Authors**: Aditya Mogadala, Marimuthu Kalimuthu, Dietrich Klakow
- **Comment**: Published at the Journal of Artificial Intelligence Research (JAIR);
  135 pages
- **Journal**: Journal of Artificial Intelligence Research, Vol. 71, 2021
- **Summary**: Interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as machine learning, computer vision, and natural language processing. Much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. This has created significant interest in the integration of vision and language. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey stimulates innovative thoughts and ideas to address the existing challenges and build new applications.



### DeepIris: Iris Recognition Using A Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1907.09380v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09380v1)
- **Published**: 2019-07-22 15:48:48+00:00
- **Updated**: 2019-07-22 15:48:48+00:00
- **Authors**: Shervin Minaee, Amirali Abdolrashidi
- **Comment**: None
- **Journal**: None
- **Summary**: Iris recognition has been an active research area during last few decades, because of its wide applications in security, from airports to homeland security border control. Different features and algorithms have been proposed for iris recognition in the past. In this paper, we propose an end-to-end deep learning framework for iris recognition based on residual convolutional neural network (CNN), which can jointly learn the feature representation and perform recognition. We train our model on a well-known iris recognition dataset using only a few training images from each class, and show promising results and improvements over previous approaches. We also present a visualization technique which is able to detect the important areas in iris images which can mostly impact the recognition results. We believe this framework can be widely used for other biometrics recognition tasks, helping to have a more scalable and accurate systems.



### Visualizing the Invisible: Occluded Vehicle Segmentation and Recovery
- **Arxiv ID**: http://arxiv.org/abs/1907.09381v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09381v1)
- **Published**: 2019-07-22 15:49:03+00:00
- **Updated**: 2019-07-22 15:49:03+00:00
- **Authors**: Xiaosheng Yan, Yuanlong Yu, Feigege Wang, Wenxi Liu, Shengfeng He, Jia Pan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel iterative multi-task framework to complete the segmentation mask of an occluded vehicle and recover the appearance of its invisible parts. In particular, to improve the quality of the segmentation completion, we present two coupled discriminators and introduce an auxiliary 3D model pool for sampling authentic silhouettes as adversarial samples. In addition, we propose a two-path structure with a shared network to enhance the appearance recovery capability. By iteratively performing the segmentation completion and the appearance recovery, the results will be progressively refined. To evaluate our method, we present a dataset, the Occluded Vehicle dataset, containing synthetic and real-world occluded vehicle images. We conduct comparison experiments on this dataset and demonstrate that our model outperforms the state-of-the-art in tasks of recovering segmentation mask and appearance for occluded vehicles. Moreover, we also demonstrate that our appearance recovery approach can benefit the occluded vehicle tracking in real-world videos.



### Domain-Specific Priors and Meta Learning for Few-Shot First-Person Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1907.09382v3
- **DOI**: 10.1109/TPAMI.2021.3058606
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09382v3)
- **Published**: 2019-07-22 15:52:21+00:00
- **Updated**: 2021-12-07 23:33:50+00:00
- **Authors**: Huseyin Coskun, Zeeshan Zia, Bugra Tekin, Federica Bogo, Nassir Navab, Federico Tombari, Harpreet Sawhney
- **Comment**: Paper has been accepted in Transactions on Pattern Analysis and
  Machine Intelligence
- **Journal**: year = {5555}, volume = {}, number = {01}, issn = {1939-3539},
  pages = {1-1},
- **Summary**: The lack of large-scale real datasets with annotations makes transfer learning a necessity for video activity understanding. We aim to develop an effective method for few-shot transfer learning for first-person action classification. We leverage independently trained local visual cues to learn representations that can be transferred from a source domain, which provides primitive action labels, to a different target domain using only a handful of examples. Visual cues we employ include object-object interactions, hand grasps and motion within regions that are a function of hand locations. We employ a framework based on meta-learning to extract the distinctive and domain invariant components of the deployed visual cues. This enables transfer of action classification models across public datasets captured with diverse scene and action configurations. We present comparative results of our transfer learning methodology and report superior results over state-of-the-art action classification approaches for both inter-class and inter-dataset transfer.



### Markerless Augmented Advertising for Sports Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.09394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09394v1)
- **Published**: 2019-07-22 16:10:34+00:00
- **Updated**: 2019-07-22 16:10:34+00:00
- **Authors**: Hallee E. Wong, Osman Akar, Emmanuel Antonio Cuevas, Iuliana Tabian, Divyaa Ravichandran, Iris Fu, Cambron Carter
- **Comment**: None
- **Journal**: None
- **Summary**: Markerless augmented reality can be a challenging computer vision task, especially in live broadcast settings and in the absence of information related to the video capture such as the intrinsic camera parameters. This typically requires the assistance of a skilled artist, along with the use of advanced video editing tools in a post-production environment. We present an automated video augmentation pipeline that identifies textures of interest and overlays an advertisement onto these regions. We constrain the advertisement to be placed in a way that is aesthetic and natural. The aim is to augment the scene such that there is no longer a need for commercial breaks. In order to achieve seamless integration of the advertisement with the original video we build a 3D representation of the scene, place the advertisement in 3D, and then project it back onto the image plane. After successful placement in a single frame, we use homography-based, shape-preserving tracking such that the advertisement appears perspective correct for the duration of a video clip. The tracker is designed to handle smooth camera motion and shot boundaries.



### STD: Sparse-to-Dense 3D Object Detector for Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1907.10471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.10471v1)
- **Published**: 2019-07-22 16:20:44+00:00
- **Updated**: 2019-07-22 16:20:44+00:00
- **Authors**: Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, Jiaya Jia
- **Comment**: arXiv admin note: text overlap with arXiv:1812.05276
- **Journal**: None
- **Summary**: We present a new two-stage 3D object detection framework, named sparse-to-dense 3D Object Detector (STD). The first stage is a bottom-up proposal generation network that uses raw point cloud as input to generate accurate proposals by seeding each point with a new spherical anchor. It achieves a high recall with less computation compared with prior works. Then, PointsPool is applied for generating proposal features by transforming their interior point features from sparse expression to compact representation, which saves even more computation time. In box prediction, which is the second stage, we implement a parallel intersection-over-union (IoU) branch to increase awareness of localization accuracy, resulting in further improved performance. We conduct experiments on KITTI dataset, and evaluate our method in terms of 3D object and Bird's Eye View (BEV) detection. Our method outperforms other state-of-the-arts by a large margin, especially on the hard set, with inference speed more than 10 FPS.



### Deep Learning Approaches for Image Retrieval and Pattern Spotting in Ancient Documents
- **Arxiv ID**: http://arxiv.org/abs/1907.09404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.09404v1)
- **Published**: 2019-07-22 16:27:19+00:00
- **Updated**: 2019-07-22 16:27:19+00:00
- **Authors**: Kelly Lais Wiggers, Alceu de Souza Britto Junior, Alessandro Lameiras Koerich, Laurent Heutte, Luiz Eduardo Soares de Oliveira
- **Comment**: The paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: This paper describes two approaches for content-based image retrieval and pattern spotting in document images using deep learning. The first approach uses a pre-trained CNN model to cope with the lack of training data, which is fine-tuned to achieve a compact yet discriminant representation of queries and image candidates. The second approach uses a Siamese Convolution Neural Network trained on a previously prepared subset of image pairs from the ImageNet dataset to provide the similarity-based feature maps. In both methods, the learned representation scheme considers feature maps of different sizes which are evaluated in terms of retrieval performance. A robust experimental protocol using two public datasets (Tobacoo-800 and DocExplore) has shown that the proposed methods compare favorably against state-of-the-art document image retrieval and pattern spotting methods.



### Satellite-Net: Automatic Extraction of Land Cover Indicators from Satellite Imagery by Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.09423v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T45, I.4.8; I.4.9; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/1907.09423v1)
- **Published**: 2019-07-22 16:50:35+00:00
- **Updated**: 2019-07-22 16:50:35+00:00
- **Authors**: Eleonora Bernasconi, Francesco Pugliese, Diego Zardetto, Monica Scannapieco
- **Comment**: New Techniques and Technologies for Statistics 2019, Brussels
- **Journal**: None
- **Summary**: In this paper we address the challenge of land cover classification for satellite images via Deep Learning (DL). Land Cover aims to detect the physical characteristics of the territory and estimate the percentage of land occupied by a certain category of entities: vegetation, residential buildings, industrial areas, forest areas, rivers, lakes, etc. DL is a new paradigm for Big Data analytics and in particular for Computer Vision. The application of DL in images classification for land cover purposes has a great potential owing to the high degree of automation and computing performance. In particular, the invention of Convolution Neural Networks (CNNs) was a fundament for the advancements in this field. In [1], the Satellite Task Team of the UN Global Working Group describes the results achieved so far with respect to the use of earth observation for Official Statistics. However, in that study, CNNs have not yet been explored for automatic classification of imagery. This work investigates the usage of CNNs for the estimation of land cover indicators, providing evidence of the first promising results. In particular, the paper proposes a customized model, called Satellite-Net, able to reach an accuracy level up to 98% on test sets.



### k-t NEXT: Dynamic MR Image Reconstruction Exploiting Spatio-temporal Correlations
- **Arxiv ID**: http://arxiv.org/abs/1907.09425v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09425v1)
- **Published**: 2019-07-22 16:51:59+00:00
- **Updated**: 2019-07-22 16:51:59+00:00
- **Authors**: Chen Qin, Jo Schlemper, Jinming Duan, Gavin Seegoolam, Anthony Price, Joseph Hajnal, Daniel Rueckert
- **Comment**: This paper is accepted by MICCAI 2019
- **Journal**: None
- **Summary**: Dynamic magnetic resonance imaging (MRI) exhibits high correlations in k-space and time. In order to accelerate the dynamic MR imaging and to exploit k-t correlations from highly undersampled data, here we propose a novel deep learning based approach for dynamic MR image reconstruction, termed k-t NEXT (k-t NEtwork with X-f Transform). In particular, inspired by traditional methods such as k-t BLAST and k-t FOCUSS, we propose to reconstruct the true signals from aliased signals in x-f domain to exploit the spatio-temporal redundancies. Building on that, the proposed method then learns to recover the signals by alternating the reconstruction process between the x-f space and image space in an iterative fashion. This enables the network to effectively capture useful information and jointly exploit spatio-temporal correlations from both complementary domains. Experiments conducted on highly undersampled short-axis cardiac cine MRI scans demonstrate that our proposed method outperforms the current state-of-the-art dynamic MR reconstruction approaches both quantitatively and qualitatively.



### Multi-Class Lane Semantic Segmentation using Efficient Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.09438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09438v1)
- **Published**: 2019-07-22 17:22:03+00:00
- **Updated**: 2019-07-22 17:22:03+00:00
- **Authors**: Shao-Yuan Lo, Hsueh-Ming Hang, Sheng-Wei Chan, Jing-Jhih Lin
- **Comment**: Accepted in IEEE International Workshop on Multimedia Signal
  Processing (MMSP) 2019
- **Journal**: None
- **Summary**: Lane detection plays an important role in a self-driving vehicle. Several studies leverage a semantic segmentation network to extract robust lane features, but few of them can distinguish different types of lanes. In this paper, we focus on the problem of multi-class lane semantic segmentation. Based on the observation that the lane is a small-size and narrow-width object in a road scene image, we propose two techniques, Feature Size Selection (FSS) and Degressive Dilation Block (DD Block). The FSS allows a network to extract thin lane features using appropriate feature sizes. To acquire fine-grained spatial information, the DD Block is made of a series of dilated convolutions with degressive dilation rates. Experimental results show that the proposed techniques provide obvious improvement in accuracy, while they achieve the same or faster inference speed compared to the baseline system, and can run at real-time on high-resolution images.



### Automatic detection of rare pathologies in fundus photographs using few-shot learning
- **Arxiv ID**: http://arxiv.org/abs/1907.09449v3
- **DOI**: 10.1016/j.media.2020.101660
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09449v3)
- **Published**: 2019-07-22 17:48:27+00:00
- **Updated**: 2020-02-10 17:41:05+00:00
- **Authors**: GwenolÃ© Quellec, Mathieu Lamard, Pierre-Henri Conze, Pascale Massin, BÃ©atrice Cochener
- **Comment**: None
- **Journal**: Medical Image Analysis, Volume 61, April 2020, 101660
- **Summary**: In the last decades, large datasets of fundus photographs have been collected in diabetic retinopathy (DR) screening networks. Through deep learning, these datasets were used to train automatic detectors for DR and a few other frequent pathologies, with the goal to automate screening. One challenge limits the adoption of such systems so far: automatic detectors ignore rare conditions that ophthalmologists currently detect, such as papilledema or anterior ischemic optic neuropathy. The reason is that standard deep learning requires too many examples of these conditions. However, this limitation can be addressed with few-shot learning, a machine learning paradigm where a classifier has to generalize to a new category not seen in training, given only a few examples of this category. This paper presents a new few-shot learning framework that extends convolutional neural networks (CNNs), trained for frequent conditions, with an unsupervised probabilistic model for rare condition detection. It is based on the observation that CNNs often perceive photographs containing the same anomalies as similar, even though these CNNs were trained to detect unrelated conditions. This observation was based on the t-SNE visualization tool, which we decided to incorporate in our probabilistic model. Experiments on a dataset of 164,660 screening examinations from the OPHDIAT screening network show that 37 conditions, out of 41, can be detected with an area under the ROC curve (AUC) greater than 0.8 (average AUC: 0.938). In particular, this framework significantly outperforms other frameworks for detecting rare conditions, including multitask learning, transfer learning and Siamese networks, another few-shot learning solution. We expect these richer predictions to trigger the adoption of automated eye pathology screening, which will revolutionize clinical practice in ophthalmology.



### Switchable Normalization for Learning-to-Normalize Deep Representation
- **Arxiv ID**: http://arxiv.org/abs/1907.10473v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.10473v1)
- **Published**: 2019-07-22 17:50:31+00:00
- **Updated**: 2019-07-22 17:50:31+00:00
- **Authors**: Ping Luo, Ruimao Zhang, Jiamin Ren, Zhanglin Peng, Jingyu Li
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence, 18
  pages, 15 figures, 11 tables. arXiv admin note: substantial text overlap with
  arXiv:1806.10779
- **Journal**: None
- **Summary**: We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks. Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, MegaFace, and Kinetics. Analyses of SN are also presented to answer the following three questions: (a) Is it useful to allow each normalization layer to select its own normalizer? (b) What impacts the choices of normalizers? (c) Do different tasks and datasets prefer different normalizers? We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN has been released at https://github.com/switchablenorms.



### deepCR: Cosmic Ray Rejection with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.09500v2
- **DOI**: 10.3847/1538-4357/ab3fa6
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.09500v2)
- **Published**: 2019-07-22 18:02:19+00:00
- **Updated**: 2019-08-29 06:04:47+00:00
- **Authors**: Keming Zhang, Joshua S. Bloom
- **Comment**: Accepted for publication in ApJ. 12 pages, 6 figures. An open-source
  Python package, deepCR, which implements the approach in this paper is at
  https://github.com/profjsb/deepCR. Figures and benchmarks can be reproduced
  at https://github.com/kmzzhang/deepCR-paper. Blog post describing this work
  is at https://medium.com/@kemingzhang/deepcr-2fa610655655
- **Journal**: None
- **Summary**: Cosmic ray (CR) identification and replacement are critical components of imaging and spectroscopic reduction pipelines involving solid-state detectors. We present deepCR, a deep learning based framework for CR identification and subsequent image inpainting based on the predicted CR mask. To demonstrate the effectiveness of this framework, we train and evaluate models on Hubble Space Telescope ACS/WFC images of sparse extragalactic fields, globular clusters, and resolved galaxies. We demonstrate that at a false positive rate of 0.5%, deepCR achieves close to 100% detection rates in both extragalactic and globular cluster fields, and 91% in resolved galaxy fields, which is a significant improvement over the current state-of-the-art method LACosmic. Compared to a multicore CPU implementation of LACosmic, deepCR CR mask predictions run up to 6.5 times faster on CPU and 90 times faster on a single GPU. For image inpainting, the mean squared errors of deepCR predictions are 20 times lower in globular cluster fields, 5 times lower in resolved galaxy fields, and 2.5 times lower in extragalactic fields, compared to the best performing non-neural technique tested. We present our framework and the trained models as an open-source Python project, with a simple-to-use API. To facilitate reproducibility of the results we also provide a benchmarking codebase.



### Universal Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1907.09511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09511v1)
- **Published**: 2019-07-22 18:17:56+00:00
- **Updated**: 2019-07-22 18:17:56+00:00
- **Authors**: Xu Lan, Xiatian Zhu, Shaogang Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Most state-of-the-art person re-identification (re-id) methods depend on supervised model learning with a large set of cross-view identity labelled training data. Even worse, such trained models are limited to only the same-domain deployment with significantly degraded cross-domain generalization capability, i.e. "domain specific". To solve this limitation, there are a number of recent unsupervised domain adaptation and unsupervised learning methods that leverage unlabelled target domain training data. However, these methods need to train a separate model for each target domain as supervised learning methods. This conventional "{\em train once, run once}" pattern is unscalable to a large number of target domains typically encountered in real-world deployments. We address this problem by presenting a "train once, run everywhere" pattern industry-scale systems are desperate for. We formulate a "universal model learning' approach enabling domain-generic person re-id using only limited training data of a "{\em single}" seed domain. Specifically, we train a universal re-id deep model to discriminate between a set of transformed person identity classes. Each of such classes is formed by applying a variety of random appearance transformations to the images of that class, where the transformations simulate the camera viewing conditions of any domains for making the model training domain generic. Extensive evaluations show the superiority of our method for universal person re-id over a wide variety of state-of-the-art unsupervised domain adaptation and unsupervised learning re-id methods on five standard benchmarks: Market-1501, DukeMTMC, CUHK03, MSMT17, and VIPeR.



### Green AI
- **Arxiv ID**: http://arxiv.org/abs/1907.10597v3
- **DOI**: None
- **Categories**: **cs.CY**, cs.CL, cs.CV, cs.LG, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1907.10597v3)
- **Published**: 2019-07-22 19:36:18+00:00
- **Updated**: 2019-08-13 20:09:57+00:00
- **Authors**: Roy Schwartz, Jesse Dodge, Noah A. Smith, Oren Etzioni
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research.   This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or "price tag" of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.



### Product of Orthogonal Spheres Parameterization for Disentangled Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.09554v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09554v1)
- **Published**: 2019-07-22 20:20:00+00:00
- **Updated**: 2019-07-22 20:20:00+00:00
- **Authors**: Ankita Shukla, Sarthak Bhagat, Shagun Uppal, Saket Anand, Pavan Turaga
- **Comment**: Accepted at British Machine Vision Conference (BMVC) 2019
- **Journal**: None
- **Summary**: Learning representations that can disentangle explanatory attributes underlying the data improves interpretabilty as well as provides control on data generation. Various learning frameworks such as VAEs, GANs and auto-encoders have been used in the literature to learn such representations. Most often, the latent space is constrained to a partitioned representation or structured by a prior to impose disentangling. In this work, we advance the use of a latent representation based on a product space of Orthogonal Spheres PrOSe. The PrOSe model is motivated by the reasoning that latent-variables related to the physics of image-formation can under certain relaxed assumptions lead to spherical-spaces. Orthogonality between the spheres is motivated via physical independence models. Imposing the orthogonal-sphere constraint is much simpler than other complicated physical models, is fairly general and flexible, and extensible beyond the factors used to motivate its development. Under further relaxed assumptions of equal-sized latent blocks per factor, the constraint can be written down in closed form as an ortho-normality term in the loss function. We show that our approach improves the quality of disentanglement significantly. We find consistent improvement in disentanglement compared to several state-of-the-art approaches, across several benchmarks and metrics.



### The Effect of Visual Design in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1907.09567v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, q-fin.CP, q-fin.TR
- **Links**: [PDF](http://arxiv.org/pdf/1907.09567v2)
- **Published**: 2019-07-22 20:47:56+00:00
- **Updated**: 2019-08-20 14:41:00+00:00
- **Authors**: Naftali Cohen, Tucker Balch, Manuela Veloso
- **Comment**: None
- **Journal**: None
- **Summary**: Financial companies continuously analyze the state of the markets to rethink and adjust their investment strategies. While the analysis is done on the digital form of data, decisions are often made based on graphical representations in white papers or presentation slides. In this study, we examine whether binary decisions are better to be decided based on the numeric or the visual representation of the same data. Using two data sets, a matrix of numerical data with spatial dependencies and financial data describing the state of the S&P index, we compare the results of supervised classification based on the original numerical representation and the visual transformation of the same data. We show that, for these data sets, the visual transformation results in higher predictability skill compared to the original form of the data. We suggest thinking of the visual representation of numeric data, effectively, as a combination of dimensional reduction and feature engineering techniques. In particular, if the visual layout encapsulates the full complexity of the data. In this view, thoughtful visual design can guard against overfitting, or introduce new features -- all of which benefit the learning process, and effectively lead to better recognition of meaningful patterns.



### MemNet: Memory-Efficiency Guided Neural Architecture Search with Augment-Trim learning
- **Arxiv ID**: http://arxiv.org/abs/1907.09569v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.09569v2)
- **Published**: 2019-07-22 20:49:53+00:00
- **Updated**: 2020-06-10 20:12:57+00:00
- **Authors**: Peiye Liu, Bo Wu, Huadong Ma, Mingoo Seok
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies on automatic neural architectures search have demonstrated significant performance, competitive to or even better than hand-crafted neural architectures. However, most of the existing network architecture tend to use residual, parallel structures and concatenation block between shallow and deep features to construct a large network. This requires large amounts of memory for storing both weights and feature maps. This is challenging for mobile and embedded devices since they may not have enough memory to perform inference with the designed large network model. To close this gap, we propose MemNet, an augment-trim learning-based neural network search framework that optimizes not only performance but also memory requirement. Specifically, it employs memory consumption based ranking score which forces an upper bound on memory consumption for navigating the search process. Experiment results show that, as compared to the state-of-the-art efficient designing methods, MemNet can find an architecture which can achieve competitive accuracy and save an average of 24.17% on the total memory needed.



### Information-Bottleneck Approach to Salient Region Discovery
- **Arxiv ID**: http://arxiv.org/abs/1907.09578v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1907.09578v2)
- **Published**: 2019-07-22 21:13:30+00:00
- **Updated**: 2020-02-14 22:14:54+00:00
- **Authors**: Andrey Zhmoginov, Ian Fischer, Mark Sandler
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new method for learning image attention masks in a semi-supervised setting based on the Information Bottleneck principle. Provided with a set of labeled images, the mask generation model is minimizing mutual information between the input and the masked image while maximizing the mutual information between the same masked image and the image label. In contrast with other approaches, our attention model produces a Boolean rather than a continuous mask, entirely concealing the information in masked-out pixels. Using a set of synthetic datasets based on MNIST and CIFAR10 and the SVHN datasets, we demonstrate that our method can successfully attend to features known to define the image class.



### Understanding the Political Ideology of Legislators from Social Media Images
- **Arxiv ID**: http://arxiv.org/abs/1907.09594v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV, cs.HC, cs.MM, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1907.09594v1)
- **Published**: 2019-07-22 21:43:49+00:00
- **Updated**: 2019-07-22 21:43:49+00:00
- **Authors**: Nan Xi, Di Ma, Marcus Liou, Zachary C. Steinert-Threlkeld, Jason Anastasopoulos, Jungseock Joo
- **Comment**: To appear in the Proceedings of International AAAI Conference on Web
  and Social Media (ICWSM 2020)
- **Journal**: None
- **Summary**: In this paper, we seek to understand how politicians use images to express ideological rhetoric through Facebook images posted by members of the U.S. House and Senate. In the era of social media, politics has become saturated with imagery, a potent and emotionally salient form of political rhetoric which has been used by politicians and political organizations to influence public sentiment and voting behavior for well over a century. To date, however, little is known about how images are used as political rhetoric. Using deep learning techniques to automatically predict Republican or Democratic party affiliation solely from the Facebook photographs of the members of the 114th U.S. Congress, we demonstrate that predicted class probabilities from our model function as an accurate proxy of the political ideology of images along a left-right (liberal-conservative) dimension. After controlling for the gender and race of politicians, our method achieves an accuracy of 59.28% from single photographs and 82.35% when aggregating scores from multiple photographs (up to 150) of the same person. To better understand image content distinguishing liberal from conservative images, we also perform in-depth content analyses of the photographs. Our findings suggest that conservatives tend to use more images supporting status quo political institutions and hierarchy maintenance, featuring individuals from dominant social groups, and displaying greater happiness than liberals.



### MixConv: Mixed Depthwise Convolutional Kernels
- **Arxiv ID**: http://arxiv.org/abs/1907.09595v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.09595v3)
- **Published**: 2019-07-22 21:49:25+00:00
- **Updated**: 2019-12-01 06:29:57+00:00
- **Authors**: Mingxing Tan, Quoc V. Le
- **Comment**: BMVC 2019
- **Journal**: BMVC 2019
- **Summary**: Depthwise convolution is becoming increasingly popular in modern efficient ConvNets, but its kernel size is often overlooked. In this paper, we systematically study the impact of different kernel sizes, and observe that combining the benefits of multiple kernel sizes can lead to better accuracy and efficiency. Based on this observation, we propose a new mixed depthwise convolution (MixConv), which naturally mixes up multiple kernel sizes in a single convolution. As a simple drop-in replacement of vanilla depthwise convolution, our MixConv improves the accuracy and efficiency for existing MobileNets on both ImageNet classification and COCO object detection. To demonstrate the effectiveness of MixConv, we integrate it into AutoML search space and develop a new family of models, named as MixNets, which outperform previous mobile models including MobileNetV2 [20] (ImageNet top-1 accuracy +4.2%), ShuffleNetV2 [16] (+3.5%), MnasNet [26] (+1.3%), ProxylessNAS [2] (+2.2%), and FBNet [27] (+2.0%). In particular, our MixNet-L achieves a new state-of-the-art 78.9% ImageNet top-1 accuracy under typical mobile settings (<600M FLOPS). Code is at https://github.com/ tensorflow/tpu/tree/master/models/official/mnasnet/mixnet



### Bayesian Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1907.09624v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.09624v3)
- **Published**: 2019-07-22 23:14:51+00:00
- **Updated**: 2020-08-26 18:58:55+00:00
- **Authors**: Sarkhan Badirli, Zeynep Akata, Murat Dundar
- **Comment**: Accepted to ECCV 2020, TASK-CV Workshop
- **Journal**: None
- **Summary**: Object classes that surround us have a natural tendency to emerge at varying levels of abstraction. We propose a Bayesian approach to zero-shot learning (ZSL) that introduces the notion of meta-classes and implements a Bayesian hierarchy around these classes to effectively blend data likelihood with local and global priors. Local priors driven by data from seen classes, i.e. classes that are available at training time, become instrumental in recovering unseen classes, i.e. classes that are missing at training time, in a generalized ZSL setting. Hyperparameters of the Bayesian model offer a convenient way to optimize the trade-off between seen and unseen class accuracy in addition to guiding other aspects of model fitting. We conduct experiments on seven benchmark datasets including the large scale ImageNet and show that our model improves the current state of the art in the challenging generalized ZSL setting.




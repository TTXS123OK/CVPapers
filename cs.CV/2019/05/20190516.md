# Arxiv Papers in cs.CV on 2019-05-16
### Semi-supervised learning based on generative adversarial network: a comparison between good GAN and bad GAN approach
- **Arxiv ID**: http://arxiv.org/abs/1905.06484v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06484v2)
- **Published**: 2019-05-16 00:43:47+00:00
- **Updated**: 2019-05-17 01:53:45+00:00
- **Authors**: Wenyuan Li, Zichen Wang, Jiayun Li, Jennifer Polson, William Speier, Corey Arnold
- **Comment**: This paper appears at CVPR 2019 Weakly Supervised Learning for
  Real-World Computer Vision Applications (LID) Workshop
- **Journal**: None
- **Summary**: Recently, semi-supervised learning methods based on generative adversarial networks (GANs) have received much attention. Among them, two distinct approaches have achieved competitive results on a variety of benchmark datasets. Bad GAN learns a classifier with unrealistic samples distributed on the complement of the support of the input data. Conversely, Triple GAN consists of a three-player game that tries to leverage good generated samples to boost classification results. In this paper, we perform a comprehensive comparison of these two approaches on different benchmark datasets. We demonstrate their different properties on image generation, and sensitivity to the amount of labeled data provided. By comprehensively comparing these two methods, we hope to shed light on the future of GAN-based semi-supervised learning.



### Investigating Channel Pruning through Structural Redundancy Reduction -- A Statistical Study
- **Arxiv ID**: http://arxiv.org/abs/1905.06498v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06498v3)
- **Published**: 2019-05-16 02:10:05+00:00
- **Updated**: 2019-07-16 16:41:15+00:00
- **Authors**: Chengcheng Li, Zi Wang, Dali Wang, Xiangyang Wang, Hairong Qi
- **Comment**: 2019 ICML Workshop, Joint Workshop on On-Device Machine Learning &
  Compact Deep Neural Network Representations (ODML-CDNNR)
- **Journal**: None
- **Summary**: Most existing channel pruning methods formulate the pruning task from a perspective of inefficiency reduction which iteratively rank and remove the least important filters, or find the set of filters that minimizes some reconstruction errors after pruning. In this work, we investigate the channel pruning from a new perspective with statistical modeling. We hypothesize that the number of filters at a certain layer reflects the level of 'redundancy' in that layer and thus formulate the pruning problem from the aspect of redundancy reduction. Based on both theoretic analysis and empirical studies, we make an important discovery: randomly pruning filters from layers of high redundancy outperforms pruning the least important filters across all layers based on the state-of-the-art ranking criterion. These results advance our understanding of pruning and further testify to the recent findings that the structure of the pruned model plays a key role in the network efficiency as compared to inherited weights.



### Bimodal Stereo: Joint Shape and Pose Estimation from Color-Depth Image Pair
- **Arxiv ID**: http://arxiv.org/abs/1905.06499v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.06499v1)
- **Published**: 2019-05-16 02:15:01+00:00
- **Updated**: 2019-05-16 02:15:01+00:00
- **Authors**: Chi Zhang, Yuehu Liu, Ying Wu, Qilin Zhang, Le Wang
- **Comment**: Preprinted version on May 15, 2019
- **Journal**: None
- **Summary**: Mutual calibration between color and depth cameras is a challenging topic in multi-modal data registration. In this paper, we are confronted with a "Bimodal Stereo" problem, which aims to solve camera pose from a pair of an uncalibrated color image and a depth map from different views automatically. To address this problem, an iterative Shape-from-Shading (SfS) based framework is proposed to estimate shape and pose simultaneously. In the pipeline, the estimated shape is refined by the shape prior from the given depth map under the estimated pose. Meanwhile, the estimated pose is improved by the registration of estimated shape and shape from given depth map. We also introduce a shading based refinement in the pipeline to address noisy depth map with holes. Extensive experiments showed that through our method, both the depth map, the recovered shape as well as its pose can be desirably refined and recovered.



### Learning Robust 3D Face Reconstruction and Discriminative Identity Representation
- **Arxiv ID**: http://arxiv.org/abs/1905.06505v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/1905.06505v1)
- **Published**: 2019-05-16 02:31:34+00:00
- **Updated**: 2019-05-16 02:31:34+00:00
- **Authors**: Yao Luo, Xiaoguang Tu, Mei Xie
- **Comment**: 5 pages, 6 figures, IEEE International Conference on Information
  Communication and Signal Processing
- **Journal**: None
- **Summary**: 3D face reconstruction from a single 2D image is a very important topic in computer vision. However, the current reconstruction methods are usually non-sensitive to face identities and over-sensitive to facial poses, which may result in similar 3D geometries for faces of different identities, or obtain different shapes for the same identity with different poses. When such methods are applied practically, their 3D estimates are either changeable for different photos of the same subject or over-regularized and generic to distinguish face identities. In this paper, we propose a robust solution to solve this problem by carefully designing a novel Siamese Convolutional Neural Network (SCNN). Specifically, regarding the 3D Morphable face Model (3DMM) parameters of the same individual as the same class, we employ the contrastive loss to enlarge the inter-class distance and meanwhile reduce the intra-class distance for the output 3DMM parameters. We also propose an identity loss to preserve the identity information for the same individual in the feature space. Training with these two losses, our SCNN could learn representations that are more discriminative for face identity and generalizable for pose variants. Experiments on the challenging database 300W-LP and AFLW2000-3D have shown the effectiveness of our method by comparing with state-of-the-arts.



### TRk-CNN: Transferable Ranking-CNN for image classification of glaucoma, glaucoma suspect, and normal eyes
- **Arxiv ID**: http://arxiv.org/abs/1905.06509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06509v1)
- **Published**: 2019-05-16 02:45:04+00:00
- **Updated**: 2019-05-16 02:45:04+00:00
- **Authors**: Tae Joon Jun, Youngsub Eom, Dohyeun Kim, Cherry Kim, Ji-Hye Park, Hoang Minh Nguyen, Daeyoung Kim
- **Comment**: 49 pages, 12 figures
- **Journal**: None
- **Summary**: In this paper, we proposed Transferable Ranking Convolutional Neural Network (TRk-CNN) that can be effectively applied when the classes of images to be classified show a high correlation with each other. The multi-class classification method based on the softmax function, which is generally used, is not effective in this case because the inter-class relationship is ignored. Although there is a Ranking-CNN that takes into account the ordinal classes, it cannot reflect the inter-class relationship to the final prediction. TRk-CNN, on the other hand, combines the weights of the primitive classification model to reflect the inter-class information to the final classification phase. We evaluated TRk-CNN in glaucoma image dataset that was labeled into three classes: normal, glaucoma suspect, and glaucoma eyes. Based on the literature we surveyed, this study is the first to classify three status of glaucoma fundus image dataset into three different classes. We compared the evaluation results of TRk-CNN with Ranking-CNN (Rk-CNN) and multi-class CNN (MC-CNN) using the DenseNet as the backbone CNN model. As a result, TRk-CNN achieved an average accuracy of 92.96%, specificity of 93.33%, sensitivity for glaucoma suspect of 95.12% and sensitivity for glaucoma of 93.98%. Based on average accuracy, TRk-CNN is 8.04% and 9.54% higher than Rk-CNN and MC-CNN and surprisingly 26.83% higher for sensitivity for suspicious than multi-class CNN. Our TRk-CNN is expected to be effectively applied to the medical image classification problem where the disease state is continuous and increases in the positive class direction.



### ReshapeGAN: Object Reshaping by Providing A Single Reference Image
- **Arxiv ID**: http://arxiv.org/abs/1905.06514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.06514v1)
- **Published**: 2019-05-16 03:54:12+00:00
- **Updated**: 2019-05-16 03:54:12+00:00
- **Authors**: Ziqiang Zheng, Yang Wu, Zhibin Yu, Yang Yang, Haiyong Zheng, Takeo Kanade
- **Comment**: 25 pages, 23 figures
- **Journal**: None
- **Summary**: The aim of this work is learning to reshape the object in an input image to an arbitrary new shape, by just simply providing a single reference image with an object instance in the desired shape. We propose a new Generative Adversarial Network (GAN) architecture for such an object reshaping problem, named ReshapeGAN. The network can be tailored for handling all kinds of problem settings, including both within-domain (or single-dataset) reshaping and cross-domain (typically across mutiple datasets) reshaping, with paired or unpaired training data. The appearance of the input object is preserved in all cases, and thus it is still identifiable after reshaping, which has never been achieved as far as we are aware. We present the tailored models of the proposed ReshapeGAN for all the problem settings, and have them tested on 8 kinds of reshaping tasks with 13 different datasets, demonstrating the ability of ReshapeGAN on generating convincing and superior results for object reshaping. To the best of our knowledge, we are the first to be able to make one GAN framework work on all such object reshaping tasks, especially the cross-domain tasks on handling multiple diverse datasets. We present here both ablation studies on our proposed ReshapeGAN models and comparisons with the state-of-the-art models when they are made comparable, using all kinds of applicable metrics that we are aware of.



### Joint Learning of Neural Networks via Iterative Reweighted Least Squares
- **Arxiv ID**: http://arxiv.org/abs/1905.06526v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.06526v2)
- **Published**: 2019-05-16 04:38:55+00:00
- **Updated**: 2019-06-11 04:41:46+00:00
- **Authors**: Zaiwei Zhang, Xiangru Huang, Qixing Huang, Xiao Zhang, Yuan Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce the problem of jointly learning feed-forward neural networks across a set of relevant but diverse datasets. Compared to learning a separate network from each dataset in isolation, joint learning enables us to extract correlated information across multiple datasets to significantly improve the quality of learned networks. We formulate this problem as joint learning of multiple copies of the same network architecture and enforce the network weights to be shared across these networks. Instead of hand-encoding the shared network layers, we solve an optimization problem to automatically determine how layers should be shared between each pair of datasets. Experimental results show that our approach outperforms baselines without joint learning and those using pretraining-and-fine-tuning. We show the effectiveness of our approach on three tasks: image classification, learning auto-encoders, and image generation.



### FH-GAN: Face Hallucination and Recognition using Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1905.06537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06537v1)
- **Published**: 2019-05-16 05:49:01+00:00
- **Updated**: 2019-05-16 05:49:01+00:00
- **Authors**: Bayram Bayramli, Usman Ali, Te Qi, Hongtao Lu
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: There are many factors affecting visual face recognition, such as low resolution images, aging, illumination and pose variance, etc. One of the most important problem is low resolution face images which can result in bad performance on face recognition. Most of the general face recognition algorithms usually assume a sufficient resolution for the face images. However, in practice many applications often do not have sufficient image resolutions. The modern face hallucination models demonstrate reasonable performance to reconstruct high-resolution images from its corresponding low resolution images. However, they do not consider identity level information during hallucination which directly affects results of the recognition of low resolution faces. To address this issue, we propose a Face Hallucination Generative Adversarial Network (FH-GAN) which improves the quality of low resolution face images and accurately recognize those low quality images. Concretely, we make the following contributions: 1) we propose FH-GAN network, an end-to-end system, that improves both face hallucination and face recognition simultaneously. The novelty of this proposed network depends on incorporating identity information in a GAN-based face hallucination algorithm via combining a face recognition network for identity preserving. 2) We also propose a new face hallucination network, namely Dense Sparse Network (DSNet), which improves upon the state-of-art in face hallucination. 3) We demonstrate benefits of training the face recognition and GAN-based DSNet jointly by reporting good result on face hallucination and recognition.



### Title Redacted
- **Arxiv ID**: http://arxiv.org/abs/1905.06540v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06540v2)
- **Published**: 2019-05-16 05:55:38+00:00
- **Updated**: 2019-05-27 07:11:10+00:00
- **Authors**: Shivang Bharadwaj, Bhupendra Niranjan, Anant Kumar
- **Comment**: This paper has been withdrawn as it is the proprietary property of an
  organization. A revision might or might not be uploaded in the future after
  further internal reviews and revisions. arXiv admin note: Title redacted
- **Journal**: None
- **Summary**: arXiv admin note: This version removed by arXiv administrators as the submitter did not have the right to agree to the license at the time of submission



### Deep Reference Generation with Multi-Domain Hierarchical Constraints for Inter Prediction
- **Arxiv ID**: http://arxiv.org/abs/1905.06567v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06567v1)
- **Published**: 2019-05-16 07:23:20+00:00
- **Updated**: 2019-05-16 07:23:20+00:00
- **Authors**: Jiaying Liu, Sifeng Xia, Wenhan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Inter prediction is an important module in video coding for temporal redundancy removal, where similar reference blocks are searched from previously coded frames and employed to predict the block to be coded. Although traditional video codecs can estimate and compensate for block-level motions, their inter prediction performance is still heavily affected by the remaining inconsistent pixel-wise displacement caused by irregular rotation and deformation. In this paper, we address the problem by proposing a deep frame interpolation network to generate additional reference frames in coding scenarios. First, we summarize the previous adaptive convolutions used for frame interpolation and propose a factorized kernel convolutional network to improve the modeling capacity and simultaneously keep its compact form. Second, to better train this network, multi-domain hierarchical constraints are introduced to regularize the training of our factorized kernel convolutional network. For spatial domain, we use a gradually down-sampled and up-sampled auto-encoder to generate the factorized kernels for frame interpolation at different scales. For quality domain, considering the inconsistent quality of the input frames, the factorized kernel convolution is modulated with quality-related features to learn to exploit more information from high quality frames. For frequency domain, a sum of absolute transformed difference loss that performs frequency transformation is utilized to facilitate network optimization from the view of coding performance. With the well-designed frame interpolation network regularized by multi-domain hierarchical constraints, our method surpasses HEVC on average 6.1% BD-rate saving and up to 11.0% BD-rate saving for the luma component under the random access configuration.



### Quality-based Pulse Estimation from NIR Face Video with Application to Driver Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1905.06568v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1905.06568v2)
- **Published**: 2019-05-16 07:25:51+00:00
- **Updated**: 2019-05-20 06:22:20+00:00
- **Authors**: Javier Hernandez-Ortega, Shigenori Nagae, Julian Fierrez, Aythami Morales
- **Comment**: Preprint of the paper presented to IbPRIA 2019
- **Journal**: None
- **Summary**: In this paper we develop a robust for heart rate (HR) estimation method using face video for challenging scenarios with high variability sources such as head movement, illumination changes, vibration, blur, etc. Our method employs a quality measure Q to extract a remote Plethysmography (rPPG) signal as clean as possible from a specific face video segment. Our main motivation is developing robust technology for driver monitoring. Therefore, for our experiments we use a self-collected dataset consisting of Near Infrared (NIR) videos acquired with a camera mounted in the dashboard of a real moving car. We compare the performance of a classic rPPG algorithm, and the performance of the same method, but using Q for selecting which video segments present a lower amount of variability. Our results show that using the video segments with the highest quality in a realistic driving setup improves the HR estimation with a relative accuracy improvement larger than 20%.



### STAR: A Concise Deep Learning Framework for Citywide Human Mobility Prediction
- **Arxiv ID**: http://arxiv.org/abs/1905.06576v1
- **DOI**: 10.1109/MDM.2019.00-44
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06576v1)
- **Published**: 2019-05-16 07:42:17+00:00
- **Updated**: 2019-05-16 07:42:17+00:00
- **Authors**: Hongnian Wang, Han Su
- **Comment**: Accepted by MDM 2019
- **Journal**: 2019 20th IEEE International Conference on Mobile Data Management
  (MDM), Hong Kong, 2019, pp. 304-309
- **Summary**: Human mobility forecasting in a city is of utmost importance to transportation and public safety, but with the process of urbanization and the generation of big data, intensive computing and determination of mobility pattern have become challenging. This study focuses on how to improve the accuracy and efficiency of predicting citywide human mobility via a simpler solution. A spatio-temporal mobility event prediction framework based on a single fully-convolutional residual network (STAR) is proposed. STAR is a highly simple, general and effective method for learning a single tensor representing the mobility event. Residual learning is utilized for training the deep network to derive the detailed result for scenarios of citywide prediction. Extensive benchmark evaluation results on real-world data demonstrate that STAR outperforms state-of-the-art approaches in single- and multi-step prediction while utilizing fewer parameters and achieving higher efficiency.



### Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization
- **Arxiv ID**: http://arxiv.org/abs/1905.06635v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.06635v2)
- **Published**: 2019-05-16 10:14:20+00:00
- **Updated**: 2022-10-18 15:48:04+00:00
- **Authors**: Seungyong Moon, Gaon An, Hyun Oh Song
- **Comment**: Accepted and to appear at ICML 2019
- **Journal**: None
- **Summary**: Solving for adversarial examples with projected gradient descent has been demonstrated to be highly effective in fooling the neural network based classifiers. However, in the black-box setting, the attacker is limited only to the query access to the network and solving for a successful adversarial example becomes much more difficult. To this end, recent methods aim at estimating the true gradient signal based on the input queries but at the cost of excessive queries. We propose an efficient discrete surrogate to the optimization problem which does not require estimating the gradient and consequently becomes free of the first order update hyperparameters to tune. Our experiments on Cifar-10 and ImageNet show the state of the art black-box attack performance with significant reduction in the required queries compared to a number of recently proposed methods. The source code is available at https://github.com/snu-mllab/parsimonious-blackbox-attack.



### Remove Cosine Window from Correlation Filter-based Visual Trackers: When and How
- **Arxiv ID**: http://arxiv.org/abs/1905.06648v1
- **DOI**: 10.1109/TIP.2020.2997521
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06648v1)
- **Published**: 2019-05-16 10:34:57+00:00
- **Updated**: 2019-05-16 10:34:57+00:00
- **Authors**: Feng Li, Xiaohe Wu, Wangmeng Zuo, David Zhang, Lei Zhang
- **Comment**: 13 pages, 7 figures, submitted to IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Correlation filters (CFs) have been continuously advancing the state-of-the-art tracking performance and have been extensively studied in the recent few years. Most of the existing CF trackers adopt a cosine window to spatially reweight base image to alleviate boundary discontinuity. However, cosine window emphasizes more on the central region of base image and has the risk of contaminating negative training samples during model learning. On the other hand, spatial regularization deployed in many recent CF trackers plays a similar role as cosine window by enforcing spatial penalty on CF coefficients. Therefore, we in this paper investigate the feasibility to remove cosine window from CF trackers with spatial regularization. When simply removing cosine window, CF with spatial regularization still suffers from small degree of boundary discontinuity. To tackle this issue, binary and Gaussian shaped mask functions are further introduced for eliminating boundary discontinuity while reweighting the estimation error of each training sample, and can be incorporated with multiple CF trackers with spatial regularization. In comparison to the counterparts with cosine window, our methods are effective in handling boundary discontinuity and sample contamination, thereby benefiting tracking performance. Extensive experiments on three benchmarks show that our methods perform favorably against the state-of-the-art trackers using either handcrafted or deep CNN features. The code is publicly available at https://github.com/lifeng9472/Removing_cosine_window_from_CF_trackers.



### Robust Real-time Pedestrian Detection in Aerial Imagery on Jetson TX2
- **Arxiv ID**: http://arxiv.org/abs/1905.06653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06653v1)
- **Published**: 2019-05-16 10:54:07+00:00
- **Updated**: 2019-05-16 10:54:07+00:00
- **Authors**: Mohamed Afifi, Yara Ali, Karim Amer, Mahmoud Shaker, Mohamed ElHelw
- **Comment**: None
- **Journal**: None
- **Summary**: Detection of pedestrians in aerial imagery captured by drones has many applications including intersection monitoring, patrolling, and surveillance, to name a few. However, the problem is involved due to continuouslychanging camera viewpoint and object appearance as well as the need for lightweight algorithms to run on on-board embedded systems. To address this issue, the paper proposes a framework for pedestrian detection in videos based on the YOLO object detection network [6] while having a high throughput of more than 5 FPS on the Jetson TX2 embedded board. The framework exploits deep learning for robust operation and uses a pre-trained model without the need for any additional training which makes it flexible to apply on different setups with minimum amount of tuning. The method achieves ~81 mAP when applied on a sample video from the Embedded Real-Time Inference (ERTI) Challenge where pedestrians are monitored by a UAV.



### One-Shot Texture Retrieval with Global Context Metric
- **Arxiv ID**: http://arxiv.org/abs/1905.06656v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.06656v2)
- **Published**: 2019-05-16 11:00:49+00:00
- **Updated**: 2020-04-12 03:25:11+00:00
- **Authors**: Kai Zhu, Wei Zhai, Zheng-Jun Zha, Yang Cao
- **Comment**: ijcai2019-lastest
- **Journal**: None
- **Summary**: In this paper, we tackle one-shot texture retrieval: given an example of a new reference texture, detect and segment all the pixels of the same texture category within an arbitrary image. To address this problem, we present an OS-TR network to encode both reference and query image, leading to achieve texture segmentation towards the reference category. Unlike the existing texture encoding methods that integrate CNN with orderless pooling, we propose a directionality-aware module to capture the texture variations at each direction, resulting in spatially invariant representation. To segment new categories given only few examples, we incorporate a self-gating mechanism into relation network to exploit global context information for adjusting per-channel modulation weights of local relation features. Extensive experiments on benchmark texture datasets and real scenarios demonstrate the above-par segmentation performance and robust generalization across domains of our proposed method.



### Vision-based Robotic Grasping From Object Localization, Object Pose Estimation to Grasp Estimation for Parallel Grippers: A Review
- **Arxiv ID**: http://arxiv.org/abs/1905.06658v4
- **DOI**: 10.1007/s10462-020-09888-5
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06658v4)
- **Published**: 2019-05-16 11:12:01+00:00
- **Updated**: 2020-10-26 03:03:20+00:00
- **Authors**: Guoguang Du, Kai Wang, Shiguo Lian, Kaiyong Zhao
- **Comment**: This is a pre-print of an article published in Artificial
  Intelligence Review. The final authenticated version is available online at:
  https://doi.org/10.1007/s10462-020-09888-5. Related refs are summarized at:
  https://github.com/GeorgeDu/vision-based-robotic-grasping
- **Journal**: None
- **Summary**: This paper presents a comprehensive survey on vision-based robotic grasping. We conclude three key tasks during vision-based robotic grasping, which are object localization, object pose estimation and grasp estimation. In detail, the object localization task contains object localization without classification, object detection and object instance segmentation. This task provides the regions of the target object in the input data. The object pose estimation task mainly refers to estimating the 6D object pose and includes correspondence-based methods, template-based methods and voting-based methods, which affords the generation of grasp poses for known objects. The grasp estimation task includes 2D planar grasp methods and 6DoF grasp methods, where the former is constrained to grasp from one direction. These three tasks could accomplish the robotic grasping with different combinations. Lots of object pose estimation methods need not object localization, and they conduct object localization and object pose estimation jointly. Lots of grasp estimation methods need not object localization and object pose estimation, and they conduct grasp estimation in an end-to-end manner. Both traditional methods and latest deep learning-based methods based on the RGB-D image inputs are reviewed elaborately in this survey. Related datasets and comparisons between state-of-the-art methods are summarized as well. In addition, challenges about vision-based robotic grasping and future directions in addressing these challenges are also pointed out.



### Uneven illumination surface defects inspection based on convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1905.06683v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06683v3)
- **Published**: 2019-05-16 12:18:42+00:00
- **Updated**: 2023-07-15 03:33:32+00:00
- **Authors**: Hao Wu, Yulong Liu, Wenbin Gao, Xiangrong Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Surface defect inspection based on machine vision is often affected by uneven illumination. In order to improve the inspection rate of surface defects inspection under uneven illumination condition, this paper proposes a method for detecting surface image defects based on convolutional neural network, which is based on the adjustment of convolutional neural networks, training parameters, changing the structure of the network, to achieve the purpose of accurately identifying various defects. Experimental on defect inspection of copper strip and steel images shows that the convolutional neural network can automatically learn features without preprocessing the image, and correct identification of various types of image defects affected by uneven illumination, thus overcoming the drawbacks of traditional machine vision inspection methods under uneven illumination.



### RGB-T Image Saliency Detection via Collaborative Graph Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.06741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06741v1)
- **Published**: 2019-05-16 13:35:21+00:00
- **Updated**: 2019-05-16 13:35:21+00:00
- **Authors**: Zhengzheng Tu, Tian Xia, Chenglong Li, Xiaoxiao Wang, Yan Ma, Jin Tang
- **Comment**: 14 pages, 14 figures, 7 tables, accepted by IEEE Transactions on
  Multimedia with minor revisions
- **Journal**: None
- **Summary**: Image saliency detection is an active research topic in the community of computer vision and multimedia. Fusing complementary RGB and thermal infrared data has been proven to be effective for image saliency detection. In this paper, we propose an effective approach for RGB-T image saliency detection. Our approach relies on a novel collaborative graph learning algorithm. In particular, we take superpixels as graph nodes, and collaboratively use hierarchical deep features to jointly learn graph affinity and node saliency in a unified optimization framework. Moreover, we contribute a more challenging dataset for the purpose of RGB-T image saliency detection, which contains 1000 spatially aligned RGB-T image pairs and their ground truth annotations. Extensive experiments on the public dataset and the newly created dataset suggest that the proposed approach performs favorably against the state-of-the-art RGB-T saliency detection methods.



### Inductive Guided Filter: Real-time Deep Image Matting with Weakly Annotated Masks on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1905.06747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06747v1)
- **Published**: 2019-05-16 13:39:41+00:00
- **Updated**: 2019-05-16 13:39:41+00:00
- **Authors**: Yaoyi Li, Jianfu Zhang, Weijie Zhao, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, significant progress has been achieved in deep image matting. Most of the classical image matting methods are time-consuming and require an ideal trimap which is difficult to attain in practice. A high efficient image matting method based on a weakly annotated mask is in demand for mobile applications. In this paper, we propose a novel method based on Deep Learning and Guided Filter, called Inductive Guided Filter, which can tackle the real-time general image matting task on mobile devices. We design a lightweight hourglass network to parameterize the original Guided Filter method that takes an image and a weakly annotated mask as input. Further, the use of Gabor loss is proposed for training networks for complicated textures in image matting. Moreover, we create an image matting dataset MAT-2793 with a variety of foreground objects. Experimental results demonstrate that our proposed method massively reduces running time with robust accuracy.



### Stroke extraction for offline handwritten mathematical expression recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.06749v2
- **DOI**: 10.1109/ACCESS.2020.2984627
- **Categories**: **cs.CV**, 68T10 (Primary) 68T45, 68U10 (Secondary), I.7.5
- **Links**: [PDF](http://arxiv.org/pdf/1905.06749v2)
- **Published**: 2019-05-16 13:40:43+00:00
- **Updated**: 2020-01-16 14:08:58+00:00
- **Authors**: Chungkwong Chan
- **Comment**: 22 pages, 7 figures
- **Journal**: IEEE Access, vol. 8, pp. 61565-61575, 2020
- **Summary**: Offline handwritten mathematical expression recognition is often considered much harder than its online counterpart due to the absence of temporal information. In order to take advantage of the more mature methods for online recognition and save resources, an oversegmentation approach is proposed to recover strokes from textual bitmap images automatically. The proposed algorithm first breaks down the skeleton of a binarized image into junctions and segments, then segments are merged to form strokes, finally stroke order is normalized by using recursive projection and topological sort. Good offline accuracy was obtained in combination with ordinary online recognizers, which are not specially designed for extracted strokes. Given a ready-made state-of-the-art online handwritten mathematical expression recognizer, the proposed procedure correctly recognized 58.22%, 65.65%, and 65.22% of the offline formulas rendered from the datasets of the Competitions on Recognition of Online Handwritten Mathematical Expressions(CROHME) in 2014, 2016, and 2019 respectively. Furthermore, given a trainable online recognition system, retraining it with extracted strokes resulted in an offline recognizer with the same level of accuracy. On the other hand, the speed of the entire pipeline was fast enough to facilitate on-device recognition on mobile phones with limited resources. To conclude, stroke extraction provides an attractive way to build optical character recognition software.



### Learning Visually Consistent Label Embeddings for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.06764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06764v1)
- **Published**: 2019-05-16 14:10:20+00:00
- **Updated**: 2019-05-16 14:10:20+00:00
- **Authors**: Berkan Demirel, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis
- **Comment**: To appear at IEEE Int. Conference on Image Processing (ICIP) 2019
- **Journal**: None
- **Summary**: In this work, we propose a zero-shot learning method to effectively model knowledge transfer between classes via jointly learning visually consistent word vectors and label embedding model in an end-to-end manner. The main idea is to project the vector space word vectors of attributes and classes into the visual space such that word representations of semantically related classes become more closer, and use the projected vectors in the proposed embedding model to identify unseen classes. We evaluate the proposed approach on two benchmark datasets and the experimental results show that our method yields significant improvements in recognition accuracy.



### Richly Activated Graph Convolutional Network for Action Recognition with Incomplete Skeletons
- **Arxiv ID**: http://arxiv.org/abs/1905.06774v2
- **DOI**: 10.1109/ICIP.2019.8802917
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06774v2)
- **Published**: 2019-05-16 14:22:07+00:00
- **Updated**: 2019-05-17 01:31:03+00:00
- **Authors**: Yi-Fan Song, Zhang Zhang, Liang Wang
- **Comment**: Accepted by ICIP 2019, 5 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: Current methods for skeleton-based human action recognition usually work with completely observed skeletons. However, in real scenarios, it is prone to capture incomplete and noisy skeletons, which will deteriorate the performance of traditional models. To enhance the robustness of action recognition models to incomplete skeletons, we propose a multi-stream graph convolutional network (GCN) for exploring sufficient discriminative features distributed over all skeleton joints. Here, each stream of the network is only responsible for learning features from currently unactivated joints, which are distinguished by the class activation maps (CAM) obtained by preceding streams, so that the activated joints of the proposed method are obviously more than traditional methods. Thus, the proposed method is termed richly activated GCN (RA-GCN), where the richly discovered features will improve the robustness of the model. Compared to the state-of-the-art methods, the RA-GCN achieves comparable performance on the NTU RGB+D dataset. Moreover, on a synthetic occlusion dataset, the performance deterioration can be alleviated by the RA-GCN significantly.



### Harvesting Information from Captions for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.06784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06784v1)
- **Published**: 2019-05-16 14:35:09+00:00
- **Updated**: 2019-05-16 14:35:09+00:00
- **Authors**: Johann Sawatzky, Debayan Banerjee, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: Since acquiring pixel-wise annotations for training convolutional neural networks for semantic image segmentation is time-consuming, weakly supervised approaches that only require class tags have been proposed. In this work, we propose another form of supervision, namely image captions as they can be found on the Internet. These captions have two advantages. They do not require additional curation as it is the case for the clean class tags used by current weakly supervised approaches and they provide textual context for the classes present in an image. To leverage such textual context, we deploy a multi-modal network that learns a joint embedding of the visual representation of the image and the textual representation of the caption. The network estimates text activation maps (TAMs) for class names as well as compound concepts, i.e. combinations of nouns and their attributes. The TAMs of compound concepts describing classes of interest substantially improve the quality of the estimated class activation maps which are then used to train a network for semantic segmentation. We evaluate our method on the COCO dataset where it achieves state of the art results for weakly supervised image segmentation.



### How is Gaze Influenced by Image Transformations? Dataset and Model
- **Arxiv ID**: http://arxiv.org/abs/1905.06803v4
- **DOI**: 10.1109/TIP.2019.2945857
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06803v4)
- **Published**: 2019-05-16 14:48:29+00:00
- **Updated**: 2019-10-03 16:10:46+00:00
- **Authors**: Zhaohui Che, Ali Borji, Guangtao Zhai, Xiongkuo Min, Guodong Guo, Patrick Le Callet
- **Comment**: None
- **Journal**: None
- **Summary**: Data size is the bottleneck for developing deep saliency models, because collecting eye-movement data is very time consuming and expensive. Most of current studies on human attention and saliency modeling have used high quality stereotype stimuli. In real world, however, captured images undergo various types of transformations. Can we use these transformations to augment existing saliency datasets? Here, we first create a novel saliency dataset including fixations of 10 observers over 1900 images degraded by 19 types of transformations. Second, by analyzing eye movements, we find that observers look at different locations over transformed versus original images. Third, we utilize the new data over transformed images, called data augmentation transformation (DAT), to train deep saliency models. We find that label preserving DATs with negligible impact on human gaze boost saliency prediction, whereas some other DATs that severely impact human gaze degrade the performance. These label preserving valid augmentation transformations provide a solution to enlarge existing saliency datasets. Finally, we introduce a novel saliency model based on generative adversarial network (dubbed GazeGAN). A modified UNet is proposed as the generator of the GazeGAN, which combines classic skip connections with a novel center-surround connection (CSC), in order to leverage multi level features. We also propose a histogram loss based on Alternative Chi Square Distance (ACS HistLoss) to refine the saliency map in terms of luminance distribution. Extensive experiments and comparisons over 3 datasets indicate that GazeGAN achieves the best performance in terms of popular saliency evaluation metrics, and is more robust to various perturbations. Our code and data are available at: https://github.com/CZHQuality/Sal-CFS-GAN.



### Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision
- **Arxiv ID**: http://arxiv.org/abs/1905.06817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06817v1)
- **Published**: 2019-05-16 14:57:45+00:00
- **Updated**: 2019-05-16 14:57:45+00:00
- **Authors**: Soubhik Sanyal, Timo Bolkart, Haiwen Feng, Michael J. Black
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: The estimation of 3D face shape from a single image must be robust to variations in lighting, head pose, expression, facial hair, makeup, and occlusions. Robustness requires a large training set of in-the-wild images, which by construction, lack ground truth 3D shape. To train a network without any 2D-to-3D supervision, we present RingNet, which learns to compute 3D face shape from a single image. Our key observation is that an individual's face shape is constant across images, regardless of expression, pose, lighting, etc. RingNet leverages multiple images of a person and automatically detected 2D face features. It uses a novel loss that encourages the face shape to be similar when the identity is the same and different for different people. We achieve invariance to expression by representing the face using the FLAME model. Once trained, our method takes a single image and outputs the parameters of FLAME, which can be readily animated. Additionally we create a new database of faces `not quite in-the-wild' (NoW) with 3D head scans and high-resolution images of the subjects in a wide variety of conditions. We evaluate publicly available methods and find that RingNet is more accurate than methods that use 3D supervision. The dataset, model, and results are available for research purposes at http://ringnet.is.tuebingen.mpg.de.



### Dealing with Label Scarcity in Computational Pathology: A Use Case in Prostate Cancer Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.06820v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06820v1)
- **Published**: 2019-05-16 14:59:22+00:00
- **Updated**: 2019-05-16 14:59:22+00:00
- **Authors**: Koen Dercksen, Wouter Bulten, Geert Litjens
- **Comment**: 4 pages, 3 figures,MIDL 2019 [arXiv:1907.08612] extended abstract
- **Journal**: None
- **Summary**: Large amounts of unlabelled data are commonplace for many applications in computational pathology, whereas labelled data is often expensive, both in time and cost, to acquire. We investigate the performance of unsupervised and supervised deep learning methods when few labelled data are available. Three methods are compared: clustering autoencoder latent vectors (unsupervised), a single layer classifier combined with a pre-trained autoencoder (semi-supervised), and a supervised CNN. We apply these methods on hematoxylin and eosin (H&E) stained prostatectomy images to classify tumour versus non-tumour tissue. Results show that semi-/unsupervised methods have an advantage over supervised learning when few labels are available. Additionally, we show that incorporating immunohistochemistry (IHC) stained data provides an increase in performance over only using H&E.



### Derived Codebooks for High-Accuracy Nearest Neighbor Search
- **Arxiv ID**: http://arxiv.org/abs/1905.06900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/1905.06900v1)
- **Published**: 2019-05-16 16:47:30+00:00
- **Updated**: 2019-05-16 16:47:30+00:00
- **Authors**: Fabien Andr√©, Anne-Marie Kermarrec, Nicolas Le Scouarnec
- **Comment**: None
- **Journal**: None
- **Summary**: High-dimensional Nearest Neighbor (NN) search is central in multimedia search systems. Product Quantization (PQ) is a widespread NN search technique which has a high performance and good scalability. PQ compresses high-dimensional vectors into compact codes thanks to a combination of quantizers. Large databases can, therefore, be stored entirely in RAM, enabling fast responses to NN queries. In almost all cases, PQ uses 8-bit quantizers as they offer low response times. In this paper, we advocate the use of 16-bit quantizers. Compared to 8-bit quantizers, 16-bit quantizers boost accuracy but they increase response time by a factor of 3 to 10. We propose a novel approach that allows 16-bit quantizers to offer the same response time as 8-bit quantizers, while still providing a boost of accuracy. Our approach builds on two key ideas: (i) the construction of derived codebooks that allow a fast and approximate distance evaluation, and (ii) a two-pass NN search procedure which builds a candidate set using the derived codebooks, and then refines it using 16-bit quantizers. On 1 billion SIFT vectors, with an inverted index, our approach offers a Recall@100 of 0.85 in 5.2 ms. By contrast, 16-bit quantizers alone offer a Recall@100 of 0.85 in 39 ms, and 8-bit quantizers a Recall@100 of 0.82 in 3.8 ms.



### X2CT-GAN: Reconstructing CT from Biplanar X-Rays with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.06902v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.06902v1)
- **Published**: 2019-05-16 16:50:26+00:00
- **Updated**: 2019-05-16 16:50:26+00:00
- **Authors**: Xingde Ying, Heng Guo, Kai Ma, Jian Wu, Zhengxin Weng, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Computed tomography (CT) can provide a 3D view of the patient's internal organs, facilitating disease diagnosis, but it incurs more radiation dose to a patient and a CT scanner is much more cost prohibitive than an X-ray machine too. Traditional CT reconstruction methods require hundreds of X-ray projections through a full rotational scan of the body, which cannot be performed on a typical X-ray machine. In this work, we propose to reconstruct CT from two orthogonal X-rays using the generative adversarial network (GAN) framework. A specially designed generator network is exploited to increase data dimension from 2D (X-rays) to 3D (CT), which is not addressed in previous research of GAN. A novel feature fusion method is proposed to combine information from two X-rays.The mean squared error (MSE) loss and adversarial loss are combined to train the generator, resulting in a high-quality CT volume both visually and quantitatively. Extensive experiments on a publicly available chest CT dataset demonstrate the effectiveness of the proposed method. It could be a nice enhancement of a low-cost X-ray machine to provide physicians a CT-like 3D volume in several niche applications.



### Finding Rats in Cats: Detecting Stealthy Attacks using Group Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1905.07273v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07273v2)
- **Published**: 2019-05-16 17:16:52+00:00
- **Updated**: 2019-05-20 13:22:04+00:00
- **Authors**: Aditya Kuppa, Slawomir Grzonkowski, Muhammad Rizwan Asghar, Nhien-An Le-Khac
- **Comment**: Preprint: Modified, Extended Version will be presented at TrustCom
  2019
- **Journal**: None
- **Summary**: Advanced attack campaigns span across multiple stages and stay stealthy for long time periods. There is a growing trend of attackers using off-the-shelf tools and pre-installed system applications (such as \emph{powershell} and \emph{wmic}) to evade the detection because the same tools are also used by system administrators and security analysts for legitimate purposes for their routine tasks. To start investigations, event logs can be collected from operational systems; however, these logs are generic enough and it often becomes impossible to attribute a potential attack to a specific attack group. Recent approaches in the literature have used anomaly detection techniques, which aim at distinguishing between malicious and normal behavior of computers or network systems. Unfortunately, anomaly detection systems based on point anomalies are too rigid in a sense that they could miss the malicious activity and classify the attack, not an outlier. Therefore, there is a research challenge to make better detection of malicious activities. To address this challenge, in this paper, we leverage Group Anomaly Detection (GAD), which detects anomalous collections of individual data points.   Our approach is to build a neural network model utilizing Adversarial Autoencoder (AAE-$\alpha$) in order to detect the activity of an attacker who leverages off-the-shelf tools and system applications. In addition, we also build \textit{Behavior2Vec} and \textit{Command2Vec} sentence embedding deep learning models specific for feature extraction tasks. We conduct extensive experiments to evaluate our models on real-world datasets collected for a period of two months. The empirical results demonstrate that our approach is effective and robust in discovering targeted attacks, pen-tests, and attack campaigns leveraging custom tools.



### Fooling Computer Vision into Inferring the Wrong Body Mass Index
- **Arxiv ID**: http://arxiv.org/abs/1905.06916v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.06916v1)
- **Published**: 2019-05-16 17:29:08+00:00
- **Updated**: 2019-05-16 17:29:08+00:00
- **Authors**: Owen Levin, Zihang Meng, Vikas Singh, Xiaojin Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently it's been shown that neural networks can use images of human faces to accurately predict Body Mass Index (BMI), a widely used health indicator. In this paper we demonstrate that a neural network performing BMI inference is indeed vulnerable to test-time adversarial attacks. This extends test-time adversarial attacks from classification tasks to regression. The application we highlight is BMI inference in the insurance industry, where such adversarial attacks imply a danger of insurance fraud.



### Monocular Plan View Networks for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1905.06937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.06937v1)
- **Published**: 2019-05-16 17:56:33+00:00
- **Updated**: 2019-05-16 17:56:33+00:00
- **Authors**: Dequan Wang, Coline Devin, Qi-Zhi Cai, Philipp Kr√§henb√ºhl, Trevor Darrell
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: Convolutions on monocular dash cam videos capture spatial invariances in the image plane but do not explicitly reason about distances and depth. We propose a simple transformation of observations into a bird's eye view, also known as plan view, for end-to-end control. We detect vehicles and pedestrians in the first person view and project them into an overhead plan view. This representation provides an abstraction of the environment from which a deep network can easily deduce the positions and directions of entities. Additionally, the plan view enables us to leverage advances in 3D object detection in conjunction with deep policy learning. We evaluate our monocular plan view network on the photo-realistic Grand Theft Auto V simulator. A network using both a plan view and front view causes less than half as many collisions as previous detection-based methods and an order of magnitude fewer collisions than pure pixel-based policies.



### PoreNet: CNN-based Pore Descriptor for High-resolution Fingerprint Recognition
- **Arxiv ID**: http://arxiv.org/abs/1905.06981v2
- **DOI**: 10.1109/JSEN.2020.2987287
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.06981v2)
- **Published**: 2019-05-16 18:23:30+00:00
- **Updated**: 2019-08-29 06:55:07+00:00
- **Authors**: Vijay Anand, Vivek Kanhangad
- **Comment**: 7 pages, 4 figures, 6tables
- **Journal**: None
- **Summary**: With the development of high-resolution fingerprint scanners, high-resolution fingerprint-based biometric recognition has received increasing attention in recent years. This paper presents a pore feature-based approach for biometric recognition. Our approach employs a convolutional neural network (CNN) model, DeepResPore, to detect pores in the input fingerprint image. Thereafter, a CNN-based descriptor is computed for a patch around each detected pore. Specifically, we have designed a residual learning-based CNN, referred to as PoreNet that learns distinctive feature representation from pore patches. For verification, the match score is generated by comparing pore descriptors obtained from a pair of fingerprint images in bi-directional manner using the Euclidean distance. The proposed approach for high-resolution fingerprint recognition achieves 2.91% and 0.57% equal error rates (EERs) on partial (DBI) and complete (DBII) fingerprints of the benchmark PolyU HRF dataset. Most importantly, it achieves lower FMR1000 and FMR10000 values than the current state-of-the-art approach on both the datasets.



### How do neural networks see depth in single images?
- **Arxiv ID**: http://arxiv.org/abs/1905.07005v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.07005v1)
- **Published**: 2019-05-16 19:19:13+00:00
- **Updated**: 2019-05-16 19:19:13+00:00
- **Authors**: Tom van Dijk, Guido C. H. E. de Croon
- **Comment**: Submitted
- **Journal**: None
- **Summary**: Deep neural networks have lead to a breakthrough in depth estimation from single images. Recent work often focuses on the accuracy of the depth map, where an evaluation on a publicly available test set such as the KITTI vision benchmark is often the main result of the article. While such an evaluation shows how well neural networks can estimate depth, it does not show how they do this. To the best of our knowledge, no work currently exists that analyzes what these networks have learned.   In this work we take the MonoDepth network by Godard et al. and investigate what visual cues it exploits for depth estimation. We find that the network ignores the apparent size of known obstacles in favor of their vertical position in the image. Using the vertical position requires the camera pose to be known; however we find that MonoDepth only partially corrects for changes in camera pitch and roll and that these influence the estimated depth towards obstacles. We further show that MonoDepth's use of the vertical image position allows it to estimate the distance towards arbitrary obstacles, even those not appearing in the training set, but that it requires a strong edge at the ground contact point of the object to do so. In future work we will investigate whether these observations also apply to other neural networks for monocular depth estimation.



### Clustered Multitask Nonnegative Matrix Factorization for Spectral Unmixing of Hyperspectral Data
- **Arxiv ID**: http://arxiv.org/abs/1905.08032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.08032v1)
- **Published**: 2019-05-16 20:22:37+00:00
- **Updated**: 2019-05-16 20:22:37+00:00
- **Authors**: Sara Khoshsokhan, Roozbeh Rajabi, Hadi Zayyani
- **Comment**: one column, 22 pages, 12 figures, journal. arXiv admin note:
  substantial text overlap with arXiv:1902.07593, arXiv:1812.10788
- **Journal**: None
- **Summary**: In this paper, the new algorithm based on clustered multitask network is proposed to solve spectral unmixing problem in hyperspectral imagery. In the proposed algorithm, the clustered network is employed. Each pixel in the hyperspectral image considered as a node in this network. The nodes in the network are clustered using the fuzzy c-means clustering method. Diffusion least mean square strategy has been used to optimize the proposed cost function. To evaluate the proposed method, experiments are conducted on synthetic and real datasets. Simulation results based on spectral angle distance, abundance angle distance and reconstruction error metrics illustrate the advantage of the proposed algorithm compared with other methods.



### Fonts-2-Handwriting: A Seed-Augment-Train framework for universal digit classification
- **Arxiv ID**: http://arxiv.org/abs/1905.08633v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1905.08633v1)
- **Published**: 2019-05-16 20:38:05+00:00
- **Updated**: 2019-05-16 20:38:05+00:00
- **Authors**: Vinay Uday Prabhu, Sanghyun Han, Dian Ang Yap, Mihail Douhaniaris, Preethi Seshadri, John Whaley
- **Comment**: Published as a workshop paper at ICLR 2019 (DeepGenStruct-2019)
- **Journal**: None
- **Summary**: In this paper, we propose a Seed-Augment-Train/Transfer (SAT) framework that contains a synthetic seed image dataset generation procedure for languages with different numeral systems using freely available open font file datasets. This seed dataset of images is then augmented to create a purely synthetic training dataset, which is in turn used to train a deep neural network and test on held-out real world handwritten digits dataset spanning five Indic scripts, Kannada, Tamil, Gujarati, Malayalam, and Devanagari. We showcase the efficacy of this approach both qualitatively, by training a Boundary-seeking GAN (BGAN) that generates realistic digit images in the five languages, and also quantitatively by testing a CNN trained on the synthetic data on the real-world datasets. This establishes not only an interesting nexus between the font-datasets-world and transfer learning but also provides a recipe for universal-digit classification in any script.



### GlidarCo: gait recognition by 3D skeleton estimation and biometric feature correction of flash lidar data
- **Arxiv ID**: http://arxiv.org/abs/1905.07058v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.07058v2)
- **Published**: 2019-05-16 23:22:16+00:00
- **Updated**: 2019-05-20 14:18:48+00:00
- **Authors**: Nasrin Sadeghzadehyazdi, Tamal Batabyal, Nibir K. Dhar, B. O. Familoni, K. M. Iftekharuddin, Scott T. Acton
- **Comment**: None
- **Journal**: None
- **Summary**: Gait recognition using noninvasively acquired data has been attracting an increasing interest in the last decade. Among various modalities of data sources, it is experimentally found that the data involving skeletal representation are amenable for reliable feature compaction and fast processing. Model-based gait recognition methods that exploit features from a fitted model, like skeleton, are recognized for their view and scale-invariant properties. We propose a model-based gait recognition method, using sequences recorded by a single flash lidar. Existing state-of-the-art model-based approaches that exploit features from high quality skeletal data collected by Kinect and Mocap are limited to controlled laboratory environments. The performance of conventional research efforts is negatively affected by poor data quality. We address the problem of gait recognition under challenging scenarios, such as lower quality and noisy imaging process of lidar, that degrades the performance of state-of-the-art skeleton-based systems. We present GlidarCo to attain high accuracy on gait recognition under the described conditions. A filtering mechanism corrects faulty skeleton joint measurements, and robust statistics are integrated to conventional feature moments to encode the dynamic of the motion. As a comparison, length-based and vector-based features extracted from the noisy skeletons are investigated for outlier removal. Experimental results illustrate the efficacy of the proposed methodology in improving gait recognition given noisy low resolution lidar data.



### Non-Parametric Priors For Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.07061v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.07061v1)
- **Published**: 2019-05-16 23:31:20+00:00
- **Updated**: 2019-05-16 23:31:20+00:00
- **Authors**: Rajhans Singh, Pavan Turaga, Suren Jayasuriya, Ravi Garg, Martin W. Braun
- **Comment**: None
- **Journal**: International Conference on Machine Learning (2019)
- **Summary**: The advent of generative adversarial networks (GAN) has enabled new capabilities in synthesis, interpolation, and data augmentation heretofore considered very challenging. However, one of the common assumptions in most GAN architectures is the assumption of simple parametric latent-space distributions. While easy to implement, a simple latent-space distribution can be problematic for uses such as interpolation. This is due to distributional mismatches when samples are interpolated in the latent space. We present a straightforward formalization of this problem; using basic results from probability theory and off-the-shelf-optimization tools, we develop ways to arrive at appropriate non-parametric priors. The obtained prior exhibits unusual qualitative properties in terms of its shape, and quantitative benefits in terms of lower divergence with its mid-point distribution. We demonstrate that our designed prior helps improve image generation along any Euclidean straight line during interpolation, both qualitatively and quantitatively, without any additional training or architectural modifications. The proposed formulation is quite flexible, paving the way to impose newer constraints on the latent-space statistics.




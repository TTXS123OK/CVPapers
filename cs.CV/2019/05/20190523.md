# Arxiv Papers in cs.CV on 2019-05-23
### Depth Estimation on Underwater Omni-directional Images Using a Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1905.09441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09441v1)
- **Published**: 2019-05-23 02:58:32+00:00
- **Updated**: 2019-05-23 02:58:32+00:00
- **Authors**: Haofei Kuang, Qingwen Xu, Sören Schwertfeger
- **Comment**: 7 pages, 8 figures, 1 table, accepted by 2019 ICRA workshop
  "Underwater Robotics Perception"
- **Journal**: None
- **Summary**: In this work, we exploit a depth estimation Fully Convolutional Residual Neural Network (FCRN) for in-air perspective images to estimate the depth of underwater perspective and omni-directional images. We train one conventional and one spherical FCRN for underwater perspective and omni-directional images, respectively. The spherical FCRN is derived from the perspective FCRN via a spherical longitude-latitude mapping. For that, the omni-directional camera is modeled as a sphere, while images captured by it are displayed in the longitude-latitude form. Due to the lack of underwater datasets, we synthesize images in both data-driven and theoretical ways, which are used in training and testing. Finally, experiments are conducted on these synthetic images and results are displayed in both qualitative and quantitative way. The comparison between ground truth and the estimated depth map indicates the effectiveness of our method.



### Variational Prototype Replays for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.09447v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.09447v3)
- **Published**: 2019-05-23 03:25:33+00:00
- **Updated**: 2020-02-15 05:54:07+00:00
- **Authors**: Mengmi Zhang, Tao Wang, Joo Hwee Lim, Gabriel Kreiman, Jiashi Feng
- **Comment**: under submission
- **Journal**: None
- **Summary**: Continual learning refers to the ability to acquire and transfer knowledge without catastrophically forgetting what was previously learned. In this work, we consider \emph{few-shot} continual learning in classification tasks, and we propose a novel method, Variational Prototype Replays, that efficiently consolidates and recalls previous knowledge to avoid catastrophic forgetting. In each classification task, our method learns a set of variational prototypes with their means and variances, where embedding of the samples from the same class can be represented in a prototypical distribution and class-representative prototypes are separated apart. To alleviate catastrophic forgetting, our method replays one sample per class from previous tasks, and correspondingly matches newly predicted embeddings to their nearest class-representative prototypes stored from previous tasks. Compared with recent continual learning approaches, our method can readily adapt to new tasks with more classes without requiring the addition of new units. Furthermore, our method is more memory efficient since only class-representative prototypes with their means and variances, as well as only one sample per class from previous tasks need to be stored. Without tampering with the performance on initial tasks, our method learns novel concepts given a few training examples of each class in new tasks.



### Constrained Design of Deep Iris Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.09481v1
- **DOI**: 10.1109/TIP.2020.2999211
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09481v1)
- **Published**: 2019-05-23 05:24:53+00:00
- **Updated**: 2019-05-23 05:24:53+00:00
- **Authors**: Kien Nguyen, Clinton Fookes, Sridha Sridharan
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the promise of recent deep neural networks in the iris recognition setting, there are vital properties of the classic IrisCode which are almost unable to be achieved with current deep iris networks: the compactness of model and the small number of computing operations (FLOPs). This paper re-models the iris network design process as a constrained optimization problem which takes model size and computation into account as learning criteria. On one hand, this allows us to fully automate the network design process to search for the best iris network confined to the computation and model compactness constraints. On the other hand, it allows us to investigate the optimality of the classic IrisCode and recent iris networks. It also allows us to learn an optimal iris network and demonstrate state-of-the-art performance with less computation and memory requirements.



### Pose estimator and tracker using temporal flow maps for limbs
- **Arxiv ID**: http://arxiv.org/abs/1905.09500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09500v1)
- **Published**: 2019-05-23 07:08:44+00:00
- **Updated**: 2019-05-23 07:08:44+00:00
- **Authors**: Jihye Hwang, Jieun Lee, Sungheon Park, Nojun Kwak
- **Comment**: Won the Honorable Mention Award in the 18'ECCV PoseTrack challenge.
  Accepted in the 19'IJCNN conference
- **Journal**: None
- **Summary**: For human pose estimation in videos, it is significant how to use temporal information between frames. In this paper, we propose temporal flow maps for limbs (TML) and a multi-stride method to estimate and track human poses. The proposed temporal flow maps are unit vectors describing the limbs' movements. We constructed a network to learn both spatial information and temporal information end-to-end. Spatial information such as joint heatmaps and part affinity fields is regressed in the spatial network part, and the TML is regressed in the temporal network part. We also propose a data augmentation method to learn various types of TML better. The proposed multi-stride method expands the data by randomly selecting two frames within a defined range. We demonstrate that the proposed method efficiently estimates and tracks human poses on the PoseTrack 2017 and 2018 datasets.



### Hierarchical Annotation of Images with Two-Alternative-Forced-Choice Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1905.09523v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.09523v2)
- **Published**: 2019-05-23 08:11:15+00:00
- **Updated**: 2019-06-05 13:55:18+00:00
- **Authors**: Niels Hellinga, Vlado Menkovski
- **Comment**: presented at 2019 ICML Workshop on Human in the Loop Learning (HILL
  2019), Long Beach, USA
- **Journal**: None
- **Summary**: Many tasks such as retrieval and recommendations can significantly benefit from structuring the data, commonly in a hierarchical way. To achieve this through annotations of high dimensional data such as images or natural text can be significantly labor intensive. We propose an approach for uncovering the hierarchical structure of data based on efficient discriminative testing rather than annotations of individual datapoints. Using two-alternative-forced-choice (2AFC) testing and deep metric learning we achieve embedding of the data in semantic space where we are able to successfully hierarchically cluster. We actively select triplets for the 2AFC test such that the modeling process is highly efficient with respect to the number of tests presented to the annotator. We empirically demonstrate the feasibility of the method by confirming the shape bias on synthetic data and extract hierarchical structure on the Fashion-MNIST dataset to a finer granularity than the original labels.



### Convolutional Restricted Boltzmann Machine Based-Radiomics for Prediction of Pathological Complete Response to Neoadjuvant Chemotherapy in Breast Cancer
- **Arxiv ID**: http://arxiv.org/abs/1905.13312v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.13312v1)
- **Published**: 2019-05-23 08:17:08+00:00
- **Updated**: 2019-05-23 08:17:08+00:00
- **Authors**: Li Wang, Lihui Wang, Qijian Chen, Caixia Sun, Xinyu Cheng, Yuemin Zhu
- **Comment**: 8 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: We proposed a novel convolutional restricted Boltzmann machine CRBM-based radiomic method for predicting pathologic complete response (pCR) to neoadjuvant chemotherapy treatment (NACT) in breast cancer. The method consists of extracting semantic features from CRBM network, and pCR prediction. It was evaluated on the dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) data of 57 patients and using the area under the receiver operating characteristic curve (AUC). Traditional radiomics features and the semantic features learned from CRBM network were extracted from the images acquired before and after the administration of NACT. After the feature selection, the support vector machine (SVM), logistic regression (LR) and random forest (RF) were trained to predict the pCR status. Compared to traditional radiomic methods, the proposed CRBM-based radiomic method yielded an AUC of 0.92 for the prediction with the images acquired before and after NACT, and an AUC of 0.87 for the pretreatment prediction, which was increased by about 38%. The results showed that the CRBM-based radiomic method provided a potential means for accurately predicting the pCR to NACT in breast cancer before the treatment, which is very useful for making more appropriate and personalized treatment regimens.



### Incorporating Human Domain Knowledge in 3D LiDAR-based Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.09533v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09533v1)
- **Published**: 2019-05-23 08:41:35+00:00
- **Updated**: 2019-05-23 08:41:35+00:00
- **Authors**: Jilin Mei, Huijing Zhao
- **Comment**: 8 Pages
- **Journal**: None
- **Summary**: This work studies semantic segmentation using 3D LiDAR data. Popular deep learning methods applied for this task require a large number of manual annotations to train the parameters. We propose a new method that makes full use of the advantages of traditional methods and deep learning methods via incorporating human domain knowledge into the neural network model to reduce the demand for large numbers of manual annotations and improve the training efficiency. We first pretrain a model with autogenerated samples from a rule-based classifier so that human knowledge can be propagated into the network. Based on the pretrained model, only a small set of annotations is required for further fine-tuning. Quantitative experiments show that the pretrained model achieves better performance than random initialization in almost all cases; furthermore, our method can achieve similar performance with fewer manual annotations.



### Implicit Background Estimation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1905.13306v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13306v1)
- **Published**: 2019-05-23 09:20:52+00:00
- **Updated**: 2019-05-23 09:20:52+00:00
- **Authors**: Charles Lehman, Dogancan Temel, Ghassan AlRegib
- **Comment**: None
- **Journal**: None
- **Summary**: Scene understanding and semantic segmentation are at the core of many computer vision tasks, many of which, involve interacting with humans in potentially dangerous ways. It is therefore paramount that techniques for principled design of robust models be developed. In this paper, we provide analytic and empirical evidence that correcting potentially errant non-distinct mappings that result from the softmax function can result in improving robustness characteristics on a state-of-the-art semantic segmentation model with minimal impact to performance and minimal changes to the code base.



### Countering Noisy Labels By Learning From Auxiliary Clean Labels
- **Arxiv ID**: http://arxiv.org/abs/1905.13305v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13305v2)
- **Published**: 2019-05-23 10:20:17+00:00
- **Updated**: 2019-09-12 06:20:03+00:00
- **Authors**: Tsung Wei Tsai, Chongxuan Li, Jun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the learning from noisy labels (NL) problem which emerges in many real-world applications. In addition to the widely-studied synthetic noise in the NL literature, we also consider the pseudo labels in semi-supervised learning (Semi-SL) as a special case of NL. For both types of noise, we argue that the generalization performance of existing methods is highly coupled with the quality of noisy labels. Therefore, we counter the problem from a novel and unified perspective: learning from the auxiliary clean labels. Specifically, we propose the Rotational-Decoupling Consistency Regularization (RDCR) framework that integrates the consistency-based methods with the self-supervised rotation task to learn noise-tolerant representations. The experiments show that RDCR achieves comparable or superior performance than the state-of-the-art methods under small noise, while outperforms the existing methods significantly when there is large noise.



### Underwater Stereo using Refraction-free Image Synthesized from Light Field Camera
- **Arxiv ID**: http://arxiv.org/abs/1905.09588v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09588v1)
- **Published**: 2019-05-23 11:12:12+00:00
- **Updated**: 2019-05-23 11:12:12+00:00
- **Authors**: Kazuto Ichimaru, Hiroshi Kawasaki
- **Comment**: Accepted in 2019 IEEE International Conference on Image Processing
  (ICIP)
- **Journal**: None
- **Summary**: There is a strong demand on capturing underwater scenes without distortions caused by refraction. Since a light field camera can capture several light rays at each point of an image plane from various directions, if geometrically correct rays are chosen, it is possible to synthesize a refraction-free image. In this paper, we propose a novel technique to efficiently select such rays to synthesize a refraction-free image from an underwater image captured by a light field camera. In addition, we propose a stereo technique to reconstruct 3D shapes using a pair of our refraction-free images, which are central projection. In the experiment, we captured several underwater scenes by two light field cameras, synthesized refraction free images and applied stereo technique to reconstruct 3D shapes. The results are compared with previous techniques which are based on approximation, showing the strength of our method.



### A Direct Approach to Robust Deep Learning Using Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.09591v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.09591v1)
- **Published**: 2019-05-23 11:32:28+00:00
- **Updated**: 2019-05-23 11:32:28+00:00
- **Authors**: Huaxia Wang, Chun-Nam Yu
- **Comment**: 15 pages
- **Journal**: ICLR 2019
- **Summary**: Deep neural networks have been shown to perform well in many classical machine learning problems, especially in image classification tasks. However, researchers have found that neural networks can be easily fooled, and they are surprisingly sensitive to small perturbations imperceptible to humans. Carefully crafted input images (adversarial examples) can force a well-trained neural network to provide arbitrary outputs. Including adversarial examples during training is a popular defense mechanism against adversarial attacks. In this paper we propose a new defensive mechanism under the generative adversarial network (GAN) framework. We model the adversarial noise using a generative network, trained jointly with a classification discriminative network as a minimax game. We show empirically that our adversarial network approach works well against black box attacks, with performance on par with state-of-art methods such as ensemble adversarial training and adversarial training with projected gradient descent.



### CUDA-Self-Organizing feature map based visual sentiment analysis of bank customer complaints for Analytical CRM
- **Arxiv ID**: http://arxiv.org/abs/1905.09598v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, 68T99
- **Links**: [PDF](http://arxiv.org/pdf/1905.09598v1)
- **Published**: 2019-05-23 11:49:48+00:00
- **Updated**: 2019-05-23 11:49:48+00:00
- **Authors**: Rohit Gavval, Vadlamani Ravi, Kalavala Revanth Harshal, Akhilesh Gangwar, Kumar Ravi
- **Comment**: None
- **Journal**: None
- **Summary**: With the widespread use of social media, companies now have access to a wealth of customer feedback data which has valuable applications to Customer Relationship Management (CRM). Analyzing customer grievances data, is paramount as their speedy non-redressal would lead to customer churn resulting in lower profitability. In this paper, we propose a descriptive analytics framework using Self-organizing feature map (SOM), for Visual Sentiment Analysis of customer complaints. The network learns the inherent grouping of the complaints automatically which can then be visualized too using various techniques. Analytical Customer Relationship Management (ACRM) executives can draw useful business insights from the maps and take timely remedial action. We also propose a high-performance version of the algorithm CUDASOM (CUDA based Self Organizing feature Map) implemented using NVIDIA parallel computing platform, CUDA, which speeds up the processing of high-dimensional text data and generates fast results. The efficacy of the proposed model has been demonstrated on the customer complaints data regarding the products and services of four leading Indian banks. CUDASOM achieved an average speed up of 44 times. Our approach can expand research into intelligent grievance redressal system to provide rapid solutions to the complaining customers.



### A model of brain morphological changes related to aging and Alzheimer's disease from cross-sectional assessments
- **Arxiv ID**: http://arxiv.org/abs/1905.09826v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1905.09826v1)
- **Published**: 2019-05-23 12:26:30+00:00
- **Updated**: 2019-05-23 12:26:30+00:00
- **Authors**: Raphaël Sivera, Hervé Delingette, Marco Lorenzi, Xavier Pennec, Nicholas Ayache
- **Comment**: NeuroImage, Elsevier, In press
- **Journal**: None
- **Summary**: In this study we propose a deformation-based framework to jointly model the influence of aging and Alzheimer's disease (AD) on the brain morphological evolution. Our approach combines a spatio-temporal description of both processes into a generative model. A reference morphology is deformed along specific trajectories to match subject specific morphologies. It is used to define two imaging progression markers: 1) a morphological age and 2) a disease score. These markers can be computed locally in any brain region. The approach is evaluated on brain structural magnetic resonance images (MRI) from the ADNI database. The generative model is first estimated on a control population, then, for each subject, the markers are computed for each acquisition. The longitudinal evolution of these markers is then studied in relation with the clinical diagnosis of the subjects and used to generate possible morphological evolution. In the model, the morphological changes associated with normal aging are mainly found around the ventricles, while the Alzheimer's disease specific changes are more located in the temporal lobe and the hippocampal area. The statistical analysis of these markers highlights differences between clinical conditions even though the inter-subject variability is quiet high. In this context, the model can be used to generate plausible morphological trajectories associated with the disease. Our method gives two interpretable scalar imaging biomarkers assessing the effects of aging and disease on brain morphology at the individual and population level. These markers confirm an acceleration of apparent aging for Alzheimer's subjects and can help discriminate clinical conditions even in prodromal stages. More generally, the joint modeling of normal and pathological evolutions shows promising results to describe age-related brain diseases over long time scales.



### Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1905.09634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1905.09634v1)
- **Published**: 2019-05-23 13:10:21+00:00
- **Updated**: 2019-05-23 13:10:21+00:00
- **Authors**: Ziquan Lan, Zi Jian Yew, Gim Hee Lee
- **Comment**: CVPR 2019, 8 pages, 5 figures
- **Journal**: None
- **Summary**: Outlier feature matches and loop-closures that survived front-end data association can lead to catastrophic failures in the back-end optimization of large-scale point cloud based 3D reconstruction. To alleviate this problem, we propose a probabilistic approach for robust back-end optimization in the presence of outliers. More specifically, we model the problem as a Bayesian network and solve it using the Expectation-Maximization algorithm. Our approach leverages on a long-tail Cauchy distribution to suppress outlier feature matches in the odometry constraints, and a Cauchy-Uniform mixture model with a set of binary latent variables to simultaneously suppress outlier loop-closure constraints and outlier feature matches in the inlier loop-closure constraints. Furthermore, we show that by using a Gaussian-Uniform mixture model, our approach degenerates to the formulation of a state-of-the-art approach for robust indoor reconstruction. Experimental results demonstrate that our approach has comparable performance with the state-of-the-art on a benchmark indoor dataset, and outperforms it on a large-scale outdoor dataset. Our source code can be found on the project website.



### Image Fusion via Sparse Regularization with Non-Convex Penalties
- **Arxiv ID**: http://arxiv.org/abs/1905.09645v3
- **DOI**: 10.1016/j.patrec.2020.01.020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09645v3)
- **Published**: 2019-05-23 13:33:26+00:00
- **Updated**: 2020-01-29 18:58:12+00:00
- **Authors**: Nantheera Anantrasirichai, Rencheng Zheng, Ivan Selesnick, Alin Achim
- **Comment**: None
- **Journal**: None
- **Summary**: The L1 norm regularized least squares method is often used for finding sparse approximate solutions and is widely used in 1-D signal restoration. Basis pursuit denoising (BPD) performs noise reduction in this way. However, the shortcoming of using L1 norm regularization is the underestimation of the true solution. Recently, a class of non-convex penalties have been proposed to improve this situation. This kind of penalty function is non-convex itself, but preserves the convexity property of the whole cost function. This approach has been confirmed to offer good performance in 1-D signal denoising. This paper demonstrates the aforementioned method to 2-D signals (images) and applies it to multisensor image fusion. The problem is posed as an inverse one and a corresponding cost function is judiciously designed to include two data attachment terms. The whole cost function is proved to be convex upon suitably choosing the non-convex penalty, so that the cost function minimization can be tackled by convex optimization approaches, which comprise simple computations. The performance of the proposed method is benchmarked against a number of state-of-the-art image fusion techniques and superior performance is demonstrated both visually and in terms of various assessment measures.



### Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.09646v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09646v2)
- **Published**: 2019-05-23 13:36:54+00:00
- **Updated**: 2019-05-25 08:07:40+00:00
- **Authors**: Xiang Li, Xiaolin Hu, Jian Yang
- **Comment**: Code available at: https://github.com/implus/PytorchInsight
- **Journal**: None
- **Summary**: The Convolutional Neural Networks (CNNs) generate the feature representation of complex objects by collecting hierarchical and different parts of semantic sub-features. These sub-features can usually be distributed in grouped form in the feature vector of each layer, representing various semantic entities. However, the activation of these sub-features is often spatially affected by similar patterns and noisy backgrounds, resulting in erroneous localization and identification. We propose a Spatial Group-wise Enhance (SGE) module that can adjust the importance of each sub-feature by generating an attention factor for each spatial location in each semantic group, so that every individual group can autonomously enhance its learnt expression and suppress possible noise. The attention factors are only guided by the similarities between the global and local feature descriptors inside each group, thus the design of SGE module is extremely lightweight with \emph{almost no extra parameters and calculations}. Despite being trained with only category supervisions, the SGE component is extremely effective in highlighting multiple active areas with various high-order semantics (such as the dog's eyes, nose, etc.). When integrated with popular CNN backbones, SGE can significantly boost the performance of image recognition tasks. Specifically, based on ResNet50 backbones, SGE achieves 1.2\% Top-1 accuracy improvement on the ImageNet benchmark and 1.0$\sim$2.0\% AP gain on the COCO benchmark across a wide range of detectors (Faster/Mask/Cascade RCNN and RetinaNet). Codes and pretrained models are available at https://github.com/implus/PytorchInsight.



### Data-Driven Crowd Simulation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.09661v1
- **DOI**: 10.1145/3328756.3328769
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09661v1)
- **Published**: 2019-05-23 13:53:31+00:00
- **Updated**: 2019-05-23 13:53:31+00:00
- **Authors**: Javad Amirian, Wouter van Toll, Jean-Bernard Hayet, Julien Pettré
- **Comment**: Accepted in CASA '19 (Computer Animation and Social Agents)
- **Journal**: None
- **Summary**: This paper presents a novel data-driven crowd simulation method that can mimic the observed traffic of pedestrians in a given environment. Given a set of observed trajectories, we use a recent form of neural networks, Generative Adversarial Networks (GANs), to learn the properties of this set and generate new trajectories with similar properties. We define a way for simulated pedestrians (agents) to follow such a trajectory while handling local collision avoidance. As such, the system can generate a crowd that behaves similarly to observations, while still enabling real-time interactions between agents. Via experiments with real-world data, we show that our simulated trajectories preserve the statistical properties of their input. Our method simulates crowds in real time that resemble existing crowds, while also allowing insertion of extra agents, combination with other simulation methods, and user interaction.



### Watermark retrieval from 3D printed objects via synthetic data training
- **Arxiv ID**: http://arxiv.org/abs/1905.09706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09706v1)
- **Published**: 2019-05-23 15:12:01+00:00
- **Updated**: 2019-05-23 15:12:01+00:00
- **Authors**: Xin Zhang, Ning Jia, Ioannis Ivrissimtzis
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep neural network based method for the retrieval of watermarks from images of 3D printed objects. To deal with the variability of all possible 3D printing and image acquisition settings we train the network with synthetic data. The main simulator parameters such as texture, illumination and camera position are dynamically randomized in non-realistic ways, forcing the neural network to learn the intrinsic features of the 3D printed watermarks. At the end of the pipeline, the watermark, in the form of a two-dimensional bit array, is retrieved through a series of simple image processing and statistical operations applied on the confidence map generated by the neural network. The results demonstrate that the inclusion of synthetic DR data in the training set increases the generalization power of the network, which performs better on images from previously unseen 3D printed objects. We conclude that in our application domain of information retrieval from 3D printed objects, where access to the exact CAD files of the printed objects can be assumed, one can use inexpensive synthetic data to enhance neural network training, reducing the need for the labour intensive process of creating large amounts of hand labelled real data or the need to generate photorealistic synthetic data.



### A Convolutional Cost-Sensitive Crack Localization Algorithm for Automated and Reliable RC Bridge Inspection
- **Arxiv ID**: http://arxiv.org/abs/1905.09716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09716v1)
- **Published**: 2019-05-23 15:22:16+00:00
- **Updated**: 2019-05-23 15:22:16+00:00
- **Authors**: Seyed Omid Sajedi, Xiao Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Bridges are an essential part of the transportation infrastructure and need to be monitored periodically. Visual inspections by dedicated teams have been one of the primary tools in structural health monitoring (SHM) of bridge structures. However, such conventional methods have certain shortcomings. Manual inspections may be challenging in harsh environments and are commonly biased in nature. In the last decade, camera-equipped unmanned aerial vehicles (UAVs) have been widely used for visual inspections; however, the task of automatically extracting useful information from raw images is still challenging. In this paper, a deep learning semantic segmentation framework is proposed to automatically localize surface cracks. Due to the high imbalance of crack and background classes in images, different strategies are investigated to improve performance and reliability. The trained models are tested on real-world crack images showing impressive robustness in terms of the metrics defined by the concepts of precision and recall. These techniques can be used in SHM of bridges to extract useful information from the unprocessed images taken from UAVs.



### Network Pruning via Transformable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1905.09717v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09717v5)
- **Published**: 2019-05-23 15:22:25+00:00
- **Updated**: 2019-10-16 05:11:01+00:00
- **Authors**: Xuanyi Dong, Yi Yang
- **Comment**: Published in the 33rd Conference on Neural Information Processing
  Systems (NeurIPS 2019)
- **Journal**: None
- **Summary**: Network pruning reduces the computation costs of an over-parameterized network without performance damage. Prevailing pruning algorithms pre-define the width and depth of the pruned networks, and then transfer parameters from the unpruned network to pruned networks. To break the structure limitation of the pruned networks, we propose to apply neural architecture search to search directly for a network with flexible channel and layer sizes. The number of the channels/layers is learned by minimizing the loss of the pruned networks. The feature map of the pruned network is an aggregation of K feature map fragments (generated by K networks of different sizes), which are sampled based on the probability distribution.The loss can be back-propagated not only to the network weights, but also to the parameterized distribution to explicitly tune the size of the channels/layers. Specifically, we apply channel-wise interpolation to keep the feature map with different channel sizes aligned in the aggregation procedure. The maximum probability for the size in each distribution serves as the width and depth of the pruned network, whose parameters are learned by knowledge transfer, e.g., knowledge distillation, from the original networks. Experiments on CIFAR-10, CIFAR-100 and ImageNet demonstrate the effectiveness of our new perspective of network pruning compared to traditional network pruning algorithms. Various searching and knowledge transfer approaches are conducted to show the effectiveness of the two components. Code is at: https://github.com/D-X-Y/NAS-Projects.



### Adversarially Robust Distillation
- **Arxiv ID**: http://arxiv.org/abs/1905.09747v2
- **DOI**: 10.1609/aaai.v34i04.5816
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.09747v2)
- **Published**: 2019-05-23 16:09:20+00:00
- **Updated**: 2019-12-02 22:37:09+00:00
- **Authors**: Micah Goldblum, Liam Fowl, Soheil Feizi, Tom Goldstein
- **Comment**: Accepted to AAAI Conference on Artificial Intelligence, 2020
- **Journal**: None
- **Summary**: Knowledge distillation is effective for producing small, high-performance neural networks for classification, but these small networks are vulnerable to adversarial attacks. This paper studies how adversarial robustness transfers from teacher to student during knowledge distillation. We find that a large amount of robustness may be inherited by the student even when distilled on only clean images. Second, we introduce Adversarially Robust Distillation (ARD) for distilling robustness onto student networks. In addition to producing small models with high test accuracy like conventional distillation, ARD also passes the superior robustness of large networks onto the student. In our experiments, we find that ARD student models decisively outperform adversarially trained networks of identical architecture in terms of robust accuracy, surpassing state-of-the-art methods on standard robustness benchmarks. Finally, we adapt recent fast adversarial training methods to ARD for accelerated robust distillation.



### Speech2Face: Learning the Face Behind a Voice
- **Arxiv ID**: http://arxiv.org/abs/1905.09773v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1905.09773v1)
- **Published**: 2019-05-23 16:54:17+00:00
- **Updated**: 2019-05-23 16:54:17+00:00
- **Authors**: Tae-Hyun Oh, Tali Dekel, Changil Kim, Inbar Mosseri, William T. Freeman, Michael Rubinstein, Wojciech Matusik
- **Comment**: To appear in CVPR2019. Project page: http://speech2face.github.io
- **Journal**: None
- **Summary**: How much can we infer about a person's looks from the way they speak? In this paper, we study the task of reconstructing a facial image of a person from a short audio recording of that person speaking. We design and train a deep neural network to perform this task using millions of natural Internet/YouTube videos of people speaking. During training, our model learns voice-face correlations that allow it to produce images that capture various physical attributes of the speakers such as age, gender and ethnicity. This is done in a self-supervised manner, by utilizing the natural co-occurrence of faces and speech in Internet videos, without the need to model attributes explicitly. We evaluate and numerically quantify how--and in what manner--our Speech2Face reconstructions, obtained directly from audio, resemble the true face images of the speakers.



### Plane-Based Optimization of Geometry and Texture for RGB-D Reconstruction of Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1905.09829v1
- **DOI**: 10.1109/3DV.2018.00067
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09829v1)
- **Published**: 2019-05-23 17:03:52+00:00
- **Updated**: 2019-05-23 17:03:52+00:00
- **Authors**: Chao Wang, Xiaohu Guo
- **Comment**: in International Conference on 3D Vision 2018; Models and Code: see
  https://github.com/chaowang15/plane-opt-rgbd. arXiv admin note: text overlap
  with arXiv:1905.08853
- **Journal**: International Conference on 3D Vision (2018) 533--541
- **Summary**: We present a novel approach to reconstruct RGB-D indoor scene with plane primitives. Our approach takes as input a RGB-D sequence and a dense coarse mesh reconstructed by some 3D reconstruction method on the sequence, and generate a lightweight, low-polygonal mesh with clear face textures and sharp features without losing geometry details from the original scene. To achieve this, we firstly partition the input mesh with plane primitives, simplify it into a lightweight mesh next, then optimize plane parameters, camera poses and texture colors to maximize the photometric consistency across frames, and finally optimize mesh geometry to maximize consistency between geometry and planes. Compared to existing planar reconstruction methods which only cover large planar regions in the scene, our method builds the entire scene by adaptive planes without losing geometry details and preserves sharp features in the final mesh. We demonstrate the effectiveness of our approach by applying it onto several RGB-D scans and comparing it to other state-of-the-art reconstruction methods.



### Multi-Sample Dropout for Accelerated Training and Better Generalization
- **Arxiv ID**: http://arxiv.org/abs/1905.09788v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.09788v3)
- **Published**: 2019-05-23 17:22:57+00:00
- **Updated**: 2020-10-21 02:39:55+00:00
- **Authors**: Hiroshi Inoue
- **Comment**: None
- **Journal**: None
- **Summary**: Dropout is a simple but efficient regularization technique for achieving better generalization of deep neural networks (DNNs); hence it is widely used in tasks based on DNNs. During training, dropout randomly discards a portion of the neurons to avoid overfitting. This paper presents an enhanced dropout technique, which we call multi-sample dropout, for both accelerating training and improving generalization over the original dropout. The original dropout creates a randomly selected subset (called a dropout sample) from the input in each training iteration while the multi-sample dropout creates multiple dropout samples. The loss is calculated for each sample, and then the sample losses are averaged to obtain the final loss. This technique can be easily implemented by duplicating a part of the network after the dropout layer while sharing the weights among the duplicated fully connected layers. Experimental results using image classification tasks including ImageNet, CIFAR-10, and CIFAR-100 showed that multi-sample dropout accelerates training. Moreover, the networks trained using multi-sample dropout achieved lower error rates compared to networks trained with the original dropout. The additional computation cost due to the duplicated operations is not significant for deep convolutional networks because most of the computation time is consumed in the convolution layers before the dropout layer, which are not duplicated.



### Interpreting Adversarially Trained Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.09797v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.09797v1)
- **Published**: 2019-05-23 17:40:41+00:00
- **Updated**: 2019-05-23 17:40:41+00:00
- **Authors**: Tianyuan Zhang, Zhanxing Zhu
- **Comment**: To apper in ICML19
- **Journal**: None
- **Summary**: We attempt to interpret how adversarially trained convolutional neural networks (AT-CNNs) recognize objects. We design systematic approaches to interpret AT-CNNs in both qualitative and quantitative ways and compare them with normally trained models. Surprisingly, we find that adversarial training alleviates the texture bias of standard CNNs when trained on object recognition tasks, and helps CNNs learn a more shape-biased representation. We validate our hypothesis from two aspects. First, we compare the salience maps of AT-CNNs and standard CNNs on clean images and images under different transformations. The comparison could visually show that the prediction of the two types of CNNs is sensitive to dramatically different types of features. Second, to achieve quantitative verification, we construct additional test datasets that destroy either textures or shapes, such as style-transferred version of clean data, saturated images and patch-shuffled ones, and then evaluate the classification accuracy of AT-CNNs and normal CNNs on these datasets. Our findings shed some light on why AT-CNNs are more robust than those normally trained ones and contribute to a better understanding of adversarial training over CNNs from an interpretation perspective.



### Thwarting finite difference adversarial attacks with output randomization
- **Arxiv ID**: http://arxiv.org/abs/1905.09871v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.09871v1)
- **Published**: 2019-05-23 18:58:39+00:00
- **Updated**: 2019-05-23 18:58:39+00:00
- **Authors**: Haidar Khan, Daniel Park, Azer Khan, Bülent Yener
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples pose a threat to deep neural network models in a variety of scenarios, from settings where the adversary has complete knowledge of the model and to the opposite "black box" setting. Black box attacks are particularly threatening as the adversary only needs access to the input and output of the model. Defending against black box adversarial example generation attacks is paramount as currently proposed defenses are not effective. Since these types of attacks rely on repeated queries to the model to estimate gradients over input dimensions, we investigate the use of randomization to thwart such adversaries from successfully creating adversarial examples. Randomization applied to the output of the deep neural network model has the potential to confuse potential attackers, however this introduces a tradeoff between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart two adaptive black box adversarial attack algorithms.



### Generative Imaging and Image Processing via Generative Encoder
- **Arxiv ID**: http://arxiv.org/abs/1905.13300v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.13300v1)
- **Published**: 2019-05-23 19:11:00+00:00
- **Updated**: 2019-05-23 19:11:00+00:00
- **Authors**: Lin Chen, Haizhao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel generative encoder (GE) model for generative imaging and image processing with applications in compressed sensing and imaging, image compression, denoising, inpainting, deblurring, and super-resolution. The GE model consists of a pre-training phase and a solving phase. In the pre-training phase, we separately train two deep neural networks: a generative adversarial network (GAN) with a generator $\G$ that captures the data distribution of a given image set, and an auto-encoder (AE) network with an encoder $\EN$ that compresses images following the estimated distribution by GAN. In the solving phase, given a noisy image $x=\mathcal{P}(x^*)$, where $x^*$ is the target unknown image, $\mathcal{P}$ is an operator adding an addictive, or multiplicative, or convolutional noise, or equivalently given such an image $x$ in the compressed domain, i.e., given $m=\EN(x)$, we solve the optimization problem   \[   z^*=\underset{z}{\mathrm{argmin}} \|\EN(\G(z))-m\|_2^2+\lambda\|z\|_2^2   \] to recover the image $x^*$ in a generative way via $\hat{x}:=\G(z^*)\approx x^*$, where $\lambda>0$ is a hyperparameter. The GE model unifies the generative capacity of GANs and the stability of AEs in an optimization framework above instead of stacking GANs and AEs into a single network or combining their loss functions into one as in existing literature. Numerical experiments show that the proposed model outperforms several state-of-the-art algorithms.



### Prognostic Value of Transfer Learning Based Features in Resectable Pancreatic Ductal Adenocarcinoma
- **Arxiv ID**: http://arxiv.org/abs/1905.09888v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09888v2)
- **Published**: 2019-05-23 19:35:41+00:00
- **Updated**: 2019-08-21 15:16:00+00:00
- **Authors**: Yucheng Zhang, Edrise M. Lobo-Mueller, Paul Karanicolas, Steven Gallinger, Masoom A. Haider, Farzad Khalvati
- **Comment**: None
- **Journal**: None
- **Summary**: Pancreatic Ductal Adenocarcinoma (PDAC) is one of the most aggressive cancers with an extremely poor prognosis. Radiomics has shown prognostic ability in multiple types of cancer including PDAC. However, the prognostic value of traditional radiomics pipelines, which are based on hand-crafted radiomic features alone is limited. Convolutional neural networks (CNNs) have been shown to outperform these feature-based models in computer vision tasks. However, training a CNN from scratch needs a large sample size which is not feasible in most medical imaging studies. As an alternative solution, CNN-based transfer learning has shown potential for achieving reasonable performance using small datasets. In this work, we developed and validated a CNN-based transfer learning approach for prognostication of PDAC patients for overall survival using two independent resectable PDAC cohorts. The proposed deep transfer learning model for prognostication of PDAC achieved the area under the receiver operating characteristic curve of 0.74, which was significantly higher than that of the traditional radiomics model (0.56) as well as a CNN model trained from scratch (0.50). These results suggest that deep transfer learning may significantly improve prognosis performance using small datasets in medical imaging.



### Adding Intuitive Physics to Neural-Symbolic Capsules Using Interaction Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.09891v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09891v2)
- **Published**: 2019-05-23 19:39:07+00:00
- **Updated**: 2019-06-22 15:26:00+00:00
- **Authors**: Michael Kissner, Helmut Mayer
- **Comment**: None
- **Journal**: None
- **Summary**: Many current methods to learn intuitive physics are based on interaction networks and similar approaches. However, they rely on information that has proven difficult to estimate directly from image data in the past. We aim to narrow this gap by inferring all the semantic information needed from raw pixel data in the form of a scene-graph. Our approach is based on neural-symbolic capsules, which identify which objects in the scene are static, dynamic, elastic or rigid, possible joints between them, as well as their collision information. By integrating all this with interaction networks, we demonstrate how our method is able to learn intuitive physics directly from image sequences and apply its knowledge to new scenes and objects, resulting in an inverse-simulation pipeline.



### Multi-level Texture Encoding and Representation (MuLTER) based on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1905.09907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09907v1)
- **Published**: 2019-05-23 20:25:37+00:00
- **Updated**: 2019-05-23 20:25:37+00:00
- **Authors**: Yuting Hu, Zhiling Long, Ghassan AlRegib
- **Comment**: Proceedings of IEEE International Conference on Image Processing
  (ICIP), Sep. 2019
- **Journal**: None
- **Summary**: In this paper, we propose a multi-level texture encoding and representation network (MuLTER) for texture-related applications. Based on a multi-level pooling architecture, the MuLTER network simultaneously leverages low- and high-level features to maintain both texture details and spatial information. Such a pooling architecture involves few extra parameters and keeps feature dimensions fixed despite of the changes of image sizes. In comparison with state-of-the-art texture descriptors, the MuLTER network yields higher recognition accuracy on typical texture datasets such as MINC-2500 and GTOS-mobile with a discriminative and compact representation. In addition, we analyze the impact of combining features from different levels, which supports our claim that the fusion of multi-level features efficiently enhances recognition performance. Our source code will be published on GitHub (https://github.com/olivesgatech).



### Precipitation Nowcasting with Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1905.09932v1
- **DOI**: 10.1145/3292500.3330762
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09932v1)
- **Published**: 2019-05-23 21:11:16+00:00
- **Updated**: 2019-05-23 21:11:16+00:00
- **Authors**: Vadim Lebedev, Vladimir Ivashkin, Irina Rudenko, Alexander Ganshin, Alexander Molchanov, Sergey Ovcharenko, Ruslan Grokhovetskiy, Ivan Bushmarinov, Dmitry Solomentsev
- **Comment**: Final KDD 2019 version
- **Journal**: in Proceedings of The 25th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining, KDD 2019
- **Summary**: Precipitation nowcasting is a short-range forecast of rain/snow (up to 2 hours), often displayed on top of the geographical map by the weather service. Modern precipitation nowcasting algorithms rely on the extrapolation of observations by ground-based radars via optical flow techniques or neural network models. Dependent on these radars, typical nowcasting is limited to the regions around their locations. We have developed a method for precipitation nowcasting based on geostationary satellite imagery and incorporated the resulting data into the Yandex.Weather precipitation map (including an alerting service with push notifications for products in the Yandex ecosystem), thus expanding its coverage and paving the way to a truly global nowcasting service.



### Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for Investigating Learned Representations
- **Arxiv ID**: http://arxiv.org/abs/1905.13308v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1905.13308v2)
- **Published**: 2019-05-23 21:20:58+00:00
- **Updated**: 2021-06-09 17:13:10+00:00
- **Authors**: Jesse A. Livezey, Ahyeon Hwang, Jacob Yeung, Kristofer E. Bouchard
- **Comment**: None
- **Journal**: None
- **Summary**: Hierarchy and compositionality are common latent properties in many natural and scientific datasets. Determining when a deep network's hidden activations represent hierarchy and compositionality is important both for understanding deep representation learning and for applying deep networks in domains where interpretability is crucial. However, current benchmark machine learning datasets either have little hierarchical or compositional structure, or the structure is not known. This gap impedes precise analysis of a network's representations and thus hinders development of new methods that can learn such properties. To address this gap, we developed a new benchmark dataset with known hierarchical and compositional structure. The Hangul Fonts Dataset (HFD) is comprised of 35 fonts from the Korean writing system (Hangul), each with 11,172 blocks (syllables) composed from the product of initial consonant, medial vowel, and final consonant glyphs. All blocks can be grouped into a few geometric types which induces a hierarchy across blocks. In addition, each block is composed of individual glyphs with rotations, translations, scalings, and naturalistic style variation across fonts. We find that both shallow and deep unsupervised methods only show modest evidence of hierarchy and compositionality in their representations of the HFD compared to supervised deep networks. Supervised deep network representations contain structure related to the geometrical hierarchy of the characters, but the compositional structure of the data is not evident. Thus, HFD enables the identification of shortcomings in existing methods, a critical first step toward developing new machine learning algorithms to extract hierarchical and compositional structure in the context of naturalistic variability.



### Bi-objective Framework for Sensor Fusion in RGB-D Multi-View Systems: Applications in Calibration
- **Arxiv ID**: http://arxiv.org/abs/1905.09939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.09939v1)
- **Published**: 2019-05-23 21:25:42+00:00
- **Updated**: 2019-05-23 21:25:42+00:00
- **Authors**: Hassan Afzal, Djamila Aouada, Michel Antunes, David Fofi, Bruno Mirbach, Björn Ottersten
- **Comment**: 23 pages
- **Journal**: None
- **Summary**: Complete and textured 3D reconstruction of dynamic scenes has been facilitated by mapped RGB and depth information acquired by RGB-D cameras based multi-view systems. One of the most critical steps in such multi-view systems is to determine the relative poses of all cameras via a process known as extrinsic calibration. In this work, we propose a sensor fusion framework based on a weighted bi-objective optimization for refinement of extrinsic calibration tailored for RGB-D multi-view systems. The weighted bi-objective cost function, which makes use of 2D information from RGB images and 3D information from depth images, is analytically derived via the Maximum Likelihood (ML) method. The weighting factor appears as a function of noise in 2D and 3D measurements and takes into account the affect of residual errors on the optimization. We propose an iterative scheme to estimate noise variances in 2D and 3D measurements, for simultaneously computing the weighting factor together with the camera poses. An extensive quantitative and qualitative evaluation of the proposed approach shows improved calibration performance as compared to refinement schemes which use only 2D or 3D measurement information.



### Scene Induced Multi-Modal Trajectory Forecasting via Planning
- **Arxiv ID**: http://arxiv.org/abs/1905.09949v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.09949v1)
- **Published**: 2019-05-23 22:00:17+00:00
- **Updated**: 2019-05-23 22:00:17+00:00
- **Authors**: Nachiket Deo, Mohan M. Trivedi
- **Comment**: ICRA Workshop on Long Term Human Motion Prediction (extended
  abstract)
- **Journal**: None
- **Summary**: We address multi-modal trajectory forecasting of agents in unknown scenes by formulating it as a planning problem. We present an approach consisting of three models; a goal prediction model to identify potential goals of the agent, an inverse reinforcement learning model to plan optimal paths to each goal, and a trajectory generator to obtain future trajectories along the planned paths. Analysis of predictions on the Stanford drone dataset, shows generalizability of our approach to novel scenes.



### Shift R-CNN: Deep Monocular 3D Object Detection with Closed-Form Geometric Constraints
- **Arxiv ID**: http://arxiv.org/abs/1905.09970v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.09970v1)
- **Published**: 2019-05-23 23:41:07+00:00
- **Updated**: 2019-05-23 23:41:07+00:00
- **Authors**: Andretti Naiden, Vlad Paunescu, Gyeongmo Kim, ByeongMoon Jeon, Marius Leordeanu
- **Comment**: v1: Accepted to be published in 2019 IEEE International Conference on
  Image Processing, Sep 22-25, 2019, Taipei. IEEE Copyright notice added. Minor
  changes for camera-ready version. (updated May. 15, 2019)
- **Journal**: None
- **Summary**: We propose Shift R-CNN, a hybrid model for monocular 3D object detection, which combines deep learning with the power of geometry. We adapt a Faster R-CNN network for regressing initial 2D and 3D object properties and combine it with a least squares solution for the inverse 2D to 3D geometric mapping problem, using the camera projection matrix. The closed-form solution of the mathematical system, along with the initial output of the adapted Faster R-CNN are then passed through a final ShiftNet network that refines the result using our newly proposed Volume Displacement Loss. Our novel, geometrically constrained deep learning approach to monocular 3D object detection obtains top results on KITTI 3D Object Detection Benchmark, being the best among all monocular methods that do not use any pre-trained network for depth estimation.




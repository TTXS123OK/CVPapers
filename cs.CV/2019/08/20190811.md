# Arxiv Papers in cs.CV on 2019-08-11
### AutoGAN: Neural Architecture Search for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.03835v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03835v1)
- **Published**: 2019-08-11 00:52:30+00:00
- **Updated**: 2019-08-11 00:52:30+00:00
- **Authors**: Xinyu Gong, Shiyu Chang, Yifan Jiang, Zhangyang Wang
- **Comment**: accepted by ICCV 2019
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has witnessed prevailing success in image classification and (very recently) segmentation tasks. In this paper, we present the first preliminary study on introducing the NAS algorithm to generative adversarial networks (GANs), dubbed AutoGAN. The marriage of NAS and GANs faces its unique challenges. We define the search space for the generator architectural variations and use an RNN controller to guide the search, with parameter sharing and dynamic-resetting to accelerate the process. Inception score is adopted as the reward, and a multi-level search strategy is introduced to perform NAS in a progressive way. Experiments validate the effectiveness of AutoGAN on the task of unconditional image generation. Specifically, our discovered architectures achieve highly competitive performance compared to current state-of-the-art hand-crafted GANs, e.g., setting new state-of-the-art FID scores of 12.42 on CIFAR-10, and 31.01 on STL-10, respectively. We also conclude with a discussion of the current limitations and future potential of AutoGAN. The code is available at https://github.com/TAMU-VITA/AutoGAN



### MobileFAN: Transferring Deep Hidden Representation for Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1908.03839v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03839v3)
- **Published**: 2019-08-11 02:33:38+00:00
- **Updated**: 2019-11-20 06:19:50+00:00
- **Authors**: Yang Zhao, Yifan Liu, Chunhua Shen, Yongsheng Gao, Shengwu Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Facial landmark detection is a crucial prerequisite for many face analysis applications. Deep learning-based methods currently dominate the approach of addressing the facial landmark detection. However, such works generally introduce a large number of parameters, resulting in high memory cost. In this paper, we aim for a lightweight as well as effective solution to facial landmark detection. To this end, we propose an effective lightweight model, namely Mobile Face Alignment Network (MobileFAN), using a simple backbone MobileNetV2 as the encoder and three deconvolutional layers as the decoder. The proposed MobileFAN, with only 8% of the model size and lower computational cost, achieves superior or equivalent performance compared with state-of-the-art models. Moreover, by transferring the geometric structural information of a face graph from a large complex model to our proposed MobileFAN through feature-aligned distillation and feature-similarity distillation, the performance of MobileFAN is further improved in effectiveness and efficiency for face alignment. Extensive experiment results on three challenging facial landmark estimation benchmarks including COFW, 300W and WFLW show the superiority of our proposed MobileFAN against state-of-the-art methods.



### Exploiting Temporal Relationships in Video Moment Localization with Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1908.03846v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1908.03846v1)
- **Published**: 2019-08-11 03:59:18+00:00
- **Updated**: 2019-08-11 03:59:18+00:00
- **Authors**: Songyang Zhang, Jinsong Su, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of video moment localization with natural language, i.e. localizing a video segment described by a natural language sentence. While most prior work focuses on grounding the query as a whole, temporal dependencies and reasoning between events within the text are not fully considered. In this paper, we propose a novel Temporal Compositional Modular Network (TCMN) where a tree attention network first automatically decomposes a sentence into three descriptions with respect to the main event, context event and temporal signal. Two modules are then utilized to measure the visual similarity and location similarity between each segment and the decomposed descriptions. Moreover, since the main event and context event may rely on different modalities (RGB or optical flow), we use late fusion to form an ensemble of four models, where each model is independently trained by one combination of the visual input. Experiments show that our model outperforms the state-of-the-art methods on the TEMPO dataset.



### Semi-Supervised Self-Growing Generative Adversarial Networks for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.03850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03850v1)
- **Published**: 2019-08-11 04:10:33+00:00
- **Updated**: 2019-08-11 04:10:33+00:00
- **Authors**: Haoqian Wang, Zhiwei Xu, Jun Xu, Wangpeng An, Lei Zhang, Qionghai Dai
- **Comment**: 13 pages, 11 figures, 8 tables. arXiv admin note: text overlap with
  arXiv:1606.03498 by other authors
- **Journal**: None
- **Summary**: Image recognition is an important topic in computer vision and image processing, and has been mainly addressed by supervised deep learning methods, which need a large set of labeled images to achieve promising performance. However, in most cases, labeled data are expensive or even impossible to obtain, while unlabeled data are readily available from numerous free on-line resources and have been exploited to improve the performance of deep neural networks. To better exploit the power of unlabeled data for image recognition, in this paper, we propose a semi-supervised and generative approach, namely the semi-supervised self-growing generative adversarial network (SGGAN). Label inference is a key step for the success of semi-supervised learning approaches. There are two main problems in label inference: how to measure the confidence of the unlabeled data and how to generalize the classifier. We address these two problems via the generative framework and a novel convolution-block-transformation technique, respectively. To stabilize and speed up the training process of SGGAN, we employ the metric Maximum Mean Discrepancy as the feature matching objective function and achieve larger gain than the standard semi-supervised GANs (SSGANs), narrowing the gap to the supervised methods. Experiments on several benchmark datasets show the effectiveness of the proposed SGGAN on image recognition and facial attribute recognition tasks. By using the training data with only 4% labeled facial attributes, the SGGAN approach can achieve comparable accuracy with leading supervised deep learning methods with all labeled facial attributes.



### IoU Loss for 2D/3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.03851v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03851v1)
- **Published**: 2019-08-11 04:19:25+00:00
- **Updated**: 2019-08-11 04:19:25+00:00
- **Authors**: Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo Yin, Yuchao Dai, Ruigang Yang
- **Comment**: Accepted by international conference on 3d vision 2019
- **Journal**: None
- **Summary**: In 2D/3D object detection task, Intersection-over-Union (IoU) has been widely employed as an evaluation metric to evaluate the performance of different detectors in the testing stage. However, during the training stage, the common distance loss (\eg, $L_1$ or $L_2$) is often adopted as the loss function to minimize the discrepancy between the predicted and ground truth Bounding Box (Bbox). To eliminate the performance gap between training and testing, the IoU loss has been introduced for 2D object detection in \cite{yu2016unitbox} and \cite{rezatofighi2019generalized}. Unfortunately, all these approaches only work for axis-aligned 2D Bboxes, which cannot be applied for more general object detection task with rotated Bboxes. To resolve this issue, we investigate the IoU computation for two rotated Bboxes first and then implement a unified framework, IoU loss layer for both 2D and 3D object detection tasks. By integrating the implemented IoU loss into several state-of-the-art 3D object detectors, consistent improvements have been achieved for both bird-eye-view 2D detection and point cloud 3D detection on the public KITTI benchmark.



### StructureFlow: Image Inpainting via Structure-aware Appearance Flow
- **Arxiv ID**: http://arxiv.org/abs/1908.03852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03852v1)
- **Published**: 2019-08-11 04:23:07+00:00
- **Updated**: 2019-08-11 04:23:07+00:00
- **Authors**: Yurui Ren, Xiaoming Yu, Ruonan Zhang, Thomas H. Li, Shan Liu, Ge Li
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting techniques have shown significant improvements by using deep neural networks recently. However, most of them may either fail to reconstruct reasonable structures or restore fine-grained textures. In order to solve this problem, in this paper, we propose a two-stage model which splits the inpainting task into two parts: structure reconstruction and texture generation. In the first stage, edge-preserved smooth images are employed to train a structure reconstructor which completes the missing structures of the inputs. In the second stage, based on the reconstructed structures, a texture generator using appearance flow is designed to yield image details. Experiments on multiple publicly available datasets show the superior performance of the proposed network.



### Delving into Robust Object Detection from Unmanned Aerial Vehicles: A Deep Nuisance Disentanglement Approach
- **Arxiv ID**: http://arxiv.org/abs/1908.03856v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03856v2)
- **Published**: 2019-08-11 05:24:25+00:00
- **Updated**: 2020-10-02 18:27:09+00:00
- **Authors**: Zhenyu Wu, Karthik Suresh, Priya Narayanan, Hongyu Xu, Heesung Kwon, Zhangyang Wang
- **Comment**: Accepted by International Conference on Computer Vision (ICCV) 2019
- **Journal**: None
- **Summary**: Object detection from images captured by Unmanned Aerial Vehicles (UAVs) is becoming increasingly useful. Despite the great success of the generic object detection methods trained on ground-to-ground images, a huge performance drop is observed when they are directly applied to images captured by UAVs. The unsatisfactory performance is owing to many UAV-specific nuisances, such as varying flying altitudes, adverse weather conditions, dynamically changing viewing angles, etc. Those nuisances constitute a large number of fine-grained domains, across which the detection model has to stay robust. Fortunately, UAVs will record meta-data that depict those varying attributes, which are either freely available along with the UAV images, or can be easily obtained. We propose to utilize those free meta-data in conjunction with associated UAV images to learn domain-robust features via an adversarial training framework dubbed Nuisance Disentangled Feature Transform (NDFT), for the specific challenging problem of object detection in UAV images, achieving a substantial gain in robustness to those nuisances. We demonstrate the effectiveness of our proposed algorithm, by showing state-of-the-art performance (single model) on two existing UAV-based object detection benchmarks. The code is available at https://github.com/TAMU-VITA/UAV-NDFT.



### Efficient Structurally-Strengthened Generative Adversarial Network for MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1908.03858v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03858v1)
- **Published**: 2019-08-11 06:13:00+00:00
- **Updated**: 2019-08-11 06:13:00+00:00
- **Authors**: Wenzhong Zhou, Huiqian Du, Wenbo Mei, Liping Fang
- **Comment**: 28 pages
- **Journal**: None
- **Summary**: Compressed sensing based magnetic resonance imaging (CS-MRI) provides an efficient way to reduce scanning time of MRI. Recently deep learning has been introduced into CS-MRI to further improve the image quality and shorten reconstruction time. In this paper, we propose an efficient structurally strengthened Generative Adversarial Network, termed ESSGAN, for reconstructing MR images from highly under-sampled k-space data. ESSGAN consists of a structurally strengthened generator (SG) and a discriminator. In SG, we introduce strengthened connections (SCs) to improve the utilization of the feature maps between the proposed strengthened convolutional autoencoders (SCAEs), where each SCAE is a variant of a typical convolutional autoencoder. In addition, we creatively introduce a residual in residual block (RIRB) to SG. RIRB increases the depth of SG, thus enhances feature expression ability of SG. Moreover, it can give the encoder blocks and the decoder blocks richer texture features. To further reduce artifacts and preserve more image details, we introduce an enhanced structural loss to SG. ESSGAN can provide higher image quality with less model parameters than the state-of-the-art deep learning-based methods at different undersampling rates of different subsampling masks, and reconstruct a 256*256 MR image in tens of milliseconds.



### To Beta or Not To Beta: Information Bottleneck for DigitaL Image Forensics
- **Arxiv ID**: http://arxiv.org/abs/1908.03864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03864v1)
- **Published**: 2019-08-11 07:28:35+00:00
- **Updated**: 2019-08-11 07:28:35+00:00
- **Authors**: Aurobrata Ghosh, Zheng Zhong, Steve Cruz, Subbu Veeravasarapu, Terrance E Boult, Maneesh Singh
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We consider an information theoretic approach to address the problem of identifying fake digital images. We propose an innovative method to formulate the issue of localizing manipulated regions in an image as a deep representation learning problem using the Information Bottleneck (IB), which has recently gained popularity as a framework for interpreting deep neural networks. Tampered images pose a serious predicament since digitized media is a ubiquitous part of our lives. These are facilitated by the easy availability of image editing software and aggravated by recent advances in deep generative models such as GANs. We propose InfoPrint, a computationally efficient solution to the IB formulation using approximate variational inference and compare it to a numerical solution that is computationally expensive. Testing on a number of standard datasets, we demonstrate that InfoPrint outperforms the state-of-the-art and the numerical solution. Additionally, it also has the ability to detect alterations made by inpainting GANs.



### PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial Networks for Classification of Noisy Handwritten Bangla Characters
- **Arxiv ID**: http://arxiv.org/abs/1908.08987v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1908.08987v1)
- **Published**: 2019-08-11 08:01:58+00:00
- **Updated**: 2019-08-11 08:01:58+00:00
- **Authors**: Qun Liu, Edward Collier, Supratik Mukhopadhyay
- **Comment**: Paper was accepted at the 21st International Conference on
  Asia-Pacific Digital Libraries (ICADL 2019)
- **Journal**: None
- **Summary**: Due to the sparsity of features, noise has proven to be a great inhibitor in the classification of handwritten characters. To combat this, most techniques perform denoising of the data before classification. In this paper, we consolidate the approach by training an all-in-one model that is able to classify even noisy characters. For classification, we progressively train a classifier generative adversarial network on the characters from low to high resolution. We show that by learning the features at each resolution independently a trained model is able to accurately classify characters even in the presence of noise. We experimentally demonstrate the effectiveness of our approach by classifying noisy versions of MNIST, handwritten Bangla Numeral, and Basic Character datasets.



### Unsupervised Neural Quantization for Compressed-Domain Similarity Search
- **Arxiv ID**: http://arxiv.org/abs/1908.03883v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.03883v1)
- **Published**: 2019-08-11 10:46:16+00:00
- **Updated**: 2019-08-11 10:46:16+00:00
- **Authors**: Stanislav Morozov, Artem Babenko
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of unsupervised visual descriptors compression, which is a key ingredient of large-scale image retrieval systems. While the deep learning machinery has benefited literally all computer vision pipelines, the existing state-of-the-art compression methods employ shallow architectures, and we aim to close this gap by our paper. In more detail, we introduce a DNN architecture for the unsupervised compressed-domain retrieval, based on multi-codebook quantization. The proposed architecture is designed to incorporate both fast data encoding and efficient distances computation via lookup tables. We demonstrate the exceptional advantage of our scheme over existing quantization approaches on several datasets of visual descriptors via outperforming the previous state-of-the-art by a large margin.



### UM-Adapt: Unsupervised Multi-Task Adaptation Using Adversarial Cross-Task Distillation
- **Arxiv ID**: http://arxiv.org/abs/1908.03884v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03884v3)
- **Published**: 2019-08-11 10:52:07+00:00
- **Updated**: 2019-09-16 12:03:44+00:00
- **Authors**: Jogendra Nath Kundu, Nishank Lakkakula, R. Venkatesh Babu
- **Comment**: ICCV 2019 (Oral)
- **Journal**: None
- **Summary**: Aiming towards human-level generalization, there is a need to explore adaptable representation learning methods with greater transferability. Most existing approaches independently address task-transferability and cross-domain adaptation, resulting in limited generalization. In this paper, we propose UM-Adapt - a unified framework to effectively perform unsupervised domain adaptation for spatially-structured prediction tasks, simultaneously maintaining a balanced performance across individual tasks in a multi-task setting. To realize this, we propose two novel regularization strategies; a) Contour-based content regularization (CCR) and b) exploitation of inter-task coherency using a cross-task distillation module. Furthermore, avoiding a conventional ad-hoc domain discriminator, we re-utilize the cross-task distillation loss as output of an energy function to adversarially minimize the input domain discrepancy. Through extensive experiments, we demonstrate superior generalizability of the learned representations simultaneously for multiple tasks under domain-shifts from synthetic to natural environments. UM-Adapt yields state-of-the-art transfer learning results on ImageNet classification and comparable performance on PASCAL VOC 2007 detection task, even with a smaller backbone-net. Moreover, the resulting semi-supervised framework outperforms the current fully-supervised multi-task learning state-of-the-art on both NYUD and Cityscapes dataset.



### Index Network
- **Arxiv ID**: http://arxiv.org/abs/1908.09895v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09895v2)
- **Published**: 2019-08-11 10:52:47+00:00
- **Updated**: 2020-04-05 14:12:48+00:00
- **Authors**: Hao Lu, Yutong Dai, Chunhua Shen, Songcen Xu
- **Comment**: 17 pages. Extended version of "Indices Matter: Learning to Index for
  Deep Image Matting" at arXiv:1908.00672
- **Journal**: None
- **Summary**: We show that existing upsampling operators can be unified using the notion of the index function. This notion is inspired by an observation in the decoding process of deep image matting where indices-guided unpooling can often recover boundary details considerably better than other upsampling operators such as bilinear interpolation. By viewing the indices as a function of the feature map, we introduce the concept of "learning to index", and present a novel index-guided encoder-decoder framework where indices are self-learned adaptively from data and are used to guide the downsampling and upsampling stages, without extra training supervision. At the core of this framework is a new learnable module, termed Index Network (IndexNet), which dynamically generates indices conditioned on the feature map itself. IndexNet can be used as a plug-in applying to almost all off-the-shelf convolutional networks that have coupled downsampling and upsampling stages, giving the networks the ability to dynamically capture variations of local patterns. In particular, we instantiate and investigate five families of IndexNet and demonstrate their effectiveness on four dense prediction tasks, including image denoising, image matting, semantic segmentation, and monocular depth estimation. Code and models have been made available at: https://tinyurl.com/IndexNetV1



### Temporal Knowledge Propagation for Image-to-Video Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1908.03885v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03885v3)
- **Published**: 2019-08-11 11:22:27+00:00
- **Updated**: 2019-10-10 02:10:27+00:00
- **Authors**: Xinqian Gu, Bingpeng Ma, Hong Chang, Shiguang Shan, Xilin Chen
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: In many scenarios of Person Re-identification (Re-ID), the gallery set consists of lots of surveillance videos and the query is just an image, thus Re-ID has to be conducted between image and videos. Compared with videos, still person images lack temporal information. Besides, the information asymmetry between image and video features increases the difficulty in matching images and videos. To solve this problem, we propose a novel Temporal Knowledge Propagation (TKP) method which propagates the temporal knowledge learned by the video representation network to the image representation network. Specifically, given the input videos, we enforce the image representation network to fit the outputs of video representation network in a shared feature space. With back propagation, temporal knowledge can be transferred to enhance the image features and the information asymmetry problem can be alleviated. With additional classification and integrated triplet losses, our model can learn expressive and discriminative image and video features for image-to-video re-identification. Extensive experiments demonstrate the effectiveness of our method and the overall results on two widely used datasets surpass the state-of-the-art methods by a large margin. Code is available at: https://github.com/guxinqian/TKP



### HBONet: Harmonious Bottleneck on Two Orthogonal Dimensions
- **Arxiv ID**: http://arxiv.org/abs/1908.03888v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03888v1)
- **Published**: 2019-08-11 11:37:39+00:00
- **Updated**: 2019-08-11 11:37:39+00:00
- **Authors**: Duo Li, Aojun Zhou, Anbang Yao
- **Comment**: Accepted by ICCV 2019. Code and pretrained models are available at
  https://github.com/d-li14/HBONet
- **Journal**: None
- **Summary**: MobileNets, a class of top-performing convolutional neural network architectures in terms of accuracy and efficiency trade-off, are increasingly used in many resourceaware vision applications. In this paper, we present Harmonious Bottleneck on two Orthogonal dimensions (HBO), a novel architecture unit, specially tailored to boost the accuracy of extremely lightweight MobileNets at the level of less than 40 MFLOPs. Unlike existing bottleneck designs that mainly focus on exploring the interdependencies among the channels of either groupwise or depthwise convolutional features, our HBO improves bottleneck representation while maintaining similar complexity via jointly encoding the feature interdependencies across both spatial and channel dimensions. It has two reciprocal components, namely spatial contraction-expansion and channel expansion-contraction, nested in a bilaterally symmetric structure. The combination of two interdependent transformations performing on orthogonal dimensions of feature maps enhances the representation and generalization ability of our proposed module, guaranteeing compelling performance with limited computational resource and power. By replacing the original bottlenecks in MobileNetV2 backbone with HBO modules, we construct HBONets which are evaluated on ImageNet classification, PASCAL VOC object detection and Market-1501 person re-identification. Extensive experiments show that with the severe constraint of computational budget our models outperform MobileNetV2 counterparts by remarkable margins of at most 6.6%, 6.3% and 5.0% on the above benchmarks respectively. Code and pretrained models are available at https://github.com/d-li14/HBONet.



### DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction
- **Arxiv ID**: http://arxiv.org/abs/1908.03918v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.03918v3)
- **Published**: 2019-08-11 15:03:24+00:00
- **Updated**: 2021-09-11 06:09:26+00:00
- **Authors**: Changhao Chen, Chris Xiaoxuan Lu, Bing Wang, Niki Trigoni, Andrew Markham
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)
- **Journal**: None
- **Summary**: Dynamical models estimate and predict the temporal evolution of physical systems. State Space Models (SSMs) in particular represent the system dynamics with many desirable properties, such as being able to model uncertainty in both the model and measurements, and optimal (in the Bayesian sense) recursive formulations e.g. the Kalman Filter. However, they require significant domain knowledge to derive the parametric form and considerable hand-tuning to correctly set all the parameters. Data driven techniques e.g. Recurrent Neural Networks have emerged as compelling alternatives to SSMs with wide success across a number of challenging tasks, in part due to their ability to extract relevant features from rich inputs. They however lack interpretability and robustness to unseen conditions. In this work, we present DynaNet, a hybrid deep learning and time-varying state-space model which can be trained end-to-end. Our neural Kalman dynamical model allows us to exploit the relative merits of each approach. We demonstrate state-of-the-art estimation and prediction on a number of physically challenging tasks, including visual odometry, sensor fusion for visual-inertial navigation and pendulum control. In addition we show how DynaNet can indicate failures through investigation of properties such as the rate of innovation (Kalman Gain).



### GAN-Tree: An Incrementally Learned Hierarchical Generative Framework for Multi-Modal Data Distributions
- **Arxiv ID**: http://arxiv.org/abs/1908.03919v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03919v3)
- **Published**: 2019-08-11 15:15:49+00:00
- **Updated**: 2019-09-16 11:56:58+00:00
- **Authors**: Jogendra Nath Kundu, Maharshi Gor, Dakshit Agrawal, R. Venkatesh Babu
- **Comment**: ICCV 2019 (code available at https://github.com/val-iisc/GANTree)
- **Journal**: None
- **Summary**: Despite the remarkable success of generative adversarial networks, their performance seems less impressive for diverse training sets, requiring learning of discontinuous mapping functions. Though multi-mode prior or multi-generator models have been proposed to alleviate this problem, such approaches may fail depending on the empirically chosen initial mode components. In contrast to such bottom-up approaches, we present GAN-Tree, which follows a hierarchical divisive strategy to address such discontinuous multi-modal data. Devoid of any assumption on the number of modes, GAN-Tree utilizes a novel mode-splitting algorithm to effectively split the parent mode to semantically cohesive children modes, facilitating unsupervised clustering. Further, it also enables incremental addition of new data modes to an already trained GAN-Tree, by updating only a single branch of the tree structure. As compared to prior approaches, the proposed framework offers a higher degree of flexibility in choosing a large variety of mutually exclusive and exhaustive tree nodes called GAN-Set. Extensive experiments on synthetic and natural image datasets including ImageNet demonstrate the superiority of GAN-Tree against the prior state-of-the-arts.



### ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks
- **Arxiv ID**: http://arxiv.org/abs/1908.03930v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1908.03930v3)
- **Published**: 2019-08-11 16:06:58+00:00
- **Updated**: 2019-08-31 12:50:35+00:00
- **Authors**: Xiaohan Ding, Yuchen Guo, Guiguang Ding, Jungong Han
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: As designing appropriate Convolutional Neural Network (CNN) architecture in the context of a given application usually involves heavy human works or numerous GPU hours, the research community is soliciting the architecture-neutral CNN structures, which can be easily plugged into multiple mature architectures to improve the performance on our real-world applications. We propose Asymmetric Convolution Block (ACB), an architecture-neutral structure as a CNN building block, which uses 1D asymmetric convolutions to strengthen the square convolution kernels. For an off-the-shelf architecture, we replace the standard square-kernel convolutional layers with ACBs to construct an Asymmetric Convolutional Network (ACNet), which can be trained to reach a higher level of accuracy. After training, we equivalently convert the ACNet into the same original architecture, thus requiring no extra computations anymore. We have observed that ACNet can improve the performance of various models on CIFAR and ImageNet by a clear margin. Through further experiments, we attribute the effectiveness of ACB to its capability of enhancing the model's robustness to rotational distortions and strengthening the central skeleton parts of square convolution kernels.



### Efficiency and Scalability of Multi-Lane Capsule Networks (MLCN)
- **Arxiv ID**: http://arxiv.org/abs/1908.03935v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03935v1)
- **Published**: 2019-08-11 17:04:14+00:00
- **Updated**: 2019-08-11 17:04:14+00:00
- **Authors**: Vanderson M. do Rosario, Mauricio Breternitz Jr., Edson Borin
- **Comment**: None
- **Journal**: None
- **Summary**: Some Deep Neural Networks (DNN) have what we call lanes, or they can be reorganized as such. Lanes are paths in the network which are data-independent and typically learn different features or add resilience to the network. Given their data-independence, lanes are amenable for parallel processing. The Multi-lane CapsNet (MLCN) is a proposed reorganization of the Capsule Network which is shown to achieve better accuracy while bringing highly-parallel lanes. However, the efficiency and scalability of MLCN had not been systematically examined. In this work, we study the MLCN network with multiple GPUs finding that it is 2x more efficient than the original CapsNet when using model-parallelism. Further, we present the load balancing problem of distributing heterogeneous lanes in homogeneous or heterogeneous accelerators and show that a simple greedy heuristic can be almost 50% faster than a naive random approach.



### Robust Online Multi-target Visual Tracking using a HISP Filter with Discriminative Deep Appearance Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.03945v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03945v6)
- **Published**: 2019-08-11 18:15:57+00:00
- **Updated**: 2020-10-11 11:54:32+00:00
- **Authors**: Nathanael L. Baisa
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel online multi-target visual tracker based on the recently developed Hypothesized and Independent Stochastic Population (HISP) filter. The HISP filter combines advantages of traditional tracking approaches like MHT and point-process-based approaches like PHD filter, and it has linear complexity while maintaining track identities. We apply this filter for tracking multiple targets in video sequences acquired under varying environmental conditions and targets density using a tracking-by-detection approach. We also adopt deep CNN appearance representation by training a verification-identification network (VerIdNet) on large-scale person re-identification data sets. We construct an augmented likelihood in a principled manner using this deep CNN appearance features and spatio-temporal information. Furthermore, we solve the problem of two or more targets having identical label considering the weight propagated with each confirmed hypothesis. Extensive experiments on MOT16 and MOT17 benchmark data sets show that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy.



### Structural Similarity based Anatomical and Functional Brain Imaging Fusion
- **Arxiv ID**: http://arxiv.org/abs/1908.03958v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03958v4)
- **Published**: 2019-08-11 20:44:52+00:00
- **Updated**: 2019-09-18 20:20:35+00:00
- **Authors**: Nishant Kumar, Nico Hoffmann, Martin Oelschl√§gel, Edmund Koch, Matthias Kirsch, Stefan Gumhold
- **Comment**: Accepted at MICCAI-MBIA 2019
- **Journal**: None
- **Summary**: Multimodal medical image fusion helps in combining contrasting features from two or more input imaging modalities to represent fused information in a single image. One of the pivotal clinical applications of medical image fusion is the merging of anatomical and functional modalities for fast diagnosis of malignant tissues. In this paper, we present a novel end-to-end unsupervised learning-based Convolutional Neural Network (CNN) for fusing the high and low frequency components of MRI-PET grayscale image pairs, publicly available at ADNI, by exploiting Structural Similarity Index (SSIM) as the loss function during training. We then apply color coding for the visualization of the fused image by quantifying the contribution of each input image in terms of the partial derivatives of the fused image. We find that our fusion and visualization approach results in better visual perception of the fused image, while also comparing favorably to previous methods when applying various quantitative assessment metrics.




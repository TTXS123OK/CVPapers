# Arxiv Papers in cs.CV on 2019-12-10
### HalluciNet-ing Spatiotemporal Representations Using a 2D-CNN
- **Arxiv ID**: http://arxiv.org/abs/1912.04430v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04430v3)
- **Published**: 2019-12-10 00:44:25+00:00
- **Updated**: 2020-10-21 07:05:24+00:00
- **Authors**: Paritosh Parmar, Brendan Morris
- **Comment**: Codebase: https://github.com/ParitoshParmar/HalluciNet
- **Journal**: None
- **Summary**: Spatiotemporal representations learned using 3D convolutional neural networks (CNN) are currently used in state-of-the-art approaches for action related tasks. However, 3D-CNN are notorious for being memory and compute resource intensive as compared with more simple 2D-CNN architectures. We propose to hallucinate spatiotemporal representations from a 3D-CNN teacher with a 2D-CNN student. By requiring the 2D-CNN to predict the future and intuit upcoming activity, it is encouraged to gain a deeper understanding of actions and how they evolve. The hallucination task is treated as an auxiliary task, which can be used with any other action related task in a multitask learning setting. Thorough experimental evaluation shows that the hallucination task indeed helps improve performance on action recognition, action quality assessment, and dynamic scene recognition tasks. From a practical standpoint, being able to hallucinate spatiotemporal representations without an actual 3D-CNN can enable deployment in resource-constrained scenarios, such as with limited computing power and/or lower bandwidth. Codebase is available here: https://github.com/ParitoshParmar/HalluciNet.



### HR-SAR-Net: A Deep Neural Network for Urban Scene Segmentation from High-Resolution SAR Data
- **Arxiv ID**: http://arxiv.org/abs/1912.04441v2
- **DOI**: 10.1109/SAS48726.2020.9220068
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04441v2)
- **Published**: 2019-12-10 01:24:21+00:00
- **Updated**: 2023-01-16 17:05:44+00:00
- **Authors**: Xiaying Wang, Lukas Cavigelli, Manuel Eggimann, Michele Magno, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic aperture radar (SAR) data is becoming increasingly available to a wide range of users through commercial service providers with resolutions reaching 0.5m/px. Segmenting SAR data still requires skilled personnel, limiting the potential for large-scale use. We show that it is possible to automatically and reliably perform urban scene segmentation from next-gen resolution SAR data (0.15m/px) using deep neural networks (DNNs), achieving a pixel accuracy of 95.19% and a mean IoU of 74.67% with data collected over a region of merely 2.2km${}^2$. The presented DNN is not only effective, but is very small with only 63k parameters and computationally simple enough to achieve a throughput of around 500Mpx/s using a single GPU. We further identify that additional SAR receive antennas and data from multiple flights massively improve the segmentation accuracy. We describe a procedure for generating a high-quality segmentation ground truth from multiple inaccurate building and road annotations, which has been crucial to achieving these segmentation results.



### AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos
- **Arxiv ID**: http://arxiv.org/abs/1912.04443v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.04443v3)
- **Published**: 2019-12-10 01:36:18+00:00
- **Updated**: 2020-06-21 20:16:28+00:00
- **Authors**: Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, Sergey Levine
- **Comment**: Robotics: Science and Systems (RSS) 2020 camera ready submission.
  Project website: https://sites.google.com/view/rss20avid
- **Journal**: None
- **Summary**: Robotic reinforcement learning (RL) holds the promise of enabling robots to learn complex behaviors through experience. However, realizing this promise for long-horizon tasks in the real world requires mechanisms to reduce human burden in terms of defining the task and scaffolding the learning process. In this paper, we study how these challenges can be alleviated with an automated robotic learning framework, in which multi-stage tasks are defined simply by providing videos of a human demonstrator and then learned autonomously by the robot from raw image observations. A central challenge in imitating human videos is the difference in appearance between the human and robot, which typically requires manual correspondence. We instead take an automated approach and perform pixel-level image translation via CycleGAN to convert the human demonstration into a video of a robot, which can then be used to construct a reward function for a model-based RL algorithm. The robot then learns the task one stage at a time, automatically learning how to reset each stage to retry it multiple times without human-provided resets. This makes the learning process largely automatic, from intuitive task specification via a video to automated training with minimal human intervention. We demonstrate that our approach is capable of learning complex tasks, such as operating a coffee machine, directly from raw image observations, requiring only 20 minutes to provide human demonstrations and about 180 minutes of robot interaction.



### DeOccNet: Learning to See Through Foreground Occlusions in Light Fields
- **Arxiv ID**: http://arxiv.org/abs/1912.04459v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04459v1)
- **Published**: 2019-12-10 02:35:29+00:00
- **Updated**: 2019-12-10 02:35:29+00:00
- **Authors**: Yingqian Wang, Tianhao Wu, Jungang Yang, Longguang Wang, Wei An, Yulan Guo
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Background objects occluded in some views of a light field (LF) camera can be seen by other views. Consequently, occluded surfaces are possible to be reconstructed from LF images. In this paper, we handle the LF de-occlusion (LF-DeOcc) problem using a deep encoder-decoder network (namely, DeOccNet). In our method, sub-aperture images (SAIs) are first given to the encoder to incorporate both spatial and angular information. The encoded representations are then used by the decoder to render an occlusionfree center-view SAI. To the best of our knowledge, DeOccNet is the first deep learning-based LF-DeOcc method. To handle the insufficiency of training data, we propose an LF synthesis approach to embed selected occlusion masks into existing LF images. Besides, several synthetic and realworld LFs are developed for performance evaluation. Experimental results show that, after training on the generated data, our DeOccNet can effectively remove foreground occlusions and achieves superior performance as compared to other state-of-the-art methods. Source codes are available at: https://github.com/YingqianWang/DeOccNet.



### Learning to Discriminate Information for Online Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.04461v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04461v3)
- **Published**: 2019-12-10 02:45:37+00:00
- **Updated**: 2020-03-31 03:48:06+00:00
- **Authors**: Hyunjun Eun, Jinyoung Moon, Jongyoul Park, Chanho Jung, Changick Kim
- **Comment**: To appear in CVPR 2020
- **Journal**: None
- **Summary**: From a streaming video, online action detection aims to identify actions in the present. For this task, previous methods use recurrent networks to model the temporal sequence of current action frames. However, these methods overlook the fact that an input image sequence includes background and irrelevant actions as well as the action of interest. For online action detection, in this paper, we propose a novel recurrent unit to explicitly discriminate the information relevant to an ongoing action from others. Our unit, named Information Discrimination Unit (IDU), decides whether to accumulate input information based on its relevance to the current action. This enables our recurrent network with IDU to learn a more discriminative representation for identifying ongoing actions. In experiments on two benchmark datasets, TVSeries and THUMOS-14, the proposed method outperforms state-of-the-art methods by a significant margin. Moreover, we demonstrate the effectiveness of our recurrent unit by conducting comprehensive ablation studies.



### Flow-Distilled IP Two-Stream Networks for Compressed Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.04462v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04462v2)
- **Published**: 2019-12-10 02:52:09+00:00
- **Updated**: 2019-12-12 16:44:21+00:00
- **Authors**: Shiyuan Huang, Xudong Lin, Svebor Karaman, Shih-Fu Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Two-stream networks have achieved great success in video recognition. A two-stream network combines a spatial stream of RGB frames and a temporal stream of Optical Flow to make predictions. However, the temporal redundancy of RGB frames as well as the high-cost of optical flow computation creates challenges for both the performance and efficiency. Recent works instead use modern compressed video modalities as an alternative to the RGB spatial stream and improve the inference speed by orders of magnitudes. Previous works create one stream for each modality which are combined with an additional temporal stream through late fusion. This is redundant since some modalities like motion vectors already contain temporal information. Based on this observation, we propose a compressed domain two-stream network IP TSN for compressed video recognition, where the two streams are represented by the two types of frames (I and P frames) in compressed videos, without needing a separate temporal stream. With this goal, we propose to fully exploit the motion information of P-stream through generalized distillation from optical flow, which largely improves the efficiency and accuracy. Our P-stream runs 60 times faster than using optical flow while achieving higher accuracy. Our full IP TSN, evaluated over public action recognition benchmarks (UCF101, HMDB51 and a subset of Kinetics), outperforms other compressed domain methods by large margins while improving the total inference speed by 20%.



### SoccerDB: A Large-Scale Database for Comprehensive Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1912.04465v4
- **DOI**: 10.1145/3422844.3423051
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04465v4)
- **Published**: 2019-12-10 02:57:28+00:00
- **Updated**: 2020-09-08 13:27:22+00:00
- **Authors**: Yudong Jiang, Kaixu Cui, Leilei Chen, Canjin Wang, Changliang Xu
- **Comment**: accepted by MM2020 sports workshop
- **Journal**: None
- **Summary**: Soccer videos can serve as a perfect research object for video understanding because soccer games are played under well-defined rules while complex and intriguing enough for researchers to study. In this paper, we propose a new soccer video database named SoccerDB, comprising 171,191 video segments from 346 high-quality soccer games. The database contains 702,096 bounding boxes, 37,709 essential event labels with time boundary and 17,115 highlight annotations for object detection, action recognition, temporal action localization, and highlight detection tasks. To our knowledge, it is the largest database for comprehensive sports video understanding on various aspects. We further survey a collection of strong baselines on SoccerDB, which have demonstrated state-of-the-art performances on independent tasks. Our evaluation suggests that we can benefit significantly when jointly considering the inner correlations among those tasks. We believe the release of SoccerDB will tremendously advance researches around comprehensive video understanding. {\itshape Our dataset and code published on https://github.com/newsdata/SoccerDB.}



### Low-rank representations with incoherent dictionary for face recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.04478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04478v1)
- **Published**: 2019-12-10 03:44:25+00:00
- **Updated**: 2019-12-10 03:44:25+00:00
- **Authors**: Pei Xie, He-Feng Yin, Xiao-Jun Wu
- **Comment**: 27 pages, 7 figures
- **Journal**: None
- **Summary**: Face recognition remains a hot topic in computer vision, and it is challenging to tackle the problem that both the training and testing images are corrupted. In this paper, we propose a novel semi-supervised method based on the theory of the low-rank matrix recovery for face recognition, which can simultaneously learn discriminative low-rank and sparse representations for both training and testing images. To this end, a correlation penalty term is introduced into the formulation of our proposed method to learn an incoherent dictionary. Experimental results on several face image databases demonstrate the effectiveness of our method, i.e., the proposed method is robust to the illumination, expression and pose variations, as well as images with noises such as block occlusion or uniform noises.



### NeuRoRA: Neural Robust Rotation Averaging
- **Arxiv ID**: http://arxiv.org/abs/1912.04485v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04485v3)
- **Published**: 2019-12-10 04:04:06+00:00
- **Updated**: 2020-07-28 06:13:42+00:00
- **Authors**: Pulak Purkait, Tat-Jun Chin, Ian Reid
- **Comment**: None
- **Journal**: ECCV 2020
- **Summary**: Multiple rotation averaging is an essential task for structure from motion, mapping, and robot navigation. The task is to estimate the absolute orientations of several cameras given some of their noisy relative orientation measurements. The conventional methods for this task seek parameters of the absolute orientations that agree best with the observed noisy measurements according to a robust cost function. These robust cost functions are highly nonlinear and are designed based on certain assumptions about the noise and outlier distributions. In this work, we aim to build a neural network that learns the noise patterns from the data and predict/regress the model parameters from the noisy relative orientations. The proposed network is a combination of two networks: (1) a view-graph cleaning network, which detects outlier edges in the view-graph and rectifies noisy measurements; and (2) a fine-tuning network, which fine-tunes an initialization of absolute orientations bootstrapped from the cleaned graph, in a single step. The proposed combined network is very fast, moreover, being trained on a large number of synthetic graphs, it is more accurate than the conventional iterative optimization methods. Although the idea of replacing robust optimization methods by a graph-based network is demonstrated only for multiple rotation averaging, it could easily be extended to other graph-based geometric problems, for example, pose-graph optimization.



### To Balance or Not to Balance: A Simple-yet-Effective Approach for Learning with Long-Tailed Distributions
- **Arxiv ID**: http://arxiv.org/abs/1912.04486v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04486v2)
- **Published**: 2019-12-10 04:11:53+00:00
- **Updated**: 2020-03-10 00:02:37+00:00
- **Authors**: Junjie Zhang, Lingqiao Liu, Peng Wang, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world visual data often exhibits a long-tailed distribution, where some ''head'' classes have a large number of samples, yet only a few samples are available for ''tail'' classes. Such imbalanced distribution causes a great challenge for learning a deep neural network, which can be boiled down into a dilemma: on the one hand, we prefer to increase the exposure of tail class samples to avoid the excessive dominance of head classes in the classifier training. On the other hand, oversampling tail classes makes the network prone to over-fitting, since head class samples are often consequently under-represented. To resolve this dilemma, in this paper, we propose a simple-yet-effective auxiliary learning approach. The key idea is to split a network into a classifier part and a feature extractor part, and then employ different training strategies for each part. Specifically, to promote the awareness of tail-classes, a class-balanced sampling scheme is utilised for training both the classifier and the feature extractor. For the feature extractor, we also introduce an auxiliary training task, which is to train a classifier under the regular random sampling scheme. In this way, the feature extractor is jointly trained from both sampling strategies and thus can take advantage of all training data and avoid the over-fitting issue. Apart from this basic auxiliary task, we further explore the benefit of using self-supervised learning as the auxiliary task. Without using any bells and whistles, our model achieves superior performance over the state-of-the-art solutions.



### Listen to Look: Action Recognition by Previewing Audio
- **Arxiv ID**: http://arxiv.org/abs/1912.04487v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1912.04487v3)
- **Published**: 2019-12-10 04:15:24+00:00
- **Updated**: 2020-03-28 04:53:38+00:00
- **Authors**: Ruohan Gao, Tae-Hyun Oh, Kristen Grauman, Lorenzo Torresani
- **Comment**: Appears in CVPR 2020; Project page:
  http://vision.cs.utexas.edu/projects/listen_to_look/
- **Journal**: None
- **Summary**: In the face of the video data deluge, today's expensive clip-level classifiers are increasingly impractical. We propose a framework for efficient action recognition in untrimmed video that uses audio as a preview mechanism to eliminate both short-term and long-term visual redundancies. First, we devise an ImgAud2Vid framework that hallucinates clip-level features by distilling from lighter modalities---a single frame and its accompanying audio---reducing short-term temporal redundancy for efficient clip-level recognition. Second, building on ImgAud2Vid, we further propose ImgAud-Skimming, an attention-based long short-term memory network that iteratively selects useful moments in untrimmed videos, reducing long-term temporal redundancy for efficient video-level recognition. Extensive experiments on four action recognition datasets demonstrate that our method achieves the state-of-the-art in terms of both recognition accuracy and speed.



### SOLO: Segmenting Objects by Locations
- **Arxiv ID**: http://arxiv.org/abs/1912.04488v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04488v3)
- **Published**: 2019-12-10 04:22:41+00:00
- **Updated**: 2020-07-19 08:22:38+00:00
- **Authors**: Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, Lei Li
- **Comment**: Accepted to Proc. Eur. Conf. Computer Vision (ECCV) 2020. Code is
  available at https://git.io/AdelaiDet
- **Journal**: None
- **Summary**: We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-thensegment' strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of "instance categories", which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.



### Feature Losses for Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/1912.04497v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04497v1)
- **Published**: 2019-12-10 04:58:45+00:00
- **Updated**: 2019-12-10 04:58:45+00:00
- **Authors**: Kirthi Shankar Sivamani
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has made tremendous advances in computer vision tasks such as image classification. However, recent studies have shown that deep learning models are vulnerable to specifically crafted adversarial inputs that are quasi-imperceptible to humans. In this work, we propose a novel approach to defending adversarial attacks. We employ an input processing technique based on denoising autoencoders as a defense. It has been shown that the input perturbations grow and accumulate as noise in feature maps while propagating through a convolutional neural network (CNN). We exploit the noisy feature maps by using an additional subnetwork to extract image feature maps and train an auto-encoder on perceptual losses of these feature maps. This technique achieves close to state-of-the-art results on defending MNIST and CIFAR10 datasets, but more importantly, shows a new way of employing a defense that cannot be trivially trained end-to-end by the attacker. Empirical results demonstrate the effectiveness of this approach on the MNIST and CIFAR10 datasets on simple as well as iterative LP attacks. Our method can be applied as a preprocessing technique to any off the shelf CNN.



### MDFN: Multi-Scale Deep Feature Learning Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.04514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04514v1)
- **Published**: 2019-12-10 05:57:04+00:00
- **Updated**: 2019-12-10 05:57:04+00:00
- **Authors**: Wenchi Ma, Yuanwei Wu, Feng Cen, Guanghui Wang
- **Comment**: None
- **Journal**: Pattern Recognition 2019
- **Summary**: This paper proposes an innovative object detector by leveraging deep features learned in high-level layers. Compared with features produced in earlier layers, the deep features are better at expressing semantic and contextual information. The proposed deep feature learning scheme shifts the focus from concrete features with details to abstract ones with semantic information. It considers not only individual objects and local contexts but also their relationships by building a multi-scale deep feature learning network (MDFN). MDFN efficiently detects the objects by introducing information square and cubic inception modules into the high-level layers, which employs parameter-sharing to enhance the computational efficiency. MDFN provides a multi-scale object detector by integrating multi-box, multi-scale and multi-level technologies. Although MDFN employs a simple framework with a relatively small base network (VGG-16), it achieves better or competitive detection results than those with a macro hierarchical structure that is either very deep or very wide for stronger ability of feature extraction. The proposed technique is evaluated extensively on KITTI, PASCAL VOC, and COCO datasets, which achieves the best results on KITTI and leading performance on PASCAL VOC and COCO. This study reveals that deep features provide prominent semantic information and a variety of contextual contents, which contribute to its superior performance in detecting small or occluded objects. In addition, the MDFN model is computationally efficient, making a good trade-off between the accuracy and speed.



### Arithmetic addition of two integers by deep image classification networks: experiments to quantify their autonomous reasoning ability
- **Arxiv ID**: http://arxiv.org/abs/1912.04518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04518v1)
- **Published**: 2019-12-10 06:02:59+00:00
- **Updated**: 2019-12-10 06:02:59+00:00
- **Authors**: Shuaicheng Liu, Zehao Zhang, Kai Song, Bing Zeng
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: The unprecedented performance achieved by deep convolutional neural networks for image classification is linked primarily to their ability of capturing rich structural features at various layers within networks. Here we design a series of experiments, inspired by children's learning of the arithmetic addition of two integers, to showcase that such deep networks can go beyond the structural features to learn deeper knowledge. In our experiments, a set of images is constructed, each image containing an arithmetic addition $n+m$ in its central area, and several classification networks are then trained over a subset of images, using the sum as the label. Tests on the excluded images show that, as the image set gets larger, the networks have well learnt the law of arithmetic additions so as to build up their autonomous reasoning ability strongly. For instance, networks trained over a small percentage of images can classify a big majority of the remaining images correctly, and many arithmetic additions involving some integers that have never been seen during the training can also be solved correctly by the trained networks.



### Context-Dependent Models for Predicting and Characterizing Facial Expressiveness
- **Arxiv ID**: http://arxiv.org/abs/1912.04523v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1912.04523v1)
- **Published**: 2019-12-10 06:10:25+00:00
- **Updated**: 2019-12-10 06:10:25+00:00
- **Authors**: Victoria Lin, Jeffrey M. Girard, Louis-Philippe Morency
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, extensive research has emerged in affective computing on topics like automatic emotion recognition and determining the signals that characterize individual emotions. Much less studied, however, is expressiveness, or the extent to which someone shows any feeling or emotion. Expressiveness is related to personality and mental health and plays a crucial role in social interaction. As such, the ability to automatically detect or predict expressiveness can facilitate significant advancements in areas ranging from psychiatric care to artificial social intelligence. Motivated by these potential applications, we present an extension of the BP4D+ dataset with human ratings of expressiveness and develop methods for (1) automatically predicting expressiveness from visual data and (2) defining relationships between interpretable visual signals and expressiveness. In addition, we study the emotional context in which expressiveness occurs and hypothesize that different sets of signals are indicative of expressiveness in different contexts (e.g., in response to surprise or in response to pain). Analysis of our statistical models confirms our hypothesis. Consequently, by looking at expressiveness separately in distinct emotional contexts, our predictive models show significant improvements over baselines and achieve comparable results to human performance in terms of correlation with the ground truth.



### Learning Pose Estimation for UAV Autonomous Navigation andLanding Using Visual-Inertial Sensor Data
- **Arxiv ID**: http://arxiv.org/abs/1912.04527v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.04527v2)
- **Published**: 2019-12-10 06:37:30+00:00
- **Updated**: 2020-04-09 11:18:55+00:00
- **Authors**: Francesca Baldini, Animashree Anandkumar, Richard M. Murray
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a new learning approach for autonomous navigation and landing of an Unmanned-Aerial-Vehicle (UAV). We develop a multimodal fusion of deep neural architectures for visual-inertial odometry. We train the model in an end-to-end fashion to estimate the current vehicle pose from streams of visual and inertial measurements. We first evaluate the accuracy of our estimation by comparing the prediction of the model to traditional algorithms on the publicly available EuRoC MAV dataset. The results illustrate a $25 \%$ improvement in estimation accuracy over the baseline. Finally, we integrate the architecture in the closed-loop flight control system of Airsim - a plugin simulator for Unreal Engine - and we provide simulation results for autonomous navigation and landing.



### Automatic Analysis System of Calcaneus Radiograph: Rotation-Invariant Landmark Detection for Calcaneal Angle Measurement, Fracture Identification and Fracture Region Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.04536v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04536v5)
- **Published**: 2019-12-10 06:53:29+00:00
- **Updated**: 2021-03-19 10:24:04+00:00
- **Authors**: Jia Guo, Yuxuan Mu, Dong Xue, Huiqi Li, Junxian Chen, Huanxin Yan, Hailin Xu, Wei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Calcaneus is the largest tarsal bone to withstand the daily stresses of weight-bearing. The calcaneal fracture is the most common type in the tarsal bone fractures. After a fracture is suspected, plain radiographs should be taken first. Bohler's Angle (BA) and Critical Angle of Gissane (CAG), measured by four anatomic landmarks in lateral foot radiograph, can guide fracture diagnosis and facilitate operative recovery of the fractured calcaneus. This study aims to develop an analysis system that can automatically locate four anatomic landmarks, measure BA and CAG for fracture assessment, identify fractured calcaneus, and segment fractured regions. For landmark detection, we proposed a coarse-to-fine Rotation-Invariant Regression-Voting (RIRV) landmark detection method based on regressive Multi-Layer Perceptron (MLP) and Scale Invariant Feature Transform (SIFT) patch descriptor, which solves the problem of fickle rotation of calcaneus. By implementing a novel normalization approach, the RIRV method is explicitly rotation-invariance comparing with traditional regressive methods. For fracture identification and segmentation, a convolution neural network (CNN) based on U-Net with auxiliary classification head (U-Net-CH) is designed. The input ROIs of the CNN are normalized by detected landmarks to uniform view, orientation, and scale. The advantage of this approach is the multi-task learning that combines classification and segmentation. Our system can accurately measure BA and CAG with a mean angle error of 3.8 and 6.2 respectively. For fracture identification and fracture region segmentation, our system presents good performance with an F1-score of 96.55%, recall of 94.99%, and segmentation IoU-score of 0.586.



### Appending Adversarial Frames for Universal Video Attack
- **Arxiv ID**: http://arxiv.org/abs/1912.04538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.04538v1)
- **Published**: 2019-12-10 06:54:20+00:00
- **Updated**: 2019-12-10 06:54:20+00:00
- **Authors**: Zhikai Chen, Lingxi Xie, Shanmin Pang, Yong He, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: There have been many efforts in attacking image classification models with adversarial perturbations, but the same topic on video classification has not yet been thoroughly studied. This paper presents a novel idea of video-based attack, which appends a few dummy frames (e.g., containing the texts of `thanks for watching') to a video clip and then adds adversarial perturbations only on these new frames. Our approach enjoys three major benefits, namely, a high success rate, a low perceptibility, and a strong ability in transferring across different networks. These benefits mostly come from the common dummy frame which pushes all samples towards the boundary of classification. On the other hand, such attacks are easily to be concealed since most people would not notice the abnormality behind the perturbed video clips. We perform experiments on two popular datasets with six state-of-the-art video classification models, and demonstrate the effectiveness of our approach in the scenario of universal video attacks.



### SG-VAE: Scene Grammar Variational Autoencoder to generate new indoor scenes
- **Arxiv ID**: http://arxiv.org/abs/1912.04554v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04554v2)
- **Published**: 2019-12-10 07:53:36+00:00
- **Updated**: 2020-08-21 03:31:42+00:00
- **Authors**: Pulak Purkait, Christopher Zach, Ian Reid
- **Comment**: None
- **Journal**: ECCV 2020
- **Summary**: Deep generative models have been used in recent years to learn coherent latent representations in order to synthesize high-quality images. In this work, we propose a neural network to learn a generative model for sampling consistent indoor scene layouts. Our method learns the co-occurrences, and appearance parameters such as shape and pose, for different objects categories through a grammar-based auto-encoder, resulting in a compact and accurate representation for scene layouts. In contrast to existing grammar-based methods with a user-specified grammar, we construct the grammar automatically by extracting a set of production rules on reasoning about object co-occurrences in training data. The extracted grammar is able to represent a scene by an augmented parse tree. The proposed auto-encoder encodes these parse trees to a latent code, and decodes the latent code to a parse tree, thereby ensuring the generated scene is always valid. We experimentally demonstrate that the proposed auto-encoder learns not only to generate valid scenes (i.e. the arrangements and appearances of objects), but it also learns coherent latent representations where nearby latent samples decode to similar scene outputs. The obtained generative model is applicable to several computer vision tasks such as 3D pose and layout estimation from RGB-D data.



### A Feasible Framework for Arbitrary-Shaped Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.04561v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04561v2)
- **Published**: 2019-12-10 08:13:34+00:00
- **Updated**: 2019-12-12 05:38:23+00:00
- **Authors**: Jinjin Zhang, Wei Wang, Di Huang, Qingjie Liu, Yunhong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based methods have achieved surprising progress in Scene Text Recognition (STR), one of classic problems in computer vision. In this paper, we propose a feasible framework for multi-lingual arbitrary-shaped STR, including instance segmentation based text detection and language model based attention mechanism for text recognition. Our STR algorithm not only recognizes Latin and Non-Latin characters, but also supports arbitrary-shaped text recognition. Our method wins the championship on Scene Text Spotting Task (Latin Only, Latin and Chinese) of ICDAR2019 Robust Reading Challenge on ArbitraryShaped Text Competition. Code is available at https://github.com/zhang0jhon/AttentionOCR.



### Understanding 3D CNN Behavior for Alzheimer's Disease Diagnosis from Brain PET Scan
- **Arxiv ID**: http://arxiv.org/abs/1912.04563v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.04563v2)
- **Published**: 2019-12-10 08:17:22+00:00
- **Updated**: 2019-12-26 03:35:41+00:00
- **Authors**: Jyoti Islam, Yanqing Zhang
- **Comment**: Science Meets Engineering of Deep Learning (SEDL) Workshop at NeurIPS
  2019
- **Journal**: None
- **Summary**: In recent days, Convolutional Neural Networks (CNN) have demonstrated impressive performance in medical image analysis. However, there is a lack of clear understanding of why and how the Convolutional Neural Network performs so well for image analysis task. How CNN analyzes an image and discriminates among samples of different classes are usually considered as non-transparent. As a result, it becomes difficult to apply CNN based approaches in clinical procedures and automated disease diagnosis systems. In this paper, we consider this issue and work on visualizing and understanding the decision of Convolutional Neural Network for Alzheimer's Disease (AD) Diagnosis. We develop a 3D deep convolutional neural network for AD diagnosis using brain PET scans and propose using five visualizations techniques - Sensitivity Analysis (Backpropagation), Guided Backpropagation, Occlusion, Brain Area Occlusion, and Layer-wise Relevance Propagation (LRP) to understand the decision of the CNN by highlighting the relevant areas in the PET data.



### MaskAAE: Latent space optimization for Adversarial Auto-Encoders
- **Arxiv ID**: http://arxiv.org/abs/1912.04564v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.04564v2)
- **Published**: 2019-12-10 08:18:13+00:00
- **Updated**: 2020-05-17 11:26:24+00:00
- **Authors**: Arnab Kumar Mondal, Sankalan Pal Chowdhury, Aravind Jayendran, Parag Singla, Himanshu Asnani, Prathosh AP
- **Comment**: To be presented at UAI 2020
- **Journal**: None
- **Summary**: The field of neural generative models is dominated by the highly successful Generative Adversarial Networks (GANs) despite their challenges, such as training instability and mode collapse. Auto-Encoders (AE) with regularized latent space provide an alternative framework for generative models, albeit their performance levels have not reached that of GANs. In this work, we hypothesise that the dimensionality of the AE model's latent space has a critical effect on the quality of generated data. Under the assumption that nature generates data by sampling from a "true" generative latent space followed by a deterministic function, we show that the optimal performance is obtained when the dimensionality of the latent space of the AE-model matches with that of the "true" generative latent space. Further, we propose an algorithm called the Mask Adversarial Auto-Encoder (MaskAAE), in which the dimensionality of the latent space of an adversarial auto encoder is brought closer to that of the "true" generative latent space, via a procedure to mask the spurious latent dimensions. We demonstrate through experiments on synthetic and several real-world datasets that the proposed formulation yields betterment in the generation quality.



### Scalable Fine-grained Generated Image Classification Based on Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.11082v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.11082v1)
- **Published**: 2019-12-10 08:19:37+00:00
- **Updated**: 2019-12-10 08:19:37+00:00
- **Authors**: Xinsheng Xuan, Bo Peng, Wei Wang, Jing Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, generated images could reach very high quality, even human eyes could not tell them apart from real images. Although there are already some methods for detecting generated images in current forensic community, most of these methods are used to detect a single type of generated images. The new types of generated images are emerging one after another, and the existing detection methods cannot cope well. These problems prompted us to propose a scalable framework for multi-class classification based on deep metric learning, which aims to classify the generated images finer. In addition, we have increased the scalability of our framework to cope with the constant emergence of new types of generated images, and through fine-tuning to make the model obtain better detection performance on the new type of generated data.



### Classifying, Segmenting, and Tracking Object Instances in Video with Mask Propagation
- **Arxiv ID**: http://arxiv.org/abs/1912.04573v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04573v4)
- **Published**: 2019-12-10 08:40:10+00:00
- **Updated**: 2021-07-09 19:21:00+00:00
- **Authors**: Gedas Bertasius, Lorenzo Torresani
- **Comment**: CVPR 2020 Best Paper Nominee
- **Journal**: None
- **Summary**: We introduce a method for simultaneously classifying, segmenting and tracking object instances in a video sequence. Our method, named MaskProp, adapts the popular Mask R-CNN to video by adding a mask propagation branch that propagates frame-level object instance masks from each video frame to all the other frames in a video clip. This allows our system to predict clip-level instance tracks with respect to the object instances segmented in the middle frame of the clip. Clip-level instance tracks generated densely for each frame in the sequence are finally aggregated to produce video-level object instance segmentation and classification. Our experiments demonstrate that our clip-level instance segmentation makes our approach robust to motion blur and object occlusions in video. MaskProp achieves the best reported accuracy on the YouTube-VIS dataset, outperforming the ICCV 2019 video instance segmentation challenge winner despite being much simpler and using orders of magnitude less labeled data (1.3M vs 1B images and 860K vs 14M bounding boxes).



### Modelling curvature of a bent paper leaf
- **Arxiv ID**: http://arxiv.org/abs/1912.04898v1
- **DOI**: 10.7287/PEERJ.PREPRINTS.161
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04898v1)
- **Published**: 2019-12-10 09:10:31+00:00
- **Updated**: 2019-12-10 09:10:31+00:00
- **Authors**: Sasikanth Raghava Goteti
- **Comment**: 5 pages , 5 figures
- **Journal**: None
- **Summary**: In this article, we briefly describe various tools and approaches that algebraic geometry has to offer to straighten bent objects. Throughout this article we will consider a specific example of a bent or curved piece of paper which in our case acts very much like an elastica curve. We conclude this article with a suggestion to algebraic geometry as a viable and fast performance alternative of neural networks in vision and machine learning. The purpose of this article is not to build a full blown framework but to show possibility of using algebraic geometry as an alternative to neural networks for recognizing or extracting features on manifolds.



### FootAndBall: Integrated player and ball detector
- **Arxiv ID**: http://arxiv.org/abs/1912.05445v2
- **DOI**: 10.5220/0008916000470056
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05445v2)
- **Published**: 2019-12-10 09:21:49+00:00
- **Updated**: 2020-10-26 22:04:30+00:00
- **Authors**: Jacek Komorowski, Grzegorz Kurzejamski, Grzegorz Sarwas
- **Comment**: arXiv admin note: text overlap with arXiv:1902.07304
- **Journal**: None
- **Summary**: The paper describes a deep neural network-based detector dedicated for ball and players detection in high resolution, long shot, video recordings of soccer matches. The detector, dubbed FootAndBall, has an efficient fully convolutional architecture and can operate on input video stream with an arbitrary resolution. It produces ball confidence map encoding the position of the detected ball, player confidence map and player bounding boxes tensor encoding players' positions and bounding boxes. The network uses Feature Pyramid Network desing pattern, where lower level features with higher spatial resolution are combined with higher level features with bigger receptive field. This improves discriminability of small objects (the ball) as larger visual context around the object of interest is taken into account for the classification. Due to its specialized design, the network has two orders of magnitude less parameters than a generic deep neural network-based object detector, such as SSD or YOLO. This allows real-time processing of high resolution input video stream. Our code and pre-trained model can be found on the project website: https://github.com/jac99/FootAndBall .



### Neural Voxel Renderer: Learning an Accurate and Controllable Rendering Tool
- **Arxiv ID**: http://arxiv.org/abs/1912.04591v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1912.04591v2)
- **Published**: 2019-12-10 09:30:03+00:00
- **Updated**: 2020-04-06 14:34:47+00:00
- **Authors**: Konstantinos Rematas, Vittorio Ferrari
- **Comment**: Additional results: http://www.krematas.com/nvr/index.html
- **Journal**: None
- **Summary**: We present a neural rendering framework that maps a voxelized scene into a high quality image. Highly-textured objects and scene element interactions are realistically rendered by our method, despite having a rough representation as an input. Moreover, our approach allows controllable rendering: geometric and appearance modifications in the input are accurately propagated to the output. The user can move, rotate and scale an object, change its appearance and texture or modify the position of the light and all these edits are represented in the final rendering. We demonstrate the effectiveness of our approach by rendering scenes with varying appearance, from single color per object to complex, high-frequency textures. We show that our rerendering network can generate very detailed images that represent precisely the appearance of the input scene. Our experiments illustrate that our approach achieves more accurate image synthesis results compared to alternatives and can also handle low voxel grid resolutions. Finally, we show how our neural rendering framework can capture and faithfully render objects from real images and from a diverse set of classes.



### Severity Detection Tool for Patients with Infectious Disease
- **Arxiv ID**: http://arxiv.org/abs/1912.05345v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05345v1)
- **Published**: 2019-12-10 09:51:37+00:00
- **Updated**: 2019-12-10 09:51:37+00:00
- **Authors**: Girmaw Abebe Tadesse, Tingting Zhu, Nhan Le Nguyen Thanh, Nguyen Thanh Hung, Ha Thi Hai Duong, Truong Huu Khanh, Pham Van Quang, Duc Duong Tran, LamMinh Yen, H Rogier Van Doorn, Nguyen Van Hao, John Prince, Hamza Javed, DaniKiyasseh, Le Van Tan, Louise Thwaites, David A. Clifton
- **Comment**: None
- **Journal**: None
- **Summary**: Hand, foot and mouth disease (HFMD) and tetanus are serious infectious diseases in low and middle income countries. Tetanus in particular has a high mortality rate and its treatment is resource-demanding. Furthermore, HFMD often affects a large number of infants and young children. As a result, its treatment consumes enormous healthcare resources, especially when outbreaks occur. Autonomic nervous system dysfunction (ANSD) is the main cause of death for both HFMD and tetanus patients. However, early detection of ANSD is a difficult and challenging problem. In this paper, we aim to provide a proof-of-principle to detect the ANSD level automatically by applying machine learning techniques to physiological patient data, such as electrocardiogram (ECG) and photoplethysmogram (PPG) waveforms, which can be collected using low-cost wearable sensors. Efficient features are extracted that encode variations in the waveforms in the time and frequency domains. A support vector machine is employed to classify the ANSD levels. The proposed approach is validated on multiple datasets of HFMD and tetanus patients in Vietnam. Results show that encouraging performance is achieved in classifying ANSD levels. Moreover, the proposed features are simple, more generalisable and outperformed the standard heart rate variability (HRV) analysis. The proposed approach would facilitate both the diagnosis and treatment of infectious diseases in low and middle income countries, and thereby improve overall patient care.



### Forecasting future action sequences with attention: a new approach to weakly supervised action forecasting
- **Arxiv ID**: http://arxiv.org/abs/1912.04608v3
- **DOI**: 10.1109/TIP.2020.3021497
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04608v3)
- **Published**: 2019-12-10 10:08:21+00:00
- **Updated**: 2022-02-03 14:28:38+00:00
- **Authors**: Yan Bin Ng, Basura Fernando
- **Comment**: None
- **Journal**: in IEEE Transactions on Image Processing, vol. 29, pp. 8880-8891,
  2020
- **Summary**: Future human action forecasting from partial observations of activities is an important problem in many practical applications such as assistive robotics, video surveillance and security. We present a method to forecast actions for the unseen future of the video using a neural machine translation technique that uses encoder-decoder architecture. The input to this model is the observed RGB video, and the objective is to forecast the correct future symbolic action sequence. Unlike prior methods that make action predictions for some unseen percentage of video one for each frame, we predict the complete action sequence that is required to accomplish the activity. We coin this task action sequence forecasting. To cater for two types of uncertainty in the future predictions, we propose a novel loss function. We show a combination of optimal transport and future uncertainty losses help to improve results.   We extend our action sequence forecasting model to perform weakly supervised action forecasting on two challenging datasets, the Breakfast and the 50Salads. Specifically, we propose a model to predict actions of future unseen frames without using frame level annotations during training. Using Fisher vector features, our supervised model outperforms the state-of-the-art action forecasting model by 0.83% and 7.09% on the Breakfast and the 50Salads datasets respectively. Our weakly supervised model is only 0.6% behind the most recent state-of-the-art supervised model and obtains comparable results to other published fully supervised methods, and sometimes even outperforms them on the Breakfast dataset. Most interestingly, our weakly supervised model outperforms prior models by 1.04% leveraging on proposed weakly supervised architecture, and effective use of attention mechanism and loss functions.



### Deep Attention Based Semi-Supervised 2D-Pose Estimation for Surgical Instruments
- **Arxiv ID**: http://arxiv.org/abs/1912.04618v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04618v2)
- **Published**: 2019-12-10 10:27:22+00:00
- **Updated**: 2021-01-11 13:42:25+00:00
- **Authors**: Mert Kayhan, Okan Kpkl, Mhd Hasan Sarhan, Mehmet Yigitsoy, Abouzar Eslami, Gerhard Rigoll
- **Comment**: None
- **Journal**: None
- **Summary**: For many practical problems and applications, it is not feasible to create a vast and accurately labeled dataset, which restricts the application of deep learning in many areas. Semi-supervised learning algorithms intend to improve performance by also leveraging unlabeled data. This is very valuable for 2D-pose estimation task where data labeling requires substantial time and is subject to noise. This work aims to investigate if semi-supervised learning techniques can achieve acceptable performance level that makes using these algorithms during training justifiable. To this end, a lightweight network architecture is introduced and mean teacher, virtual adversarial training and pseudo-labeling algorithms are evaluated on 2D-pose estimation for surgical instruments. For the applicability of pseudo-labelling algorithm, we propose a novel confidence measure, total variation. Experimental results show that utilization of semi-supervised learning improves the performance on unseen geometries drastically while maintaining high accuracy for seen geometries. For RMIT benchmark, our lightweight architecture outperforms state-of-the-art with supervised learning. For Endovis benchmark, pseudo-labelling algorithm improves the supervised baseline achieving the new state-of-the-art performance.



### Inception Architecture and Residual Connections in Classification of Breast Cancer Histology Images
- **Arxiv ID**: http://arxiv.org/abs/1912.04619v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04619v1)
- **Published**: 2019-12-10 10:27:29+00:00
- **Updated**: 2019-12-10 10:27:29+00:00
- **Authors**: Mohammad Ibrahim Sarker, Hyongsuk Kim, Denis Tarasov, Dinar Akhmetzanov
- **Comment**: Achieved 23rd place out if 50 accepted positions (ICIAR Grand
  Challenge on Brest cancer histology images)
- **Journal**: None
- **Summary**: This paper presents results of applying Inception v4 deep convolutional neural network to ICIAR-2018 Breast Cancer Classification Grand Challenge, part a. The Challenge task is to classify breast cancer biopsy results, presented in form of hematoxylin and eosin stained images. Breast cancer classification is of primary interest to the medical practitioners and thus binary classification of breast cancer images have been under investigation by many researchers, but multi-class categorization of histology breast images have been challenging due to the subtle differences among the categories. In this work extensive data augmentation is conducted to reduce overfitting and effectiveness of committee of several Inception v4 networks is studied. We report 89% accuracy on 4 class classification task and 93.7% on carcinoma/non-carcinoma two class classification task using our test set of 80 images.



### SuperNCN: Neighbourhood consensus network for robust outdoor scenes matching
- **Arxiv ID**: http://arxiv.org/abs/1912.04627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04627v1)
- **Published**: 2019-12-10 10:36:38+00:00
- **Updated**: 2019-12-10 10:36:38+00:00
- **Authors**: Grzegorz Kurzejamski, Jacek Komorowski, Lukasz Dabala, Konrad Czarnota, Simon Lynen, Tomasz Trzcinski
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a framework for computing dense keypoint correspondences between images under strong scene appearance changes. Traditional methods, based on nearest neighbour search in the feature descriptor space, perform poorly when environmental conditions vary, e.g. when images are taken at different times of the day or seasons. Our method improves finding keypoint correspondences in such difficult conditions. First, we use Neighbourhood Consensus Networks to build spatially consistent matching grid between two images at a coarse scale. Then, we apply Superpoint-like corner detector to achieve pixel-level accuracy. Both parts use features learned with domain adaptation to increase robustness against strong scene appearance variations. The framework has been tested on a RobotCar Seasons dataset, proving large improvement on pose estimation task under challenging environmental conditions.



### WCE Polyp Detection with Triplet based Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1912.04643v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.04643v3)
- **Published**: 2019-12-10 11:08:45+00:00
- **Updated**: 2020-10-02 11:51:38+00:00
- **Authors**: Pablo Laiz, Jordi Vitri, Hagen Wenzek, Carolina Malagelada, Fernando Azpiroz, Santi Segu
- **Comment**: 19 pages, 13 figures, 9 tables, Accepted in Computerized Medical
  Imaging and Graphics
- **Journal**: None
- **Summary**: Wireless capsule endoscopy is a medical procedure used to visualize the entire gastrointestinal tract and to diagnose intestinal conditions, such as polyps or bleeding. Current analyses are performed by manually inspecting nearly each one of the frames of the video, a tedious and error-prone task. Automatic image analysis methods can be used to reduce the time needed for physicians to evaluate a capsule endoscopy video, however these methods are still in a research phase. In this paper we focus on computer-aided polyp detection in capsule endoscopy images. This is a challenging problem because of the diversity of polyp appearance, the imbalanced dataset structure and the scarcity of data. We have developed a new polyp computer-aided decision system that combines a deep convolutional neural network and metric learning. The key point of the method is the use of the triplet loss function with the aim of improving feature extraction from the images when having small dataset. The triplet loss function allows to train robust detectors by forcing images from the same category to be represented by similar embedding vectors while ensuring that images from different categories are represented by dissimilar vectors. Empirical results show a meaningful increase of AUC values compared to baseline methods. A good performance is not the only requirement when considering the adoption of this technology to clinical practice. Trust and explainability of decisions are as important as performance. With this purpose, we also provide a method to generate visual explanations of the outcome of our polyp detector. These explanations can be used to build a physician's trust in the system and also to convey information about the inner working of the method to the designer for debugging purposes.



### Neural Point Cloud Rendering via Multi-Plane Projection
- **Arxiv ID**: http://arxiv.org/abs/1912.04645v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04645v2)
- **Published**: 2019-12-10 11:11:50+00:00
- **Updated**: 2020-06-25 04:28:05+00:00
- **Authors**: Peng Dai, Yinda Zhang, Zhuwen Li, Shuaicheng Liu, Bing Zeng
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: We present a new deep point cloud rendering pipeline through multi-plane projections. The input to the network is the raw point cloud of a scene and the output are image or image sequences from a novel view or along a novel camera trajectory. Unlike previous approaches that directly project features from 3D points onto 2D image domain, we propose to project these features into a layered volume of camera frustum. In this way, the visibility of 3D points can be automatically learnt by the network, such that ghosting effects due to false visibility check as well as occlusions caused by noise interferences are both avoided successfully. Next, the 3D feature volume is fed into a 3D CNN to produce multiple layers of images w.r.t. the space division in the depth directions. The layered images are then blended based on learned weights to produce the final rendering results. Experiments show that our network produces more stable renderings compared to previous methods, especially near the object boundaries. Moreover, our pipeline is robust to noisy and relatively sparse point cloud for a variety of challenging scenes.



### 3D-GMNet: Single-View 3D Shape Recovery as A Gaussian Mixture
- **Arxiv ID**: http://arxiv.org/abs/1912.04663v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04663v2)
- **Published**: 2019-12-10 12:23:24+00:00
- **Updated**: 2020-08-15 14:18:31+00:00
- **Authors**: Kohei Yamashita, Shohei Nobuhara, Ko Nishino
- **Comment**: BMVC 2020
- **Journal**: None
- **Summary**: In this paper, we introduce 3D-GMNet, a deep neural network for 3D object shape reconstruction from a single image. As the name suggests, 3D-GMNet recovers 3D shape as a Gaussian mixture. In contrast to voxels, point clouds, or meshes, a Gaussian mixture representation provides an analytical expression with a small memory footprint while accurately representing the target 3D shape. At the same time, it offers a number of additional advantages including instant pose estimation and controllable level-of-detail reconstruction, while also enabling interpretation as a point cloud, volume, and a mesh model. We train 3D-GMNet end-to-end with single input images and corresponding 3D models by introducing two novel loss functions, a 3D Gaussian mixture loss and a 2D multi-view loss, which collectively enable accurate shape reconstruction as kernel density estimation. We thoroughly evaluate the effectiveness of 3D-GMNet with synthetic and real images of objects. The results show accurate reconstruction with a compact representation that also realizes novel applications of single-image 3D reconstruction.



### DR-GAN: Conditional Generative Adversarial Network for Fine-Grained Lesion Synthesis on Diabetic Retinopathy Images
- **Arxiv ID**: http://arxiv.org/abs/1912.04670v3
- **DOI**: 10.1109/JBHI.2020.3045475
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04670v3)
- **Published**: 2019-12-10 12:53:09+00:00
- **Updated**: 2020-11-11 14:20:50+00:00
- **Authors**: Yi Zhou, Boyang Wang, Xiaodong He, Shanshan Cui, Ling Shao
- **Comment**: Extension work of our MICCAI paper
- **Journal**: IEEE Journal of Biomedical and Health Informatics 2020
- **Summary**: Diabetic retinopathy (DR) is a complication of diabetes that severely affects eyes. It can be graded into five levels of severity according to international protocol. However, optimizing a grading model to have strong generalizability requires a large amount of balanced training data, which is difficult to collect particularly for the high severity levels. Typical data augmentation methods, including random flipping and rotation, cannot generate data with high diversity. In this paper, we propose a diabetic retinopathy generative adversarial network (DR-GAN) to synthesize high-resolution fundus images which can be manipulated with arbitrary grading and lesion information. Thus, large-scale generated data can be used for more meaningful augmentation to train a DR grading and lesion segmentation model. The proposed retina generator is conditioned on the structural and lesion masks, as well as adaptive grading vectors sampled from the latent grading space, which can be adopted to control the synthesized grading severity. Moreover, a multi-scale spatial and channel attention module is devised to improve the generation ability to synthesize details. Multi-scale discriminators are designed to operate from large to small receptive fields, and joint adversarial losses are adopted to optimize the whole network in an end-to-end manner. With extensive experiments evaluated on the EyePACS dataset connected to Kaggle, as well as the FGADR dataset, we validate the effectiveness of our method, which can both synthesize highly realistic (1280 x 1280) controllable fundus images and contribute to the DR grading task.



### Bias Remediation in Driver Drowsiness Detection systems using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.12123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12123v1)
- **Published**: 2019-12-10 13:04:52+00:00
- **Updated**: 2019-12-10 13:04:52+00:00
- **Authors**: Mkhuseli Ngxande, Jules-Raymond Tapamo, Michael Burke
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Datasets are crucial when training a deep neural network. When datasets are unrepresentative, trained models are prone to bias because they are unable to generalise to real world settings. This is particularly problematic for models trained in specific cultural contexts, which may not represent a wide range of races, and thus fail to generalise. This is a particular challenge for Driver drowsiness detection, where many publicly available datasets are unrepresentative as they cover only certain ethnicity groups. Traditional augmentation methods are unable to improve a model's performance when tested on other groups with different facial attributes, and it is often challenging to build new, more representative datasets. In this paper, we introduce a novel framework that boosts the performance of detection of drowsiness for different ethnicity groups. Our framework improves Convolutional Neural Network (CNN) trained for prediction by using Generative Adversarial networks (GAN) for targeted data augmentation based on a population bias visualisation strategy that groups faces with similar facial attributes and highlights where the model is failing. A sampling method selects faces where the model is not performing well, which are used to fine-tune the CNN. Experiments show the efficacy of our approach in improving driver drowsiness detection for under represented ethnicity groups. Here, models trained on publicly available datasets are compared with a model trained using the proposed data augmentation strategy. Although developed in the context of driver drowsiness detection, the proposed framework is not limited to the driver drowsiness detection task, but can be applied to other applications.



### End-to-end facial and physiological model for Affective Computing and applications
- **Arxiv ID**: http://arxiv.org/abs/1912.04711v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1912.04711v2)
- **Published**: 2019-12-10 14:37:15+00:00
- **Updated**: 2020-01-20 11:25:55+00:00
- **Authors**: Joaquim Comas, Decky Aspandi, Xavier Binefa
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, Affective Computing and its applications have become a fast-growing research topic. Furthermore, the rise of Deep Learning has introduced significant improvements in the emotion recognition system compared to classical methods. In this work, we propose a multi-modal emotion recognition model based on deep learning techniques using the combination of peripheral physiological signals and facial expressions. Moreover, we present an improvement to proposed models by introducing latent features extracted from our internal Bio Auto-Encoder (BAE). Both models are trained and evaluated on AMIGOS datasets reporting valence, arousal, and emotion state classification. Finally, to demonstrate a possible medical application in affective computing using deep learning techniques, we applied the proposed method to the assessment of anxiety therapy. To this purpose, a reduced multi-modal database has been collected by recording facial expressions and peripheral signals such as Electrocardiogram (ECG) and Galvanic Skin Response (GSR) of each patient. Valence and arousal estimation was extracted using the proposed model from the beginning until the end of the therapy, with successful evaluation to the different emotional changes in the temporal domain.



### Efficient Differentiable Neural Architecture Search with Meta Kernels
- **Arxiv ID**: http://arxiv.org/abs/1912.04749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04749v1)
- **Published**: 2019-12-10 15:08:50+00:00
- **Updated**: 2019-12-10 15:08:50+00:00
- **Authors**: Shoufa Chen, Yunpeng Chen, Shuicheng Yan, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: The searching procedure of neural architecture search (NAS) is notoriously time consuming and cost prohibitive.To make the search space continuous, most existing gradient-based NAS methods relax the categorical choice of a particular operation to a softmax over all possible operations and calculate the weighted sum of multiple features, resulting in a large memory requirement and a huge computation burden. In this work, we propose an efficient and novel search strategy with meta kernels. We directly encode the supernet from the perspective on convolution kernels and "shrink" multiple convolution kernel candidates into a single one before these candidates operate on the input feature. In this way, only a single feature is generated between two intermediate nodes. The memory for storing intermediate features and the resource budget for conducting convolution operations are both reduced remarkably. Despite high efficiency, our search strategy can search in a more fine-grained way than existing works and increases the capacity for representing possible networks. We demonstrate the effectiveness of our search strategy by conducting extensive experiments. Specifically, our method achieves 77.0% top-1 accuracy on ImageNet benchmark dataset with merely 357M FLOPs, outperforming both EfficientNet and MobileNetV3 under the same FLOPs constraints. Compared to models discovered by the start-of-the-art NAS method, our method achieves the same (sometimes even better) performance, while faster by three orders of magnitude.



### Context-Aware Dynamic Feature Extraction for 3D Object Detection in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.04775v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04775v3)
- **Published**: 2019-12-10 15:46:28+00:00
- **Updated**: 2020-07-28 15:42:39+00:00
- **Authors**: Yonglin Tian, Lichao Huang, Xuesong Li, Kunfeng Wang, Zilei Wang, Fei-Yue Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Varying density of point clouds increases the difficulty of 3D detection. In this paper, we present a context-aware dynamic network (CADNet) to capture the variance of density by considering both point context and semantic context. Point-level contexts are generated from original point clouds to enlarge the effective receptive filed. They are extracted around the voxelized pillars based on our extended voxelization method and processed with the context encoder in parallel with the pillar features. With a large perception range, we are able to capture the variance of features for potential objects and generate attentive spatial guidance to help adjust the strengths for different regions. In the region proposal network, considering the limited representation ability of traditional convolution where same kernels are shared among different samples and positions, we propose a decomposable dynamic convolutional layer to adapt to the variance of input features by learning from local semantic context. It adaptively generates the position-dependent coefficients for multiple fixed kernels and combines them to convolve with local feature windows. Based on our dynamic convolution, we design a dual-path convolution block to further improve the representation ability. We conduct experiments with our Network on KITTI dataset and achieve good performance on 3D detection task for both precision and speed. Our one-stage detector outperforms SECOND and PointPillars by a large margin and achieves the speed of 30 FPS.



### Frivolous Units: Wider Networks Are Not Really That Wide
- **Arxiv ID**: http://arxiv.org/abs/1912.04783v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.04783v5)
- **Published**: 2019-12-10 15:53:45+00:00
- **Updated**: 2021-05-31 23:42:59+00:00
- **Authors**: Stephen Casper, Xavier Boix, Vanessa D'Amario, Ling Guo, Martin Schrimpf, Kasper Vinken, Gabriel Kreiman
- **Comment**: None
- **Journal**: Proceedings of the AAAI Conference on Artificial Intelligence,
  2021
- **Summary**: A remarkable characteristic of overparameterized deep neural networks (DNNs) is that their accuracy does not degrade when the network's width is increased. Recent evidence suggests that developing compressible representations is key for adjusting the complexity of large networks to the learning task at hand. However, these compressible representations are poorly understood. A promising strand of research inspired from biology is understanding representations at the unit level as it offers a more granular and intuitive interpretation of the neural mechanisms. In order to better understand what facilitates increases in width without decreases in accuracy, we ask: Are there mechanisms at the unit level by which networks control their effective complexity as their width is increased? If so, how do these depend on the architecture, dataset, and training parameters? We identify two distinct types of "frivolous" units that proliferate when the network's width is increased: prunable units which can be dropped out of the network without significant change to the output and redundant units whose activities can be expressed as a linear combination of others. These units imply complexity constraints as the function the network represents could be expressed by a network without them. We also identify how the development of these units can be influenced by architecture and a number of training factors. Together, these results help to explain why the accuracy of DNNs does not degrade when width is increased and highlight the importance of frivolous units toward understanding implicit regularization in DNNs.



### Learning Depth-Guided Convolutions for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.04799v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04799v2)
- **Published**: 2019-12-10 16:31:22+00:00
- **Updated**: 2019-12-13 08:46:29+00:00
- **Authors**: Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu, Ping Luo
- **Comment**: 12 pages, 8 figures, modify email and add code
- **Journal**: None
- **Summary**: 3D object detection from a single image without LiDAR is a challenging task due to the lack of accurate depth information. Conventional 2D convolutions are unsuitable for this task because they fail to capture local object and its scale information, which are vital for 3D object detection. To better represent 3D structure, prior arts typically transform depth maps estimated from 2D images into a pseudo-LiDAR representation, and then apply existing 3D point-cloud based object detectors. However, their results depend heavily on the accuracy of the estimated depth maps, resulting in suboptimal performance. In this work, instead of using pseudo-LiDAR representation, we improve the fundamental 2D fully convolutions by proposing a new local convolutional network (LCN), termed Depth-guided Dynamic-Depthwise-Dilated LCN (D$^4$LCN), where the filters and their receptive fields can be automatically learned from image-based depth maps, making different pixels of different images have different filters. D$^4$LCN overcomes the limitation of conventional 2D convolutions and narrows the gap between image representation and 3D point cloud representation. Extensive experiments show that D$^4$LCN outperforms existing works by large margins. For example, the relative improvement of D$^4$LCN against the state-of-the-art on KITTI is 9.1\% in the moderate setting. The code is available at https://github.com/dingmyu/D4LCN.



### Detection of Collision-Prone Vehicle Behavior at Intersections using Siamese Interaction LSTM
- **Arxiv ID**: http://arxiv.org/abs/1912.04801v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04801v1)
- **Published**: 2019-12-10 16:35:10+00:00
- **Updated**: 2019-12-10 16:35:10+00:00
- **Authors**: Debaditya Roy, Tetsuhiro Ishizaka, Krishna Mohan C., Atsushi Fukuda
- **Comment**: 10 pages, 4 figures, submitted to IEEE Transactions on Intelligent
  Transportation Systems
- **Journal**: None
- **Summary**: As a large proportion of road accidents occur at intersections, monitoring traffic safety of intersections is important. Existing approaches are designed to investigate accidents in lane-based traffic. However, such approaches are not suitable in a lane-less mixed-traffic environment where vehicles often ply very close to each other. Hence, we propose an approach called Siamese Interaction Long Short-Term Memory network (SILSTM) to detect collision prone vehicle behavior. The SILSTM network learns the interaction trajectory of a vehicle that describes the interactions of a vehicle with its neighbors at an intersection. Among the hundreds of interactions for every vehicle, there maybe only some interactions which may be unsafe and hence, a temporal attention layer is used in the SILSTM network. Furthermore, the comparison of interaction trajectories requires labeling the trajectories as either unsafe or safe, but such a distinction is highly subjective, especially in lane-less traffic. Hence, in this work, we compute the characteristics of interaction trajectories involved in accidents using the collision energy model. The interaction trajectories that match accident characteristics are labeled as unsafe while the rest are considered safe. Finally, there is no existing dataset that allows us to monitor a particular intersection for a long duration. Therefore, we introduce the SkyEye dataset that contains 1 hour of continuous aerial footage from each of the 4 chosen intersections in the city of Ahmedabad in India. A detailed evaluation of SILSTM on the SkyEye dataset shows that unsafe (collision-prone) interaction trajectories can be effectively detected at different intersections.



### Scalability in Perception for Autonomous Driving: Waymo Open Dataset
- **Arxiv ID**: http://arxiv.org/abs/1912.04838v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.04838v7)
- **Published**: 2019-12-10 17:28:55+00:00
- **Updated**: 2020-05-12 23:28:05+00:00
- **Authors**: Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Sheng Zhao, Shuyang Cheng, Yu Zhang, Jonathon Shlens, Zhifeng Chen, Dragomir Anguelov
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.



### SKD: Keypoint Detection for Point Clouds using Saliency Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.04943v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04943v3)
- **Published**: 2019-12-10 19:30:00+00:00
- **Updated**: 2021-02-27 19:24:06+00:00
- **Authors**: Georgi Tinchev, Adrian Penate-Sanchez, Maurice Fallon
- **Comment**: Accepted for publication at 2021 IEEE Robotics and Automation Letters
  (RA-L) + IEEE International Conference on Robotics and Automation (ICRA)
  presentation option. Video preview available here:
  https://youtu.be/Wx6FEWCgWDk
- **Journal**: None
- **Summary**: We present SKD, a novel keypoint detector that uses saliency to determine the best candidates from a point cloud for tasks such as registration and reconstruction. The approach can be applied to any differentiable deep learning descriptor by using the gradients of that descriptor with respect to the 3D position of the input points as a measure of their saliency. The saliency is combined with the original descriptor and context information in a neural network, which is trained to learn robust keypoint candidates. The key intuition behind this approach is that keypoints are not extracted solely as a result of the geometry surrounding a point, but also take into account the descriptor's response. The approach was evaluated on two large LIDAR datasets - the Oxford RobotCar dataset and the KITTI dataset, where we obtain up to 50% improvement over the state-of-the-art in both matchability and repeatability. When performing sparse matching with the keypoints computed by our method we achieve a higher inlier ratio and faster convergence.



### HyperCon: Image-To-Video Model Transfer for Video-To-Video Translation Tasks
- **Arxiv ID**: http://arxiv.org/abs/1912.04950v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04950v2)
- **Published**: 2019-12-10 19:47:53+00:00
- **Updated**: 2020-11-10 16:18:34+00:00
- **Authors**: Ryan Szeto, Mostafa El-Khamy, Jungwon Lee, Jason J. Corso
- **Comment**: Accepted to WACV 2021
- **Journal**: None
- **Summary**: Video-to-video translation is more difficult than image-to-image translation due to the temporal consistency problem that, if unaddressed, leads to distracting flickering effects. Although video models designed from scratch produce temporally consistent results, training them to match the vast visual knowledge captured by image models requires an intractable number of videos. To combine the benefits of image and video models, we propose an image-to-video model transfer method called Hyperconsistency (HyperCon) that transforms any well-trained image model into a temporally consistent video model without fine-tuning. HyperCon works by translating a temporally interpolated video frame-wise and then aggregating over temporally localized windows on the interpolated video. It handles both masked and unmasked inputs, enabling support for even more video-to-video translation tasks than prior image-to-video model transfer techniques. We demonstrate HyperCon on video style transfer and inpainting, where it performs favorably compared to prior state-of-the-art methods without training on a single stylized or incomplete video. Our project website is available at https://ryanszeto.com/projects/hypercon .



### Occlusion-Robust Online Multi-Object Visual Tracking using a GM-PHD Filter with CNN-Based Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1912.05949v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05949v6)
- **Published**: 2019-12-10 20:18:42+00:00
- **Updated**: 2021-08-05 11:14:04+00:00
- **Authors**: Nathanael L. Baisa
- **Comment**: arXiv admin note: text overlap with arXiv:1908.03945
- **Journal**: None
- **Summary**: We propose a novel online multi-object visual tracker using a Gaussian mixture Probability Hypothesis Density (GM-PHD) filter and deep appearance learning. The GM-PHD filter has a linear complexity with the number of objects and observations while estimating the states and cardinality of time-varying number of objects, however, it is susceptible to miss-detections and does not include the identity of objects. We use visual-spatio-temporal information obtained from object bounding boxes and deeply learned appearance representations to perform estimates-to-tracks data association for target labeling as well as formulate an augmented likelihood and then integrate into the update step of the GM-PHD filter. We also employ additional unassigned tracks prediction after the data association step to overcome the susceptibility of the GM-PHD filter towards miss-detections caused by occlusion. Extensive evaluations on MOT16, MOT17 and HiEve benchmark datasets show that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy and identification.



### A Two-Stage Approach to Few-Shot Learning for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.04973v1
- **DOI**: 10.1109/TIP.2019.2959254
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.04973v1)
- **Published**: 2019-12-10 20:45:35+00:00
- **Updated**: 2019-12-10 20:45:35+00:00
- **Authors**: Debasmit Das, C. S. George Lee
- **Comment**: To Appear in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: This paper proposes a multi-layer neural network structure for few-shot image recognition of novel categories. The proposed multi-layer neural network architecture encodes transferable knowledge extracted from a large annotated dataset of base categories. This architecture is then applied to novel categories containing only a few samples. The transfer of knowledge is carried out at the feature-extraction and the classification levels distributed across the two training stages. In the first-training stage, we introduce the relative feature to capture the structure of the data as well as obtain a low-dimensional discriminative space. Secondly, we account for the variable variance of different categories by using a network to predict the variance of each class. Classification is then performed by computing the Mahalanobis distance to the mean-class representation in contrast to previous approaches that used the Euclidean distance. In the second-training stage, a category-agnostic mapping is learned from the mean-sample representation to its corresponding class-prototype representation. This is because the mean-sample representation may not accurately represent the novel category prototype. Finally, we evaluate the proposed network structure on four standard few-shot image recognition datasets, where our proposed few-shot learning system produces competitive performance compared to previous work. We also extensively studied and analyzed the contribution of each component of our proposed framework.



### Learning to Optimally Segment Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.04976v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04976v1)
- **Published**: 2019-12-10 20:53:18+00:00
- **Updated**: 2019-12-10 20:53:18+00:00
- **Authors**: Peiyun Hu, David Held, Deva Ramanan
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on the problem of class-agnostic instance segmentation of LiDAR point clouds. We propose an approach that combines graph-theoretic search with data-driven learning: it searches over a set of candidate segmentations and returns one where individual segments score well according to a data-driven point-based model of "objectness". We prove that if we score a segmentation by the worst objectness among its individual segments, there is an efficient algorithm that finds the optimal worst-case segmentation among an exponentially large number of candidate segmentations. We also present an efficient algorithm for the average-case. For evaluation, we repurpose KITTI 3D detection as a segmentation benchmark and empirically demonstrate that our algorithms significantly outperform past bottom-up segmentation approaches and top-down object-based algorithms on segmenting point clouds.



### Advances in Online Audio-Visual Meeting Transcription
- **Arxiv ID**: http://arxiv.org/abs/1912.04979v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CL, cs.CV, cs.SD, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04979v1)
- **Published**: 2019-12-10 20:59:24+00:00
- **Updated**: 2019-12-10 20:59:24+00:00
- **Authors**: Takuya Yoshioka, Igor Abramovski, Cem Aksoylar, Zhuo Chen, Moshe David, Dimitrios Dimitriadis, Yifan Gong, Ilya Gurvich, Xuedong Huang, Yan Huang, Aviv Hurvitz, Li Jiang, Sharon Koubi, Eyal Krupka, Ido Leichter, Changliang Liu, Partha Parthasarathy, Alon Vinnikov, Lingfeng Wu, Xiong Xiao, Wayne Xiong, Huaming Wang, Zhenghao Wang, Jun Zhang, Yong Zhao, Tianyan Zhou
- **Comment**: To appear in Proc. IEEE ASRU Workshop 2019
- **Journal**: None
- **Summary**: This paper describes a system that generates speaker-annotated transcripts of meetings by using a microphone array and a 360-degree camera. The hallmark of the system is its ability to handle overlapped speech, which has been an unsolved problem in realistic settings for over a decade. We show that this problem can be addressed by using a continuous speech separation approach. In addition, we describe an online audio-visual speaker diarization method that leverages face tracking and identification, sound source localization, speaker identification, and, if available, prior speaker information for robustness to various real world challenges. All components are integrated in a meeting transcription framework called SRD, which stands for "separate, recognize, and diarize". Experimental results using recordings of natural meetings involving up to 11 attendees are reported. The continuous speech separation improves a word error rate (WER) by 16.1% compared with a highly tuned beamformer. When a complete list of meeting attendees is available, the discrepancy between WER and speaker-attributed WER is only 1.0%, indicating accurate word-to-speaker association. This increases marginally to 1.6% when 50% of the attendees are unknown to the system.



### Phase Retrieval Using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.04981v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.04981v2)
- **Published**: 2019-12-10 21:03:59+00:00
- **Updated**: 2020-07-08 07:37:49+00:00
- **Authors**: Tobias Uelwer, Alexander Oberstra, Stefan Harmeling
- **Comment**: Accepted at the 25th International Conference on Pattern Recognition
  2020 (ICPR)
- **Journal**: None
- **Summary**: In this paper, we propose the application of conditional generative adversarial networks to solve various phase retrieval problems. We show that including knowledge of the measurement process at training time leads to an optimization at test time that is more robust to initialization than existing approaches involving generative models. In addition, conditioning the generator network on the measurements enables us to achieve much more detailed results. We empirically demonstrate that these advantages provide meaningful solutions to the Fourier and the compressive phase retrieval problem and that our method outperforms well-established projection-based methods as well as existing methods that are based on neural networks. Like other deep learning methods, our approach is very robust to noise and can therefore be very useful for real-world applications.



### What You See is What You Get: Exploiting Visibility for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.04986v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.04986v3)
- **Published**: 2019-12-10 21:15:37+00:00
- **Updated**: 2020-12-21 22:42:25+00:00
- **Authors**: Peiyun Hu, Jason Ziglar, David Held, Deva Ramanan
- **Comment**: CVPR'20. More at https://www.cs.cmu.edu/~peiyunh/wysiwyg
- **Journal**: None
- **Summary**: Recent advances in 3D sensing have created unique challenges for computer vision. One fundamental challenge is finding a good representation for 3D sensor data. Most popular representations (such as PointNet) are proposed in the context of processing truly 3D data (e.g. points sampled from mesh models), ignoring the fact that 3D sensored data such as a LiDAR sweep is in fact 2.5D. We argue that representing 2.5D data as collections of (x, y, z) points fundamentally destroys hidden information about freespace. In this paper, we demonstrate such knowledge can be efficiently recovered through 3D raycasting and readily incorporated into batch-based gradient learning. We describe a simple approach to augmenting voxel-based networks with visibility: we add a voxelized visibility map as an additional input stream. In addition, we show that visibility can be combined with two crucial modifications common to state-of-the-art 3D detectors: synthetic data augmentation of virtual objects and temporal aggregation of LiDAR sweeps over multiple time frames. On the NuScenes 3D detection benchmark, we show that, by adding an additional stream for visibility input, we can significantly improve the overall detection accuracy of a state-of-the-art 3D detector.



### Learning Domain Adaptive Features with Unlabeled Domain Bridges
- **Arxiv ID**: http://arxiv.org/abs/1912.05004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05004v1)
- **Published**: 2019-12-10 22:07:59+00:00
- **Updated**: 2019-12-10 22:07:59+00:00
- **Authors**: Yichen Li, Xingchao Peng
- **Comment**: Both authors contributed equally
- **Journal**: None
- **Summary**: Conventional cross-domain image-to-image translation or unsupervised domain adaptation methods assume that the source domain and target domain are closely related. This neglects a practical scenario where the domain discrepancy between the source and target is excessively large. In this paper, we propose a novel approach to learn domain adaptive features between the largely-gapped source and target domains with unlabeled domain bridges. Firstly, we introduce the framework of Cycle-consistency Flow Generative Adversarial Networks (CFGAN) that utilizes domain bridges to perform image-to-image translation between two distantly distributed domains. Secondly, we propose the Prototypical Adversarial Domain Adaptation (PADA) model which utilizes unlabeled bridge domains to align feature distribution between source and target with a large discrepancy. Extensive quantitative and qualitative experiments are conducted to demonstrate the effectiveness of our proposed models.



### SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization
- **Arxiv ID**: http://arxiv.org/abs/1912.05027v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05027v3)
- **Published**: 2019-12-10 22:13:42+00:00
- **Updated**: 2020-06-17 16:37:15+00:00
- **Authors**: Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, Quoc V. Le, Xiaodan Song
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Convolutional neural networks typically encode an input image into a series of intermediate features with decreasing resolutions. While this structure is suited to classification tasks, it does not perform well for tasks requiring simultaneous recognition and localization (e.g., object detection). The encoder-decoder architectures are proposed to resolve this by applying a decoder network onto a backbone model designed for classification tasks. In this paper, we argue encoder-decoder architecture is ineffective in generating strong multi-scale features because of the scale-decreased backbone. We propose SpineNet, a backbone with scale-permuted intermediate features and cross-scale connections that is learned on an object detection task by Neural Architecture Search. Using similar building blocks, SpineNet models outperform ResNet-FPN models by ~3% AP at various scales while using 10-20% fewer FLOPs. In particular, SpineNet-190 achieves 52.5% AP with a MaskR-CNN detector and achieves 52.1% AP with a RetinaNet detector on COCO for a single model without test-time augmentation, significantly outperforms prior art of detectors. SpineNet can transfer to classification tasks, achieving 5% top-1 accuracy improvement on a challenging iNaturalist fine-grained dataset. Code is at: https://github.com/tensorflow/tpu/tree/master/models/official/detection.



### Deep Adaptive Wavelet Network
- **Arxiv ID**: http://arxiv.org/abs/1912.05035v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05035v1)
- **Published**: 2019-12-10 22:43:16+00:00
- **Updated**: 2019-12-10 22:43:16+00:00
- **Authors**: Maria Ximena Bastidas Rodriguez, Adrien Gruson, Luisa F. Polania, Shin Fujieda, Flavio Prieto Ortiz, Kohei Takayama, Toshiya Hachisuka
- **Comment**: None
- **Journal**: None
- **Summary**: Even though convolutional neural networks have become the method of choice in many fields of computer vision, they still lack interpretability and are usually designed manually in a cumbersome trial-and-error process. This paper aims at overcoming those limitations by proposing a deep neural network, which is designed in a systematic fashion and is interpretable, by integrating multiresolution analysis at the core of the deep neural network design. By using the lifting scheme, it is possible to generate a wavelet representation and design a network capable of learning wavelet coefficients in an end-to-end form. Compared to state-of-the-art architectures, the proposed model requires less hyper-parameter tuning and achieves competitive accuracy in image classification tasks




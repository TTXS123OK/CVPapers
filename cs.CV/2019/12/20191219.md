# Arxiv Papers in cs.CV on 2019-12-19
### Fashion Outfit Complementary Item Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1912.08967v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08967v2)
- **Published**: 2019-12-19 00:53:41+00:00
- **Updated**: 2020-03-04 22:24:34+00:00
- **Authors**: Yen-Liang Lin, Son Tran, Larry S. Davis
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Complementary fashion item recommendation is critical for fashion outfit completion. Existing methods mainly focus on outfit compatibility prediction but not in a retrieval setting. We propose a new framework for outfit complementary item retrieval. Specifically, a category-based subspace attention network is presented, which is a scalable approach for learning the subspace attentions. In addition, we introduce an outfit ranking loss that better models the item relationships of an entire outfit. We evaluate our method on the outfit compatibility, FITB and new retrieval tasks. Experimental results demonstrate that our approach outperforms state-of-the-art methods in both compatibility prediction and complementary item retrieval



### Learning a Spatio-Temporal Embedding for Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.08969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08969v1)
- **Published**: 2019-12-19 00:59:50+00:00
- **Updated**: 2019-12-19 00:59:50+00:00
- **Authors**: Anthony Hu, Alex Kendall, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel embedding approach for video instance segmentation. Our method learns a spatio-temporal embedding integrating cues from appearance, motion, and geometry; a 3D causal convolutional network models motion, and a monocular self-supervised depth loss models geometry. In this embedding space, video-pixels of the same instance are clustered together while being separated from other instances, to naturally track instances over time without any complex post-processing. Our network runs in real-time as our architecture is entirely causal - we do not incorporate information from future frames, contrary to previous methods. We show that our model can accurately track and segment instances, even with occlusions and missed detections, advancing the state-of-the-art on the KITTI Multi-Object and Tracking Dataset.



### Metamorphic Testing for Object Detection Systems
- **Arxiv ID**: http://arxiv.org/abs/1912.12162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12162v1)
- **Published**: 2019-12-19 02:03:46+00:00
- **Updated**: 2019-12-19 02:03:46+00:00
- **Authors**: Shuai Wang, Zhendong Su
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep neural networks (DNNs) have led to object detectors that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based object detection as a standard computer vision service, object detection systems - similar to traditional software - may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users of these object detection systems. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, principled, systematic methods for testing object detection systems do not yet exist, despite their importance.   To fill this critical gap, we introduce the design and realization of MetaOD, the first metamorphic testing system for object detectors to effectively reveal erroneous detection results by commercial object detectors. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of object detection results between the original and synthetic images after excluding the prediction results on the inserted objects. MetaOD is designed as a streamlined workflow that performs object extraction, selection, and insertion. Evaluated on four commercial object detection services and four pretrained models provided by the TensorFlow API, MetaOD found tens of thousands of detection defects in these object detectors. To further demonstrate the practical usage of MetaOD, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is increased significantly, from an mAP score of 9.3 to an mAP score of 10.5.



### TextTubes for Detecting Curved Text in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1912.08990v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.08990v1)
- **Published**: 2019-12-19 02:13:08+00:00
- **Updated**: 2019-12-19 02:13:08+00:00
- **Authors**: Joël Seytre, Jon Wu, Alessandro Achille
- **Comment**: None
- **Journal**: None
- **Summary**: We present a detector for curved text in natural images. We model scene text instances as tubes around their medial axes and introduce a parametrization-invariant loss function. We train a two-stage curved text detector, and evaluate it on the curved text benchmarks CTW-1500 and Total-Text. Our approach achieves state-of-the-art results or improves upon them, notably for CTW-1500 by over 8 percentage points in F-score.



### AANet: Attribute Attention Network for Person Re-Identifications
- **Arxiv ID**: http://arxiv.org/abs/1912.09021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09021v1)
- **Published**: 2019-12-19 05:19:45+00:00
- **Updated**: 2019-12-19 05:19:45+00:00
- **Authors**: Chiat-Pin Tay, Sharmili Roy, Kim-Hui Yap
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: This paper proposes Attribute Attention Network (AANet), a new architecture that integrates person attributes and attribute attention maps into a classification framework to solve the person re-identification (re-ID) problem. Many person re-ID models typically employ semantic cues such as body parts or human pose to improve the re-ID performance. Attribute information, however, is often not utilized. The proposed AANet leverages on a baseline model that uses body parts and integrates the key attribute information in an unified learning framework. The AANet consists of a global person ID task, a part detection task and a crucial attribute detection task. By estimating the class responses of individual attributes and combining them to form the attribute attention map (AAM), a very strong discriminatory representation is constructed. The proposed AANet outperforms the best state-of-the-art method arXiv:1711.09349v3 [cs.CV] using ResNet-50 by 3.36% in mAP and 3.12% in Rank-1 accuracy on DukeMTMC-reID dataset. On Market1501 dataset, AANet achieves 92.38% mAP and 95.10% Rank-1 accuracy with re-ranking, outperforming arXiv:1804.00216v1 [cs.CV], another state of the art method using ResNet-152, by 1.42% in mAP and 0.47% in Rank-1 accuracy. In addition, AANet can perform person attribute prediction (e.g., gender, hair length, clothing length etc.), and localize the attributes in the query image.



### Bounded Manifold Completion
- **Arxiv ID**: http://arxiv.org/abs/1912.09026v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T05, H.2.8; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1912.09026v1)
- **Published**: 2019-12-19 05:42:21+00:00
- **Updated**: 2019-12-19 05:42:21+00:00
- **Authors**: Kelum Gajamannage, Randy Paffenroth
- **Comment**: 12 pages, 7 figures, submitted to Pattern Recognition Journal
- **Journal**: None
- **Summary**: Nonlinear dimensionality reduction or, equivalently, the approximation of high-dimensional data using a low-dimensional nonlinear manifold is an active area of research. In this paper, we will present a thematically different approach to detect the existence of a low-dimensional manifold of a given dimension that lies within a set of bounds derived from a given point cloud. A matrix representing the appropriately defined distances on a low-dimensional manifold is low-rank, and our method is based on current techniques for recovering a partially observed matrix from a small set of fully observed entries that can be implemented as a low-rank Matrix Completion (MC) problem. MC methods are currently used to solve challenging real-world problems, such as image inpainting and recommender systems, and we leverage extent efficient optimization techniques that use a nuclear norm convex relaxation as a surrogate for non-convex and discontinuous rank minimization. Our proposed method provides several advantages over current nonlinear dimensionality reduction techniques, with the two most important being theoretical guarantees on the detection of low-dimensional embeddings and robustness to non-uniformity in the sampling of the manifold. We validate the performance of this approach using both a theoretical analysis as well as synthetic and real-world benchmark datasets.



### Scale-wise Convolution for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1912.09028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09028v1)
- **Published**: 2019-12-19 05:50:23+00:00
- **Updated**: 2019-12-19 05:50:23+00:00
- **Authors**: Yuchen Fan, Jiahui Yu, Ding Liu, Thomas S. Huang
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: While scale-invariant modeling has substantially boosted the performance of visual recognition tasks, it remains largely under-explored in deep networks based image restoration. Naively applying those scale-invariant techniques (e.g. multi-scale testing, random-scale data augmentation) to image restoration tasks usually leads to inferior performance. In this paper, we show that properly modeling scale-invariance into neural networks can bring significant benefits to image restoration performance. Inspired from spatial-wise convolution for shift-invariance, "scale-wise convolution" is proposed to convolve across multiple scales for scale-invariance. In our scale-wise convolutional network (SCN), we first map the input image to the feature space and then build a feature pyramid representation via bi-linear down-scaling progressively. The feature pyramid is then passed to a residual network with scale-wise convolutions. The proposed scale-wise convolution learns to dynamically activate and aggregate features from different input scales in each residual building block, in order to exploit contextual information on multiple scales. In experiments, we compare the restoration accuracy and parameter efficiency among our model and many different variants of multi-scale neural networks. The proposed network with scale-wise convolution achieves superior performance in multiple image restoration tasks including image super-resolution, image denoising and image compression artifacts removal. Code and models are available at: https://github.com/ychfan/scn_sr



### TransMatch: A Transfer-Learning Scheme for Semi-Supervised Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.09033v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.09033v2)
- **Published**: 2019-12-19 06:50:45+00:00
- **Updated**: 2020-03-09 19:25:18+00:00
- **Authors**: Zhongjie Yu, Lin Chen, Zhongwei Cheng, Jiebo Luo
- **Comment**: Accepted at CVPR2020
- **Journal**: None
- **Summary**: The successful application of deep learning to many visual recognition tasks relies heavily on the availability of a large amount of labeled data which is usually expensive to obtain. The few-shot learning problem has attracted increasing attention from researchers for building a robust model upon only a few labeled samples. Most existing works tackle this problem under the meta-learning framework by mimicking the few-shot learning task with an episodic training strategy. In this paper, we propose a new transfer-learning framework for semi-supervised few-shot learning to fully utilize the auxiliary information from labeled base-class data and unlabeled novel-class data. The framework consists of three components: 1) pre-training a feature extractor on base-class data; 2) using the feature extractor to initialize the classifier weights for the novel classes; and 3) further updating the model with a semi-supervised learning method. Under the proposed framework, we develop a novel method for semi-supervised few-shot learning called TransMatch by instantiating the three components with Imprinting and MixMatch. Extensive experiments on two popular benchmark datasets for few-shot learning, CUB-200-2011 and miniImageNet, demonstrate that our proposed method can effectively utilize the auxiliary information from labeled base-class data and unlabeled novel-class data to significantly improve the accuracy of few-shot learning task.



### PointVoteNet: Accurate Object Detection and 6 DoF Pose Estimation in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.09057v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09057v2)
- **Published**: 2019-12-19 08:19:46+00:00
- **Updated**: 2020-06-17 12:00:38+00:00
- **Authors**: Frederik Hagelskjær, Anders Glent Buch
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: We present a learning-based method for 6 DoF pose estimation of rigid objects in point cloud data. Many recent learning-based approaches use primarily RGB information for detecting objects, in some cases with an added refinement step using depth data. Our method consumes unordered point sets with/without RGB information, from initial detection to the final transformation estimation stage. This allows us to achieve accurate pose estimates, in some cases surpassing state of the art methods trained on the same data.



### $n$-ML: Mitigating Adversarial Examples via Ensembles of Topologically Manipulated Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1912.09059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1912.09059v1)
- **Published**: 2019-12-19 08:24:07+00:00
- **Updated**: 2019-12-19 08:24:07+00:00
- **Authors**: Mahmood Sharif, Lujo Bauer, Michael K. Reiter
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new defense called $n$-ML against adversarial examples, i.e., inputs crafted by perturbing benign inputs by small amounts to induce misclassifications by classifiers. Inspired by $n$-version programming, $n$-ML trains an ensemble of $n$ classifiers, and inputs are classified by a vote of the classifiers in the ensemble. Unlike prior such approaches, however, the classifiers in the ensemble are trained specifically to classify adversarial examples differently, rendering it very difficult for an adversarial example to obtain enough votes to be misclassified. We show that $n$-ML roughly retains the benign classification accuracies of state-of-the-art models on the MNIST, CIFAR10, and GTSRB datasets, while simultaneously defending against adversarial examples with better resilience than the best defenses known to date and, in most cases, with lower classification-time overhead.



### Advanced Variations of Two-Dimensional Principal Component Analysis for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.09970v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1912.09970v1)
- **Published**: 2019-12-19 08:38:24+00:00
- **Updated**: 2019-12-19 08:38:24+00:00
- **Authors**: Meixiang Zhao, Zhigang Jia, Yunfeng Cai, Xiao Chen, Dunwei Gong
- **Comment**: arXiv admin note: text overlap with arXiv:1905.06458
- **Journal**: None
- **Summary**: The two-dimensional principal component analysis (2DPCA) has become one of the most powerful tools of artificial intelligent algorithms. In this paper, we review 2DPCA and its variations, and propose a general ridge regression model to extract features from both row and column directions. To enhance the generalization ability of extracted features, a novel relaxed 2DPCA (R2DPCA) is proposed with a new ridge regression model. R2DPCA generates a weighting vector with utilizing the label information, and maximizes a relaxed criterion with applying an optimal algorithm to get the essential features. The R2DPCA-based approaches for face recognition and image reconstruction are also proposed and the selected principle components are weighted to enhance the role of main components. Numerical experiments on well-known standard databases indicate that R2DPCA has high generalization ability and can achieve a higher recognition rate than the state-of-the-art methods, including in the deep learning methods such as CNNs, DBNs, and DNNs.



### Malware Makeover: Breaking ML-based Static Analysis by Modifying Executable Bytes
- **Arxiv ID**: http://arxiv.org/abs/1912.09064v2
- **DOI**: 10.1145/3433210.3453086
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09064v2)
- **Published**: 2019-12-19 08:41:16+00:00
- **Updated**: 2021-10-25 16:29:50+00:00
- **Authors**: Keane Lucas, Mahmood Sharif, Lujo Bauer, Michael K. Reiter, Saurabh Shintre
- **Comment**: Code for transformations at
  https://github.com/pwwl/enhanced-binary-diversification. Presentation at
  https://dl.acm.org/doi/10.1145/3433210.3453086. An author of a related work
  [32] contacted us regarding our characterization of their defense (Sec 2.2).
  They point out that our attack is not within the stated scope of their
  defense, but agree their defense would be ineffective against our attack
- **Journal**: None
- **Summary**: Motivated by the transformative impact of deep neural networks (DNNs) in various domains, researchers and anti-virus vendors have proposed DNNs for malware detection from raw bytes that do not require manual feature engineering. In this work, we propose an attack that interweaves binary-diversification techniques and optimization frameworks to mislead such DNNs while preserving the functionality of binaries. Unlike prior attacks, ours manipulates instructions that are a functional part of the binary, which makes it particularly challenging to defend against. We evaluated our attack against three DNNs in white- and black-box settings, and found that it often achieved success rates near 100%. Moreover, we found that our attack can fool some commercial anti-viruses, in certain cases with a success rate of 85%. We explored several defenses, both new and old, and identified some that can foil over 80% of our evasion attempts. However, these defenses may still be susceptible to evasion by attacks, and so we advocate for augmenting malware-detection systems with methods that do not rely on machine learning.



### Intra-Variable Handwriting Inspection Reinforced with Idiosyncrasy Analysis
- **Arxiv ID**: http://arxiv.org/abs/1912.12168v2
- **DOI**: 10.1109/TIFS.2020.2991833
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12168v2)
- **Published**: 2019-12-19 10:56:52+00:00
- **Updated**: 2020-05-07 15:51:51+00:00
- **Authors**: Chandranath Adak, Bidyut B. Chaudhuri, Chin-Teng Lin, Michael Blumenstein
- **Comment**: None
- **Journal**: IEEE Transactions on Information Forensics and Security, 2020
- **Summary**: In this paper, we work on intra-variable handwriting, where the writing samples of an individual can vary significantly. Such within-writer variation throws a challenge for automatic writer inspection, where the state-of-the-art methods do not perform well. To deal with intra-variability, we analyze the idiosyncrasy in individual handwriting. We identify/verify the writer from highly idiosyncratic text-patches. Such patches are detected using a deep recurrent reinforcement learning-based architecture. An idiosyncratic score is assigned to every patch, which is predicted by employing deep regression analysis. For writer identification, we propose a deep neural architecture, which makes the final decision by the idiosyncratic score-induced weighted average of patch-based decisions. For writer verification, we propose two algorithms for patch-fed deep feature aggregation, which assist in authentication using a triplet network. The experiments were performed on two databases, where we obtained encouraging results.



### SCAttNet: Semantic Segmentation Network with Spatial and Channel Attention Mechanism for High-Resolution Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1912.09121v2
- **DOI**: 10.1109/LGRS.2020.2988294
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09121v2)
- **Published**: 2019-12-19 10:58:01+00:00
- **Updated**: 2020-05-07 11:03:57+00:00
- **Authors**: Haifeng Li, Kaijian Qiu, Li Chen, Xiaoming Mei, Liang Hong, Chao Tao
- **Comment**: 5 pages, 3 figures, 2 tables
- **Journal**: IEEE Geoscience and Remote Sensing Letters 2020
- **Summary**: High-resolution remote sensing images (HRRSIs) contain substantial ground object information, such as texture, shape, and spatial location. Semantic segmentation, which is an important task for element extraction, has been widely used in processing mass HRRSIs. However, HRRSIs often exhibit large intraclass variance and small interclass variance due to the diversity and complexity of ground objects, thereby bringing great challenges to a semantic segmentation task. In this paper, we propose a new end-to-end semantic segmentation network, which integrates lightweight spatial and channel attention modules that can refine features adaptively. We compare our method with several classic methods on the ISPRS Vaihingen and Potsdam datasets. Experimental results show that our method can achieve better semantic segmentation results. The source codes are available at https://github.com/lehaifeng/SCAttNet.



### Semantic Segmentation from Remote Sensor Data and the Exploitation of Latent Learning for Classification of Auxiliary Tasks
- **Arxiv ID**: http://arxiv.org/abs/1912.09216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09216v1)
- **Published**: 2019-12-19 14:25:26+00:00
- **Updated**: 2019-12-19 14:25:26+00:00
- **Authors**: Bodhiswatta Chatterjee, Charalambos Poullis
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: In this paper we address three different aspects of semantic segmentation from remote sensor data using deep neural networks. Firstly, we focus on the semantic segmentation of buildings from remote sensor data and propose ICT-Net. The proposed network has been tested on the INRIA and AIRS benchmark datasets and is shown to outperform all other state of the art by more than 1.5% and 1.8% on the Jaccard index, respectively.   Secondly, as the building classification is typically the first step of the reconstruction process, we investigate the relationship of the classification accuracy to the reconstruction accuracy.   Finally, we present the simple yet compelling concept of latent learning and the implications it carries within the context of deep learning. We posit that a network trained on a primary task (i.e. building classification) is unintentionally learning about auxiliary tasks (e.g. the classification of road, tree, etc) which are complementary to the primary task. We extensively tested the proposed technique on the ISPRS benchmark dataset which contains multi-label ground truth, and report an average classification accuracy (F1 score) of 54.29% (SD=17.03) for roads, 10.15% (SD=2.54) for cars, 24.11% (SD=5.25) for trees, 42.74% (SD=6.62) for low vegetation, and 18.30% (SD=16.08) for clutter.   The source code and supplemental material is publicly available at http://www.theICTlab.org/lp/2019ICT-Net/.



### So2Sat LCZ42: A Benchmark Dataset for Global Local Climate Zones Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.12171v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12171v1)
- **Published**: 2019-12-19 14:42:01+00:00
- **Updated**: 2019-12-19 14:42:01+00:00
- **Authors**: Xiao Xiang Zhu, Jingliang Hu, Chunping Qiu, Yilei Shi, Jian Kang, Lichao Mou, Hossein Bagheri, Matthias Häberle, Yuansheng Hua, Rong Huang, Lloyd Hughes, Hao Li, Yao Sun, Guichen Zhang, Shiyao Han, Michael Schmitt, Yuanyuan Wang
- **Comment**: Article submitted to IEEE Geoscience and Remote Sensing Magazine
- **Journal**: None
- **Summary**: Access to labeled reference data is one of the grand challenges in supervised machine learning endeavors. This is especially true for an automated analysis of remote sensing images on a global scale, which enables us to address global challenges such as urbanization and climate change using state-of-the-art machine learning techniques. To meet these pressing needs, especially in urban research, we provide open access to a valuable benchmark dataset named "So2Sat LCZ42," which consists of local climate zone (LCZ) labels of about half a million Sentinel-1 and Sentinel-2 image patches in 42 urban agglomerations (plus 10 additional smaller areas) across the globe. This dataset was labeled by 15 domain experts following a carefully designed labeling work flow and evaluation process over a period of six months. As rarely done in other labeled remote sensing dataset, we conducted rigorous quality assessment by domain experts. The dataset achieved an overall confidence of 85%. We believe this LCZ dataset is a first step towards an unbiased globallydistributed dataset for urban growth monitoring using machine learning methods, because LCZ provide a rather objective measure other than many other semantic land use and land cover classifications. It provides measures of the morphology, compactness, and height of urban areas, which are less dependent on human and culture. This dataset can be accessed from http://doi.org/10.14459/2018mp1483140.



### HAMBox: Delving into Online High-quality Anchors Mining for Detecting Outer Faces
- **Arxiv ID**: http://arxiv.org/abs/1912.09231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09231v1)
- **Published**: 2019-12-19 14:44:26+00:00
- **Updated**: 2019-12-19 14:44:26+00:00
- **Authors**: Yang Liu, Xu Tang, Xiang Wu, Junyu Han, Jingtuo Liu, Errui Ding
- **Comment**: 9 pages, 6 figures. arXiv admin note: text overlap with 1802.09058 by
  other authors
- **Journal**: None
- **Summary**: Current face detectors utilize anchors to frame a multi-task learning problem which combines classification and bounding box regression. Effective anchor design and anchor matching strategy enable face detectors to localize faces under large pose and scale variations. However, we observe that more than 80% correctly predicted bounding boxes are regressed from the unmatched anchors (the IoUs between anchors and target faces are lower than a threshold) in the inference phase. It indicates that these unmatched anchors perform excellent regression ability, but the existing methods neglect to learn from them. In this paper, we propose an Online High-quality Anchor Mining Strategy (HAMBox), which explicitly helps outer faces compensate with high-quality anchors. Our proposed HAMBox method could be a general strategy for anchor-based single-stage face detection. Experiments on various datasets, including WIDER FACE, FDDB, AFW and PASCAL Face, demonstrate the superiority of the proposed method. Furthermore, our team win the championship on the Face Detection test track of WIDER Face and Pedestrian Challenge 2019. We will release the codes with PaddlePaddle.



### Towards automated mobile-phone-based plant pathology management
- **Arxiv ID**: http://arxiv.org/abs/1912.09239v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09239v2)
- **Published**: 2019-12-19 14:55:05+00:00
- **Updated**: 2019-12-20 01:39:17+00:00
- **Authors**: Nantheera Anantrasirichai, Sion Hannuna, Nishan Canagarajah
- **Comment**: 13 pages, India-UK Advanced Technology Centre of Excellence in Next
  Generation Networks, Systems and Services (IU-ATC), 2010
- **Journal**: None
- **Summary**: This paper presents a framework which uses computer vision algorithms to standardise images and analyse them for identifying crop diseases automatically. The tools are created to bridge the information gap between farmers, advisory call centres and agricultural experts using the images of diseased/infected crop captured by mobile-phones. These images are generally sensitive to a number of factors including camera type and lighting. We therefore propose a technique for standardising the colour of plant images within the context of the advisory system. Subsequently, to aid the advisory process, the disease recognition process is automated using image processing in conjunction with machine learning techniques. We describe our proposed leaf extraction, affected area segmentation and disease classification techniques. The proposed disease recognition system is tested using six mango diseases and the results show over 80% accuracy. The final output of our system is a list of possible diseases with relevant management advice.



### Analyzing an Imitation Learning Network for Fundus Image Registration Using a Divide-and-Conquer Approach
- **Arxiv ID**: http://arxiv.org/abs/1912.10837v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.10837v1)
- **Published**: 2019-12-19 15:01:49+00:00
- **Updated**: 2019-12-19 15:01:49+00:00
- **Authors**: Siming Bayer, Xia Zhong, Weilin Fu, Nishant Ravikumar, Andreas Maier
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: Comparison of microvascular circulation on fundoscopic images is a non-invasive clinical indication for the diagnosis and monitoring of diseases, such as diabetes and hypertensions. The differences between intra-patient images can be assessed quantitatively by registering serial acquisitions. Due to the variability of the images (i.e. contrast, luminosity) and the anatomical changes of the retina, the registration of fundus images remains a challenging task. Recently, several deep learning approaches have been proposed to register fundus images in an end-to-end fashion, achieving remarkable results. However, the results are difficult to interpret and analyze. In this work, we propose an imitation learning framework for the registration of 2D color funduscopic images for a wide range of applications such as disease monitoring, image stitching and super-resolution. We follow a divide-and-conquer approach to improve the interpretability of the proposed network, and analyze both the influence of the input image and the hyperparameters on the registration result. The results show that the proposed registration network reduces the initial target registration error up to 95\%.



### Evaluation of Multi-Slice Inputs to Convolutional Neural Networks for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.09287v2
- **DOI**: 10.1002/mp.14391
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.09287v2)
- **Published**: 2019-12-19 15:26:16+00:00
- **Updated**: 2019-12-22 19:31:15+00:00
- **Authors**: Minh H. Vu, Guus Grimbergen, Tufve Nyholm, Tommy Löfstedt
- **Comment**: None
- **Journal**: None
- **Summary**: When using Convolutional Neural Networks (CNNs) for segmentation of organs and lesions in medical images, the conventional approach is to work with inputs and outputs either as single slice (2D) or whole volumes (3D). One common alternative, in this study denoted as pseudo-3D, is to use a stack of adjacent slices as input and produce a prediction for at least the central slice. This approach gives the network the possibility to capture 3D spatial information, with only a minor additional computational cost. In this study, we systematically evaluate the segmentation performance and computational costs of this pseudo-3D approach as a function of the number of input slices, and compare the results to conventional end-to-end 2D and 3D CNNs. The standard pseudo-3D method regards the neighboring slices as multiple input image channels. We additionally evaluate a simple approach where the input stack is a volumetric input that is repeatably convolved in 3D to obtain a 2D feature map. This 2D map is in turn fed into a standard 2D network. We conducted experiments using two different CNN backbone architectures and on five diverse data sets covering different anatomical regions, imaging modalities, and segmentation tasks. We found that while both pseudo-3D methods can process a large number of slices at once and still be computationally much more efficient than fully 3D CNNs, a significant improvement over a regular 2D CNN was only observed for one of the five data sets. An analysis of the structural properties of the segmentation masks revealed no relations to the segmentation performance with respect to the number of input slices. The conclusion is therefore that in the general case, multi-slice inputs appear to not significantly improve segmentation results over using 2D or 3D CNNs.



### P$^2$GNet: Pose-Guided Point Cloud Generating Networks for 6-DoF Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.09316v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.09316v2)
- **Published**: 2019-12-19 15:57:05+00:00
- **Updated**: 2019-12-20 02:55:54+00:00
- **Authors**: Peiyu Yu, Yongming Rao, Jiwen Lu, Jie Zhou
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Humans are able to perform fast and accurate object pose estimation even under severe occlusion by exploiting learned object model priors from everyday life. However, most recently proposed pose estimation algorithms neglect to utilize the information of object models, often end up with limited accuracy, and tend to fall short in cluttered scenes. In this paper, we present a novel learning-based model, \underline{P}ose-Guided \underline{P}oint Cloud \underline{G}enerating Networks for 6D Object Pose Estimation (P$^2$GNet), designed to effectively exploit object model priors to facilitate 6D object pose estimation. We achieve this with an end-to-end estimation-by-generation workflow that combines the appearance information from the RGB-D image and the structure knowledge from object point cloud to enable accurate and robust pose estimation. Experiments on two commonly used benchmarks for 6D pose estimation, YCB-Video dataset and LineMOD dataset, demonstrate that P$^2$GNet outperforms the state-of-the-art method by a large margin and shows marked robustness towards heavy occlusion, while achieving real-time inference.



### VizWiz Dataset Browser: A Tool for Visualizing Machine Learning Datasets
- **Arxiv ID**: http://arxiv.org/abs/1912.09336v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1912.09336v1)
- **Published**: 2019-12-19 16:18:34+00:00
- **Updated**: 2019-12-19 16:18:34+00:00
- **Authors**: Nilavra Bhattacharya, Danna Gurari
- **Comment**: None
- **Journal**: None
- **Summary**: We present a visualization tool to exhaustively search and browse through a set of large-scale machine learning datasets. Built on the top of the VizWiz dataset, our dataset browser tool has the potential to support and enable a variety of qualitative and quantitative research, and open new directions for visualizing and researching with multimodal information. The tool is publicly available at https://vizwiz.org/browse.



### Instance-wise Depth and Motion Learning from Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/1912.09351v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.09351v2)
- **Published**: 2019-12-19 16:35:30+00:00
- **Updated**: 2020-04-08 11:53:52+00:00
- **Authors**: Seokju Lee, Sunghoon Im, Stephen Lin, In So Kweon
- **Comment**: Project page at https://sites.google.com/site/seokjucv/home/instadm
- **Journal**: None
- **Summary**: We present an end-to-end joint training framework that explicitly models 6-DoF motion of multiple dynamic objects, ego-motion and depth in a monocular camera setup without supervision. Our technical contributions are three-fold. First, we propose a differentiable forward rigid projection module that plays a key role in our instance-wise depth and motion learning. Second, we design an instance-wise photometric and geometric consistency loss that effectively decomposes background and moving object regions. Lastly, we introduce a new auto-annotation scheme to produce video instance segmentation maps that will be utilized as input to our training pipeline. These proposed elements are validated in a detailed ablation study. Through extensive experiments conducted on the KITTI dataset, our framework is shown to outperform the state-of-the-art depth and motion estimation methods. Our code and dataset will be available at https://github.com/SeokjuLee/Insta-DM.



### Tangent Images for Mitigating Spherical Distortion
- **Arxiv ID**: http://arxiv.org/abs/1912.09390v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09390v3)
- **Published**: 2019-12-19 17:06:20+00:00
- **Updated**: 2020-05-22 14:43:39+00:00
- **Authors**: Marc Eder, Mykhailo Shvets, John Lim, Jan-Michael Frahm
- **Comment**: Updated version of CVPR 2020 publication (9 pages, 13 pages
  supplementary). Code: https://github.com/meder411/Tangent-Images
- **Journal**: None
- **Summary**: In this work, we propose "tangent images," a spherical image representation that facilitates transferable and scalable $360^\circ$ computer vision. Inspired by techniques in cartography and computer graphics, we render a spherical image to a set of distortion-mitigated, locally-planar image grids tangent to a subdivided icosahedron. By varying the resolution of these grids independently of the subdivision level, we can effectively represent high resolution spherical images while still benefiting from the low-distortion icosahedral spherical approximation. We show that training standard convolutional neural networks on tangent images compares favorably to the many specialized spherical convolutional kernels that have been developed, while also scaling efficiently to handle significantly higher spherical resolutions. Furthermore, because our approach does not require specialized kernels, we show that we can transfer networks trained on perspective images to spherical data without fine-tuning and with limited performance drop-off. Finally, we demonstrate that tangent images can be used to improve the quality of sparse feature detection on spherical images, illustrating its usefulness for traditional computer vision tasks like structure-from-motion and SLAM.



### Making Better Mistakes: Leveraging Class Hierarchies with Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.09393v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.09393v2)
- **Published**: 2019-12-19 17:08:51+00:00
- **Updated**: 2020-06-12 16:19:22+00:00
- **Authors**: Luca Bertinetto, Romain Mueller, Konstantinos Tertikas, Sina Samangooei, Nicholas A. Lord
- **Comment**: To appear at CVPR 2020. Code available at
  https://github.com/fiveai/making-better-mistakes
- **Journal**: None
- **Summary**: Deep neural networks have improved image classification dramatically over the past decade, but have done so by focusing on performance measures that treat all classes other than the ground truth as equally wrong. This has led to a situation in which mistakes are less likely to be made than before, but are equally likely to be absurd or catastrophic when they do occur. Past works have recognised and tried to address this issue of mistake severity, often by using graph distances in class hierarchies, but this has largely been neglected since the advent of the current deep learning era in computer vision. In this paper, we aim to renew interest in this problem by reviewing past approaches and proposing two simple modifications of the cross-entropy loss which outperform the prior art under several metrics on two large datasets with complex class hierarchies: tieredImageNet and iNaturalist'19.



### Neural Networks-based Regularization for Large-Scale Medical Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.09395v2
- **DOI**: 10.1088/1361-6560/ab990e
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.09395v2)
- **Published**: 2019-12-19 17:15:27+00:00
- **Updated**: 2020-01-22 11:52:58+00:00
- **Authors**: Andreas Kofler, Markus Haltmeier, Tobias Schaeffter, Marc Kachelrieß, Marc Dewey, Christian Wald, Christoph Kolbitsch
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a generalized Deep Learning-based approach for solving ill-posed large-scale inverse problems occuring in medical image reconstruction. Recently, Deep Learning methods using iterative neural networks and cascaded neural networks have been reported to achieve state-of-the-art results with respect to various quantitative quality measures as PSNR, NRMSE and SSIM across different imaging modalities. However, the fact that these approaches employ the forward and adjoint operators repeatedly in the network architecture requires the network to process the whole images or volumes at once, which for some applications is computationally infeasible. In this work, we follow a different reconstruction strategy by decoupling the regularization of the solution from ensuring consistency with the measured data. The regularization is given in the form of an image prior obtained by the output of a previously trained neural network which is used in a Tikhonov regularization framework. By doing so, more complex and sophisticated network architectures can be used for the removal of the artefacts or noise than it is usually the case in iterative networks. Due to the large scale of the considered problems and the resulting computational complexity of the employed networks, the priors are obtained by processing the images or volumes as patches or slices. We evaluated the method for the cases of 3D cone-beam low dose CT and undersampled 2D radial cine MRI and compared it to a total variation-minimization-based reconstruction algorithm as well as to a method with regularization based on learned overcomplete dictionaries. The proposed method outperformed all the reported methods with respect to all chosen quantitative measures and further accelerates the regularization step in the reconstruction by several orders of magnitude.



### Explaining Classifiers using Adversarial Perturbations on the Perceptual Ball
- **Arxiv ID**: http://arxiv.org/abs/1912.09405v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.09405v4)
- **Published**: 2019-12-19 17:25:07+00:00
- **Updated**: 2021-03-30 21:51:19+00:00
- **Authors**: Andrew Elliott, Stephen Law, Chris Russell
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We present a simple regularization of adversarial perturbations based upon the perceptual loss. While the resulting perturbations remain imperceptible to the human eye, they differ from existing adversarial perturbations in that they are semi-sparse alterations that highlight objects and regions of interest while leaving the background unaltered. As a semantically meaningful adverse perturbations, it forms a bridge between counterfactual explanations and adversarial perturbations in the space of images. We evaluate our approach on several standard explainability benchmarks, namely, weak localization, insertion deletion, and the pointing game demonstrating that perceptually regularized counterfactuals are an effective explanation for image-based classifiers.



### Neural Design Network: Graphic Layout Generation with Constraints
- **Arxiv ID**: http://arxiv.org/abs/1912.09421v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09421v2)
- **Published**: 2019-12-19 17:47:46+00:00
- **Updated**: 2020-07-16 23:32:45+00:00
- **Authors**: Hsin-Ying Lee, Lu Jiang, Irfan Essa, Phuong B Le, Haifeng Gong, Ming-Hsuan Yang, Weilong Yang
- **Comment**: European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: Graphic design is essential for visual communication with layouts being fundamental to composing attractive designs. Layout generation differs from pixel-level image synthesis and is unique in terms of the requirement of mutual relations among the desired components. We propose a method for design layout generation that can satisfy user-specified constraints. The proposed neural design network (NDN) consists of three modules. The first module predicts a graph with complete relations from a graph with user-specified relations. The second module generates a layout from the predicted graph. Finally, the third module fine-tunes the predicted layout. Quantitative and qualitative experiments demonstrate that the generated layouts are visually similar to real design layouts. We also construct real designs based on predicted layouts for a better understanding of the visual quality. Finally, we demonstrate a practical application on layout recommendation.



### Benchmark for Generic Product Detection: A Low Data Baseline for Dense Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.09476v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09476v2)
- **Published**: 2019-12-19 18:57:45+00:00
- **Updated**: 2020-01-08 18:50:23+00:00
- **Authors**: Srikrishna Varadarajan, Sonaal Kant, Muktabh Mayank Srivastava
- **Comment**: corrected a mistake in evaluation; added more comparisons
- **Journal**: None
- **Summary**: Object detection in densely packed scenes is a new area where standard object detectors fail to train well. Dense object detectors like RetinaNet trained on large and dense datasets show great performance. We train a standard object detector on a small, normally packed dataset with data augmentation techniques. This dataset is 265 times smaller than the standard dataset, in terms of number of annotations. This low data baseline achieves satisfactory results (mAP=0.56) at standard IoU of 0.5. We also create a varied benchmark for generic SKU product detection by providing full annotations for multiple public datasets. It can be accessed at https://github.com/ParallelDots/generic-sku-detection-benchmark. We hope that this benchmark helps in building robust detectors that perform reliably across different settings in the wild.



### Anisotropic Super Resolution in Prostate MRI using Super Resolution Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.09497v1
- **DOI**: 10.1109/ISBI.2019.8759237
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09497v1)
- **Published**: 2019-12-19 19:05:18+00:00
- **Updated**: 2019-12-19 19:05:18+00:00
- **Authors**: Rewa Sood, Mirabela Rusu
- **Comment**: International Symposium on Biomedical Imaging, 4 pages, 4 figures, 1
  table
- **Journal**: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2019), Venice, Italy, 2019, pp. 1688-1691
- **Summary**: Acquiring High Resolution (HR) Magnetic Resonance (MR) images requires the patient to remain still for long periods of time, which causes patient discomfort and increases the probability of motion induced image artifacts. A possible solution is to acquire low resolution (LR) images and to process them with the Super Resolution Generative Adversarial Network (SRGAN) to create a super-resolved version. This work applies SRGAN to MR images of the prostate and performs three experiments. The first experiment explores improving the in-plane MR image resolution by factors of 4 and 8, and shows that, while the PSNR and SSIM (Structural SIMilarity) metrics are lower than the isotropic bicubic interpolation baseline, the SRGAN is able to create images that have high edge fidelity. The second experiment explores anisotropic super-resolution via synthetic images, in that the input images to the network are anisotropically downsampled versions of HR images. This experiment demonstrates the ability of the modified SRGAN to perform anisotropic super-resolution, with quantitative image metrics that are comparable to those of the anisotropic bicubic interpolation baseline. Finally, the third experiment applies a modified version of the SRGAN to super-resolve anisotropic images obtained from the through-plane slices of the volumetric MR data. The output super-resolved images contain a significant amount of high frequency information that make them visually close to their HR counterparts. Overall, the promising results from each experiment show that super-resolution for MR images is a successful technique and that producing isotropic MR image volumes from anisotropic slices is an achievable goal.



### Image Analytics for Legal Document Review: A Transfer Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1912.12169v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12169v1)
- **Published**: 2019-12-19 19:11:15+00:00
- **Updated**: 2019-12-19 19:11:15+00:00
- **Authors**: Nathaniel Huber-Fliflet, Fusheng Wei, Haozhen Zhao, Han Qin, Shi Ye, Amy Tsang
- **Comment**: 2019 IEEE International Conference on Big Data (Big Data)
- **Journal**: None
- **Summary**: Though technology assisted review in electronic discovery has been focusing on text data, the need of advanced analytics to facilitate reviewing multimedia content is on the rise. In this paper, we present several applications of deep learning in computer vision to Technology Assisted Review of image data in legal industry. These applications include image classification, image clustering, and object detection. We use transfer learning techniques to leverage established pretrained models for feature extraction and fine tuning. These applications are first of their kind in the legal industry for image document review. We demonstrate effectiveness of these applications with solving real world business challenges.



### An Application of Generative Adversarial Networks for Super Resolution Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1912.09507v1
- **DOI**: 10.1109/ICMLA.2018.00055
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09507v1)
- **Published**: 2019-12-19 19:20:04+00:00
- **Updated**: 2019-12-19 19:20:04+00:00
- **Authors**: Rewa Sood, Binit Topiwala, Karthik Choutagunta, Rohit Sood, Mirabela Rusu
- **Comment**: International Conference on Machine Learning Applications, 6 pages, 5
  figures, 2 tables
- **Journal**: 17th IEEE International Conference on Machine Learning and
  Applications,2018, pp. 326-331
- **Summary**: Acquiring High Resolution (HR) Magnetic Resonance (MR) images requires the patient to remain still for long periods of time, which causes patient discomfort and increases the probability of motion induced image artifacts. A possible solution is to acquire low resolution (LR) images and to process them with the Super Resolution Generative Adversarial Network (SRGAN) to create an HR version. Acquiring LR images requires a lower scan time than acquiring HR images, which allows for higher patient comfort and scanner throughput. This work applies SRGAN to MR images of the prostate to improve the in-plane resolution by factors of 4 and 8. The term 'super resolution' in the context of this paper defines the post processing enhancement of medical images as opposed to 'high resolution' which defines native image resolution acquired during the MR acquisition phase. We also compare the SRGAN to three other models: SRCNN, SRResNet, and Sparse Representation. While the SRGAN results do not have the best Peak Signal to Noise Ratio (PSNR) or Structural Similarity (SSIM) metrics, they are the visually most similar to the original HR images, as portrayed by the Mean Opinion Score (MOS) results.



### UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/1912.09513v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09513v2)
- **Published**: 2019-12-19 19:31:07+00:00
- **Updated**: 2020-04-03 02:42:27+00:00
- **Authors**: Weisong Wen, Yiyang Zhou, Guohao Zhang, Saman Fahandezh-Saadi, Xiwei Bai, Wei Zhan, Masayoshi Tomizuka, Li-Ta Hsu
- **Comment**: 7 pages, ICRA2020
- **Journal**: None
- **Summary**: Mapping and localization is a critical module of autonomous driving, and significant achievements have been reached in this field. Beyond Global Navigation Satellite System (GNSS), research in point cloud registration, visual feature matching, and inertia navigation has greatly enhanced the accuracy and robustness of mapping and localization in different scenarios. However, highly urbanized scenes are still challenging: LIDAR- and camera-based methods perform poorly with numerous dynamic objects; the GNSS-based solutions experience signal loss and multipath problems; the inertia measurement units (IMU) suffer from drifting. Unfortunately, current public datasets either do not adequately address this urban challenge or do not provide enough sensor information related to mapping and localization. Here we present UrbanLoco: a mapping/localization dataset collected in highly-urbanized environments with a full sensor-suite. The dataset includes 13 trajectories collected in San Francisco and Hong Kong, covering a total length of over 40 kilometers. Our dataset includes a wide variety of urban terrains: urban canyons, bridges, tunnels, sharp turns, etc. More importantly, our dataset includes information from LIDAR, cameras, IMU, and GNSS receivers. Now the dataset is publicly available through the link in the footnote. Dataset Link: https://advdataset2019.wixsite.com/urbanloco.



### LS-Net: Fast Single-Shot Line-Segment Detector
- **Arxiv ID**: http://arxiv.org/abs/1912.09532v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09532v2)
- **Published**: 2019-12-19 20:19:51+00:00
- **Updated**: 2020-01-24 11:39:00+00:00
- **Authors**: Van Nhan Nguyen, Robert Jenssen, Davide Roverso
- **Comment**: Highlighted the paper's contributions
- **Journal**: None
- **Summary**: In low-altitude Unmanned Aerial Vehicle (UAV) flights, power lines are considered as one of the most threatening hazards and one of the most difficult obstacles to avoid. In recent years, many vision-based techniques have been proposed to detect power lines to facilitate self-driving UAVs and automatic obstacle avoidance. However, most of the proposed methods are typically based on a common three-step approach: (i) edge detection, (ii) the Hough transform, and (iii) spurious line elimination based on power line constrains. These approaches not only are slow and inaccurate but also require a huge amount of effort in post-processing to distinguish between power lines and spurious lines. In this paper, we introduce LS-Net, a fast single-shot line-segment detector, and apply it to power line detection. The LS-Net is by design fully convolutional and consists of three modules: (i) a fully convolutional feature extractor, (ii) a classifier, and (iii) a line segment regressor. Due to the unavailability of large datasets with annotations of power lines, we render synthetic images of power lines using the Physically Based Rendering (PBR) approach and propose a series of effective data augmentation techniques to generate more training data. With a customized version of the VGG-16 network as the backbone, the proposed approach outperforms existing state-of-the-art approaches. In addition, the LS-Net can detect power lines in near real-time (20.4 FPS). This suggests that our proposed approach has a promising role in automatic obstacle avoidance and as a valuable component of self-driving UAVs, especially for automatic autonomous power line inspection.



### Interactive Open-Ended Learning for 3D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.09539v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09539v1)
- **Published**: 2019-12-19 20:46:51+00:00
- **Updated**: 2019-12-19 20:46:51+00:00
- **Authors**: S. Hamidreza Kasaei
- **Comment**: PhD thesis
- **Journal**: None
- **Summary**: The thesis contributes in several important ways to the research area of 3D object category learning and recognition. To cope with the mentioned limitations, we look at human cognition, in particular at the fact that human beings learn to recognize object categories ceaselessly over time. This ability to refine knowledge from the set of accumulated experiences facilitates the adaptation to new environments. Inspired by this capability, we seek to create a cognitive object perception and perceptual learning architecture that can learn 3D object categories in an open-ended fashion. In this context, ``open-ended'' implies that the set of categories to be learned is not known in advance, and the training instances are extracted from actual experiences of a robot, and thus become gradually available, rather than being available since the beginning of the learning process. In particular, this architecture provides perception capabilities that will allow robots to incrementally learn object categories from the set of accumulated experiences and reason about how to perform complex tasks. This framework integrates detection, tracking, teaching, learning, and recognition of objects. An extensive set of systematic experiments, in multiple experimental settings, was carried out to thoroughly evaluate the described learning approaches. Experimental results show that the proposed system is able to interact with human users, learn new object categories over time, as well as perform complex tasks. The contributions presented in this thesis have been fully implemented and evaluated on different standard object and scene datasets and empirically evaluated on different robotic platforms.



### Deep Exemplar Networks for VQA and VQG
- **Arxiv ID**: http://arxiv.org/abs/1912.09551v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.09551v1)
- **Published**: 2019-12-19 21:29:22+00:00
- **Updated**: 2019-12-19 21:29:22+00:00
- **Authors**: Badri N. Patro, Vinay P. Namboodiri
- **Comment**: This work is an extension of CVPR-2018 accepted paper
  arXiv:1804.00298 and EMNLP-2018 accepted paper arXiv:1808.03986
- **Journal**: None
- **Summary**: In this paper, we consider the problem of solving semantic tasks such as `Visual Question Answering' (VQA), where one aims to answers related to an image and `Visual Question Generation' (VQG), where one aims to generate a natural question pertaining to an image. Solutions for VQA and VQG tasks have been proposed using variants of encoder-decoder deep learning based frameworks that have shown impressive performance. Humans however often show generalization by relying on exemplar based approaches. For instance, the work by Tversky and Kahneman suggests that humans use exemplars when making categorizations and decisions. In this work, we propose the incorporation of exemplar based approaches towards solving these problems. Specifically, we incorporate exemplar based approaches and show that an exemplar based module can be incorporated in almost any of the deep learning architectures proposed in the literature and the addition of such a block results in improved performance for solving these tasks. Thus, just as the incorporation of attention is now considered de facto useful for solving these tasks, similarly, incorporating exemplars also can be considered to improve any proposed architecture for solving this task. We provide extensive empirical analysis for the same through various architectures, ablations, and state of the art comparisons.



### High Resolution Millimeter Wave Imaging For Self-Driving Cars
- **Arxiv ID**: http://arxiv.org/abs/1912.09579v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09579v2)
- **Published**: 2019-12-19 22:27:56+00:00
- **Updated**: 2019-12-27 22:35:12+00:00
- **Authors**: Junfeng Guan, Sohrab Madani, Suraj Jog, Haitham Hassanieh
- **Comment**: 14 pages, 13 figures
- **Journal**: None
- **Summary**: Recent years have witnessed much interest in expanding the use of networking signals beyond communication to sensing, localization, robotics, and autonomous systems. This paper explores how we can leverage recent advances in 5G millimeter wave (mmWave) technology for imaging in self-driving cars. Specifically, the use of mmWave in 5G has led to the creation of compact phased arrays with hundreds of antenna elements that can be electronically steered. Such phased arrays can expand the use of mmWave beyond vehicular communications and simple ranging sensors to a full-fledged imaging system that enables self-driving cars to see through fog, smog, snow, etc. Unfortunately, using mmWave signals for imaging in self-driving cars is challenging due to the very low resolution, the presence of fake artifacts resulting from multipath reflections and the absence of portions of the car due to specularity. This paper presents HawkEye, a system that can enable high resolution mmWave imaging in self driving cars. HawkEye addresses the above challenges by leveraging recent advances in deep learning known as Generative Adversarial Networks (GANs). HawkEye introduces a GAN architecture that is customized to mmWave imaging and builds a system that can significantly enhance the quality of mmWave images for self-driving cars.



### Mitigating large adversarial perturbations on X-MAS (X minus Moving Averaged Samples)
- **Arxiv ID**: http://arxiv.org/abs/1912.12170v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12170v4)
- **Published**: 2019-12-19 22:37:12+00:00
- **Updated**: 2020-01-23 14:16:36+00:00
- **Authors**: Woohyung Chun, Sung-Min Hong, Junho Huh, Inyup Kang
- **Comment**: X-MAS is the essential condition for the proposed mitigation as well
  as human beings. The codes and data for evaluation are available in
  https://github.com/stonylinux/mitigating_large_adversarial_perturbations_on_X-MAS
- **Journal**: None
- **Summary**: We propose the scheme that mitigates the adversarial perturbation $\epsilon$ on the adversarial example $X_{adv}$ ($=$ $X$ $\pm$ $\epsilon$, $X$ is a benign sample) by subtracting the estimated perturbation $\hat{\epsilon}$ from $X$ $+$ $\epsilon$ and adding $\hat{\epsilon}$ to $X$ $-$ $\epsilon$. The estimated perturbation $\hat{\epsilon}$ comes from the difference between $X_{adv}$ and its moving-averaged outcome $W_{avg}*X_{adv}$ where $W_{avg}$ is $N \times N$ moving average kernel that all the coefficients are one. Usually, the adjacent samples of an image are close to each other such that we can let $X$ $\approx$ $W_{avg}*X$ (naming this relation after X-MAS[X minus Moving Averaged Samples]). By doing that, we can make the estimated perturbation $\hat{\epsilon}$ falls within the range of $\epsilon$. The scheme is also extended to do the multi-level mitigation by configuring the mitigated adversarial example $X_{adv}$ $\pm$ $\hat{\epsilon}$ as a new adversarial example to be mitigated. The multi-level mitigation gets $X_{adv}$ closer to $X$ with a smaller (i.e. mitigated) perturbation than original unmitigated perturbation by setting the moving averaged adversarial sample $W_{avg} * X_{adv}$ (which has the smaller perturbation than $X_{adv}$ if $X$ $\approx$ $W_{avg}*X$) as the boundary condition that the multi-level mitigation cannot cross over (i.e. decreasing $\epsilon$ cannot go below and increasing $\epsilon$ cannot go beyond). With the multi-level mitigation, we can get high prediction accuracies even in the adversarial example having a large perturbation (i.e. $\epsilon$ $>$ $16$). The proposed scheme is evaluated with adversarial examples crafted by the FGSM (Fast Gradient Sign Method) based attacks on ResNet-50 trained with ImageNet dataset.



### Line Drawings of Natural Scenes Guide Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1912.09581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09581v1)
- **Published**: 2019-12-19 22:41:43+00:00
- **Updated**: 2019-12-19 22:41:43+00:00
- **Authors**: Kai-Fu Yang, Wen-Wen Jiang, Teng-Fei Zhan, Yong-Jie Li
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Visual search is an important strategy of the human visual system for fast scene perception. The guided search theory suggests that the global layout or other top-down sources of scenes play a crucial role in guiding object searching. In order to verify the specific roles of scene layout and regional cues in guiding visual attention, we executed a psychophysical experiment to record the human fixations on line drawings of natural scenes with an eye-tracking system in this work. We collected the human fixations of ten subjects from 498 natural images and of another ten subjects from the corresponding 996 human-marked line drawings of boundaries (two boundary maps per image) under free-viewing condition. The experimental results show that with the absence of some basic features like color and luminance, the distribution of the fixations on the line drawings has a high correlation with that on the natural images. Moreover, compared to the basic cues of regions, subjects pay more attention to the closed regions of line drawings which are usually related to the dominant objects of the scenes. Finally, we built a computational model to demonstrate that the fixation information on the line drawings can be used to significantly improve the performances of classical bottom-up models for fixation prediction in natural scenes. These results support that Gestalt features and scene layout are important cues for guiding fast visual object searching.




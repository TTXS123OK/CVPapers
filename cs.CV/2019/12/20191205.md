# Arxiv Papers in cs.CV on 2019-12-05
### Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1912.02314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02314v1)
- **Published**: 2019-12-05 00:06:20+00:00
- **Updated**: 2019-12-05 00:06:20+00:00
- **Authors**: Miika Aittala, Prafull Sharma, Lukas Murmann, Adam B. Yedidia, Gregory W. Wornell, William T. Freeman, Fredo Durand
- **Comment**: 14 pages, 5 figures, Advances in Neural Information Processing
  Systems 2019
- **Journal**: Aittala, Miika, et al. "Computational Mirrors: Blind Inverse Light
  Transport by Deep Matrix Factorization." Advances in Neural Information
  Processing Systems. 2019
- **Summary**: We recover a video of the motion taking place in a hidden scene by observing changes in indirect illumination in a nearby uncalibrated visible region. We solve this problem by factoring the observed video into a matrix product between the unknown hidden scene video and an unknown light transport matrix. This task is extremely ill-posed, as any non-negative factorization will satisfy the data. Inspired by recent work on the Deep Image Prior, we parameterize the factor matrices using randomly initialized convolutional neural networks trained in a one-off manner, and show that this results in decompositions that reflect the true motion in the hidden scene.



### 12-in-1: Multi-Task Vision and Language Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.02315v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02315v2)
- **Published**: 2019-12-05 00:07:35+00:00
- **Updated**: 2020-04-24 21:39:42+00:00
- **Authors**: Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee
- **Comment**: Jiasen Lu and Vedanuj Goswami contributed equally to this work
- **Journal**: None
- **Summary**: Much of vision-and-language research focuses on a small but diverse set of independent tasks and supporting datasets often studied in isolation; however, the visually-grounded language understanding skills required for success at these tasks overlap significantly. In this work, we investigate these relationships between vision-and-language tasks by developing a large-scale, multi-task training regime. Our approach culminates in a single model on 12 datasets from four broad categories of task including visual question answering, caption-based image retrieval, grounding referring expressions, and multi-modal verification. Compared to independently trained single-task models, this represents a reduction from approximately 3 billion parameters to 270 million while simultaneously improving performance by 2.05 points on average across tasks. We use our multi-task framework to perform in-depth analysis of the effect of joint training diverse tasks. Further, we show that finetuning task-specific models from our single multi-task model can lead to further improvements, achieving performance at or above the state-of-the-art.



### 15 Keypoints Is All You Need
- **Arxiv ID**: http://arxiv.org/abs/1912.02323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02323v2)
- **Published**: 2019-12-05 00:38:55+00:00
- **Updated**: 2020-03-13 17:18:57+00:00
- **Authors**: Michael Snower, Asim Kadav, Farley Lai, Hans Peter Graf
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Pose tracking is an important problem that requires identifying unique human pose-instances and matching them temporally across different frames of a video. However, existing pose tracking methods are unable to accurately model temporal relationships and require significant computation, often computing the tracks offline. We present an efficient Multi-person Pose Tracking method, KeyTrack, that only relies on keypoint information without using any RGB or optical flow information to track human keypoints in real-time. Keypoints are tracked using our Pose Entailment method, in which, first, a pair of pose estimates is sampled from different frames in a video and tokenized. Then, a Transformer-based network makes a binary classification as to whether one pose temporally follows another. Furthermore, we improve our top-down pose estimation method with a novel, parameter-free, keypoint refinement technique that improves the keypoint estimates used during the Pose Entailment step. We achieve state-of-the-art results on the PoseTrack'17 and the PoseTrack'18 benchmarks while using only a fraction of the computation required by most other methods for computing the tracking information.



### 3D Objectness Estimation via Bottom-up Regret Grouping
- **Arxiv ID**: http://arxiv.org/abs/1912.02332v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02332v1)
- **Published**: 2019-12-05 01:20:56+00:00
- **Updated**: 2019-12-05 01:20:56+00:00
- **Authors**: Zelin Ye, Yan Hao, Liang Xu, Rui Zhu, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: 3D objectness estimation, namely discovering semantic objects from 3D scene, is a challenging and significant task in 3D understanding. In this paper, we propose a 3D objectness method working in a bottom-up manner. Beginning with over-segmented 3D segments, we iteratively group them into object proposals by learning an ingenious grouping predictor to determine whether two 3D segments can be grouped or not. To enhance robustness, a novel regret mechanism is presented to withdraw incorrect grouping operations. Hence the irreparable consequences brought by mistaken grouping in prior bottom-up works can be greatly reduced. Our experiments show that our method outperforms state-of-the-art 3D objectness methods with a small number of proposals in two difficult datasets, GMU-kitchen and CTD. Further ablation study also demonstrates the effectiveness of our grouping predictor and regret mechanism.



### Static and Dynamic Fusion for Multi-modal Cross-ethnicity Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/1912.02340v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02340v2)
- **Published**: 2019-12-05 01:39:56+00:00
- **Updated**: 2019-12-16 00:55:58+00:00
- **Authors**: Ajian Liu, Zichang Tan, Xuan Li, Jun Wan, Sergio Escalera, Guodong Guo, Stan Z. Li
- **Comment**: 10 pages, 9 figures, conference
- **Journal**: None
- **Summary**: Regardless of the usage of deep learning and handcrafted methods, the dynamic information from videos and the effect of cross-ethnicity are rarely considered in face anti-spoofing. In this work, we propose a static-dynamic fusion mechanism for multi-modal face anti-spoofing. Inspired by motion divergences between real and fake faces, we incorporate the dynamic image calculated by rank pooling with static information into a conventional neural network (CNN) for each modality (i.e., RGB, Depth and infrared (IR)). Then, we develop a partially shared fusion method to learn complementary information from multiple modalities. Furthermore, in order to study the generalization capability of the proposal in terms of cross-ethnicity attacks and unknown spoofs, we introduce the largest public cross-ethnicity Face Anti-spoofing (CASIA-CeFA) dataset, covering 3 ethnicities, 3 modalities, 1607 subjects, and 2D plus 3D attack types. Experiments demonstrate that the proposed method achieves state-of-the-art results on CASIA-CeFA, CASIA-SURF, OULU-NPU and SiW.



### Spatial-Frequency Domain Nonlocal Total Variation for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1912.02357v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02357v1)
- **Published**: 2019-12-05 03:02:35+00:00
- **Updated**: 2019-12-05 03:02:35+00:00
- **Authors**: Haijuan Hu, Jacques Froment, Baoyan Wang, Xiequan Fan
- **Comment**: 36 pages, 17 figures
- **Journal**: None
- **Summary**: Following the pioneering works of Rudin, Osher and Fatemi on total variation (TV) and of Buades, Coll and Morel on non-local means (NL-means), the last decade has seen a large number of denoising methods mixing these two approaches, starting with the nonlocal total variation (NLTV) model. The present article proposes an analysis of the NLTV model for image denoising as well as a number of improvements, the most important of which being to apply the denoising both in the space domain and in the Fourier domain, in order to exploit the complementarity of the representation of image data in both domains. A local version obtained by a regionwise implementation followed by an aggregation process, called Local Spatial-Frequency NLTV (L- SFNLTV) model, is finally proposed as a new reference algorithm for image denoising among the family of approaches mixing TV and NL operators. The experiments show the great performance of L-SFNLTV, both in terms of image quality and of computational speed, comparing with other recently proposed NLTV-related methods.



### Why Should we Combine Training and Post-Training Methods for Out-of-Distribution Detection?
- **Arxiv ID**: http://arxiv.org/abs/1912.03133v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.03133v1)
- **Published**: 2019-12-05 04:24:14+00:00
- **Updated**: 2019-12-05 04:24:14+00:00
- **Authors**: Aristotelis-Angelos Papadopoulos, Nazim Shaikh, Mohammad Reza Rajati
- **Comment**: Preprint, 9 pages. arXiv admin note: text overlap with
  arXiv:1906.03509
- **Journal**: None
- **Summary**: Deep neural networks are known to achieve superior results in classification tasks. However, it has been recently shown that they are incapable to detect examples that are generated by a distribution which is different than the one they have been trained on since they are making overconfident prediction for Out-Of-Distribution (OOD) examples. OOD detection has attracted a lot of attention recently. In this paper, we review some of the most seminal recent algorithms in the OOD detection field, we divide those methods into training and post-training and we experimentally show how the combination of the former with the latter can achieve state-of-the-art results in the OOD detection task.



### Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline
- **Arxiv ID**: http://arxiv.org/abs/1912.02379v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.02379v2)
- **Published**: 2019-12-05 04:51:11+00:00
- **Updated**: 2020-03-31 03:12:26+00:00
- **Authors**: Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das
- **Comment**: None
- **Journal**: None
- **Summary**: Prior work in visual dialog has focused on training deep neural models on VisDial in isolation. Instead, we present an approach to leverage pretraining on related vision-language datasets before transferring to visual dialog. We adapt the recently proposed ViLBERT (Lu et al., 2019) model for multi-turn visually-grounded conversations. Our model is pretrained on the Conceptual Captions and Visual Question Answering datasets, and finetuned on VisDial. Our best single model outperforms prior published work (including model ensembles) by more than 1% absolute on NDCG and MRR. Next, we find that additional finetuning using "dense" annotations in VisDial leads to even higher NDCG -- more than 10% over our base model -- but hurts MRR -- more than 17% below our base model! This highlights a trade-off between the two primary metrics -- NDCG and MRR -- which we find is due to dense annotations not correlating well with the original ground-truth answers to questions.



### PSNet: Parametric Sigmoid Norm Based CNN for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.10946v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.10946v1)
- **Published**: 2019-12-05 05:10:27+00:00
- **Updated**: 2019-12-05 05:10:27+00:00
- **Authors**: Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey
- **Comment**: Accepted in IEEE CICT 2019 Conference
- **Journal**: None
- **Summary**: The Convolutional Neural Networks (CNN) have become very popular recently due to its outstanding performance in various computer vision applications. It is also used over widely studied face recognition problem. However, the existing layers of CNN are unable to cope with the problem of hard examples which generally produce lower class scores. Thus, the existing methods become biased towards the easy examples. In this paper, we resolve this problem by incorporating a Parametric Sigmoid Norm (PSN) layer just before the final fully-connected layer. We propose a PSNet CNN model by using the PSN layer. The PSN layer facilitates high gradient flow for harder examples as compared to easy examples. Thus, it forces the network to learn the visual characteristics of hard examples. We conduct the face recognition experiments to test the performance of PSN layer. The suitability of the PSN layer with different loss functions is also experimented. The widely used Labeled Faces in the Wild (LFW) and YouTube Faces (YTF) datasets are used in the experiments. The experimental results confirm the relevance of the proposed PSN layer.



### Ultrafast Photorealistic Style Transfer via Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1912.02398v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02398v2)
- **Published**: 2019-12-05 05:51:54+00:00
- **Updated**: 2020-06-22 12:56:01+00:00
- **Authors**: Jie An, Haoyi Xiong, Jun Huan, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: The key challenge in photorealistic style transfer is that an algorithm should faithfully transfer the style of a reference photo to a content photo while the generated image should look like one captured by a camera. Although several photorealistic style transfer algorithms have been proposed, they need to rely on post- and/or pre-processing to make the generated images look photorealistic. If we disable the additional processing, these algorithms would fail to produce plausible photorealistic stylization in terms of detail preservation and photorealism. In this work, we propose an effective solution to these issues. Our method consists of a construction step (C-step) to build a photorealistic stylization network and a pruning step (P-step) for acceleration. In the C-step, we propose a dense auto-encoder named PhotoNet based on a carefully designed pre-analysis. PhotoNet integrates a feature aggregation module (BFA) and instance normalized skip links (INSL). To generate faithful stylization, we introduce multiple style transfer modules in the decoder and INSLs. PhotoNet significantly outperforms existing algorithms in terms of both efficiency and effectiveness. In the P-step, we adopt a neural architecture search method to accelerate PhotoNet. We propose an automatic network pruning framework in the manner of teacher-student learning for photorealistic stylization. The network architecture named PhotoNAS resulted from the search achieves significant acceleration over PhotoNet while keeping the stylization effects almost intact. We conduct extensive experiments on both image and video transfer. The results show that our method can produce favorable results while achieving 20-30 times acceleration in comparison with the existing state-of-the-art approaches. It is worth noting that the proposed algorithm accomplishes better performance without any pre- or post-processing.



### Generating Videos of Zero-Shot Compositions of Actions and Objects
- **Arxiv ID**: http://arxiv.org/abs/1912.02401v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02401v4)
- **Published**: 2019-12-05 06:09:13+00:00
- **Updated**: 2020-07-17 06:01:52+00:00
- **Authors**: Megha Nawhal, Mengyao Zhai, Andreas Lehrmann, Leonid Sigal, Greg Mori
- **Comment**: Accepted at ECCV'20; Project Page:
  https://www.sfu.ca/~mnawhal/projects/zs_hoi_generation.html
- **Journal**: None
- **Summary**: Human activity videos involve rich, varied interactions between people and objects. In this paper we develop methods for generating such videos -- making progress toward addressing the important, open problem of video generation in complex scenes. In particular, we introduce the task of generating human-object interaction videos in a zero-shot compositional setting, i.e., generating videos for action-object compositions that are unseen during training, having seen the target action and target object separately. This setting is particularly important for generalization in human activity video generation, obviating the need to observe every possible action-object combination in training and thus avoiding the combinatorial explosion involved in modeling complex scenes. To generate human-object interaction videos, we propose a novel adversarial framework HOI-GAN which includes multiple discriminators focusing on different aspects of a video. To demonstrate the effectiveness of our proposed framework, we perform extensive quantitative and qualitative evaluation on two challenging datasets: EPIC-Kitchens and 20BN-Something-Something v2.



### BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.02413v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02413v4)
- **Published**: 2019-12-05 07:32:28+00:00
- **Updated**: 2020-03-10 09:34:38+00:00
- **Authors**: Boyan Zhou, Quan Cui, Xiu-Shen Wei, Zhao-Min Chen
- **Comment**: Accepted by CVPR 2020; Our work won the first place in the
  iNaturalist 2019 large scale species classification competition, and our code
  is open-source and available at https://github.com/Megvii-Nanjing/BBN
- **Journal**: None
- **Summary**: Our work focuses on tackling the challenging but natural visual recognition task of long-tailed data distribution (i.e., a few classes occupy most of the data, while most classes have rarely few samples). In the literature, class re-balancing strategies (e.g., re-weighting and re-sampling) are the prominent and effective methods proposed to alleviate the extreme imbalance for dealing with long-tailed problems. In this paper, we firstly discover that these re-balancing methods achieving satisfactory recognition accuracy owe to that they could significantly promote the classifier learning of deep networks. However, at the same time, they will unexpectedly damage the representative ability of the learned deep features to some extent. Therefore, we propose a unified Bilateral-Branch Network (BBN) to take care of both representation learning and classifier learning simultaneously, where each branch does perform its own duty separately. In particular, our BBN model is further equipped with a novel cumulative learning strategy, which is designed to first learn the universal patterns and then pay attention to the tail data gradually. Extensive experiments on four benchmark datasets, including the large-scale iNaturalist ones, justify that the proposed BBN can significantly outperform state-of-the-art methods. Furthermore, validation experiments can demonstrate both our preliminary discovery and effectiveness of tailored designs in BBN for long-tailed problems. Our method won the first place in the iNaturalist 2019 large scale species classification competition, and our code is open-source and available at https://github.com/Megvii-Nanjing/BBN.



### OASIS: One-pass aligned Atlas Set for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.02417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02417v1)
- **Published**: 2019-12-05 07:39:58+00:00
- **Updated**: 2019-12-05 07:39:58+00:00
- **Authors**: Qikui Zhu, Bo Du, Pingkun Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation is a fundamental task in medical image analysis. Despite that deep convolutional neural networks have gained stellar performance in this challenging task, they typically rely on large labeled datasets, which have limited their extension to customized applications. By revisiting the superiority of atlas based segmentation methods, we present a new framework of One-pass aligned Atlas Set for Images Segmentation (OASIS). To address the problem of time-consuming iterative image registration used for atlas warping, the proposed method takes advantage of the power of deep learning to achieve one-pass image registration. In addition, by applying label constraint, OASIS also makes the registration process to be focused on the regions to be segmented for improving the performance of segmentation. Furthermore, instead of using image based similarity for label fusion, which can be distracted by the large background areas, we propose a novel strategy to compute the label similarity based weights for label fusion. Our experimental results on the challenging task of prostate MR image segmentation demonstrate that OASIS is able to significantly increase the segmentation performance compared to other state-of-the-art methods.



### Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection
- **Arxiv ID**: http://arxiv.org/abs/1912.02424v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02424v4)
- **Published**: 2019-12-05 07:49:56+00:00
- **Updated**: 2020-06-20 10:54:23+00:00
- **Authors**: Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, Stan Z. Li
- **Comment**: Accepted by CVPR 2020 as Oral; Best Paper Nomination
- **Journal**: None
- **Summary**: Object detection has been dominated by anchor-based detectors for several years. Recently, anchor-free detectors have become popular due to the proposal of FPN and Focal Loss. In this paper, we first point out that the essential difference between anchor-based and anchor-free detection is actually how to define positive and negative training samples, which leads to the performance gap between them. If they adopt the same definition of positive and negative samples during training, there is no obvious difference in the final performance, no matter regressing from a box or a point. This shows that how to select positive and negative training samples is important for current object detectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object. It significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them. Finally, we discuss the necessity of tiling multiple anchors per location on the image to detect objects. Extensive experiments conducted on MS COCO support our aforementioned analysis and conclusions. With the newly introduced ATSS, we improve state-of-the-art detectors by a large margin to $50.7\%$ AP without introducing any overhead. The code is available at https://github.com/sfzhang15/ATSS



### Deep Learning Based Segmentation Free License Plate Recognition Using Roadway Surveillance Camera Images
- **Arxiv ID**: http://arxiv.org/abs/1912.02441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02441v1)
- **Published**: 2019-12-05 09:10:47+00:00
- **Updated**: 2019-12-05 09:10:47+00:00
- **Authors**: Alperen Elihos, Burak Balci, Bensu Alkan, Yusuf Artan
- **Comment**: None
- **Journal**: None
- **Summary**: Smart automated traffic enforcement solutions have been gaining popularity in recent years. These solutions are ubiquitously used for seat-belt violation detection, red-light violation detection and speed violation detection purposes. Highly accurate license plate recognition is an indispensable part of these systems. However, general license plate recognition systems require high resolution images for high performance. In this study, we propose a novel license plate recognition method for general roadway surveillance cameras. Proposed segmentation free license plate recognition algorithm utilizes deep learning based object detection techniques in the character detection and recognition process. Proposed method has been tested on 2000 images captured on a roadway.



### Fully Trainable and Interpretable Non-Local Sparse Models for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1912.02456v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02456v5)
- **Published**: 2019-12-05 09:31:13+00:00
- **Updated**: 2020-08-20 14:56:27+00:00
- **Authors**: Bruno Lecouat, Jean Ponce, Julien Mairal
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Non-local self-similarity and sparsity principles have proven to be powerful priors for natural image modeling. We propose a novel differentiable relaxation of joint sparsity that exploits both principles and leads to a general framework for image restoration which is (1) trainable end to end, (2) fully interpretable, and (3) much more compact than competing deep learning architectures. We apply this approach to denoising, jpeg deblocking, and demosaicking, and show that, with as few as 100K parameters, its performance on several standard benchmarks is on par or better than state-of-the-art methods that may have an order of magnitude or more parameters.



### Blind Inpainting of Large-scale Masks of Thin Structures with Adversarial and Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.02470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02470v1)
- **Published**: 2019-12-05 10:06:33+00:00
- **Updated**: 2019-12-05 10:06:33+00:00
- **Authors**: Hao Chen, Mario Valerio Giuffrida, Peter Doerner, Sotirios A. Tsaftaris
- **Comment**: None
- **Journal**: None
- **Summary**: Several imaging applications (vessels, retina, plant roots, road networks from satellites) require the accurate segmentation of thin structures for subsequent analysis. Discontinuities (gaps) in the extracted foreground may hinder down-stream image-based analysis of biomarkers, organ structure and topology. In this paper, we propose a general post-processing technique to recover such gaps in large-scale segmentation masks. We cast this problem as a blind inpainting task, where the regions of missing lines in the segmentation masks are not known to the algorithm, which we solve with an adversarially trained neural network. One challenge of using large images is the memory capacity of current GPUs. The typical approach of dividing a large image into smaller patches to train the network does not guarantee global coherence of the reconstructed image that preserves structure and topology. We use adversarial training and reinforcement learning (Policy Gradient) to endow the model with both global context and local details. We evaluate our method in several datasets in medical imaging, plant science, and remote sensing. Our experiments demonstrate that our model produces the most realistic and complete inpainted results, outperforming other approaches. In a dedicated study on plant roots we find that our approach is also comparable to human performance. Implementation available at \url{https://github.com/Hhhhhhhhhhao/Thin-Structure-Inpainting}.



### Smartphone Multi-modal Biometric Authentication: Database and Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1912.02487v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02487v1)
- **Published**: 2019-12-05 10:36:23+00:00
- **Updated**: 2019-12-05 10:36:23+00:00
- **Authors**: Raghavendra Ramachandra, Martin Stokkenes, Amir Mohammadi, Sushma Venkatesh, Kiran Raja, Pankaj Wasnik, Eric Poiret, SÃ©bastien Marcel, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: Biometric-based verification is widely employed on the smartphones for various applications, including financial transactions. In this work, we present a new multimodal biometric dataset (face, voice, and periocular) acquired using a smartphone. The new dataset is comprised of 150 subjects that are captured in six different sessions reflecting real-life scenarios of smartphone assisted authentication. One of the unique features of this dataset is that it is collected in four different geographic locations representing a diverse population and ethnicity. Additionally, we also present a multimodal Presentation Attack (PA) or spoofing dataset using a low-cost Presentation Attack Instrument (PAI) such as print and electronic display attacks. The novel acquisition protocols and the diversity of the data subjects collected from different geographic locations will allow developing a novel algorithm for either unimodal or multimodal biometrics. Further, we also report the performance evaluation of the baseline biometric verification and Presentation Attack Detection (PAD) on the newly collected dataset.



### E2-Capsule Neural Networks for Facial Expression Recognition Using AU-Aware Attention
- **Arxiv ID**: http://arxiv.org/abs/1912.02491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02491v1)
- **Published**: 2019-12-05 10:44:08+00:00
- **Updated**: 2019-12-05 10:44:08+00:00
- **Authors**: Shan Cao, Yuqian Yao, Gaoyun An
- **Comment**: 2 pages, 3 figures
- **Journal**: None
- **Summary**: Capsule neural network is a new and popular technique in deep learning. However, the traditional capsule neural network does not extract features sufficiently before the dynamic routing between the capsules. In this paper, the one Double Enhanced Capsule Neural Network (E2-Capsnet) that uses AU-aware attention for facial expression recognition (FER) is proposed. The E2-Capsnet takes advantage of dynamic routing between the capsules, and has two enhancement modules which are beneficial for FER. The first enhancement module is the convolutional neural network with AU-aware attention, which can help focus on the active areas of the expression. The second enhancement module is the capsule neural network with multiple convolutional layers, which enhances the ability of the feature representation. Finally, squashing function is used to classify the facial expression. We demonstrate the effectiveness of E2-Capsnet on the two public benchmark datasets, RAF-DB and EmotioNet. The experimental results show that our E2-Capsnet is superior to the state-of-the-art methods. Our implementation will be publicly available online.



### MetalGAN: Multi-Domain Label-Less Image Synthesis Using cGANs and Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.02494v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.02494v2)
- **Published**: 2019-12-05 10:47:08+00:00
- **Updated**: 2020-06-25 09:40:52+00:00
- **Authors**: Tomaso Fontanini, Eleonora Iotti, Luca Donati, Andrea Prati
- **Comment**: None
- **Journal**: None
- **Summary**: Image synthesis is currently one of the most addressed image processing topic in computer vision and deep learning fields of study. Researchers have tackled this problem focusing their efforts on its several challenging problems, e.g. image quality and size, domain and pose changing, architecture of the networks, and so on. Above all, producing images belonging to different domains by using a single architecture is a very relevant goal for image generation. In fact, a single multi-domain network would allow greater flexibility and robustness in the image synthesis task than other approaches. This paper proposes a novel architecture and a training algorithm, which are able to produce multi-domain outputs using a single network. A small portion of a dataset is intentionally used, and there are no hard-coded labels (or classes). This is achieved by combining a conditional Generative Adversarial Network (cGAN) for image generation and a Meta-Learning algorithm for domain switch, and we called our approach MetalGAN. The approach has proved to be appropriate for solving the multi-domain problem and it is validated on facial attribute transfer, using CelebA dataset.



### A Document Skew Detection Method Using Fast Hough Transform
- **Arxiv ID**: http://arxiv.org/abs/1912.02504v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02504v1)
- **Published**: 2019-12-05 11:07:14+00:00
- **Updated**: 2019-12-05 11:07:14+00:00
- **Authors**: Pavel Bezmaternykh, Dmitry Nikolaev
- **Comment**: None
- **Journal**: None
- **Summary**: The majority of document image analysis systems use a document skew detection algorithm to simplify all its further processing stages. A huge amount of such algorithms based on Hough transform (HT) analysis has already been proposed. Despite this, we managed to find only one work where the Fast Hough Transform (FHT) usage was suggested to solve the indicated problem. Unfortunately, no study of that method was provided. In this work, we propose and study a skew detection algorithm for the document images which relies on FHT analysis. To measure this algorithm quality we use the dataset from the problem oriented DISEC'13 contest and its evaluation methodology. Obtained values for AED, TOP80, and CE criteria are equal to 0.086, 0.056, 68.80 respectively.



### Post-Mortem Iris Recognition Resistant to Biological Eye Decay Processes
- **Arxiv ID**: http://arxiv.org/abs/1912.02512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02512v1)
- **Published**: 2019-12-05 11:31:56+00:00
- **Updated**: 2019-12-05 11:31:56+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an end-to-end iris recognition method designed specifically for post-mortem samples, and thus serving as a perfect application for iris biometrics in forensics. To our knowledge, it is the first method specific for verification of iris samples acquired after demise. We have fine-tuned a convolutional neural network-based segmentation model with a large set of diversified iris data (including post-mortem and diseased eyes), and combined Gabor kernels with newly designed, iris-specific kernels learnt by Siamese networks. The resulting method significantly outperforms the existing off-the-shelf iris recognition methods (both academic and commercial) on the newly collected database of post-mortem iris images and for all available time horizons since death. We make all models and the method itself available along with this paper.



### Towards Explainable Deep Neural Networks (xDNN)
- **Arxiv ID**: http://arxiv.org/abs/1912.02523v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02523v1)
- **Published**: 2019-12-05 12:01:15+00:00
- **Updated**: 2019-12-05 12:01:15+00:00
- **Authors**: Plamen Angelov, Eduardo Soares
- **Comment**: Preprint submitted to the Neural Networks Journal for publication
- **Journal**: None
- **Summary**: In this paper, we propose an elegant solution that is directly addressing the bottlenecks of the traditional deep learning approaches and offers a clearly explainable internal architecture that can outperform the existing methods, requires very little computational resources (no need for GPUs) and short training times (in the order of seconds). The proposed approach, xDNN is using prototypes. Prototypes are actual training data samples (images), which are local peaks of the empirical data distribution called typicality as well as of the data density. This generative model is identified in a closed form and equates to the pdf but is derived automatically and entirely from the training data with no user- or problem-specific thresholds, parameters or intervention. The proposed xDNN offers a new deep learning architecture that combines reasoning and learning in a synergy. It is non-iterative and non-parametric, which explains its efficiency in terms of time and computational resources. From the user perspective, the proposed approach is clearly understandable to human users. We tested it on some well-known benchmark data sets such as iRoads and Caltech-256. xDNN outperforms the other methods including deep learning in terms of accuracy, time to train and offers a clearly explainable classifier. In fact, the result on the very hard Caltech-256 problem (which has 257 classes) represents a world record.



### Towards Understanding Residual and Dilated Dense Neural Networks via Convolutional Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1912.02605v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.02605v2)
- **Published**: 2019-12-05 14:46:01+00:00
- **Updated**: 2019-12-08 03:01:51+00:00
- **Authors**: Zhiyang Zhang, Shihua Zhang
- **Comment**: 13 pages, 8 figures
- **Journal**: National Science Review (2020)
- **Summary**: Convolutional neural network (CNN) and its variants have led to many state-of-art results in various fields. However, a clear theoretical understanding about them is still lacking. Recently, multi-layer convolutional sparse coding (ML-CSC) has been proposed and proved to equal such simply stacked networks (plain networks). Here, we think three factors in each layer of it including the initialization, the dictionary design and the number of iterations greatly affect the performance of ML-CSC. Inspired by these considerations, we propose two novel multi-layer models--residual convolutional sparse coding model (Res-CSC) and mixed-scale dense convolutional sparse coding model (MSD-CSC), which have close relationship with the residual neural network (ResNet) and mixed-scale (dilated) dense neural network (MSDNet), respectively. Mathematically, we derive the shortcut connection in ResNet as a special case of a new forward propagation rule on ML-CSC. We find a theoretical interpretation of the dilated convolution and dense connection in MSDNet by analyzing MSD-CSC, which gives a clear mathematical understanding about them. We implement the iterative soft thresholding algorithm (ISTA) and its fast version to solve Res-CSC and MSD-CSC, which can employ the unfolding operation for further improvements. At last, extensive numerical experiments and comparison with competing methods demonstrate their effectiveness using three typical datasets.



### Multi-Modal Deep Clustering: Unsupervised Partitioning of Images
- **Arxiv ID**: http://arxiv.org/abs/1912.02678v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02678v3)
- **Published**: 2019-12-05 16:03:43+00:00
- **Updated**: 2020-12-15 10:16:11+00:00
- **Authors**: Guy Shiran, Daphna Weinshall
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: The clustering of unlabeled raw images is a daunting task, which has recently been approached with some success by deep learning methods. Here we propose an unsupervised clustering framework, which learns a deep neural network in an end-to-end fashion, providing direct cluster assignments of images without additional processing. Multi-Modal Deep Clustering (MMDC), trains a deep network to align its image embeddings with target points sampled from a Gaussian Mixture Model distribution. The cluster assignments are then determined by mixture component association of image embeddings. Simultaneously, the same deep network is trained to solve an additional self-supervised task of predicting image rotations. This pushes the network to learn more meaningful image representations that facilitate a better clustering. Experimental results show that MMDC achieves or exceeds state-of-the-art performance on six challenging benchmarks. On natural image datasets we improve on previous results with significant margins of up to 20% absolute accuracy points, yielding an accuracy of 82% on CIFAR-10, 45% on CIFAR-100 and 69% on STL-10.



### Fingerprint Spoof Generalization
- **Arxiv ID**: http://arxiv.org/abs/1912.02710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02710v1)
- **Published**: 2019-12-05 16:43:24+00:00
- **Updated**: 2019-12-05 16:43:24+00:00
- **Authors**: Tarang Chugh, Anil K. Jain
- **Comment**: 14 pages, 10 figures
- **Journal**: None
- **Summary**: We present a style-transfer based wrapper, called Universal Material Generator (UMG), to improve the generalization performance of any fingerprint spoof detector against spoofs made from materials not seen during training. Specifically, we transfer the style (texture) characteristics between fingerprint images of known materials with the goal of synthesizing fingerprint images corresponding to unknown materials, that may occupy the space between the known materials in the deep feature space. Synthetic live fingerprint images are also added to the training dataset to force the CNN to learn generative-noise invariant features which discriminate between lives and spoofs. The proposed approach is shown to improve the generalization performance of a state-of-the-art spoof detector, namely Fingerprint Spoof Buster, from TDR of 75.24% to 91.78% @ FDR = 0.2%. These results are based on a large-scale dataset of 5,743 live and 4,912 spoof images fabricated using 12 different materials. Additionally, the UMG wrapper is shown to improve the average cross-sensor spoof detection performance from 67.60% to 80.63% when tested on the LivDet 2017 dataset. Training the UMG wrapper requires only 100 live fingerprint images from the target sensor, alleviating the time and resources required to generate large-scale live and spoof datasets for a new sensor. We also fabricate physical spoof artifacts using a mixture of known spoof materials to explore the role of cross-material style transfer in improving generalization performance.



### Revisiting Few-Shot Learning for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.02751v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02751v2)
- **Published**: 2019-12-05 17:41:56+00:00
- **Updated**: 2019-12-11 09:20:41+00:00
- **Authors**: Anca-Nicoleta Ciubotaru, Arnout Devos, Behzad Bozorgtabar, Jean-Philippe Thiran, Maria Gabrani
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing deep neural nets on automatic facial expression recognition focus on a set of predefined emotion classes, where the amount of training data has the biggest impact on performance. However, in the standard setting over-parameterised neural networks are not amenable for learning from few samples as they can quickly over-fit. In addition, these approaches do not have such a strong generalisation ability to identify a new category, where the data of each category is too limited and significant variations exist in the expression within the same semantic category. We embrace these challenges and formulate the problem as a low-shot learning, where once the base classifier is deployed, it must rapidly adapt to recognise novel classes using a few samples. In this paper, we revisit and compare existing few-shot learning methods for the low-shot facial expression recognition in terms of their generalisation ability via episode-training. In particular, we extend our analysis on the cross-domain generalisation, where training and test tasks are not drawn from the same distribution. We demonstrate the efficacy of low-shot learning methods through extensive experiments.



### AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1912.02781v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02781v2)
- **Published**: 2019-12-05 18:18:10+00:00
- **Updated**: 2020-02-17 06:16:13+00:00
- **Authors**: Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, Balaji Lakshminarayanan
- **Comment**: Code available at https://github.com/google-research/augmix
- **Journal**: None
- **Summary**: Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.



### Self-Supervised Learning of Video-Induced Visual Invariances
- **Arxiv ID**: http://arxiv.org/abs/1912.02783v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02783v2)
- **Published**: 2019-12-05 18:20:31+00:00
- **Updated**: 2020-04-01 18:29:28+00:00
- **Authors**: Michael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Xiaohua Zhai, Neil Houlsby, Sylvain Gelly, Mario Lucic
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We propose a general framework for self-supervised learning of transferable visual representations based on Video-Induced Visual Invariances (VIVI). We consider the implicit hierarchy present in the videos and make use of (i) frame-level invariances (e.g. stability to color and contrast perturbations), (ii) shot/clip-level invariances (e.g. robustness to changes in object orientation and lighting conditions), and (iii) video-level invariances (semantic relationships of scenes across shots/clips), to define a holistic self-supervised loss. Training models using different variants of the proposed framework on videos from the YouTube-8M (YT8M) data set, we obtain state-of-the-art self-supervised transfer learning results on the 19 diverse downstream tasks of the Visual Task Adaptation Benchmark (VTAB), using only 1000 labels per task. We then show how to co-train our models jointly with labeled images, outperforming an ImageNet-pretrained ResNet-50 by 0.8 points with 10x fewer labeled images, as well as the previous best supervised model by 3.7 points using the full ImageNet data set.



### CLOTH3D: Clothed 3D Humans
- **Arxiv ID**: http://arxiv.org/abs/1912.02792v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02792v2)
- **Published**: 2019-12-05 18:34:26+00:00
- **Updated**: 2020-09-06 14:46:05+00:00
- **Authors**: Hugo Bertiche, Meysam Madadi, Sergio Escalera
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents CLOTH3D, the first big scale synthetic dataset of 3D clothed human sequences. CLOTH3D contains a large variability on garment type, topology, shape, size, tightness and fabric. Clothes are simulated on top of thousands of different pose sequences and body shapes, generating realistic cloth dynamics. We provide the dataset with a generative model for cloth generation. We propose a Conditional Variational Auto-Encoder (CVAE) based on graph convolutions (GCVAE) to learn garment latent spaces. This allows for realistic generation of 3D garments on top of SMPL model for any pose and shape.



### PolyTransform: Deep Polygon Transformer for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.02801v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02801v4)
- **Published**: 2019-12-05 18:50:20+00:00
- **Updated**: 2021-01-16 21:09:51+00:00
- **Authors**: Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Yuwen Xiong, Rui Hu, Raquel Urtasun
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose PolyTransform, a novel instance segmentation algorithm that produces precise, geometry-preserving masks by combining the strengths of prevailing segmentation approaches and modern polygon-based methods. In particular, we first exploit a segmentation network to generate instance masks. We then convert the masks into a set of polygons that are then fed to a deforming network that transforms the polygons such that they better fit the object boundaries. Our experiments on the challenging Cityscapes dataset show that our PolyTransform significantly improves the performance of the backbone instance segmentation network and ranks 1st on the Cityscapes test-set leaderboard. We also show impressive gains in the interactive annotation setting. We release the code at https://github.com/uber-research/PolyTransform.



### KeyPose: Multi-View 3D Labeling and Keypoint Estimation for Transparent Objects
- **Arxiv ID**: http://arxiv.org/abs/1912.02805v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.02805v2)
- **Published**: 2019-12-05 18:54:07+00:00
- **Updated**: 2020-05-18 21:46:19+00:00
- **Authors**: Xingyu Liu, Rico Jonschkowski, Anelia Angelova, Kurt Konolige
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Estimating the 3D pose of desktop objects is crucial for applications such as robotic manipulation. Many existing approaches to this problem require a depth map of the object for both training and prediction, which restricts them to opaque, lambertian objects that produce good returns in an RGBD sensor. In this paper we forgo using a depth sensor in favor of raw stereo input. We address two problems: first, we establish an easy method for capturing and labeling 3D keypoints on desktop objects with an RGB camera; and second, we develop a deep neural network, called $KeyPose$, that learns to accurately predict object poses using 3D keypoints, from stereo input, and works even for transparent objects. To evaluate the performance of our method, we create a dataset of 15 clear objects in five classes, with 48K 3D-keypoint labeled images. We train both instance and category models, and show generalization to new textures, poses, and objects. KeyPose surpasses state-of-the-art performance in 3D pose estimation on this dataset by factors of 1.5 to 3.5, even in cases where the competing method is provided with ground-truth depth. Stereo input is essential for this performance as it improves results compared to using monocular input by a factor of 2. We will release a public version of the data capture and labeling pipeline, the transparent object database, and the KeyPose models and evaluation code. Project website: https://sites.google.com/corp/view/keypose.



### Cross-Resolution Learning for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.02851v1
- **DOI**: 10.1016/j.imavis.2020.103927
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.6, I.2.10, I.2.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1912.02851v1)
- **Published**: 2019-12-05 19:40:35+00:00
- **Updated**: 2019-12-05 19:40:35+00:00
- **Authors**: Fabio Valerio Massoli, Giuseppe Amato, Fabrizio Falchi
- **Comment**: None
- **Journal**: Image and Vision Computing Volume 99, July 2020, 103927
- **Summary**: Convolutional Neural Networks have reached extremely high performances on the Face Recognition task. Largely used datasets, such as VGGFace2, focus on gender, pose and age variations trying to balance them to achieve better results. However, the fact that images have different resolutions is not usually discussed and resize to 256 pixels before cropping is used. While specific datasets for very low resolution faces have been proposed, less attention has been payed on the task of cross-resolution matching. Such scenarios are of particular interest for forensic and surveillance systems in which it usually happens that a low-resolution probe has to be matched with higher-resolution galleries. While it is always possible to either increase the resolution of the probe image or to reduce the size of the gallery images, to the best of our knowledge an extensive experimentation of cross-resolution matching was missing in the recent deep learning based literature. In the context of low- and cross-resolution Face Recognition, the contributions of our work are: i) we proposed a training method to fine-tune a state-of-the-art model in order to make it able to extract resolution-robust deep features; ii) we tested our models on the benchmark datasets IJB-B/C considering images at both full and low resolutions in order to show the effectiveness of the proposed training algorithm. To the best of our knowledge, this is the first work testing extensively the performance of a FR model in a cross-resolution scenario; iii) we tested our models on the low resolution and low quality datasets QMUL-SurvFace and TinyFace and showed their superior performances, even though we did not train our model on low-resolution faces only and our main focus was cross-resolution; iv) we showed that our approach can be more effective with respect to preprocessing faces with super resolution techniques.



### An Accelerated Correlation Filter Tracker
- **Arxiv ID**: http://arxiv.org/abs/1912.02854v1
- **DOI**: 10.1016/j.patcog.2019.107172
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02854v1)
- **Published**: 2019-12-05 20:03:38+00:00
- **Updated**: 2019-12-05 20:03:38+00:00
- **Authors**: Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, Josef Kittler
- **Comment**: None
- **Journal**: Pattern Recognition 102(2019) 107172
- **Summary**: Recent visual object tracking methods have witnessed a continuous improvement in the state-of-the-art with the development of efficient discriminative correlation filters (DCF) and robust deep neural network features. Despite the outstanding performance achieved by the above combination, existing advanced trackers suffer from the burden of high computational complexity of the deep feature extraction and online model learning. We propose an accelerated ADMM optimisation method obtained by adding a momentum to the optimisation sequence iterates, and by relaxing the impact of the error between DCF parameters and their norm. The proposed optimisation method is applied to an innovative formulation of the DCF design, which seeks the most discriminative spatially regularised feature channels. A further speed up is achieved by an adaptive initialisation of the filter optimisation process. The significantly increased convergence of the DCF filter is demonstrated by establishing the optimisation process equivalence with a continuous dynamical system for which the convergence properties can readily be derived. The experimental results obtained on several well-known benchmarking datasets demonstrate the efficiency and robustness of the proposed ACFT method, with a tracking accuracy comparable to the start-of-the-art trackers.



### Exposing Fake Images with Forensic Similarity Graphs
- **Arxiv ID**: http://arxiv.org/abs/1912.02861v2
- **DOI**: 10.1109/JSTSP.2020.3001516
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02861v2)
- **Published**: 2019-12-05 20:21:24+00:00
- **Updated**: 2020-04-20 19:51:18+00:00
- **Authors**: Owen Mayer, Matthew C. Stamm
- **Comment**: 16 pages, under review at IEEE Journal of Selected Topics in Signal
  Processing
- **Journal**: None
- **Summary**: We propose new image forgery detection and localization algorithms by recasting these problems as graph-based community detection problems. To do this, we introduce a novel abstract, graph-based representation of an image, which we call the Forensic Similarity Graph, that captures key forensic relationships among regions in the image. In this representation, small image patches are represented by graph vertices with edges assigned according to the forensic similarity between patches. Localized tampering introduces unique structure into this graph, which aligns with a concept called ``community structure'' in graph-theory literature. In the Forensic Similarity Graph, communities correspond to the tampered and unaltered regions in the image. As a result, forgery detection is performed by identifying whether multiple communities exist, and forgery localization is performed by partitioning these communities. We present two community detection techniques, adapted from literature, to detect and localize image forgeries. We experimentally show that our proposed community detection methods outperform existing state-of-the-art forgery detection and localization methods, which do not capture such community structure.



### Classifying Diagrams and Their Parts using Graph Neural Networks: A Comparison of Crowd-Sourced and Expert Annotations
- **Arxiv ID**: http://arxiv.org/abs/1912.02866v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1912.02866v1)
- **Published**: 2019-12-05 20:34:53+00:00
- **Updated**: 2019-12-05 20:34:53+00:00
- **Authors**: Tuomo Hiippala
- **Comment**: 9 pages; submitted to LREC 2020
- **Journal**: None
- **Summary**: This article compares two multimodal resources that consist of diagrams which describe topics in elementary school natural sciences. Both resources contain the same diagrams and represent their structure using graphs, but differ in terms of their annotation schema and how the annotations have been created - depending on the resource in question - either by crowd-sourced workers or trained experts. This article reports on two experiments that evaluate how effectively crowd-sourced and expert-annotated graphs can represent the multimodal structure of diagrams for representation learning using various graph neural networks. The results show that the identity of diagram elements can be learned from their layout features, while the expert annotations provide better representations of diagram types.



### Learning Super-resolved Depth from Active Gated Imaging
- **Arxiv ID**: http://arxiv.org/abs/1912.02889v1
- **DOI**: 10.1109/ITSC.2018.8569590
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02889v1)
- **Published**: 2019-12-05 21:35:39+00:00
- **Updated**: 2019-12-05 21:35:39+00:00
- **Authors**: Tobias Gruber, Mariia Kokhova, Werner Ritter, Norbert Haala, Klaus Dietmayer
- **Comment**: None
- **Journal**: Published in: 2018 21st International Conference on Intelligent
  Transportation Systems (ITSC)
- **Summary**: Environment perception for autonomous driving is doomed by the trade-off between range-accuracy and resolution: current sensors that deliver very precise depth information are usually restricted to low resolution because of technology or cost limitations. In this work, we exploit depth information from an active gated imaging system based on cost-sensitive diode and CMOS technology. Learning a mapping between pixel intensities of three gated slices and depth produces a super-resolved depth map image with respectable relative accuracy of 5% in between 25-80 m. By design, depth information is perfectly aligned with pixel intensity values.



### Analysis of effectiveness of thresholding in perfusion ROI detection on T2-weighted MR images with abnormal brain anatomy
- **Arxiv ID**: http://arxiv.org/abs/1912.05469v1
- **DOI**: 10.20535/kpi-sn.2019.4.180237
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05469v1)
- **Published**: 2019-12-05 22:38:21+00:00
- **Updated**: 2019-12-05 22:38:21+00:00
- **Authors**: Svitlana Alkhimova, Svitlana Sliusar
- **Comment**: None
- **Journal**: KPI Science News. - 2019. - V. 126, N. 4. - P.35-43
- **Summary**: The brain perfusion ROI detection being a preliminary step, designed to exclude non-brain tissues from analyzed DSC perfusion MR images. Its accuracy is considered as the key factor for delivering correct results of perfusion data analysis. Despite the large variety of algorithms developed on brain tissues segmentation, there is no one that works reliably and robustly on 2T-waited MR images of a human head with abnormal brain anatomy. Therefore, thresholding method is still the state-of-the-art technique that is widely used as a way of managing pixels involved in brain perfusion ROI. This paper presents the analysis of effectiveness of thresholding techniques in brain perfusion ROI detection on 2T-waited MR images of a human head with abnormal brain anatomy. Four threshold-based algorithms implementation are considered: according to Otsu method as global thresholding, according to Niblack method as local thresholding, thresholding in approximate anatomical brain location, and brute force thresholding. The analysis is done using comparison of qualitative maps produced from thresholded images and from the reference ones. Pearson correlation analysis showed strong positive (r was ranged from 0.7123 to 0.8518, p<0.01) and weak positive (r<0.35, p<0.01) relationship in case of conducted experiments with CBF, CBV, MTT and Tmax maps, respectively. Linear regression analysis showed at level of 95% confidence interval that maps produced from thresholded images were subject to scale and offset errors in all conducted experiments. The experimental results showed that widely used thresholding methods are an ineffective way of managing pixels involved in brain perfusion ROI. Thresholding as brain segmentation tool can lead to poor placement of perfusion ROI and, as a result, produced maps will be subject to artifacts and can cause falsely high or falsely low perfusion parameters assessment.



### Diagnostic Image Quality Assessment and Classification in Medical Imaging: Opportunities and Challenges
- **Arxiv ID**: http://arxiv.org/abs/1912.02907v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02907v1)
- **Published**: 2019-12-05 22:44:54+00:00
- **Updated**: 2019-12-05 22:44:54+00:00
- **Authors**: Jeffrey Ma, Ukash Nakarmi, Cedric Yue Sik Kin, Christopher Sandino, Joseph Y. Cheng, Ali B. Syed, Peter Wei, John M. Pauly, Shreyas Vasanawala
- **Comment**: 4 pages, 8 Figures, Conference Submission
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) suffers from several artifacts, the most common of which are motion artifacts. These artifacts often yield images that are of non-diagnostic quality. To detect such artifacts, images are prospectively evaluated by experts for their diagnostic quality, which necessitates patient-revisits and rescans whenever non-diagnostic quality scans are encountered. This motivates the need to develop an automated framework capable of accessing medical image quality and detecting diagnostic and non-diagnostic images. In this paper, we explore several convolutional neural network-based frameworks for medical image quality assessment and investigate several challenges therein.



### Why Having 10,000 Parameters in Your Camera Model is Better Than Twelve
- **Arxiv ID**: http://arxiv.org/abs/1912.02908v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02908v3)
- **Published**: 2019-12-05 22:48:50+00:00
- **Updated**: 2020-06-23 15:51:07+00:00
- **Authors**: Thomas SchÃ¶ps, Viktor Larsson, Marc Pollefeys, Torsten Sattler
- **Comment**: 15 pages, 12 figures, accepted to CVPR 2020 as an oral
- **Journal**: None
- **Summary**: Camera calibration is an essential first step in setting up 3D Computer Vision systems. Commonly used parametric camera models are limited to a few degrees of freedom and thus often do not optimally fit to complex real lens distortion. In contrast, generic camera models allow for very accurate calibration due to their flexibility. Despite this, they have seen little use in practice. In this paper, we argue that this should change. We propose a calibration pipeline for generic models that is fully automated, easy to use, and can act as a drop-in replacement for parametric calibration, with a focus on accuracy. We compare our results to parametric calibrations. Considering stereo depth estimation and camera pose estimation as examples, we show that the calibration error acts as a bias on the results. We thus argue that in contrast to current common practice, generic models should be preferred over parametric ones whenever possible. To facilitate this, we released our calibration pipeline at https://github.com/puzzlepaint/camera_calibration, making both easy-to-use and accurate camera calibration available to everyone.



### Deep learning with noisy labels: exploring techniques and remedies in medical image analysis
- **Arxiv ID**: http://arxiv.org/abs/1912.02911v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.02911v4)
- **Published**: 2019-12-05 22:58:55+00:00
- **Updated**: 2020-03-20 22:45:57+00:00
- **Authors**: Davood Karimi, Haoran Dou, Simon K. Warfield, Ali Gholipour
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised training of deep learning models requires large labeled datasets. There is a growing interest in obtaining such datasets for medical image analysis applications. However, the impact of label noise has not received sufficient attention. Recent studies have shown that label noise can significantly impact the performance of deep learning models in many machine learning and computer vision applications. This is especially concerning for medical applications, where datasets are typically small, labeling requires domain expertise and suffers from high inter- and intra-observer variability, and erroneous predictions may influence decisions that directly impact human health. In this paper, we first review the state-of-the-art in handling label noise in deep learning. Then, we review studies that have dealt with label noise in deep learning for medical image analysis. Our review shows that recent progress on handling label noise in deep learning has gone largely unnoticed by the medical image analysis community. To help achieve a better understanding of the extent of the problem and its potential remedies, we conducted experiments with three medical imaging datasets with different types of label noise, where we investigated several existing strategies and developed new methods to combat the negative effect of label noise. Based on the results of these experiments and our review of the literature, we have made recommendations on methods that can be used to alleviate the effects of different types of label noise on deep models trained for medical image analysis. We hope that this article helps the medical image analysis researchers and developers in choosing and devising new techniques that effectively handle label noise in deep learning.



### A Comparative Analysis of Virtual Reality Head-Mounted Display Systems
- **Arxiv ID**: http://arxiv.org/abs/1912.02913v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02913v1)
- **Published**: 2019-12-05 23:01:33+00:00
- **Updated**: 2019-12-05 23:01:33+00:00
- **Authors**: Arian Mehrfard, Javad Fotouhi, Giacomo Taylor, Tess Forster, Nassir Navab, Bernhard Fuerst
- **Comment**: 9 pages, 11 figures
- **Journal**: None
- **Summary**: With recent advances of Virtual Reality (VR) technology, the deployment of such will dramatically increase in non-entertainment environments, such as professional education and training, manufacturing, service, or low frequency/high risk scenarios. Clinical education is an area that especially stands to benefit from VR technology due to the complexity, high cost, and difficult logistics. The effectiveness of the deployment of VR systems, is subject to factors that may not be necessarily considered for devices targeting the entertainment market. In this work, we systematically compare a wide range of VR Head-Mounted Displays (HMDs) technologies and designs by defining a new set of metrics that are 1) relevant to most generic VR solutions and 2) are of paramount importance for VR-based education and training. We evaluated ten HMDs based on various criteria, including neck strain, heat development, and color accuracy. Other metrics such as text readability, comfort, and contrast perception were evaluated in a multi-user study on three selected HMDs, namely Oculus Rift S, HTC Vive Pro and Samsung Odyssey+. Results indicate that the HTC Vive Pro performs best with regards to comfort, display quality and compatibility with glasses.



### RED-NET: A Recursive Encoder-Decoder Network for Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.02914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02914v1)
- **Published**: 2019-12-05 23:05:26+00:00
- **Updated**: 2019-12-05 23:05:26+00:00
- **Authors**: Truc Le, Yuyan Li, Ye Duan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce RED-NET: A Recursive Encoder-Decoder Network with Skip-Connections for edge detection in natural images. The proposed network is a novel integration of a Recursive Neural Network with an Encoder-Decoder architecture. The recursive network enables us to increase the network depth without increasing the number of parameters. Adding skip-connections between encoder and decoder helps the gradients reach all the layers of a network more easily and allows information related to finer details in the early stage of the encoder to be fully utilized in the decoder. Based on our extensive experiments on popular boundary detection datasets including BSDS500 \cite{Arbelaez2011}, NYUD \cite{Silberman2012} and Pascal Context \cite{Mottaghi2014}, RED-NET significantly advances the state-of-the-art on edge detection regarding standard evaluation metrics such as Optimal Dataset Scale (ODS) F-measure, Optimal Image Scale (OIS) F-measure, and Average Precision (AP).



### Detection of Face Recognition Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1912.02918v1
- **DOI**: 10.1016/j.cviu.2020.103103
- **Categories**: **cs.CV**, cs.LG, I.2.0, I.2.6, I.2.0; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1912.02918v1)
- **Published**: 2019-12-05 23:24:33+00:00
- **Updated**: 2019-12-05 23:24:33+00:00
- **Authors**: Fabio Valerio Massoli, Fabio Carrara, Giuseppe Amato, Fabrizio Falchi
- **Comment**: None
- **Journal**: Computer Vision and Image Understanding Volume 202, January 2021,
  103103
- **Summary**: Deep Learning methods have become state-of-the-art for solving tasks such as Face Recognition (FR). Unfortunately, despite their success, it has been pointed out that these learning models are exposed to adversarial inputs - images to which an imperceptible amount of noise for humans is added to maliciously fool a neural network - thus limiting their adoption in real-world applications. While it is true that an enormous effort has been spent in order to train robust models against this type of threat, adversarial detection techniques have recently started to draw attention within the scientific community. A detection approach has the advantage that it does not require to re-train any model, thus it can be added on top of any system. In this context, we present our work on adversarial samples detection in forensics mainly focused on detecting attacks against FR systems in which the learning model is typically used only as a features extractor. Thus, in these cases, train a more robust classifier might not be enough to defence a FR system. In this frame, the contribution of our work is four-fold: i) we tested our recently proposed adversarial detection approach against classifier attacks, i.e. adversarial samples crafted to fool a FR neural network acting as a classifier; ii) using a k-Nearest Neighbor (kNN) algorithm as a guidance, we generated deep features attacks against a FR system based on a DL model acting as features extractor, followed by a kNN which gives back the query identity based on features similarity; iii) we used the deep features attacks to fool a FR system on the 1:1 Face Verification task and we showed their superior effectiveness with respect to classifier attacks in fooling such type of system; iv) we used the detectors trained on classifier attacks to detect deep features attacks, thus showing that such approach is generalizable to different types of offensives.



### Generating 3D People in Scenes without People
- **Arxiv ID**: http://arxiv.org/abs/1912.02923v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02923v3)
- **Published**: 2019-12-05 23:49:27+00:00
- **Updated**: 2020-04-19 18:22:59+00:00
- **Authors**: Yan Zhang, Mohamed Hassan, Heiko Neumann, Michael J. Black, Siyu Tang
- **Comment**: cvpr'20, camera ready version with appendix
- **Journal**: None
- **Summary**: We present a fully automatic system that takes a 3D scene and generates plausible 3D human bodies that are posed naturally in that 3D scene. Given a 3D scene without people, humans can easily imagine how people could interact with the scene and the objects in it. However, this is a challenging task for a computer as solving it requires that (1) the generated human bodies to be semantically plausible within the 3D environment (e.g. people sitting on the sofa or cooking near the stove), and (2) the generated human-scene interaction to be physically feasible such that the human body and scene do not interpenetrate while, at the same time, body-scene contact supports physical interactions. To that end, we make use of the surface-based 3D human model SMPL-X. We first train a conditional variational autoencoder to predict semantically plausible 3D human poses conditioned on latent scene representations, then we further refine the generated 3D bodies using scene constraints to enforce feasible physical interaction. We show that our approach is able to synthesize realistic and expressive 3D human bodies that naturally interact with 3D environment. We perform extensive experiments demonstrating that our generative framework compares favorably with existing methods, both qualitatively and quantitatively. We believe that our scene-conditioned 3D human generation pipeline will be useful for numerous applications; e.g. to generate training data for human pose estimation, in video games and in VR/AR. Our project page for data and code can be seen at: \url{https://vlg.inf.ethz.ch/projects/PSI/}.




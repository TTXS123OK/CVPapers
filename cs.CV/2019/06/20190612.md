# Arxiv Papers in cs.CV on 2019-06-12
### Learning Predicates as Functions to Enable Few-shot Scene Graph Prediction
- **Arxiv ID**: http://arxiv.org/abs/1906.04876v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04876v4)
- **Published**: 2019-06-12 01:27:15+00:00
- **Updated**: 2019-12-05 19:35:36+00:00
- **Authors**: Apoorva Dornadula, Austin Narcomey, Ranjay Krishna, Michael Bernstein, Li Fei-Fei
- **Comment**: 14 pages, 10 figures, preprint
- **Journal**: None
- **Summary**: Scene graph prediction --- classifying the set of objects and predicates in a visual scene --- requires substantial training data. However, most predicates only occur a handful of times making them difficult to learn. We introduce the first scene graph prediction model that supports few-shot learning of predicates. Existing scene graph generation models represent objects using pretrained object detectors or word embeddings that capture semantic object information at the cost of encoding information about which relationships they afford. So, these object representations are unable to generalize to new few-shot relationships. We introduce a framework that induces object representations that are structured according to their visual relationships. Unlike past methods, our framework embeds objects that afford similar relationships closer together. This property allows our model to perform well in the few-shot setting. For example, applying the 'riding' predicate transformation to 'person' modifies the representation towards objects like 'skateboard' and 'horse' that enable riding. We generate object representations by learning predicates trained as message passing functions within a new graph convolution framework. The object representations are used to build few-shot predicate classifiers for rare predicates with as few as 1 labeled example. We achieve a 5-shot performance of 22.70 recall@50, a 3.7 increase when compared to strong transfer learning baselines.



### Lidar based Detection and Classification of Pedestrians and Vehicles Using Machine Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/1906.11899v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11899v1)
- **Published**: 2019-06-12 01:39:45+00:00
- **Updated**: 2019-06-12 01:39:45+00:00
- **Authors**: Farzad Shafiei Dizaji
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this paper is to classify objects mapped by LiDAR sensor into different classes such as vehicles, pedestrians and bikers. Utilizing a LiDAR-based object detector and Neural Networks-based classifier, a novel real-time object detection is presented essentially with respect to aid self-driving vehicles in recognizing and classifying other objects encountered in the course of driving and proceed accordingly. We discuss our work using machine learning methods to tackle a common high-level problem found in machine learning applications for self-driving cars: the classification of pointcloud data obtained from a 3D LiDAR sensor.



### Using Small Proxy Datasets to Accelerate Hyperparameter Search
- **Arxiv ID**: http://arxiv.org/abs/1906.04887v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.04887v1)
- **Published**: 2019-06-12 02:01:30+00:00
- **Updated**: 2019-06-12 02:01:30+00:00
- **Authors**: Sam Shleifer, Eric Prokop
- **Comment**: None
- **Journal**: None
- **Summary**: One of the biggest bottlenecks in a machine learning workflow is waiting for models to train. Depending on the available computing resources, it can take days to weeks to train a neural network on a large dataset with many classes such as ImageNet. For researchers experimenting with new algorithmic approaches, this is impractically time consuming and costly. We aim to generate smaller "proxy datasets" where experiments are cheaper to run but results are highly correlated with experimental results on the full dataset. We generate these proxy datasets using by randomly sampling from examples or classes, training on only the easiest or hardest examples and training on synthetic examples generated by "data distillation". We compare these techniques to the more widely used baseline of training on the full dataset for fewer epochs. For each proxying strategy, we estimate three measures of "proxy quality": how much of the variance in experimental results on the full dataset can be explained by experimental results on the proxy dataset.   Experiments on Imagenette and Imagewoof (Howard, 2019) show that running hyperparameter search on the easiest 10% of examples explains 81% of the variance in experiment results on the target task, and using the easiest 50% of examples can explain 95% of the variance, significantly more than training on all the data for fewer epochs, a more widely used baseline. These "easy" proxies are higher quality than training on the full dataset for a reduced number of epochs (but equivalent computational cost), and, unexpectedly, higher quality than proxies constructed from the hardest examples. Without access to a trained model, researchers can improve proxy quality by restricting the subset to fewer classes; proxies built on half the classes are higher quality than those with an equivalent number of examples spread across all classes.



### Adaptive Navigation Scheme for Optimal Deep-Sea Localization Using Multimodal Perception Cues
- **Arxiv ID**: http://arxiv.org/abs/1906.04888v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1906.04888v1)
- **Published**: 2019-06-12 02:03:20+00:00
- **Updated**: 2019-06-12 02:03:20+00:00
- **Authors**: Arturo Gomez Chavez, Qingwen Xu, Christian A. Mueller, Sören Schwertfeger, Andreas Birk
- **Comment**: Submitted to IROS 2019
- **Journal**: None
- **Summary**: Underwater robot interventions require a high level of safety and reliability. A major challenge to address is a robust and accurate acquisition of localization estimates, as it is a prerequisite to enable more complex tasks, e.g. floating manipulation and mapping. State-of-the-art navigation in commercial operations, such as oil & gas production (OGP), rely on costly instrumentation. These can be partially replaced or assisted by visual navigation methods, especially in deep-sea scenarios where equipment deployment has high costs and risks. Our work presents a multimodal approach that adapts state-of-the-art methods from on-land robotics, i.e., dense point cloud generation in combination with plane representation and registration, to boost underwater localization performance. A two-stage navigation scheme is proposed that initially generates a coarse probabilistic map of the workspace, which is used to filter noise from computed point clouds and planes in the second stage. Furthermore, an adaptive decision-making approach is introduced that determines which perception cues to incorporate into the localization filter to optimize accuracy and computation performance. Our approach is investigated first in simulation and then validated with data from field trials in OGP monitoring and maintenance scenarios.



### Regularizing Neural Networks via Minimizing Hyperspherical Energy
- **Arxiv ID**: http://arxiv.org/abs/1906.04892v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04892v2)
- **Published**: 2019-06-12 02:12:28+00:00
- **Updated**: 2020-04-09 16:04:06+00:00
- **Authors**: Rongmei Lin, Weiyang Liu, Zhen Liu, Chen Feng, Zhiding Yu, James M. Rehg, Li Xiong, Le Song
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Inspired by the Thomson problem in physics where the distribution of multiple propelling electrons on a unit sphere can be modeled via minimizing some potential energy, hyperspherical energy minimization has demonstrated its potential in regularizing neural networks and improving their generalization power. In this paper, we first study the important role that hyperspherical energy plays in neural network training by analyzing its training dynamics. Then we show that naively minimizing hyperspherical energy suffers from some difficulties due to highly non-linear and non-convex optimization as the space dimensionality becomes higher, therefore limiting the potential to further improve the generalization. To address these problems, we propose the compressive minimum hyperspherical energy (CoMHE) as a more effective regularization for neural networks. Specifically, CoMHE utilizes projection mappings to reduce the dimensionality of neurons and minimizes their hyperspherical energy. According to different designs for the projection mapping, we propose several distinct yet well-performing variants and provide some theoretical guarantees to justify their effectiveness. Our experiments show that CoMHE consistently outperforms existing regularization methods, and can be easily applied to different neural networks.



### Privacy-Preserving Deep Action Recognition: An Adversarial Learning Framework and A New Dataset
- **Arxiv ID**: http://arxiv.org/abs/1906.05675v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05675v6)
- **Published**: 2019-06-12 02:23:32+00:00
- **Updated**: 2021-03-21 21:34:49+00:00
- **Authors**: Zhenyu Wu, Haotao Wang, Zhaowen Wang, Hailin Jin, Zhangyang Wang
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). arXiv admin note: text overlap with arXiv:1807.08379
- **Journal**: None
- **Summary**: We investigate privacy-preserving, video-based action recognition in deep learning, a problem with growing importance in smart camera applications. A novel adversarial training framework is formulated to learn an anonymization transform for input videos such that the trade-off between target utility task performance and the associated privacy budgets is explicitly optimized on the anonymized videos. Notably, the privacy budget, often defined and measured in task-driven contexts, cannot be reliably indicated using any single model performance because strong protection of privacy should sustain against any malicious model that tries to steal private information. To tackle this problem, we propose two new optimization strategies of model restarting and model ensemble to achieve stronger universal privacy protection against any attacker models. Extensive experiments have been carried out and analyzed. On the other hand, given few public datasets available with both utility and privacy labels, the data-driven (supervised) learning cannot exert its full power on this task. We first discuss an innovative heuristic of cross-dataset training and evaluation, enabling the use of multiple single-task datasets (one with target task labels and the other with privacy labels) in our problem. To further address this dataset challenge, we have constructed a new dataset, termed PA-HMDB51, with both target task labels (action) and selected privacy attributes (skin color, face, gender, nudity, and relationship) annotated on a per-frame basis. This first-of-its-kind video dataset and evaluation protocol can greatly facilitate visual privacy research and open up other opportunities. Our codes, models, and the PA-HMDB51 dataset are available at https://github.com/VITA-Group/PA-HMDB51.



### All-Weather Deep Outdoor Lighting Estimation
- **Arxiv ID**: http://arxiv.org/abs/1906.04909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04909v1)
- **Published**: 2019-06-12 03:19:08+00:00
- **Updated**: 2019-06-12 03:19:08+00:00
- **Authors**: Jinsong Zhang, Kalyan Sunkavalli, Yannick Hold-Geoffroy, Sunil Hadap, Jonathan Eisenmann, Jean-François Lalonde
- **Comment**: 8 pages, CVPR 19. Project page: http://lvsn.github.io/allweather
- **Journal**: None
- **Summary**: We present a neural network that predicts HDR outdoor illumination from a single LDR image. At the heart of our work is a method to accurately learn HDR lighting from LDR panoramas under any weather condition. We achieve this by training another CNN (on a combination of synthetic and real images) to take as input an LDR panorama, and regress the parameters of the Lalonde-Matthews outdoor illumination model. This model is trained such that it a) reconstructs the appearance of the sky, and b) renders the appearance of objects lit by this illumination. We use this network to label a large-scale dataset of LDR panoramas with lighting parameters and use them to train our single image outdoor lighting estimation network. We demonstrate, via extensive experiments, that both our panorama and single image networks outperform the state of the art, and unlike prior work, are able to handle weather conditions ranging from fully sunny to overcast skies.



### Non-Parametric Calibration for Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.04933v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.04933v3)
- **Published**: 2019-06-12 04:11:48+00:00
- **Updated**: 2020-02-27 14:20:33+00:00
- **Authors**: Jonathan Wenger, Hedvig Kjellström, Rudolph Triebel
- **Comment**: None
- **Journal**: None
- **Summary**: Many applications of classification methods not only require high accuracy but also reliable estimation of predictive uncertainty. However, while many current classification frameworks, in particular deep neural networks, achieve high accuracy, they tend to incorrectly estimate uncertainty. In this paper, we propose a method that adjusts the confidence estimates of a general classifier such that they approach the probability of classifying correctly. In contrast to existing approaches, our calibration method employs a non-parametric representation using a latent Gaussian process, and is specifically designed for multi-class classification. It can be applied to any classifier that outputs confidence estimates and is not limited to neural networks. We also provide a theoretical analysis regarding the over- and underconfidence of a classifier and its relationship to calibration, as well as an empirical outlook for calibrated active learning. In experiments we show the universally strong performance of our method across different classifiers and benchmark data sets, in particular for state-of-the art neural network architectures.



### Semi-Supervised Exploration in Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1906.04944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04944v1)
- **Published**: 2019-06-12 05:12:32+00:00
- **Updated**: 2019-06-12 05:12:32+00:00
- **Authors**: Cheng Chang, Himanshu Rai, Satya Krishna Gorti, Junwei Ma, Chundi Liu, Guangwei Yu, Maksims Volkovs
- **Comment**: None
- **Journal**: None
- **Summary**: We present our solution to Landmark Image Retrieval Challenge 2019. This challenge was based on the large Google Landmarks Dataset V2[9]. The goal was to retrieve all database images containing the same landmark for every provided query image. Our solution is a combination of global and local models to form an initial KNN graph. We then use a novel extension of the recently proposed graph traversal method EGT [1] referred to as semi-supervised EGT to refine the graph and retrieve better candidates.



### Pay Attention to Convolution Filters: Towards Fast and Accurate Fine-Grained Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.04950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04950v1)
- **Published**: 2019-06-12 05:38:24+00:00
- **Updated**: 2019-06-12 05:38:24+00:00
- **Authors**: Xiangxi Mo, Ruizhe Cheng, Tianyi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an efficient transfer learning method for adapting ImageNet pre-trained Convolutional Neural Network (CNN) to fine-grained image classification task. Conventional transfer learning methods typically face the trade-off between training time and accuracy. By adding "attention module" to each convolutional filters of the pre-trained network, we are able to rank and adjust the importance of each convolutional signal in an end-to-end pipeline. In this report, we show our method can adapt a pre-trianed ResNet50 for a fine-grained transfer learning task within few epochs and achieve accuracy above conventional transfer learning methods and close to models trained from scratch. Our model also offer interpretable result because the rank of the convolutional signal shows which convolution channels are utilized and amplified to achieve better classification result, as well as which signal should be treated as noise for the specific transfer learning task, which could be pruned to lower model size.



### Hand Orientation Estimation in Probability Density Form
- **Arxiv ID**: http://arxiv.org/abs/1906.04952v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04952v1)
- **Published**: 2019-06-12 05:42:38+00:00
- **Updated**: 2019-06-12 05:42:38+00:00
- **Authors**: Kazuaki Kondo, Daisuke Deguchi, Atsushi Shimada
- **Comment**: None
- **Journal**: None
- **Summary**: Hand orientation is an essential feature required to understand hand behaviors and subsequently support human activities. In this paper, we present a new method for estimating hand orientation in probability density form. It can solve the cyclicity problem in direct angular representation and enables the integration of multiple predictions based on different features. We validated the performance of the proposed method and an integration example using our dataset, which captured cooperative group work.



### Synthesizing Diverse Lung Nodules Wherever Massively: 3D Multi-Conditional GAN-based CT Image Augmentation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.04962v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04962v2)
- **Published**: 2019-06-12 06:31:39+00:00
- **Updated**: 2019-08-12 18:54:57+00:00
- **Authors**: Changhee Han, Yoshiro Kitamura, Akira Kudo, Akimichi Ichinose, Leonardo Rundo, Yujiro Furukawa, Kazuki Umemoto, Yuanzhong Li, Hideki Nakayama
- **Comment**: 9 pages, 6 figures, accepted to 3DV 2019
- **Journal**: None
- **Summary**: Accurate Computer-Assisted Diagnosis, relying on large-scale annotated pathological images, can alleviate the risk of overlooking the diagnosis. Unfortunately, in medical imaging, most available datasets are small/fragmented. To tackle this, as a Data Augmentation (DA) method, 3D conditional Generative Adversarial Networks (GANs) can synthesize desired realistic/diverse 3D images as additional training data. However, no 3D conditional GAN-based DA approach exists for general bounding box-based 3D object detection, while it can locate disease areas with physicians' minimum annotation cost, unlike rigorous 3D segmentation. Moreover, since lesions vary in position/size/attenuation, further GAN-based DA performance requires multiple conditions. Therefore, we propose 3D Multi-Conditional GAN (MCGAN) to generate realistic/diverse 32 X 32 X 32 nodules placed naturally on lung Computed Tomography images to boost sensitivity in 3D object detection. Our MCGAN adopts two discriminators for conditioning: the context discriminator learns to classify real vs synthetic nodule/surrounding pairs with noise box-centered surroundings; the nodule discriminator attempts to classify real vs synthetic nodules with size/attenuation conditions. The results show that 3D Convolutional Neural Network-based detection can achieve higher sensitivity under any nodule size/attenuation at fixed False Positive rates and overcome the medical data paucity with the MCGAN-generated realistic nodules---even expert physicians fail to distinguish them from the real ones in Visual Turing Test.



### CDPM: Convolutional Deformable Part Models for Semantically Aligned Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1906.04976v2
- **DOI**: 10.1109/TIP.2019.2959923
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04976v2)
- **Published**: 2019-06-12 07:20:08+00:00
- **Updated**: 2019-12-13 01:07:12+00:00
- **Authors**: Kan Wang, Changxing Ding, Stephen J. Maybank, Dacheng Tao
- **Comment**: 13 pages, 13 figures, To appear in IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Part-level representations are essential for robust person re-identification. However, common errors that arise during pedestrian detection frequently result in severe misalignment problems for body parts, which degrade the quality of part representations. Accordingly, to deal with this problem, we propose a novel model named Convolutional Deformable Part Models (CDPM). CDPM works by decoupling the complex part alignment procedure into two easier steps: first, a vertical alignment step detects each body part in the vertical direction, with the help of a multi-task learning model; second, a horizontal refinement step based on attention suppresses the background information around each detected body part. Since these two steps are performed orthogonally and sequentially, the difficulty of part alignment is significantly reduced. In the testing stage, CDPM is able to accurately align flexible body parts without any need for outside information. Extensive experimental results demonstrate the effectiveness of the proposed CDPM for part alignment. Most impressively, CDPM achieves state-of-the-art performance on three large-scale datasets: Market-1501, DukeMTMC-ReID,and CUHK03.



### DeepSquare: Boosting the Learning Power of Deep Convolutional Neural Networks with Elementwise Square Operators
- **Arxiv ID**: http://arxiv.org/abs/1906.04979v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04979v1)
- **Published**: 2019-06-12 07:27:44+00:00
- **Updated**: 2019-06-12 07:27:44+00:00
- **Authors**: Sheng Chen, Xu Wang, Chao Chen, Yifan Lu, Xijin Zhang, Linfu Wen
- **Comment**: We has submitted this paper to a conference before March 22, 2019. We
  will improve this paper according to the received reviews. The code will be
  released if the paper is accepted for publication
- **Journal**: None
- **Summary**: Modern neural network modules which can significantly enhance the learning power usually add too much computational complexity to the original neural networks. In this paper, we pursue very efficient neural network modules which can significantly boost the learning power of deep convolutional neural networks with negligible extra computational cost. We first present both theoretically and experimentally that elementwise square operator has a potential to enhance the learning power of neural networks. Then, we design four types of lightweight modules with elementwise square operators, named as Square-Pooling, Square-Softmin, Square-Excitation, and Square-Encoding. We add our four lightweight modules to Resnet18, Resnet50, and ShuffleNetV2 for better performance in the experiment on ImageNet 2012 dataset. The experimental results show that our modules can bring significant accuracy improvements to the base convolutional neural network models. The performance of our lightweight modules is even comparable to many complicated modules such as bilinear pooling, Squeeze-and-Excitation, and Gather-Excite. Our highly efficient modules are particularly suitable for mobile models. For example, when equipped with a single Square-Pooling module, the top-1 classification accuracy of ShuffleNetV2-0.5x on ImageNet 2012 is absolutely improved by 1.45% with no additional parameters and negligible inference time overhead.



### Indoor image representation by high-level semantic features
- **Arxiv ID**: http://arxiv.org/abs/1906.04987v3
- **DOI**: 10.1109/ACCESS.2019.2925002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04987v3)
- **Published**: 2019-06-12 07:53:26+00:00
- **Updated**: 2019-07-12 00:48:45+00:00
- **Authors**: Chiranjibi Sitaula, Yong Xiang, Yushu Zhang, Xuequan Lu, Sunil Aryal
- **Comment**: This paper has been accepted in IEEE Access
- **Journal**: IEEE Access 7 ,2019
- **Summary**: Indoor image features extraction is a fundamental problem in multiple fields such as image processing, pattern recognition, robotics and so on. Nevertheless, most of the existing feature extraction methods, which extract features based on pixels, color, shape/object parts or objects on images, suffer from limited capabilities in describing semantic information (e.g., object association). These techniques, therefore, involve undesired classification performance. To tackle this issue, we propose the notion of high-level semantic features and design four steps to extract them. Specifically, we first construct the objects pattern dictionary through extracting raw objects in the images, and then retrieve and extract semantic objects from the objects pattern dictionary. We finally extract our high-level semantic features based on the calculated probability and delta parameter. Experiments on three publicly available datasets (MIT-67, Scene15 and NYU V1) show that our feature extraction approach outperforms state-of-the-art feature extraction methods for indoor image classification, given a lower dimension of our features than those methods.



### Instant automatic diagnosis of diabetic retinopathy
- **Arxiv ID**: http://arxiv.org/abs/1906.11875v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11875v1)
- **Published**: 2019-06-12 08:34:57+00:00
- **Updated**: 2019-06-12 08:34:57+00:00
- **Authors**: Gwenolé Quellec, Mathieu Lamard, Bruno Lay, Alexandre Le Guilcher, Ali Erginay, Béatrice Cochener, Pascale Massin
- **Comment**: None
- **Journal**: None
- **Summary**: The purpose of this study is to evaluate the performance of the OphtAI system for the automatic detection of referable diabetic retinopathy (DR) and the automatic assessment of DR severity using color fundus photography. OphtAI relies on ensembles of convolutional neural networks trained to recognize eye laterality, detect referable DR and assess DR severity. The system can either process single images or full examination records. To document the automatic diagnoses, accurate heatmaps are generated. The system was developed and validated using a dataset of 763,848 images from 164,660 screening procedures from the OPHDIAT screening program. For comparison purposes, it was also evaluated in the public Messidor-2 dataset. Referable DR can be detected with an area under the ROC curve of AUC = 0.989 in the Messidor-2 dataset, using the University of Iowa's reference standard (95% CI: 0.984-0.994). This is significantly better than the only AI system authorized by the FDA, evaluated in the exact same conditions (AUC = 0.980). OphtAI can also detect vision-threatening DR with an AUC of 0.997 (95% CI: 0.996-0.998) and proliferative DR with an AUC of 0.997 (95% CI: 0.995-0.999). The system runs in 0.3 seconds using a graphics processing unit and less than 2 seconds without. OphtAI is safer, faster and more comprehensive than the only AI system authorized by the FDA so far. Instant DR diagnosis is now possible, which is expected to streamline DR screening and to give easy access to DR screening to more diabetic patients.



### The Worrisome Impact of an Inter-rater Bias on Neural Network Training
- **Arxiv ID**: http://arxiv.org/abs/1906.11872v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11872v2)
- **Published**: 2019-06-12 12:29:23+00:00
- **Updated**: 2020-05-31 15:36:02+00:00
- **Authors**: Or Shwartzman, Harel Gazit, Ilan Shelef, Tammy Riklin-Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of inter-rater variability is often discussed in the context of manual labeling of medical images. The emergence of data-driven approaches such as Deep Neural Networks (DNNs) brought this issue of raters' disagreement to the front-stage. In this paper, we highlight the issue of inter-rater bias as opposed to random inter-observer variability and demonstrate its influence on DNN training, leading to different segmentation results for the same input images. In fact, lower overlap scores are obtained between the outputs of a DNN trained on annotations of one rater and tested on another. Moreover, we demonstrate that inter-rater bias in the training examples is amplified and becomes more consistent, considering the segmentation predictions of the DNNs' test data. We support our findings by showing that a classifier-DNN trained to distinguish between raters based on their manual annotations performs better when the automatic segmentation predictions rather than the actual raters' annotations were tested. For this study, we used two different datasets: the ISBI 2015 Multiple Sclerosis (MS) challenge dataset, including MRI scans each with annotations provided by two raters with different levels of expertise; and Intracerebral Hemorrhage (ICH) CT scans with manual and semi-manual segmentations. The results obtained allow us to underline a worrisome clinical implication of a DNN bias induced by an inter-rater bias during training. Specifically, we present a consistent underestimate of MS-lesion loads when calculated from segmentation predictions of a DNN trained on input provided by the less experienced rater. In the same manner, the differences in ICH volumes calculated based on outputs of identical DNNs, each trained on annotations from a different source are more consistent and larger than the differences in volumes between the manual and semi-manual annotations used for training.



### Pose from Shape: Deep Pose Estimation for Arbitrary 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/1906.05105v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05105v2)
- **Published**: 2019-06-12 12:58:21+00:00
- **Updated**: 2019-08-05 14:43:03+00:00
- **Authors**: Yang Xiao, Xuchong Qiu, Pierre-Alain Langlois, Mathieu Aubry, Renaud Marlet
- **Comment**: None
- **Journal**: None
- **Summary**: Most deep pose estimation methods need to be trained for specific object instances or categories. In this work we propose a completely generic deep pose estimation approach, which does not require the network to have been trained on relevant categories, nor objects in a category to have a canonical pose. We believe this is a crucial step to design robotic systems that can interact with new objects in the wild not belonging to a predefined category. Our main insight is to dynamically condition pose estimation with a representation of the 3D shape of the target object. More precisely, we train a Convolutional Neural Network that takes as input both a test image and a 3D model, and outputs the relative 3D pose of the object in the input image with respect to the 3D model. We demonstrate that our method boosts performances for supervised category pose estimation on standard benchmarks, namely Pascal3D+, ObjectNet3D and Pix3D, on which we provide results superior to the state of the art. More importantly, we show that our network trained on everyday man-made objects from ShapeNet generalizes without any additional training to completely new types of 3D objects by providing results on the LINEMOD dataset as well as on natural entities such as animals from ImageNet.



### LED2Net: Deep Illumination-aware Dehazing with Low-light and Detail Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1906.05119v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05119v2)
- **Published**: 2019-06-12 13:09:28+00:00
- **Updated**: 2019-07-26 04:38:27+00:00
- **Authors**: Guisik Kim, Junseok Kwon
- **Comment**: we will upload new version of this paper after August
- **Journal**: None
- **Summary**: We present a novel dehazing and low-light enhancement method based on an illumination map that is accurately estimated by a convolutional neural network (CNN). In this paper, the illumination map is used as a component for three different tasks, namely, atmospheric light estimation, transmission map estimation, and low-light enhancement. To train CNNs for dehazing and low-light enhancement simultaneously based on the retinex theory, we synthesize numerous low-light and hazy images from normal hazy images from the FADE data set. In addition, we further improve the network using detail enhancement. Experimental results demonstrate that our method surpasses recent state-of-theart algorithms quantitatively and qualitatively. In particular, our haze-free images present vivid colors and enhance visibility without a halo effect or color distortion.



### High Accuracy Classification of White Blood Cells using TSLDA Classifier and Covariance Features
- **Arxiv ID**: http://arxiv.org/abs/1906.05131v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05131v2)
- **Published**: 2019-06-12 13:36:41+00:00
- **Updated**: 2019-07-16 15:23:52+00:00
- **Authors**: Hamed Talebi, Amin Ranjbar, Alireza Davoudi, Hamed Gholami, Mohammad Bagher Menhaj
- **Comment**: 7 pages, 4 tables, 3 figures, journal
- **Journal**: None
- **Summary**: creating automated processes in different areas of medical science with the application of engineering tools is a highly growing field over recent decades. In this context, many medical image processing and analyzing researchers use worthwhile methods in artificial intelligence, which can reduce necessary human power while increases accuracy of results. Among various medical images, blood microscopic images play a vital role in heart failure diagnosis, e.g., blood cancers. The prominent component in blood cancer diagnosis is white blood cells (WBCs) which due to its general characteristics in microscopic images sometimes make difficulties in recognition and classification tasks such as non-uniform colors/illuminances, different shapes, sizes, and textures. Moreover, overlapped WBCs in bone marrow images and neighboring to red blood cells are identified as reasons for errors in the classification task. In this paper, we have endeavored to segment various parts in medical images via Na\"ive Bayes clustering method and in next stage via TSLDA classifier, which is supplied by features acquired from covariance descriptor results in the accuracy of 98.02%. It seems that this result is delightful in WBCs recognition.



### Recognizing Manipulation Actions from State-Transformations
- **Arxiv ID**: http://arxiv.org/abs/1906.05147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05147v1)
- **Published**: 2019-06-12 14:02:17+00:00
- **Updated**: 2019-06-12 14:02:17+00:00
- **Authors**: Nachwa Aboubakr, James L. Crowley, Remi Ronfard
- **Comment**: Accepted for presentation at EPIC@CVPR2019 workshop
- **Journal**: None
- **Summary**: Manipulation actions transform objects from an initial state into a final state. In this paper, we report on the use of object state transitions as a mean for recognizing manipulation actions. Our method is inspired by the intuition that object states are visually more apparent than actions from a still frame and thus provide information that is complementary to spatio-temporal action recognition. We start by defining a state transition matrix that maps action labels into a pre-state and a post-state. From each keyframe, we learn appearance models of objects and their states. Manipulation actions can then be recognized from the state transition matrix. We report results on the EPIC kitchen action recognition challenge.



### Evaluation of Dataflow through layers of Deep Neural Networks in Classification and Regression Problems
- **Arxiv ID**: http://arxiv.org/abs/1906.05156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IT, cs.LG, math.IT, 68Txx
- **Links**: [PDF](http://arxiv.org/pdf/1906.05156v1)
- **Published**: 2019-06-12 14:16:10+00:00
- **Updated**: 2019-06-12 14:16:10+00:00
- **Authors**: Ahmad Kalhor, Mohsen Saffar, Melika Kheirieh, Somayyeh Hoseinipoor, Babak N. Araabi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces two straightforward, effective indices to evaluate the input data and the data flowing through layers of a feedforward deep neural network. For classification problems, the separation rate of target labels in the space of dataflow is explained as a key factor indicating the performance of designed layers in improving the generalization of the network. According to the explained concept, a shapeless distance-based evaluation index is proposed. Similarly, for regression problems, the smoothness rate of target outputs in the space of dataflow is explained as a key factor indicating the performance of designed layers in improving the generalization of the network. According to the explained smoothness concept, a shapeless distance-based smoothness index is proposed for regression problems. To consider more strictly concepts of separation and smoothness, their extended versions are introduced, and by interpreting a regression problem as a classification problem, it is shown that the separation and smoothness indices are related together. Through four case studies, the profits of using the introduced indices are shown. In the first case study, for classification and regression problems , the challenging of some known input datasets are compared respectively by the proposed separation and smoothness indices. In the second case study, the quality of dataflow is evaluated through layers of two pre-trained VGG 16 networks in classification of Cifar10 and Cifar100. In the third case study, it is shown that the correct classification rate and the separation index are almost equivalent through layers particularly while the serration index is increased. In the last case study, two multi-layer neural networks, which are designed for the prediction of Boston Housing price, are compared layer by layer by using the proposed smoothness index.



### Stereoscopic Omnidirectional Image Quality Assessment Based on Predictive Coding Theory
- **Arxiv ID**: http://arxiv.org/abs/1906.05165v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.05165v1)
- **Published**: 2019-06-12 14:25:28+00:00
- **Updated**: 2019-06-12 14:25:28+00:00
- **Authors**: Zhibo Chen, Jiahua Xu, Chaoyi Lin, Wei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Objective quality assessment of stereoscopic omnidirectional images is a challenging problem since it is influenced by multiple aspects such as projection deformation, field of view (FoV) range, binocular vision, visual comfort, etc. Existing studies show that classic 2D or 3D image quality assessment (IQA) metrics are not able to perform well for stereoscopic omnidirectional images. However, very few research works have focused on evaluating the perceptual visual quality of omnidirectional images, especially for stereoscopic omnidirectional images. In this paper, based on the predictive coding theory of the human vision system (HVS), we propose a stereoscopic omnidirectional image quality evaluator (SOIQE) to cope with the characteristics of 3D 360-degree images. Two modules are involved in SOIQE: predictive coding theory based binocular rivalry module and multi-view fusion module. In the binocular rivalry module, we introduce predictive coding theory to simulate the competition between high-level patterns and calculate the similarity and rivalry dominance to obtain the quality scores of viewport images. Moreover, we develop the multi-view fusion module to aggregate the quality scores of viewport images with the help of both content weight and location weight. The proposed SOIQE is a parametric model without necessary of regression learning, which ensures its interpretability and generalization performance. Experimental results on our published stereoscopic omnidirectional image quality assessment database (SOLID) demonstrate that our proposed SOIQE method outperforms state-of-the-art metrics. Furthermore, we also verify the effectiveness of each proposed module on both public stereoscopic image datasets and panoramic image datasets.



### Boosting Few-Shot Visual Learning with Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1906.05186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05186v1)
- **Published**: 2019-06-12 14:57:21+00:00
- **Updated**: 2019-06-12 14:57:21+00:00
- **Authors**: Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, Matthieu Cord
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning and self-supervised learning address different facets of the same problem: how to train a model with little or no labeled data. Few-shot learning aims for optimization methods and models that can learn efficiently to recognize patterns in the low data regime. Self-supervised learning focuses instead on unlabeled data and looks into it for the supervisory signal to feed high capacity deep neural networks. In this work we exploit the complementarity of these two domains and propose an approach for improving few-shot learning through self-supervision. We use self-supervision as an auxiliary task in a few-shot learning pipeline, enabling feature extractors to learn richer and more transferable visual representations while still using few annotated samples. Through self-supervision, our approach can be naturally extended towards using diverse unlabeled data from other datasets in the few-shot setting. We report consistent improvements across an array of architectures, datasets and self-supervision techniques.



### Detection and Correction of Cardiac MR Motion Artefacts during Reconstruction from K-space
- **Arxiv ID**: http://arxiv.org/abs/1906.05695v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.05695v1)
- **Published**: 2019-06-12 14:58:31+00:00
- **Updated**: 2019-06-12 14:58:31+00:00
- **Authors**: lkay Oksuz, James Clough, Bram Ruijsink, Esther Puyol-Anton, Aurelien Bustin, Gastao Cruz, Claudia Prieto, Daniel Rueckert, Andrew P. King, Julia A. Schnabel
- **Comment**: Accepted to MICCAI 2019. arXiv admin note: text overlap with
  arXiv:1808.05130
- **Journal**: None
- **Summary**: In fully sampled cardiac MR (CMR) acquisitions, motion can lead to corruption of k-space lines, which can result in artefacts in the reconstructed images. In this paper, we propose a method to automatically detect and correct motion-related artefacts in CMR acquisitions during reconstruction from k-space data. Our correction method is inspired by work on undersampled CMR reconstruction, and uses deep learning to optimize a data-consistency term for under-sampled k-space reconstruction. Our main methodological contribution is the addition of a detection network to classify motion-corrupted k-space lines to convert the problem of artefact correction to a problem of reconstruction using the data consistency term. We train our network to automatically correct for motion-related artefacts using synthetically corrupted cine CMR k-space data as well as uncorrupted CMR images. Using a test set of 50 2D+time cine CMR datasets from the UK Biobank, we achieve good image quality in the presence of synthetic motion artefacts. We quantitatively compare our method with a variety of techniques for recovering good image quality and showcase better performance compared to state of the art denoising techniques with a PSNR of 37.1. Moreover, we show that our method preserves the quality of uncorrupted images and therefore can be also utilized as a general image reconstruction algorithm.



### Vispi: Automatic Visual Perception and Interpretation of Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/1906.05190v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1906.05190v3)
- **Published**: 2019-06-12 15:01:31+00:00
- **Updated**: 2020-05-29 15:53:06+00:00
- **Authors**: Xin Li, Rui Cao, Dongxiao Zhu
- **Comment**: In the proceeding of Medical Imaging with Deep Learning (MIDL-20)
- **Journal**: None
- **Summary**: Medical imaging contains the essential information for rendering diagnostic and treatment decisions. Inspecting (visual perception) and interpreting image to generate a report are tedious clinical routines for a radiologist where automation is expected to greatly reduce the workload. Despite rapid development of natural image captioning, computer-aided medical image visual perception and interpretation remain a challenging task, largely due to the lack of high-quality annotated image-report pairs and tailor-made generative models for sufficient extraction and exploitation of localized semantic features, particularly those associated with abnormalities. To tackle these challenges, we present Vispi, an automatic medical image interpretation system, which first annotates an image via classifying and localizing common thoracic diseases with visual support and then followed by report generation from an attentive LSTM model. Analyzing an open IU X-ray dataset, we demonstrate a superior performance of Vispi in disease classification, localization and report generation using automatic performance evaluation metrics ROUGE and CIDEr.



### Tackling Partial Domain Adaptation with Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1906.05199v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05199v1)
- **Published**: 2019-06-12 15:16:31+00:00
- **Updated**: 2019-06-12 15:16:31+00:00
- **Authors**: Silvia Bucci, Antonio D'Innocente, Tatiana Tommasi
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation approaches have shown promising results in reducing the marginal distribution difference among visual domains. They allow to train reliable models that work over datasets of different nature (photos, paintings etc), but they still struggle when the domains do not share an identical label space. In the partial domain adaptation setting, where the target covers only a subset of the source classes, it is challenging to reduce the domain gap without incurring in negative transfer. Many solutions just keep the standard domain adaptation techniques by adding heuristic sample weighting strategies. In this work we show how the self-supervisory signal obtained from the spatial co-location of patches can be used to define a side task that supports adaptation regardless of the exact label sharing condition across domains. We build over a recent work that introduced a jigsaw puzzle task for domain generalization: we describe how to reformulate this approach for partial domain adaptation and we show how it boosts existing adaptive solutions when combined with them. The obtained experimental results on three datasets supports the effectiveness of our approach.



### Manifold Graph with Learned Prototypes for Semi-Supervised Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.05202v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.05202v2)
- **Published**: 2019-06-12 15:18:36+00:00
- **Updated**: 2019-06-13 02:46:21+00:00
- **Authors**: Chia-Wen Kuo, Chih-Yao Ma, Jia-Bin Huang, Zsolt Kira
- **Comment**: Project site:
  https://sites.google.com/view/manifold-graph-with-prototypes/home
- **Journal**: None
- **Summary**: Recent advances in semi-supervised learning methods rely on estimating the categories of unlabeled data using a model trained on the labeled data (pseudo-labeling) and using the unlabeled data for various consistency-based regularization. In this work, we propose to explicitly leverage the structure of the data manifold based on a Manifold Graph constructed over the image instances within the feature space. Specifically, we propose an architecture based on graph networks that jointly optimizes feature extraction, graph connectivity, and feature propagation and aggregation to unlabeled data in an end-to-end manner. Further, we present a novel Prototype Generator for producing a diverse set of prototypes that compactly represent each category, which supports feature propagation. To evaluate our method, we first contribute a strong baseline that combines two consistency-based regularizers that already achieves state-of-the-art results especially with fewer labels. We then show that when combined with these regularizers, the proposed method facilitates the propagation of information from generated prototypes to image data to further improve results. We provide extensive qualitative and quantitative experimental results on semi-supervised benchmarks demonstrating the improvements arising from our design and show that our method achieves state-of-the-art performance when compared with existing methods using a single model and comparable with ensemble methods. Specifically, we achieve error rates of 3.35% on SVHN, 8.27% on CIFAR-10, and 33.83% on CIFAR-100. With much fewer labels, we surpass the state of the arts by significant margins of 41% relative error decrease on average.



### Towards Real-Time Head Pose Estimation: Exploring Parameter-Reduced Residual Networks on In-the-wild Datasets
- **Arxiv ID**: http://arxiv.org/abs/1906.05203v2
- **DOI**: 10.1007/978-3-030-22999-3_12
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05203v2)
- **Published**: 2019-06-12 15:19:12+00:00
- **Updated**: 2019-08-01 07:38:41+00:00
- **Authors**: Ines Rieger, Thomas Hauenstein, Sebastian Hettenkofer, Jens-Uwe Garbas
- **Comment**: 32nd International Conference on Industrial, Engineering & Other
  Applications of Applied Intelligent Systems (IEA/AIE 2019)
- **Journal**: Lecture Notes in Computer Science 11606 (2019) 123-134
- **Summary**: Head poses are a key component of human bodily communication and thus a decisive element of human-computer interaction. Real-time head pose estimation is crucial in the context of human-robot interaction or driver assistance systems. The most promising approaches for head pose estimation are based on Convolutional Neural Networks (CNNs). However, CNN models are often too complex to achieve real-time performance. To face this challenge, we explore a popular subgroup of CNNs, the Residual Networks (ResNets) and modify them in order to reduce their number of parameters. The ResNets are modifed for different image sizes including low-resolution images and combined with a varying number of layers. They are trained on in-the-wild datasets to ensure real-world applicability. As a result, we demonstrate that the performance of the ResNets can be maintained while reducing the number of parameters. The modified ResNets achieve state-of-the-art accuracy and provide fast inference for real-time applicability.



### Unsupervised Monocular Depth and Ego-motion Learning with Structure and Semantics
- **Arxiv ID**: http://arxiv.org/abs/1906.05717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.05717v1)
- **Published**: 2019-06-12 15:28:57+00:00
- **Updated**: 2019-06-12 15:28:57+00:00
- **Authors**: Vincent Casser, Soeren Pirk, Reza Mahjourian, Anelia Angelova
- **Comment**: CVPR Workshop on Visual Odometry & Computer Vision Applications Based
  on Location Clues (VOCVALC), 2019. This is an extension of arXiv:1811.06152:
  Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised
  Learning from Monocular Videos. Thirty-Third AAAI Conference on Artificial
  Intelligence (AAAI'19)
- **Journal**: None
- **Summary**: We present an approach which takes advantage of both structure and semantics for unsupervised monocular learning of depth and ego-motion. More specifically, we model the motion of individual objects and learn their 3D motion vector jointly with depth and ego-motion. We obtain more accurate results, especially for challenging dynamic scenes not addressed by previous approaches. This is an extended version of Casser et al. [AAAI'19]. Code and models have been open sourced at https://sites.google.com/corp/view/struct2depth.



### Continual and Multi-Task Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1906.05226v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05226v1)
- **Published**: 2019-06-12 16:01:35+00:00
- **Updated**: 2019-06-12 16:01:35+00:00
- **Authors**: Ramakanth Pasunuru, Mohit Bansal
- **Comment**: ACL 2019 (12 pages)
- **Journal**: None
- **Summary**: Architecture search is the process of automatically learning the neural model or cell structure that best suits the given task. Recently, this approach has shown promising performance improvements (on language modeling and image classification) with reasonable training speed, using a weight sharing strategy called Efficient Neural Architecture Search (ENAS). In our work, we first introduce a novel continual architecture search (CAS) approach, so as to continually evolve the model parameters during the sequential training of several tasks, without losing performance on previously learned tasks (via block-sparsity and orthogonality constraints), thus enabling life-long learning. Next, we explore a multi-task architecture search (MAS) approach over ENAS for finding a unified, single cell structure that performs well across multiple tasks (via joint controller rewards), and hence allows more generalizable transfer of the cell structure knowledge to an unseen new task. We empirically show the effectiveness of our sequential continual learning and parallel multi-task learning based architecture search approaches on diverse sentence-pair classification tasks (GLUE) and multimodal-generation based video captioning tasks. Further, we present several ablations and analyses on the learned cell structures.



### Handwritten Text Segmentation via End-to-End Learning of Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1906.05229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05229v1)
- **Published**: 2019-06-12 16:03:57+00:00
- **Updated**: 2019-06-12 16:03:57+00:00
- **Authors**: Junho Jo, Hyung Il Koo, Jae Woong Soh, Nam Ik Cho
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new handwritten text segmentation method by training a convolutional neural network (CNN) in an end-to-end manner. Many conventional methods addressed this problem by extracting connected components and then classifying them. However, this two-step approach has limitations when handwritten components and machine-printed parts are overlapping. Unlike conventional methods, we develop an end-to-end deep CNN for this problem, which does not need any preprocessing steps. Since there is no publicly available dataset for this goal and pixel-wise annotations are time-consuming and costly, we also propose a data synthesis algorithm that generates realistic training samples. For training our network, we develop a cross-entropy based loss function that addresses the imbalance problems. Experimental results on synthetic and real images show the effectiveness of the proposed method. Specifically, the proposed network has been trained solely on synthetic images, nevertheless the removal of handwritten text in real documents improves OCR performance from 71.13% to 92.50%, showing the generalization performance of our network and synthesized images.



### VolMap: A Real-time Model for Semantic Segmentation of a LiDAR surrounding view
- **Arxiv ID**: http://arxiv.org/abs/1906.11873v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11873v1)
- **Published**: 2019-06-12 16:22:41+00:00
- **Updated**: 2019-06-12 16:22:41+00:00
- **Authors**: Hager Radi, Waleed Ali
- **Comment**: Accepted at Thirty-sixth International Conference on Machine Learning
  (ICML 2019) Workshop on AI for Autonomous Driving
- **Journal**: None
- **Summary**: This paper introduces VolMap, a real-time approach for the semantic segmentation of a 3D LiDAR surrounding view system in autonomous vehicles. We designed an optimized deep convolution neural network that can accurately segment the point cloud produced by a 360\degree{} LiDAR setup, where the input consists of a volumetric bird-eye view with LiDAR height layers used as input channels. We further investigated the usage of multi-LiDAR setup and its effect on the performance of the semantic segmentation task. Our evaluations are carried out on a large scale 3D object detection benchmark containing a LiDAR cocoon setup, along with KITTI dataset, where the per-point segmentation labels are derived from 3D bounding boxes. We show that VolMap achieved an excellent balance between high accuracy and real-time running on CPU.



### Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning CNN for Single Image De-Raining
- **Arxiv ID**: http://arxiv.org/abs/1906.11129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11129v1)
- **Published**: 2019-06-12 17:13:50+00:00
- **Updated**: 2019-06-12 17:13:50+00:00
- **Authors**: Rajeev Yasarla, Vishal M. Patel
- **Comment**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2019
- **Journal**: None
- **Summary**: Single image de-raining is an extremely challenging problem since the rainy image may contain rain streaks which may vary in size, direction and density. Previous approaches have attempted to address this problem by leveraging some prior information to remove rain streaks from a single image. One of the major limitations of these approaches is that they do not consider the location information of rain drops in the image. The proposed Uncertainty guided Multi-scale Residual Learning (UMRL) network attempts to address this issue by learning the rain content at different scales and using them to estimate the final de-rained output. In addition, we introduce a technique which guides the network to learn the network weights based on the confidence measure about the estimate. Furthermore, we introduce a new training and testing procedure based on the notion of cycle spinning to improve the final de-raining performance. Extensive experiments on synthetic and real datasets to demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods. Code is available at: https://github.com/rajeevyasarla/UMRL--using-Cycle-Spinning



### Image-Adaptive GAN based Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1906.05284v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05284v2)
- **Published**: 2019-06-12 17:39:43+00:00
- **Updated**: 2019-11-25 16:42:36+00:00
- **Authors**: Shady Abu Hussein, Tom Tirer, Raja Giryes
- **Comment**: Accepted to AAAI 2020. Code available at
  https://github.com/shadyabh/IAGAN
- **Journal**: None
- **Summary**: In the recent years, there has been a significant improvement in the quality of samples produced by (deep) generative models such as variational auto-encoders and generative adversarial networks. However, the representation capabilities of these methods still do not capture the full distribution for complex classes of images, such as human faces. This deficiency has been clearly observed in previous works that use pre-trained generative models to solve imaging inverse problems. In this paper, we suggest to mitigate the limited representation capabilities of generators by making them image-adaptive and enforcing compliance of the restoration with the observations via back-projections. We empirically demonstrate the advantages of our proposed approach for image super-resolution and compressed sensing.



### LAEO-Net: revisiting people Looking At Each Other in videos
- **Arxiv ID**: http://arxiv.org/abs/1906.05261v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05261v1)
- **Published**: 2019-06-12 17:40:38+00:00
- **Updated**: 2019-06-12 17:40:38+00:00
- **Authors**: Manuel J. Marin-Jimenez, Vicky Kalogeiton, Pablo Medina-Suarez, Andrew Zisserman
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Capturing the `mutual gaze' of people is essential for understanding and interpreting the social interactions between them. To this end, this paper addresses the problem of detecting people Looking At Each Other (LAEO) in video sequences. For this purpose, we propose LAEO-Net, a new deep CNN for determining LAEO in videos. In contrast to previous works, LAEO-Net takes spatio-temporal tracks as input and reasons about the whole track. It consists of three branches, one for each character's tracked head and one for their relative position. Moreover, we introduce two new LAEO datasets: UCO-LAEO and AVA-LAEO. A thorough experimental evaluation demonstrates the ability of LAEONet to successfully determine if two people are LAEO and the temporal window where it happens. Our model achieves state-of-the-art results on the existing TVHID-LAEO video dataset, significantly outperforming previous approaches. Finally, we apply LAEO-Net to social network analysis, where we automatically infer the social relationship between pairs of people based on the frequency and duration that they LAEO.



### Visual Wake Words Dataset
- **Arxiv ID**: http://arxiv.org/abs/1906.05721v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.2.10; B.7.1; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1906.05721v1)
- **Published**: 2019-06-12 17:47:21+00:00
- **Updated**: 2019-06-12 17:47:21+00:00
- **Authors**: Aakanksha Chowdhery, Pete Warden, Jonathon Shlens, Andrew Howard, Rocky Rhodes
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: The emergence of Internet of Things (IoT) applications requires intelligence on the edge. Microcontrollers provide a low-cost compute platform to deploy intelligent IoT applications using machine learning at scale, but have extremely limited on-chip memory and compute capability. To deploy computer vision on such devices, we need tiny vision models that fit within a few hundred kilobytes of memory footprint in terms of peak usage and model size on device storage. To facilitate the development of microcontroller friendly models, we present a new dataset, Visual Wake Words, that represents a common microcontroller vision use-case of identifying whether a person is present in the image or not, and provides a realistic benchmark for tiny vision models. Within a limited memory footprint of 250 KB, several state-of-the-art mobile models achieve accuracy of 85-90% on the Visual Wake Words dataset. We anticipate the proposed dataset will advance the research on tiny vision models that can push the pareto-optimal boundary in terms of accuracy versus memory usage for microcontroller applications.



### Differential Imaging Forensics
- **Arxiv ID**: http://arxiv.org/abs/1906.05268v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1906.05268v1)
- **Published**: 2019-06-12 17:48:47+00:00
- **Updated**: 2019-06-12 17:48:47+00:00
- **Authors**: Aurélien Bourquard, Jeff Yan
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce some new forensics based on differential imaging, where a novel category of visual evidence created via subtle interactions of light with a scene, such as dim reflections, can be computationally extracted and amplified from an image of interest through a comparative analysis with an additional reference baseline image acquired under similar conditions. This paradigm of differential imaging forensics (DIF) enables forensic examiners for the first time to retrieve the said visual evidence that is readily available in an image or video footage but would otherwise remain faint or even invisible to a human observer. We demonstrate the relevance and effectiveness of our approach through practical experiments. We also show that DIF provides a novel method for detecting forged images and video clips, including deep fakes.



### Presence-Only Geographical Priors for Fine-Grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.05272v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05272v3)
- **Published**: 2019-06-12 17:54:23+00:00
- **Updated**: 2019-10-28 06:46:10+00:00
- **Authors**: Oisin Mac Aodha, Elijah Cole, Pietro Perona
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Appearance information alone is often not sufficient to accurately differentiate between fine-grained visual categories. Human experts make use of additional cues such as where, and when, a given image was taken in order to inform their final decision. This contextual information is readily available in many online image collections but has been underutilized by existing image classifiers that focus solely on making predictions based on the image contents.   We propose an efficient spatio-temporal prior, that when conditioned on a geographical location and time, estimates the probability that a given object category occurs at that location. Our prior is trained from presence-only observation data and jointly models object categories, their spatio-temporal distributions, and photographer biases. Experiments performed on multiple challenging image classification datasets show that combining our prior with the predictions from image classifiers results in a large improvement in final classification performance.



### HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-scale Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1906.05332v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.05332v1)
- **Published**: 2019-06-12 19:12:58+00:00
- **Updated**: 2019-06-12 19:12:58+00:00
- **Authors**: Xiuye Gu, Yijie Wang, Chongruo wu, Yong-Jae lee, Panqu Wang
- **Comment**: None
- **Journal**: CVPR 2019
- **Summary**: We present a novel deep neural network architecture for end-to-end scene flow estimation that directly operates on large-scale 3D point clouds. Inspired by Bilateral Convolutional Layers (BCL), we propose novel DownBCL, UpBCL, and CorrBCL operations that restore structural information from unstructured point clouds, and fuse information from two consecutive point clouds. Operating on discrete and sparse permutohedral lattice points, our architectural design is parsimonious in computational cost. Our model can efficiently process a pair of point cloud frames at once with a maximum of 86K points per frame. Our approach achieves state-of-the-art performance on the FlyingThings3D and KITTI Scene Flow 2015 datasets. Moreover, trained on synthetic data, our approach shows great generalization ability on real-world data and on different point densities without fine-tuning.



### Uncovering Dominant Social Class in Neighborhoods through Building Footprints: A Case Study of Residential Zones in Massachusetts using Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1906.05352v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1906.05352v1)
- **Published**: 2019-06-12 19:47:09+00:00
- **Updated**: 2019-06-12 19:47:09+00:00
- **Authors**: Qianhui Liang, Zhoutong Wang
- **Comment**: Publishing as conference proceeding of 16th International Conference
  on Computers in Urban Planning and Urban Management
- **Journal**: None
- **Summary**: In urban theory, urban form is related to social and economic status. This paper explores to uncover zip-code level income through urban form by analyzing figure-ground map, a simple, prevailing and precise representation of urban form in the field of urban study. Deep learning in computer vision enables such representation maps to be studied at a large scale. We propose to train a DCNN model to identify and uncover the internal bridge between social class and urban form. Further, using hand-crafted informative visual features related with urban form properties (building size, building density, etc.), we apply a random forest classifier to interpret how morphological properties are related with social class.



### GANPOP: Generative Adversarial Network Prediction of Optical Properties from Single Snapshot Wide-field Images
- **Arxiv ID**: http://arxiv.org/abs/1906.05360v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.05360v2)
- **Published**: 2019-06-12 19:55:49+00:00
- **Updated**: 2019-06-20 18:04:02+00:00
- **Authors**: Mason T. Chen, Faisal Mahmood, Jordan A. Sweer, Nicholas J. Durr
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep learning framework for wide-field, content-aware estimation of absorption and scattering coefficients of tissues, called Generative Adversarial Network Prediction of Optical Properties (GANPOP). Spatial frequency domain imaging is used to obtain ground-truth optical properties from in vivo human hands, freshly resected human esophagectomy samples and homogeneous tissue phantoms. Images of objects with either flat-field or structured illumination are paired with registered optical property maps and are used to train conditional generative adversarial networks that estimate optical properties from a single input image. We benchmark this approach by comparing GANPOP to a single-snapshot optical property (SSOP) technique, using a normalized mean absolute error (NMAE) metric. In human gastrointestinal specimens, GANPOP estimates both reduced scattering and absorption coefficients at 660 nm from a single 0.2/mm spatial frequency illumination image with 58% higher accuracy than SSOP. When applied to both in vivo and ex vivo swine tissues, a GANPOP model trained solely on human specimens and phantoms estimates optical properties with approximately 43% improvement over SSOP, indicating adaptability to sample variety. Moreover, we demonstrate that GANPOP estimates optical properties from flat-field illumination images with similar error to SSOP, which requires structured-illumination. Given a training set that appropriately spans the target domain, GANPOP has the potential to enable rapid and accurate wide-field measurements of optical properties, even from conventional imaging systems with flat-field illumination.



### The Herbarium Challenge 2019 Dataset
- **Arxiv ID**: http://arxiv.org/abs/1906.05372v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.05372v2)
- **Published**: 2019-06-12 20:42:24+00:00
- **Updated**: 2019-06-15 06:16:28+00:00
- **Authors**: Kiat Chuan Tan, Yulong Liu, Barbara Ambrose, Melissa Tulig, Serge Belongie
- **Comment**: Part of the 6th Fine-Grained Visual Categorization Workshop (FGVC6)
  at CVPR 2019. Dataset available at
  https://github.com/visipedia/herbarium_comp
- **Journal**: None
- **Summary**: Herbarium sheets are invaluable for botanical research, and considerable time and effort is spent by experts to label and identify specimens on them. In view of recent advances in computer vision and deep learning, developing an automated approach to help experts identify specimens could significantly accelerate research in this area. Whereas most existing botanical datasets comprise photos of specimens in the wild, herbarium sheets exhibit dried specimens, which poses new challenges. We present a challenge dataset of herbarium sheet images labeled by experts, with the intent of facilitating the development of automated identification techniques for this challenging scenario.



### Eye Contact Correction using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.05378v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1906.05378v2)
- **Published**: 2019-06-12 21:15:12+00:00
- **Updated**: 2019-12-26 17:59:44+00:00
- **Authors**: Leo F. Isikdogan, Timo Gerasimow, Gilad Michael
- **Comment**: None
- **Journal**: None
- **Summary**: In a typical video conferencing setup, it is hard to maintain eye contact during a call since it requires looking into the camera rather than the display. We propose an eye contact correction model that restores the eye contact regardless of the relative position of the camera and display. Unlike previous solutions, our model redirects the gaze from an arbitrary direction to the center without requiring a redirection angle or camera/display/user geometry as inputs. We use a deep convolutional neural network that inputs a monocular image and produces a vector field and a brightness map to correct the gaze. We train this model in a bi-directional way on a large set of synthetically generated photorealistic images with perfect labels. The learned model is a robust eye contact corrector which also predicts the input gaze implicitly at no additional cost. Our system is primarily designed to improve the quality of video conferencing experience. Therefore, we use a set of control mechanisms to prevent creepy results and to ensure a smooth and natural video conferencing experience. The entire eye contact correction system runs end-to-end in real-time on a commodity CPU and does not require any dedicated hardware, making our solution feasible for a variety of devices.



### Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1906.05388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05388v1)
- **Published**: 2019-06-12 21:32:07+00:00
- **Updated**: 2019-06-12 21:32:07+00:00
- **Authors**: Mohammad Mahdi Derakhshani, Saeed Masoudnia, Amir Hossein Shaker, Omid Mersa, Mohammad Amin Sadeghi, Mohammad Rastegari, Babak N. Araabi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple and effective learning technique that significantly improves mAP of YOLO object detectors without compromising their speed. During network training, we carefully feed in localization information. We excite certain activations in order to help the network learn to better localize. In the later stages of training, we gradually reduce our assisted excitation to zero. We reached a new state-of-the-art in the speed-accuracy trade-off. Our technique improves the mAP of YOLOv2 by 3.8% and mAP of YOLOv3 by 2.2% on MSCOCO dataset.This technique is inspired from curriculum learning. It is simple and effective and it is applicable to most single-stage object detectors.



### Topology-Preserving Deep Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.05404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05404v1)
- **Published**: 2019-06-12 22:12:44+00:00
- **Updated**: 2019-06-12 22:12:44+00:00
- **Authors**: Xiaoling Hu, Li Fuxin, Dimitris Samaras, Chao Chen
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Segmentation algorithms are prone to make topological errors on fine-scale structures, e.g., broken connections. We propose a novel method that learns to segment with correct topology. In particular, we design a continuous-valued loss function that enforces a segmentation to have the same topology as the ground truth, i.e., having the same Betti number. The proposed topology-preserving loss function is differentiable and we incorporate it into end-to-end training of a deep neural network. Our method achieves much better performance on the Betti number error, which directly accounts for the topological correctness. It also performs superiorly on other topology-relevant metrics, e.g., the Adjusted Rand Index and the Variation of Information. We illustrate the effectiveness of the proposed method on a broad spectrum of natural and biomedical datasets.



### Copulas as High-Dimensional Generative Models: Vine Copula Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1906.05423v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05423v2)
- **Published**: 2019-06-12 23:29:17+00:00
- **Updated**: 2019-11-27 12:12:33+00:00
- **Authors**: Natasa Tagasovska, Damien Ackerer, Thibault Vatter
- **Comment**: None
- **Journal**: Advances in Neural Information Processing Systems 32, pages:
  6525--6537, year: 2019
- **Summary**: We introduce the vine copula autoencoder (VCAE), a flexible generative model for high-dimensional distributions built in a straightforward three-step procedure.   First, an autoencoder (AE) compresses the data into a lower dimensional representation. Second, the multivariate distribution of the encoded data is estimated with vine copulas. Third, a generative model is obtained by combining the estimated distribution with the decoder part of the AE. As such, the proposed approach can transform any already trained AE into a flexible generative model at a low computational cost. This is an advantage over existing generative models such as adversarial networks and variational AEs which can be difficult to train and can impose strong assumptions on the latent space. Experiments on MNIST, Street View House Numbers and Large-Scale CelebFaces Attributes datasets show that VCAEs can achieve competitive results to standard baselines.




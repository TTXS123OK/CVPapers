# Arxiv Papers in cs.CV on 2019-06-13
### CoopSubNet: Cooperating Subnetwork for Data-Driven Regularization of Deep Networks under Limited Training Budgets
- **Arxiv ID**: http://arxiv.org/abs/1906.05441v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05441v1)
- **Published**: 2019-06-13 01:13:53+00:00
- **Updated**: 2019-06-13 01:13:53+00:00
- **Authors**: Riddhish Bhalodia, Shireen Elhabian, Ladislav Kavan, Ross Whitaker
- **Comment**: None
- **Journal**: None
- **Summary**: Deep networks are an integral part of the current machine learning paradigm. Their inherent ability to learn complex functional mappings between data and various target variables, while discovering hidden, task-driven features, makes them a powerful technology in a wide variety of applications. Nonetheless, the success of these networks typically relies on the availability of sufficient training data to optimize a large number of free parameters while avoiding overfitting, especially for networks with large capacity. In scenarios with limited training budgets, e.g., supervised tasks with limited labeled samples, several generic and/or task-specific regularization techniques, including data augmentation, have been applied to improve the generalization of deep networks.Typically such regularizations are introduced independently of that data or training scenario, and must therefore be tuned, tested, and modified to meet the needs of a particular network. In this paper, we propose a novel regularization framework that is driven by the population-level statistics of the feature space to be learned. The regularization is in the form of a \textbf{cooperating subnetwork}, which is an auto-encoder architecture attached to the feature space and trained in conjunction with the primary network. We introduce the architecture and training methodology and demonstrate the effectiveness of the proposed cooperative network-based regularization in a variety of tasks and architectures from the literature. Our code is freely available at \url{https://github.com/riddhishb/CoopSubNet



### Robust and interpretable blind image denoising via bias-free convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1906.05478v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.05478v3)
- **Published**: 2019-06-13 04:48:21+00:00
- **Updated**: 2020-02-08 05:55:05+00:00
- **Authors**: Sreyas Mohan, Zahra Kadkhodaie, Eero P. Simoncelli, Carlos Fernandez-Granda
- **Comment**: Published as conference paper in ICLR 2020
- **Journal**: None
- **Summary**: Deep convolutional networks often append additive constant ("bias") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of "batch normalization"). Recent state-of-the-art blind denoising methods (e.g., DnCNN) seem to require these terms for their success. Here, however, we show that these networks systematically overfit the noise levels for which they are trained: when deployed at noise levels outside the training range, performance degrades dramatically. In contrast, a bias-free architecture -- obtained by removing the constant terms in every layer of the network, including those used for batch normalization-- generalizes robustly across noise levels, while preserving state-of-the-art performance within the training range. Locally, the bias-free network acts linearly on the noisy image, enabling direct analysis of network behavior via standard linear-algebraic tools. These analyses provide interpretations of network functionality in terms of nonlinear adaptive filtering, and projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology.



### S3: A Spectral-Spatial Structure Loss for Pan-Sharpening Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.05480v2
- **DOI**: 10.1109/LGRS.2019.2934493
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.05480v2)
- **Published**: 2019-06-13 05:02:26+00:00
- **Updated**: 2019-08-19 01:57:24+00:00
- **Authors**: Jae-Seok Choi, Yongwoo Kim, Munchurl Kim
- **Comment**: Accepted for publication in IEEE Geoscience and Remote Sensing
  Letters
- **Journal**: None
- **Summary**: Recently, many deep-learning-based pan-sharpening methods have been proposed for generating high-quality pan-sharpened (PS) satellite images. These methods focused on various types of convolutional neural network (CNN) structures, which were trained by simply minimizing a spectral loss between network outputs and the corresponding high-resolution multi-spectral (MS) target images. However, due to different sensor characteristics and acquisition times, high-resolution panchromatic (PAN) and low-resolution MS image pairs tend to have large pixel misalignments, especially for moving objects in the images. Conventional CNNs trained with only the spectral loss with these satellite image datasets often produce PS images of low visual quality including double-edge artifacts along strong edges and ghosting artifacts on moving objects. In this letter, we propose a novel loss function, called a spectral-spatial structure (S3) loss, based on the correlation maps between MS targets and PAN inputs. Our proposed S3 loss can be very effectively utilized for pan-sharpening with various types of CNN structures, resulting in significant visual improvements on PS images with suppressed artifacts.



### An image-driven machine learning approach to kinetic modeling of a discontinuous precipitation reaction
- **Arxiv ID**: http://arxiv.org/abs/1906.05496v1
- **DOI**: None
- **Categories**: **physics.app-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05496v1)
- **Published**: 2019-06-13 06:07:57+00:00
- **Updated**: 2019-06-13 06:07:57+00:00
- **Authors**: Elizabeth Kautz, Wufei Ma, Saumyadeep Jana, Arun Devaraj, Vineet Joshi, Bülent Yener, Daniel Lewis
- **Comment**: 30 pages, 8 figures
- **Journal**: None
- **Summary**: Micrograph quantification is an essential component of several materials science studies. Machine learning methods, in particular convolutional neural networks, have previously demonstrated performance in image recognition tasks across several disciplines (e.g. materials science, medical imaging, facial recognition). Here, we apply these well-established methods to develop an approach to microstructure quantification for kinetic modeling of a discontinuous precipitation reaction in a case study on the uranium-molybdenum system. Prediction of material processing history based on image data (classification), calculation of area fraction of phases present in the micrographs (segmentation), and kinetic modeling from segmentation results were performed. Results indicate that convolutional neural networks represent microstructure image data well, and segmentation using the k-means clustering algorithm yields results that agree well with manually annotated images. Classification accuracies of original and segmented images are both 94\% for a 5-class classification problem. Kinetic modeling results agree well with previously reported data using manual thresholding. The image quantification and kinetic modeling approach developed and presented here aims to reduce researcher bias introduced into the characterization process, and allows for leveraging information in limited image data sets.



### Understanding Human Context in 3D Scenes by Learning Spatial Affordances with Virtual Skeleton Models
- **Arxiv ID**: http://arxiv.org/abs/1906.05498v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, cs.RO, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1906.05498v1)
- **Published**: 2019-06-13 06:21:14+00:00
- **Updated**: 2019-06-13 06:21:14+00:00
- **Authors**: Lasitha Piyathilaka, Sarath Kodagoda
- **Comment**: None
- **Journal**: None
- **Summary**: Robots are often required to operate in environments where humans are not present, but yet require the human context information for better human-robot interaction. Even when humans are present in the environment, detecting their presence in cluttered environments could be challenging. As a solution to this problem, this paper presents the concept of spatial affordance map which learns human context by looking at geometric features of the environment. Instead of observing real humans to learn human context, it uses virtual human models and their relationships with the environment to map hidden human affordances in 3D scenes by placing virtual skeleton models in 3D scenes with their confidence values. The spatial affordance map learning problem is formulated as a multi-label classification problem that can be learned using Support Vector Machine (SVM) based learners. Experiments carried out in a real 3D scene dataset recorded promising results and proved the applicability of affordance-map for mapping human context.



### MIMA: MAPPER-Induced Manifold Alignment for Semi-Supervised Fusion of Optical Image and Polarimetric SAR Data
- **Arxiv ID**: http://arxiv.org/abs/1906.05512v1
- **DOI**: 10.1109/TGRS.2019.2924113
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05512v1)
- **Published**: 2019-06-13 07:24:33+00:00
- **Updated**: 2019-06-13 07:24:33+00:00
- **Authors**: Jingliang Hu, Danfeng Hong, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal data fusion has recently been shown promise in classification tasks in remote sensing. Optical data and radar data, two important yet intrinsically different data sources, are attracting more and more attention for potential data fusion. It is already widely known that, a machine learning based methodology often yields excellent performance. However, the methodology relies on a large training set, which is very expensive to achieve in remote sensing. The semi-supervised manifold alignment (SSMA), a multi-modal data fusion algorithm, has been designed to amplify the impact of an existing training set by linking labeled data to unlabeled data via unsupervised techniques. In this paper, we explore the potential of SSMA in fusing optical data and polarimetric SAR data, which are multi-sensory data sources. Furthermore, we propose a MAPPER-induced manifold alignment (MIMA) for semi-supervised fusion of multi-sensory data sources. Our proposed method unites SSMA with MAPPER, which is developed from the emerging topological data analysis (TDA) field. To our best knowledge, this is the first time that SSMA has been applied on fusing optical data and SAR data, and also the first time that TDA has been applied in remote sensing. The conventional SSMA derives a topological structure using k-nearest-neighbor (kNN), while MIMA employs MAPPER, which considers the field knowledge and derives a novel topological structure through the spectral clustering in a data-driven fashion. Experiment results on data fusion with respect to land cover land use classification and local climate zone classification suggest superior performance of MIMA.



### Illuminant Chromaticity Estimation from Interreflections
- **Arxiv ID**: http://arxiv.org/abs/1906.05526v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1906.05526v1)
- **Published**: 2019-06-13 07:50:28+00:00
- **Updated**: 2019-06-13 07:50:28+00:00
- **Authors**: Eytan Lifshitz, Dani Lischinski
- **Comment**: None
- **Journal**: None
- **Summary**: Reliable estimation of illuminant chromaticity is crucial for simulating color constancy and for white balancing digital images. However, estimating illuminant chromaticity from a single image is an ill-posed task, in general, and existing solutions typically employ a variety of assumptions and heuristics. In this paper, we present a new, physically-based, approach for estimating illuminant chromaticity from interreflections of light between diffuse surfaces. Our approach assumes that all of the direct illumination in the scene has the same chromaticity, and that at least two areas where interreflections between Lambertian surfaces occur may be detected in the image. No further assumptions or restrictions on the illuminant chromaticty or the shading in the scene are necessary. Our approach is based on representing interreflections as lines in a special 2D color space, and the chromaticity of the illuminant is estimated from the approximate intersection between two or more such lines. Experimental results are reported on a dataset of illumination and surface reflectance spectra, as well as on real images we captured. The results indicate that our approach can yield state-of-the-art results when the interreflections are significant enough to be captured by the camera.



### Deep Variational Networks with Exponential Weighting for Learning Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/1906.05528v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05528v1)
- **Published**: 2019-06-13 07:54:51+00:00
- **Updated**: 2019-06-13 07:54:51+00:00
- **Authors**: Valery Vishnevskiy, Richard Rau, Orcun Goksel
- **Comment**: Accepted to MICCAI 2019
- **Journal**: None
- **Summary**: Tomographic image reconstruction is relevant for many medical imaging modalities including X-ray, ultrasound (US) computed tomography (CT) and photoacoustics, for which the access to full angular range tomographic projections might be not available in clinical practice due to physical or time constraints. Reconstruction from incomplete data in low signal-to-noise ratio regime is a challenging and ill-posed inverse problem that usually leads to unsatisfactory image quality. While informative image priors may be learned using generic deep neural network architectures, the artefacts caused by an ill-conditioned design matrix often have global spatial support and cannot be efficiently filtered out by means of convolutions. In this paper we propose to learn an inverse mapping in an end-to-end fashion via unrolling optimization iterations of a prototypical reconstruction algorithm. We herein introduce a network architecture that performs filtering jointly in both sinogram and spatial domains. To efficiently train such deep network we propose a novel regularization approach based on deep exponential weighting. Experiments on US and X-ray CT data show that our proposed method is qualitatively and quantitatively superior to conventional non-linear reconstruction methods as well as state-of-the-art deep networks for image reconstruction. Fast inference time of the proposed algorithm allows for sophisticated reconstructions in real-time critical settings, demonstrated with US SoS imaging of an ex vivo bovine phantom.



### Enforcing temporal consistency in Deep Learning segmentation of brain MR images
- **Arxiv ID**: http://arxiv.org/abs/1906.07160v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1906.07160v1)
- **Published**: 2019-06-13 08:33:23+00:00
- **Updated**: 2019-06-13 08:33:23+00:00
- **Authors**: Malav Bateriwala, Pierrick Bourgeat
- **Comment**: None
- **Journal**: None
- **Summary**: Longitudinal analysis has great potential to reveal developmental trajectories and monitor disease progression in medical imaging. This process relies on consistent and robust joint 4D segmentation. Traditional techniques are dependent on the similarity of images over time and the use of subject-specific priors to reduce random variation and improve the robustness and sensitivity of the overall longitudinal analysis. This is however slow and computationally intensive as subject-specific templates need to be rebuilt every time. The focus of this work to accelerate this analysis with the use of deep learning. The proposed approach is based on deep CNNs and incorporates semantic segmentation and provides a longitudinal relationship for the same subject. The proposed approach is based on deep CNNs and incorporates semantic segmentation and provides a longitudinal relationship for the same subject. The state of art using 3D patches as inputs to modified Unet provides results around ${0.91 \pm 0.5}$ Dice and using multi-view atlas in CNNs provide around the same results. In this work, different models are explored, each offers better accuracy and fast results while increasing the segmentation quality. These methods are evaluated on 135 scans from the EADC-ADNI Harmonized Hippocampus Protocol. Proposed CNN based segmentation approaches demonstrate how 2D segmentation using prior slices can provide similar results to 3D segmentation while maintaining good continuity in the 3D dimension and improved speed. Just using 2D modified sagittal slices provide us a better Dice and longitudinal analysis for a given subject. For the ADNI dataset, using the simple UNet CNN technique gives us ${0.84 \pm 0.5}$ and while using modified CNN techniques on the same input yields ${0.89 \pm 0.5}$. Rate of atrophy and RMS error are calculated for several test cases using various methods and analyzed.



### Learning Spatio-Temporal Representation with Local and Global Diffusion
- **Arxiv ID**: http://arxiv.org/abs/1906.05571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05571v1)
- **Published**: 2019-06-13 09:41:00+00:00
- **Updated**: 2019-06-13 09:41:00+00:00
- **Authors**: Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xinmei Tian, Tao Mei
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for visual recognition problems. Nevertheless, the convolutional filters in these networks are local operations while ignoring the large-range dependency. Such drawback becomes even worse particularly for video recognition, since video is an information-intensive media with complex temporal variations. In this paper, we present a novel framework to boost the spatio-temporal representation learning by Local and Global Diffusion (LGD). Specifically, we construct a novel neural network architecture that learns the local and global representations in parallel. The architecture is composed of LGD blocks, where each block updates local and global features by modeling the diffusions between these two representations. Diffusions effectively interact two aspects of information, i.e., localized and holistic, for more powerful way of representation learning. Furthermore, a kernelized classifier is introduced to combine the representations from two aspects for video recognition. Our LGD networks achieve clear improvements on the large-scale Kinetics-400 and Kinetics-600 video classification datasets against the best competitors by 3.5% and 0.7%. We further examine the generalization of both the global and local representations produced by our pre-trained LGD networks on four different benchmarks for video action recognition and spatio-temporal action detection tasks. Superior performances over several state-of-the-art techniques on these benchmarks are reported. Code is available at: https://github.com/ZhaofanQiu/local-and-global-diffusion-networks.



### ATRW: A Benchmark for Amur Tiger Re-identification in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1906.05586v5
- **DOI**: 10.1145/3394171.3413569
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05586v5)
- **Published**: 2019-06-13 10:16:30+00:00
- **Updated**: 2020-10-31 17:46:09+00:00
- **Authors**: Shuyuan Li, Jianguo Li, Hanlin Tang, Rui Qian, Weiyao Lin
- **Comment**: ACM Multimedia (MM) 2020
- **Journal**: None
- **Summary**: Monitoring the population and movements of endangered species is an important task to wildlife conversation. Traditional tagging methods do not scale to large populations, while applying computer vision methods to camera sensor data requires re-identification (re-ID) algorithms to obtain accurate counts and moving trajectory of wildlife. However, existing re-ID methods are largely targeted at persons and cars, which have limited pose variations and constrained capture environments. This paper tries to fill the gap by introducing a novel large-scale dataset, the Amur Tiger Re-identification in the Wild (ATRW) dataset. ATRW contains over 8,000 video clips from 92 Amur tigers, with bounding box, pose keypoint, and tiger identity annotations. In contrast to typical re-ID datasets, the tigers are captured in a diverse set of unconstrained poses and lighting conditions. We demonstrate with a set of baseline algorithms that ATRW is a challenging dataset for re-ID. Lastly, we propose a novel method for tiger re-identification, which introduces precise pose parts modeling in deep neural networks to handle large pose variation of tigers, and reaches notable performance improvement over existing re-ID methods. The dataset is public available at https://cvwc2019.github.io/ .



### $c^+$GAN: Complementary Fashion Item Recommendation
- **Arxiv ID**: http://arxiv.org/abs/1906.05596v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05596v1)
- **Published**: 2019-06-13 10:47:56+00:00
- **Updated**: 2019-06-13 10:47:56+00:00
- **Authors**: Sudhir Kumar, Mithun Das Gupta
- **Comment**: KDD'19: Workshop on AI for Fashion. arXiv admin note: text overlap
  with arXiv:1806.08977, arXiv:1411.1784, arXiv:1609.04802 by other authors
- **Journal**: None
- **Summary**: We present a conditional generative adversarial model to draw realistic samples from paired fashion clothing distribution and provide real samples to pair with arbitrary fashion units. More concretely, given an image of a shirt, obtained from a fashion magazine, a brochure or even any random click on ones phone, we draw realistic samples from a parameterized conditional distribution learned as a conditional generative adversarial network ($c^+$GAN) to generate the possible pants which can go with the shirt. We start with a classical cGAN model as proposed by Mirza and Osindero [arXiv:1411.1784] and modify both the generator and discriminator to work on captured-in-the-wild data with no human alignment. We gather a dataset from web crawled data, systematically develop a method which counters the problems inherent to such data, and finally present plausible results based on our technique. We propose simple ideas to evaluate how these techniques can conquer the cognitive gap that exists when arbitrary clothing articles need to be paired with another relevant article, based on similarity of search results.



### A Computationally Efficient Method for Defending Adversarial Deep Learning Attacks
- **Arxiv ID**: http://arxiv.org/abs/1906.05599v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.05599v1)
- **Published**: 2019-06-13 10:56:47+00:00
- **Updated**: 2019-06-13 10:56:47+00:00
- **Authors**: Rajeev Sahay, Rehana Mahfuz, Aly El Gamal
- **Comment**: 6 pages, 6 figures, submitted to IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: The reliance on deep learning algorithms has grown significantly in recent years. Yet, these models are highly vulnerable to adversarial attacks, which introduce visually imperceptible perturbations into testing data to induce misclassifications. The literature has proposed several methods to combat such adversarial attacks, but each method either fails at high perturbation values, requires excessive computing power, or both. This letter proposes a computationally efficient method for defending the Fast Gradient Sign (FGS) adversarial attack by simultaneously denoising and compressing data. Specifically, our proposed defense relies on training a fully connected multi-layer Denoising Autoencoder (DAE) and using its encoder as a defense against the adversarial attack. Our results show that using this dimensionality reduction scheme is not only highly effective in mitigating the effect of the FGS attack in multiple threat models, but it also provides a 2.43x speedup in comparison to defense strategies providing similar robustness against the same attack.



### Dense Deformation Network for High Resolution Tissue Cleared Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1906.06180v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06180v2)
- **Published**: 2019-06-13 11:54:19+00:00
- **Updated**: 2019-09-06 00:56:27+00:00
- **Authors**: Abdullah Nazib, Clinton Fookes, Dimitri Perrin
- **Comment**: None
- **Journal**: None
- **Summary**: The recent application of deep learning in various areas of medical image analysis has brought excellent performance gains. In particular, technologies based on deep learning in medical image registration can outperform traditional optimisation-based registration algorithms both in registration time and accuracy. However, the U-net based architectures used in most of the image registration frameworks downscale the data, which removes global information and affects the deformation. In this paper, we present a densely connected convolutional architecture for deformable image registration. Our proposed dense network downsizes data only in one stage and have dense connections instead of the skip connections in U-net architecture. The training of the network is unsupervised and does not require ground-truth deformation or any synthetic deformation as a label. The proposed architecture is trained and tested on two different versions of tissue-cleared data, at 10\% and 25\% resolution of the original single-cell-resolution dataset. We demonstrate comparable registration performance to state-of-the-art registration methods and superior performance to the deep-learning based VoxelMorph method in terms of accuracy and increased resolution handling ability. In both resolutions, the proposed DenseDeformation network outperforms VoxelMorph in registration accuracy. Importantly, it can register brains in one minute where conventional methods can take hours at 25\% resolution.



### Grid R-CNN Plus: Faster and Better
- **Arxiv ID**: http://arxiv.org/abs/1906.05688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05688v1)
- **Published**: 2019-06-13 13:52:28+00:00
- **Updated**: 2019-06-13 13:52:28+00:00
- **Authors**: Xin Lu, Buyu Li, Yuxin Yue, Quanquan Li, Junjie Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Grid R-CNN is a well-performed objection detection framework. It transforms the traditional box offset regression problem into a grid point estimation problem. With the guidance of the grid points, it can obtain high-quality localization results. However, the speed of Grid R-CNN is not so satisfactory. In this technical report we present Grid R-CNN Plus, a better and faster version of Grid R-CNN. We have made several updates that significantly speed up the framework and simultaneously improve the accuracy. On COCO dataset, the Res50-FPN based Grid R-CNN Plus detector achieves an mAP of 40.4%, outperforming the baseline on the same model by 3.0 points with similar inference time. Code is available at https://github.com/STVIR/Grid-R-CNN .



### Slim DensePose: Thrifty Learning from Sparse Annotations and Motion Cues
- **Arxiv ID**: http://arxiv.org/abs/1906.05706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05706v1)
- **Published**: 2019-06-13 14:07:40+00:00
- **Updated**: 2019-06-13 14:07:40+00:00
- **Authors**: Natalia Neverova, James Thewlis, Rıza Alp Güler, Iasonas Kokkinos, Andrea Vedaldi
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: DensePose supersedes traditional landmark detectors by densely mapping image pixels to body surface coordinates. This power, however, comes at a greatly increased annotation time, as supervising the model requires to manually label hundreds of points per pose instance. In this work, we thus seek methods to significantly slim down the DensePose annotations, proposing more efficient data collection strategies. In particular, we demonstrate that if annotations are collected in video frames, their efficacy can be multiplied for free by using motion cues. To explore this idea, we introduce DensePose-Track, a dataset of videos where selected frames are annotated in the traditional DensePose manner. Then, building on geometric properties of the DensePose mapping, we use the video dynamic to propagate ground-truth annotations in time as well as to learn from Siamese equivariance constraints. Having performed exhaustive empirical evaluation of various data annotation and learning strategies, we demonstrate that doing so can deliver significantly improved pose estimation results over strong baselines. However, despite what is suggested by some recent works, we show that merely synthesizing motion patterns by applying geometric transformations to isolated frames is significantly less effective, and that motion cues help much more when they are extracted from videos.



### 2D Attentional Irregular Scene Text Recognizer
- **Arxiv ID**: http://arxiv.org/abs/1906.05708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05708v1)
- **Published**: 2019-06-13 14:10:12+00:00
- **Updated**: 2019-06-13 14:10:12+00:00
- **Authors**: Pengyuan Lyu, Zhicheng Yang, Xinhang Leng, Xiaojun Wu, Ruiyu Li, Xiaoyong Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Irregular scene text, which has complex layout in 2D space, is challenging to most previous scene text recognizers. Recently, some irregular scene text recognizers either rectify the irregular text to regular text image with approximate 1D layout or transform the 2D image feature map to 1D feature sequence. Though these methods have achieved good performance, the robustness and accuracy are still limited due to the loss of spatial information in the process of 2D to 1D transformation. Different from all of previous, we in this paper propose a framework which transforms the irregular text with 2D layout to character sequence directly via 2D attentional scheme. We utilize a relation attention module to capture the dependencies of feature maps and a parallel attention module to decode all characters in parallel, which make our method more effective and efficient. Extensive experiments on several public benchmarks as well as our collected multi-line text dataset show that our approach is effective to recognize regular and irregular scene text and outperforms previous methods both in accuracy and speed.



### Generating and Exploiting Probabilistic Monocular Depth Estimates
- **Arxiv ID**: http://arxiv.org/abs/1906.05739v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05739v2)
- **Published**: 2019-06-13 14:59:54+00:00
- **Updated**: 2019-12-10 16:40:53+00:00
- **Authors**: Zhihao Xia, Patrick Sullivan, Ayan Chakrabarti
- **Comment**: Project page at https://projects.ayanc.org/prdepth/
- **Journal**: None
- **Summary**: Beyond depth estimation from a single image, the monocular cue is useful in a broader range of depth inference applications and settings---such as when one can leverage other available depth cues for improved accuracy. Currently, different applications, with different inference tasks and combinations of depth cues, are solved via different specialized networks---trained separately for each application. Instead, we propose a versatile task-agnostic monocular model that outputs a probability distribution over scene depth given an input color image, as a sample approximation of outputs from a patch-wise conditional VAE. We show that this distributional output can be used to enable a variety of inference tasks in different settings, without needing to retrain for each application. Across a diverse set of applications (depth completion, user guided estimation, etc.), our common model yields results with high accuracy---comparable to or surpassing that of state-of-the-art methods dependent on application-specific networks.



### Learning Video Representations using Contrastive Bidirectional Transformer
- **Arxiv ID**: http://arxiv.org/abs/1906.05743v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.05743v2)
- **Published**: 2019-06-13 15:03:52+00:00
- **Updated**: 2019-09-27 21:59:59+00:00
- **Authors**: Chen Sun, Fabien Baradel, Kevin Murphy, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation) compared to existing methods. Our method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). We also show how to learn representations from sequences of visual features and sequences of words derived from ASR (automatic speech recognition), and show that such cross-modal training (when possible) helps even more.



### The iMaterialist Fashion Attribute Dataset
- **Arxiv ID**: http://arxiv.org/abs/1906.05750v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05750v2)
- **Published**: 2019-06-13 15:25:04+00:00
- **Updated**: 2019-06-14 17:36:43+00:00
- **Authors**: Sheng Guo, Weilin Huang, Xiao Zhang, Prasanna Srikhanta, Yin Cui, Yuan Li, Matthew R. Scott, Hartwig Adam, Serge Belongie
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale image databases such as ImageNet have significantly advanced image classification and other visual recognition tasks. However much of these datasets are constructed only for single-label and coarse object-level classification. For real-world applications, multiple labels and fine-grained categories are often needed, yet very few such datasets exist publicly, especially those of large-scale and high quality. In this work, we contribute to the community a new dataset called iMaterialist Fashion Attribute (iFashion-Attribute) to address this problem in the fashion domain. The dataset is constructed from over one million fashion images with a label space that includes 8 groups of 228 fine-grained attributes in total. Each image is annotated by experts with multiple, high-quality fashion attributes. The result is the first known million-scale multi-label and fine-grained image dataset. We conduct extensive experiments and provide baseline results with modern deep Convolutional Neural Networks (CNNs). Additionally, we demonstrate models pre-trained on iFashion-Attribute achieve superior transfer learning performance on fashion related tasks compared with pre-training from ImageNet or other fashion datasets. Data is available at: https://github.com/visipedia/imat_fashion_comp



### Unsupervised Image Noise Modeling with Self-Consistent GAN
- **Arxiv ID**: http://arxiv.org/abs/1906.05762v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.05762v3)
- **Published**: 2019-06-13 15:45:46+00:00
- **Updated**: 2020-06-04 12:12:52+00:00
- **Authors**: Hanshu Yan, Xuan Chen, Vincent Y. F. Tan, Wenhan Yang, Joe Wu, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Noise modeling lies in the heart of many image processing tasks. However, existing deep learning methods for noise modeling generally require clean and noisy image pairs for model training; these image pairs are difficult to obtain in many realistic scenarios. To ameliorate this problem, we propose a self-consistent GAN (SCGAN), that can directly extract noise maps from noisy images, thus enabling unsupervised noise modeling. In particular, the SCGAN introduces three novel self-consistent constraints that are complementary to one another, viz.: the noise model should produce a zero response over a clean input; the noise model should return the same output when fed with a specific pure noise input; and the noise model also should re-extract a pure noise map if the map is added to a clean image. These three constraints are simple yet effective. They jointly facilitate unsupervised learning of a noise model for various noise types. To demonstrate its wide applicability, we deploy the SCGAN on three image processing tasks including blind image denoising, rain streak removal, and noisy image super-resolution. The results demonstrate the effectiveness and superiority of our method over the state-of-the-art methods on a variety of benchmark datasets, even though the noise types vary significantly and paired clean images are not available.



### Training Image Estimators without Image Ground-Truth
- **Arxiv ID**: http://arxiv.org/abs/1906.05775v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05775v2)
- **Published**: 2019-06-13 16:04:03+00:00
- **Updated**: 2019-10-28 19:40:44+00:00
- **Authors**: Zhihao Xia, Ayan Chakrabarti
- **Comment**: Project page at https://projects.ayanc.org/unsupimg/
- **Journal**: None
- **Summary**: Deep neural networks have been very successful in image estimation applications such as compressive-sensing and image restoration, as a means to estimate images from partial, blurry, or otherwise degraded measurements. These networks are trained on a large number of corresponding pairs of measurements and ground-truth images, and thus implicitly learn to exploit domain-specific image statistics. But unlike measurement data, it is often expensive or impractical to collect a large training set of ground-truth images in many application settings. In this paper, we introduce an unsupervised framework for training image estimation networks, from a training set that contains only measurements---with two varied measurements per image---but no ground-truth for the full images desired as output. We demonstrate that our framework can be applied for both regular and blind image estimation tasks, where in the latter case parameters of the measurement model (e.g., the blur kernel) are unknown: during inference, and potentially, also during training. We evaluate our method for training networks for compressive-sensing and blind deconvolution, considering both non-blind and blind training for the latter. Our unsupervised framework yields models that are nearly as accurate as those from fully supervised training, despite not having access to any ground-truth images.



### Egocentric affordance detection with the one-shot geometry-driven Interaction Tensor
- **Arxiv ID**: http://arxiv.org/abs/1906.05794v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.05794v1)
- **Published**: 2019-06-13 16:28:54+00:00
- **Updated**: 2019-06-13 16:28:54+00:00
- **Authors**: Eduardo Ruiz, Walterio Mayol-Cuevas
- **Comment**: Accepted for presentation at EPIC@CVPR2019 workshop
- **Journal**: None
- **Summary**: In this abstract we describe recent [4,7] and latest work on the determination of affordances in visually perceived 3D scenes. Our method builds on the hypothesis that geometry on its own provides enough information to enable the detection of significant interaction possibilities in the environment. The motivation behind this is that geometric information is intimately related to the physical interactions afforded by objects in the world. The approach uses a generic representation for the interaction between everyday objects such as a mug or an umbrella with the environment, and also for more complex affordances such as humans Sitting or Riding a motorcycle. Experiments with synthetic and real RGB-D scenes show that the representation enables the prediction of affordance candidate locations in novel environments at fast rates and from a single (one-shot) training example. The determination of affordances is a crucial step towards systems that need to perceive and interact with their surroundings. We here illustrate output on two cases for a simulated robot and for an Augmented Reality setting, both perceiving in an egocentric manner.



### The Replica Dataset: A Digital Replica of Indoor Spaces
- **Arxiv ID**: http://arxiv.org/abs/1906.05797v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.05797v1)
- **Published**: 2019-06-13 16:29:58+00:00
- **Updated**: 2019-06-13 16:29:58+00:00
- **Authors**: Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, Richard Newcombe
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each scene consists of a dense mesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The goal of Replica is to enable machine learning (ML) research that relies on visually, geometrically, and semantically realistic generative models of the world - for instance, egocentric computer vision, semantic segmentation in 2D and 3D, geometric inference, and the development of embodied agents (virtual robots) performing navigation, instruction following, and question answering. Due to the high level of realism of the renderings from Replica, there is hope that ML systems trained on Replica may transfer directly to real world image and video data. Together with the data, we are releasing a minimal C++ SDK as a starting point for working with the Replica dataset. In addition, Replica is `Habitat-compatible', i.e. can be natively used with AI Habitat for training and testing embodied agents.



### Time-warping invariants of multidimensional time series
- **Arxiv ID**: http://arxiv.org/abs/1906.05823v2
- **DOI**: 10.1007/s10440-020-00333-x
- **Categories**: **math.RA**, cs.CV, cs.LG, 60L10, 16T05, 62M10, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1906.05823v2)
- **Published**: 2019-06-13 17:09:19+00:00
- **Updated**: 2020-03-11 10:55:59+00:00
- **Authors**: Joscha Diehl, Kurusch Ebrahimi-Fard, Nikolas Tapia
- **Comment**: 18 pages, 1 figure
- **Journal**: Acta Applicandae Mathematicae 170(1) (2020), 265--290
- **Summary**: In data science, one is often confronted with a time series representing measurements of some quantity of interest. Usually, as a first step, features of the time series need to be extracted. These are numerical quantities that aim to succinctly describe the data and to dampen the influence of noise. In some applications, these features are also required to satisfy some invariance properties. In this paper, we concentrate on time-warping invariants. We show that these correspond to a certain family of iterated sums of the increments of the time series, known as quasisymmetric functions in the mathematics literature. We present these invariant features in an algebraic framework, and we develop some of their basic properties.



### Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards
- **Arxiv ID**: http://arxiv.org/abs/1906.05841v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05841v2)
- **Published**: 2019-06-13 17:43:07+00:00
- **Updated**: 2019-08-02 16:34:16+00:00
- **Authors**: Gerrit Schoettler, Ashvin Nair, Jianlan Luo, Shikhar Bahl, Juan Aparicio Ojea, Eugen Solowjow, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: Connector insertion and many other tasks commonly found in modern manufacturing settings involve complex contact dynamics and friction. Since it is difficult to capture related physical effects with first-order modeling, traditional control methods often result in brittle and inaccurate controllers, which have to be manually tuned. Reinforcement learning (RL) methods have been demonstrated to be capable of learning controllers in such environments from autonomous interaction with the environment, but running RL algorithms in the real world poses sample efficiency and safety challenges. Moreover, in practical real-world settings we cannot assume access to perfect state information or dense reward signals. In this paper, we consider a variety of difficult industrial insertion tasks with visual inputs and different natural reward specifications, namely sparse rewards and goal images. We show that methods that combine RL with prior information, such as classical controllers or demonstrations, can solve these tasks from a reasonable amount of real-world interaction.



### Mask2Lesion: Mask-Constrained Adversarial Skin Lesion Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1906.05845v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.05845v2)
- **Published**: 2019-06-13 17:47:09+00:00
- **Updated**: 2019-07-15 11:24:37+00:00
- **Authors**: Kumar Abhishek, Ghassan Hamarneh
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Skin lesion segmentation is a vital task in skin cancer diagnosis and further treatment. Although deep learning based approaches have significantly improved the segmentation accuracy, these algorithms are still reliant on having a large enough dataset in order to achieve adequate results. Inspired by the immense success of generative adversarial networks (GANs), we propose a GAN-based augmentation of the original dataset in order to improve the segmentation performance. In particular, we use the segmentation masks available in the training dataset to train the Mask2Lesion model, and use the model to generate new lesion images given any arbitrary mask, which are then used to augment the original training dataset. We test Mask2Lesion augmentation on the ISBI ISIC 2017 Skin Lesion Segmentation Challenge dataset and achieve an improvement of 5.17% in the mean Dice score as compared to a model trained with only classical data augmentation techniques.



### Contrastive Multiview Coding
- **Arxiv ID**: http://arxiv.org/abs/1906.05849v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05849v5)
- **Published**: 2019-06-13 17:49:20+00:00
- **Updated**: 2020-12-18 10:00:51+00:00
- **Authors**: Yonglong Tian, Dilip Krishnan, Phillip Isola
- **Comment**: Code: http://github.com/HobbitLong/CMC/
- **Journal**: None
- **Summary**: Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a "dog" can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks. Code is released at: http://github.com/HobbitLong/CMC/.



### Detecting Photoshopped Faces by Scripting Photoshop
- **Arxiv ID**: http://arxiv.org/abs/1906.05856v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05856v2)
- **Published**: 2019-06-13 17:59:02+00:00
- **Updated**: 2019-09-05 17:59:09+00:00
- **Authors**: Sheng-Yu Wang, Oliver Wang, Andrew Owens, Richard Zhang, Alexei A. Efros
- **Comment**: None
- **Journal**: None
- **Summary**: Most malicious photo manipulations are created using standard image editing tools, such as Adobe Photoshop. We present a method for detecting one very popular Photoshop manipulation -- image warping applied to human faces -- using a model trained entirely using fake images that were automatically generated by scripting Photoshop itself. We show that our model outperforms humans at the task of recognizing manipulated images, can predict the specific location of edits, and in some cases can be used to "undo" a manipulation to reconstruct the original, unedited image. We demonstrate that the system can be successfully applied to real, artist-created image manipulations.



### Show, Match and Segment: Joint Weakly Supervised Learning of Semantic Matching and Object Co-segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.05857v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05857v2)
- **Published**: 2019-06-13 17:59:19+00:00
- **Updated**: 2020-03-29 08:58:13+00:00
- **Authors**: Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, Jia-Bin Huang
- **Comment**: PAMI 2020. Project: https://yunchunchen.github.io/MaCoSNet-web/ Code:
  https://github.com/YunChunChen/MaCoSNet-pytorch
- **Journal**: None
- **Summary**: We present an approach for jointly matching and segmenting object instances of the same category within a collection of images. In contrast to existing algorithms that tackle the tasks of semantic matching and object co-segmentation in isolation, our method exploits the complementary nature of the two tasks. The key insights of our method are two-fold. First, the estimated dense correspondence fields from semantic matching provide supervision for object co-segmentation by enforcing consistency between the predicted masks from a pair of images. Second, the predicted object masks from object co-segmentation in turn allow us to reduce the adverse effects due to background clutters for improving semantic matching. Our model is end-to-end trainable and does not require supervision from manually annotated correspondences and object masks. We validate the efficacy of our approach on five benchmark datasets: TSS, Internet, PF-PASCAL, PF-WILLOW, and SPair-71k, and show that our algorithm performs favorably against the state-of-the-art methods on both semantic matching and object co-segmentation tasks.



### Joint Concept Matching based Learning for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.05879v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05879v3)
- **Published**: 2019-06-13 18:15:29+00:00
- **Updated**: 2019-07-03 01:32:39+00:00
- **Authors**: Wen Tang, Ashkan Panahi, Hamid Krim
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) which aims to recognize unseen object classes by only training on seen object classes, has increasingly been of great interest in Machine Learning, and has registered with some successes. Most existing ZSL methods typically learn a projection map between the visual feature space and the semantic space and mainly suffer which is prone to a projection domain shift primarily due to a large domain gap between seen and unseen classes. In this paper, we propose a novel inductive ZSL model based on projecting both visual and semantic features into a common distinct latent space with class-specific knowledge, and on reconstructing both visual and semantic features by such a distinct common space to narrow the domain shift gap. We show that all these constraints on the latent space, class-specific knowledge, reconstruction of features and their combinations enhance the robustness against the projection domain shift problem, and improve the generalization ability to unseen object classes. Comprehensive experiments on four benchmark datasets demonstrate that our proposed method is superior to state-of-the-art algorithms.



### Can generalised relative pose estimation solve sparse 3D registration?
- **Arxiv ID**: http://arxiv.org/abs/1906.05888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05888v1)
- **Published**: 2019-06-13 18:41:27+00:00
- **Updated**: 2019-06-13 18:41:27+00:00
- **Authors**: Siddhant Ranade, Xin Yu, Shantnu Kakkar, Pedro Miraldo, Srikumar Ramalingam
- **Comment**: None
- **Journal**: None
- **Summary**: Popular 3D scan registration projects, such as Stanford digital Michelangelo or KinectFusion, exploit the high-resolution sensor data for scan alignment. It is particularly challenging to solve the registration of sparse 3D scans in the absence of RGB components. In this case, we can not establish point correspondences since the same 3D point cannot be captured in two successive scans. In contrast to correspondence based methods, we take a different viewpoint and formulate the sparse 3D registration problem based on the constraints from the intersection of line segments from adjacent scans. We obtain the line segments by modeling every horizontal and vertical scan-line as piece-wise linear segments. We propose a new alternating projection algorithm for solving the scan alignment problem using line intersection constraints. We develop two new minimal solvers for scan alignment in the presence of plane correspondences: 1) 3 line intersections and 1 plane correspondence, and 2) 1 line intersection and 2 plane correspondences. We outperform other competing methods on Kinect and LiDAR datasets.



### IntrinSeqNet: Learning to Estimate the Reflectance from Varying Illumination
- **Arxiv ID**: http://arxiv.org/abs/1906.05893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05893v1)
- **Published**: 2019-06-13 18:59:29+00:00
- **Updated**: 2019-06-13 18:59:29+00:00
- **Authors**: Grégoire Nieto, Mohammad Rouhani, Philippe Robert
- **Comment**: This article has been removed by arXiv administrators because the
  submitter did not have the rights to agree to the license at the time of
  submission
- **Journal**: None
- **Summary**: This article has been removed by arXiv administrators because the submitter did not have the rights to agree to the license at the time of submission



### Semantics to Space(S2S): Embedding semantics into spatial space for zero-shot verb-object query inferencing
- **Arxiv ID**: http://arxiv.org/abs/1906.05894v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05894v2)
- **Published**: 2019-06-13 19:01:05+00:00
- **Updated**: 2019-09-13 22:39:48+00:00
- **Authors**: Sungmin Eum, Heesung Kwon
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel deep zero-shot learning (ZSL) model for inferencing human-object-interaction with verb-object (VO) query. While the previous two-stream ZSL approaches only use the semantic/textual information to be fed into the query stream, we seek to incorporate and embed the semantics into the visual representation stream as well. Our approach is powered by Semantics-to-Space (S2S) architecture where semantics derived from the residing objects are embedded into a spatial space of the visual stream. This architecture allows the co-capturing of the semantic attributes of the human and the objects along with their location/size/silhouette information. To validate, we have constructed a new dataset, Verb-Transferability 60 (VT60). VT60 provides 60 different VO pairs with overlapping verbs tailored for testing two-stream ZSL approaches with VO query. Experimental evaluations show that our approach not only outperforms the state-of-the-art, but also shows the capability of consistently improving performance regardless of which ZSL baseline architecture is used.



### Learning to Forget for Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.05895v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.05895v2)
- **Published**: 2019-06-13 19:03:27+00:00
- **Updated**: 2020-06-16 03:24:44+00:00
- **Authors**: Sungyong Baik, Seokil Hong, Kyoung Mu Lee
- **Comment**: CVPR 2020. Code at https://github.com/baiksung/L2F
- **Journal**: None
- **Summary**: Few-shot learning is a challenging problem where the goal is to achieve generalization from only few examples. Model-agnostic meta-learning (MAML) tackles the problem by formulating prior knowledge as a common initialization across tasks, which is then used to quickly adapt to unseen tasks. However, forcibly sharing an initialization can lead to conflicts among tasks and the compromised (undesired by tasks) location on optimization landscape, thereby hindering the task adaptation. Further, we observe that the degree of conflict differs among not only tasks but also layers of a neural network. Thus, we propose task-and-layer-wise attenuation on the compromised initialization to reduce its influence. As the attenuation dynamically controls (or selectively forgets) the influence of prior knowledge for a given task and each layer, we name our method as L2F (Learn to Forget). The experimental results demonstrate that the proposed method provides faster adaptation and greatly improves the performance. Furthermore, L2F can be easily applied and improve other state-of-the-art MAML-based frameworks, illustrating its simplicity and generalizability.



### Learning Instance Occlusion for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.05896v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.05896v4)
- **Published**: 2019-06-13 19:04:58+00:00
- **Updated**: 2020-04-08 06:24:27+00:00
- **Authors**: Justin Lazarow, Kwonjoon Lee, Kunyu Shi, Zhuowen Tu
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Panoptic segmentation requires segments of both "things" (countable object instances) and "stuff" (uncountable and amorphous regions) within a single output. A common approach involves the fusion of instance segmentation (for "things") and semantic segmentation (for "stuff") into a non-overlapping placement of segments, and resolves overlaps. However, instance ordering with detection confidence do not correlate well with natural occlusion relationship. To resolve this issue, we propose a branch that is tasked with modeling how two instance masks should overlap one another as a binary relation. Our method, named OCFusion, is lightweight but particularly effective in the instance fusion process. OCFusion is trained with the ground truth relation derived automatically from the existing dataset annotations. We obtain state-of-the-art results on COCO and show competitive results on the Cityscapes panoptic segmentation benchmark.



### Dynamic PET cardiac and parametric image reconstruction: a fixed-point proximity gradient approach using patch-based DCT and tensor SVD regularization
- **Arxiv ID**: http://arxiv.org/abs/1906.05897v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1906.05897v1)
- **Published**: 2019-06-13 19:06:11+00:00
- **Updated**: 2019-06-13 19:06:11+00:00
- **Authors**: Ida Häggström, Yizun Lin, Si Li, Andrzej Krol, Yuesheng Xu, C. Ross Schmidtlein
- **Comment**: 11 pages, 12 figures
- **Journal**: None
- **Summary**: Our aim was to enhance visual quality and quantitative accuracy of dynamic positron emission tomography (PET)uptake images by improved image reconstruction, using sophisticated sparse penalty models that incorporate both 2D spatial+1D temporal (3DT) information. We developed two new 3DT PET reconstruction algorithms, incorporating different temporal and spatial penalties based on discrete cosine transform (DCT)w/ patches, and tensor nuclear norm (TNN) w/ patches, and compared to frame-by-frame methods; conventional 2D ordered subsets expectation maximization (OSEM) w/ post-filtering and 2D-DCT and 2D-TNN. A 3DT brain phantom with kinetic uptake (2-tissue model), and a moving 3DT cardiac/lung phantom was simulated and reconstructed. For the cardiac/lung phantom, an additional cardiac gated 2D-OSEM set was reconstructed. The structural similarity index (SSIM) and relative root mean squared error (rRMSE) relative ground truth was investigated. The image derived left ventricular (LV) volume for the cardiac/lung images was found by region growing and parametric images of the brain phantom were calculated. For the cardiac/lung phantom, 3DT-TNN yielded optimal images, and 3DT-DCT was best for the brain phantom. The optimal LV volume from the 3DT-TNN images was on average 11 and 55 percentage points closer to the true value compared to cardiac gated 2D-OSEM and 2D-OSEM respectively. Compared to 2D-OSEM, parametric images based on 3DT-DCT images generally had smaller bias and higher SSIM. Our novel methods that incorporate both 2D spatial and 1D temporal penalties produced dynamic PET images of higher quality than conventional 2D methods, w/o need for post-filtering. Breathing and cardiac motion were simultaneously captured w/o need for respiratory or cardiac gating. LV volumes were better recovered, and subsequently fitted parametric images were generally less biased and of higher quality.



### Stand-Alone Self-Attention in Vision Models
- **Arxiv ID**: http://arxiv.org/abs/1906.05909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05909v1)
- **Published**: 2019-06-13 19:43:01+00:00
- **Updated**: 2019-06-13 19:43:01+00:00
- **Authors**: Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jonathon Shlens
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.



### Hallucinating IDT Descriptors and I3D Optical Flow Features for Action Recognition with CNNs
- **Arxiv ID**: http://arxiv.org/abs/1906.05910v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05910v2)
- **Published**: 2019-06-13 19:44:17+00:00
- **Updated**: 2019-08-18 15:37:40+00:00
- **Authors**: Lei Wang, Piotr Koniusz, Du Q. Huynh
- **Comment**: First two authors contributed equally. This paper is accepted by
  ICCV'19
- **Journal**: ICCV 2019
- **Summary**: In this paper, we revive the use of old-fashioned handcrafted video representations for action recognition and put new life into these techniques via a CNN-based hallucination step. Despite of the use of RGB and optical flow frames, the I3D model (amongst others) thrives on combining its output with the Improved Dense Trajectory (IDT) and extracted with its low-level video descriptors encoded via Bag-of-Words (BoW) and Fisher Vectors (FV). Such a fusion of CNNs and handcrafted representations is time-consuming due to pre-processing, descriptor extraction, encoding and tuning parameters. Thus, we propose an end-to-end trainable network with streams which learn the IDT-based BoW/FV representations at the training stage and are simple to integrate with the I3D model. Specifically, each stream takes I3D feature maps ahead of the last 1D conv. layer and learns to `translate' these maps to BoW/FV representations. Thus, our model can hallucinate and use such synthesized BoW/FV representations at the testing stage. We show that even features of the entire I3D optical flow stream can be hallucinated thus simplifying the pipeline. Our model saves 20-55h of computations and yields state-of-the-art results on four publicly available datasets.



### Unsupervised Video Interpolation Using Cycle Consistency
- **Arxiv ID**: http://arxiv.org/abs/1906.05928v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05928v3)
- **Published**: 2019-06-13 21:04:10+00:00
- **Updated**: 2021-03-28 00:43:10+00:00
- **Authors**: Fitsum A. Reda, Deqing Sun, Aysegul Dundar, Mohammad Shoeybi, Guilin Liu, Kevin J. Shih, Andrew Tao, Jan Kautz, Bryan Catanzaro
- **Comment**: Published in ICCV 2019. Codes are available at
  https://github.com/NVIDIA/unsupervised-video-interpolation. Project website
  https://nv-adlr.github.io/publication/2019-UnsupervisedVideoInterpolation
- **Journal**: None
- **Summary**: Learning to synthesize high frame rate videos via interpolation requires large quantities of high frame rate training videos, which, however, are scarce, especially at high resolutions. Here, we propose unsupervised techniques to synthesize high frame rate videos directly from low frame rate videos using cycle consistency. For a triplet of consecutive frames, we optimize models to minimize the discrepancy between the center frame and its cycle reconstruction, obtained by interpolating back from interpolated intermediate frames. This simple unsupervised constraint alone achieves results comparable with supervision using the ground truth intermediate frames. We further introduce a pseudo supervised loss term that enforces the interpolated frames to be consistent with predictions of a pre-trained interpolation model. The pseudo supervised loss term, used together with cycle consistency, can effectively adapt a pre-trained model to a new target domain. With no additional data and in a completely unsupervised fashion, our techniques significantly improve pre-trained models on new target domains, increasing PSNR values from 32.84dB to 33.05dB on the Slowflow and from 31.82dB to 32.53dB on the Sintel evaluation datasets.



### Cross-View Policy Learning for Street Navigation
- **Arxiv ID**: http://arxiv.org/abs/1906.05930v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.05930v2)
- **Published**: 2019-06-13 21:07:31+00:00
- **Updated**: 2019-09-22 07:33:44+00:00
- **Authors**: Ang Li, Huiyi Hu, Piotr Mirowski, Mehrdad Farajtabar
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The ability to navigate from visual observations in unfamiliar environments is a core component of intelligent agents and an ongoing challenge for Deep Reinforcement Learning (RL). Street View can be a sensible testbed for such RL agents, because it provides real-world photographic imagery at ground level, with diverse street appearances; it has been made into an interactive environment called StreetLearn and used for research on navigation. However, goal-driven street navigation agents have not so far been able to transfer to unseen areas without extensive retraining, and relying on simulation is not a scalable solution. Since aerial images are easily and globally accessible, we propose instead to train a multi-modal policy on ground and aerial views, then transfer the ground view policy to unseen (target) parts of the city by utilizing aerial view observations. Our core idea is to pair the ground view with an aerial view and to learn a joint policy that is transferable across views. We achieve this by learning a similar embedding space for both views, distilling the policy across views and dropping out visual modalities. We further reformulate the transfer learning paradigm into three stages: 1) cross-modal training, when the agent is initially trained on multiple city regions, 2) aerial view-only adaptation to a new area, when the agent is adapted to a held-out region using only the easily obtainable aerial view, and 3) ground view-only transfer, when the agent is tested on navigation tasks on unseen ground views, without aerial imagery. Experimental results suggest that the proposed cross-view policy learning enables better generalization of the agent and allows for more effective transfer to unseen environments.



### Temporal Transformer Networks: Joint Learning of Invariant and Discriminative Time Warping
- **Arxiv ID**: http://arxiv.org/abs/1906.05947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05947v1)
- **Published**: 2019-06-13 22:05:15+00:00
- **Updated**: 2019-06-13 22:05:15+00:00
- **Authors**: Suhas Lohit, Qiao Wang, Pavan Turaga
- **Comment**: Published in CVPR 2019, Codes available at
  https://github.com/suhaslohit/TTN
- **Journal**: None
- **Summary**: Many time-series classification problems involve developing metrics that are invariant to temporal misalignment. In human activity analysis, temporal misalignment arises due to various reasons including differing initial phase, sensor sampling rates, and elastic time-warps due to subject-specific biomechanics. Past work in this area has only looked at reducing intra-class variability by elastic temporal alignment. In this paper, we propose a hybrid model-based and data-driven approach to learn warping functions that not just reduce intra-class variability, but also increase inter-class separation. We call this a temporal transformer network (TTN). TTN is an interpretable differentiable module, which can be easily integrated at the front end of a classification network. The module is capable of reducing intra-class variance by generating input-dependent warping functions which lead to rate-robust representations. At the same time, it increases inter-class variance by learning warping functions that are more discriminative. We show improvements over strong baselines in 3D action recognition on challenging datasets using the proposed framework. The improvements are especially pronounced when training sets are smaller.



### Multigrid Neural Memory
- **Arxiv ID**: http://arxiv.org/abs/1906.05948v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1906.05948v4)
- **Published**: 2019-06-13 22:10:01+00:00
- **Updated**: 2020-08-15 21:12:15+00:00
- **Authors**: Tri Huynh, Michael Maire, Matthew R. Walter
- **Comment**: ICML 2020; Project Website:
  http://people.cs.uchicago.edu/~trihuynh/multigrid_mem
- **Journal**: None
- **Summary**: We introduce a novel approach to endowing neural networks with emergent, long-term, large-scale memory. Distinct from strategies that connect neural networks to external memory banks via intricately crafted controllers and hand-designed attentional mechanisms, our memory is internal, distributed, co-located alongside computation, and implicitly addressed, while being drastically simpler than prior efforts. Architecting networks with multigrid structure and connectivity, while distributing memory cells alongside computation throughout this topology, we observe the emergence of coherent memory subsystems. Our hierarchical spatial organization, parameterized convolutionally, permits efficient instantiation of large-capacity memories, while multigrid topology provides short internal routing pathways, allowing convolutional networks to efficiently approximate the behavior of fully connected networks. Such networks have an implicit capacity for internal attention; augmented with memory, they learn to read and write specific memory locations in a dynamic data-dependent manner. We demonstrate these capabilities on exploration and mapping tasks, where our network is able to self-organize and retain long-term memory for trajectories of thousands of time steps. On tasks decoupled from any notion of spatial geometry: sorting, associative recall, and question answering, our design functions as a truly generic memory and yields excellent results.



### Scalable Neural Architecture Search for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.05956v1
- **DOI**: 10.1007/978-3-030-32248-9_25
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.05956v1)
- **Published**: 2019-06-13 22:39:42+00:00
- **Updated**: 2019-06-13 22:39:42+00:00
- **Authors**: Sungwoong Kim, Ildoo Kim, Sungbin Lim, Woonhyuk Baek, Chiheon Kim, Hyungjoo Cho, Boogeon Yoon, Taesup Kim
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper, a neural architecture search (NAS) framework is proposed for 3D medical image segmentation, to automatically optimize a neural architecture from a large design space. Our NAS framework searches the structure of each layer including neural connectivities and operation types in both of the encoder and decoder. Since optimizing over a large discrete architecture space is difficult due to high-resolution 3D medical images, a novel stochastic sampling algorithm based on a continuous relaxation is also proposed for scalable gradient based optimization. On the 3D medical image segmentation tasks with a benchmark dataset, an automatically designed architecture by the proposed NAS framework outperforms the human-designed 3D U-Net, and moreover this optimized architecture is well suited to be transferred for different tasks.



### Speaker-Targeted Audio-Visual Models for Speech Recognition in Cocktail-Party Environments
- **Arxiv ID**: http://arxiv.org/abs/1906.05962v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CL, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1906.05962v1)
- **Published**: 2019-06-13 23:52:16+00:00
- **Updated**: 2019-06-13 23:52:16+00:00
- **Authors**: Guan-Lin Chao, William Chan, Ian Lane
- **Comment**: Published in INTERSPEECH 2016
- **Journal**: None
- **Summary**: Speech recognition in cocktail-party environments remains a significant challenge for state-of-the-art speech recognition systems, as it is extremely difficult to extract an acoustic signal of an individual speaker from a background of overlapping speech with similar frequency and temporal characteristics. We propose the use of speaker-targeted acoustic and audio-visual models for this task. We complement the acoustic features in a hybrid DNN-HMM model with information of the target speaker's identity as well as visual features from the mouth region of the target speaker. Experimentation was performed using simulated cocktail-party data generated from the GRID audio-visual corpus by overlapping two speakers's speech on a single acoustic channel. Our audio-only baseline achieved a WER of 26.3%. The audio-visual model improved the WER to 4.4%. Introducing speaker identity information had an even more pronounced effect, improving the WER to 3.6%. Combining both approaches, however, did not significantly improve performance further. Our work demonstrates that speaker-targeted models can significantly improve the speech recognition in cocktail party environments.




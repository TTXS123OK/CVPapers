# Arxiv Papers in cs.CV on 2019-06-01
### ArcticNet: A Deep Learning Solution to Classify Arctic Wetlands
- **Arxiv ID**: http://arxiv.org/abs/1906.00133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00133v1)
- **Published**: 2019-06-01 02:40:47+00:00
- **Updated**: 2019-06-01 02:40:47+00:00
- **Authors**: Ziyu Jiang, Kate Von Ness, Julie Loisel, Zhangyang Wang
- **Comment**: Published at CVPR 2019 Detecting Objects in Aerial Images Workshop
- **Journal**: None
- **Summary**: Arctic environments are rapidly changing under the warming climate. Of particular interest are wetlands, a type of ecosystem that constitutes the most effective terrestrial long-term carbon store. As permafrost thaws, the carbon that was locked in these wetland soils for millennia becomes available for aerobic and anaerobic decomposition, which releases CO2 and CH4, respectively, back to the atmosphere.As CO2 and CH4 are potent greenhouse gases, this transfer of carbon from the land to the atmosphere further contributes to global warming, thereby increasing the rate of permafrost degradation in a positive feedback loop. Therefore, monitoring Arctic wetland health and dynamics is a key scientific task that is also of importance for policy. However, the identification and delineation of these important wetland ecosystems, remain incomplete and often inaccurate. Mapping the extent of Arctic wetlands remains a challenge for the scientific community. Conventional, coarser remote sensing methods are inadequate at distinguishing the diverse and micro-topographically complex non-vascular vegetation that characterize Arctic wetlands, presenting the need for better identification methods. To tackle this challenging problem, we constructed and annotated the first-of-its-kind Arctic Wetland Dataset (AWD). Based on that, we present ArcticNet, a deep neural network that exploits the multi-spectral, high-resolution imagery captured from nanosatellites (Planet Dove CubeSats) with additional DEM from the ArcticDEM project, to semantically label a Arctic study area into six types, in which three Arctic wetland functional types are included. We present multi-fold efforts to handle the arising challenges, including class imbalance, and the choice of fusion strategies. Preliminary results endorse the high promise of ArcticNet, achieving 93.12% in labelling a hold-out set of regions in our Arctic study area.



### Region-specific Diffeomorphic Metric Mapping
- **Arxiv ID**: http://arxiv.org/abs/1906.00139v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/1906.00139v2)
- **Published**: 2019-06-01 03:14:15+00:00
- **Updated**: 2019-11-09 03:19:44+00:00
- **Authors**: Zhengyang Shen, François-Xavier Vialard, Marc Niethammer
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: We introduce a region-specific diffeomorphic metric mapping (RDMM) registration approach. RDMM is non-parametric, estimating spatio-temporal velocity fields which parameterize the sought-for spatial transformation. Regularization of these velocity fields is necessary. However, while existing non-parametric registration approaches, e.g., the large displacement diffeomorphic metric mapping (LDDMM) model, use a fixed spatially-invariant regularization our model advects a spatially-varying regularizer with the estimated velocity field, thereby naturally attaching a spatio-temporal regularizer to deforming objects. We explore a family of RDMM registration approaches: 1) a registration model where regions with separate regularizations are pre-defined (e.g., in an atlas space), 2) a registration model where a general spatially-varying regularizer is estimated, and 3) a registration model where the spatially-varying regularizer is obtained via an end-to-end trained deep learning (DL) model. We provide a variational derivation of RDMM, show that the model can assure diffeomorphic transformations in the continuum, and that LDDMM is a particular instance of RDMM. To evaluate RDMM performance we experiment 1) on synthetic 2D data and 2) on two 3D datasets: knee magnetic resonance images (MRIs) of the Osteoarthritis Initiative (OAI) and computed tomography images (CT) of the lung. Results show that our framework achieves state-of-the-art image registration performance, while providing additional information via a learned spatio-temoporal regularizer. Further, our deep learning approach allows for very fast RDMM and LDDMM estimations. Our code will be open-sourced. Code is available at https://github.com/uncbiag/registration.



### A synthetic dataset for deep learning
- **Arxiv ID**: http://arxiv.org/abs/1906.11905v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11905v1)
- **Published**: 2019-06-01 05:16:40+00:00
- **Updated**: 2019-06-01 05:16:40+00:00
- **Authors**: Xinjie Lan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel method for generating a synthetic dataset obeying Gaussian distribution. Compared to the commonly used benchmark datasets with unknown distribution, the synthetic dataset has an explicit distribution, i.e., Gaussian distribution. Meanwhile, it has the same characteristics as the benchmark dataset MNIST. As a result, we can easily apply Deep Neural Networks (DNNs) on the synthetic dataset. This synthetic dataset provides a novel experimental tool to verify the proposed theories of deep learning.



### Temporally Coherent Full 3D Mesh Human Pose Recovery from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/1906.00161v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1906.00161v2)
- **Published**: 2019-06-01 06:39:35+00:00
- **Updated**: 2019-07-01 03:53:35+00:00
- **Authors**: Jian Liu, Naveed Akhtar, Ajmal Mian
- **Comment**: Updated bibliography
- **Journal**: None
- **Summary**: Advances in Deep Learning have recently made it possible to recover full 3D meshes of human poses from individual images. However, extension of this notion to videos for recovering temporally coherent poses still remains unexplored. A major challenge in this regard is the lack of appropriately annotated video data for learning the desired deep models. Existing human pose datasets only provide 2D or 3D skeleton joint annotations, whereas the datasets are also recorded in constrained environments. We first contribute a technique to synthesize monocular action videos with rich 3D annotations that are suitable for learning computational models for full mesh 3D human pose recovery. Compared to the existing methods which simply "texture-map" clothes onto the 3D human pose models, our approach incorporates Physics based realistic cloth deformations with the human body movements. The generated videos cover a large variety of human actions, poses, and visual appearances, whereas the annotations record accurate human pose dynamics and human body surface information. Our second major contribution is an end-to-end trainable Recurrent Neural Network for full pose mesh recovery from monocular video. Using the proposed video data and LSTM based recurrent structure, our network explicitly learns to model the temporal coherence in videos and imposes geometric consistency over the recovered meshes. We establish the effectiveness of the proposed model with quantitative and qualitative analysis using the proposed and benchmark datasets.



### Learning to Transfer: Unsupervised Meta Domain Translation
- **Arxiv ID**: http://arxiv.org/abs/1906.00181v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00181v3)
- **Published**: 2019-06-01 08:24:26+00:00
- **Updated**: 2019-09-10 06:28:53+00:00
- **Authors**: Jianxin Lin, Yijun Wang, Tianyu He, Zhibo Chen
- **Comment**: 10 pages, 2 figures
- **Journal**: None
- **Summary**: Unsupervised domain translation has recently achieved impressive performance with Generative Adversarial Network (GAN) and sufficient (unpaired) training data. However, existing domain translation frameworks form in a disposable way where the learning experiences are ignored and the obtained model cannot be adapted to a new coming domain. In this work, we take on unsupervised domain translation problems from a meta-learning perspective. We propose a model called Meta-Translation GAN (MT-GAN) to find good initialization of translation models. In the meta-training procedure, MT-GAN is explicitly trained with a primary translation task and a synthesized dual translation task. A cycle-consistency meta-optimization objective is designed to ensure the generalization ability. We demonstrate effectiveness of our model on ten diverse two-domain translation tasks and multiple face identity translation tasks. We show that our proposed approach significantly outperforms the existing domain translation methods when each domain contains no more than ten training samples.



### ZstGAN: An Adversarial Approach for Unsupervised Zero-Shot Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1906.00184v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00184v2)
- **Published**: 2019-06-01 08:43:44+00:00
- **Updated**: 2021-07-20 05:05:04+00:00
- **Authors**: Jianxin Lin, Yingce Xia, Sen Liu, Shuqin Zhao, Zhibo Chen
- **Comment**: Accepted by Nuerocomputing
- **Journal**: None
- **Summary**: Image-to-image translation models have shown remarkable ability on transferring images among different domains. Most of existing work follows the setting that the source domain and target domain keep the same at training and inference phases, which cannot be generalized to the scenarios for translating an image from an unseen domain to another unseen domain. In this work, we propose the Unsupervised Zero-Shot Image-to-image Translation (UZSIT) problem, which aims to learn a model that can translate samples from image domains that are not observed during training. Accordingly, we propose a framework called ZstGAN: By introducing an adversarial training scheme, ZstGAN learns to model each domain with domain-specific feature distribution that is semantically consistent on vision and attribute modalities. Then the domain-invariant features are disentangled with an shared encoder for image generation. We carry out extensive experiments on CUB and FLO datasets, and the results demonstrate the effectiveness of proposed method on UZSIT task. Moreover, ZstGAN shows significant accuracy improvements over state-of-the-art zero-shot learning methods on CUB and FLO.



### Perceptual Evaluation of Adversarial Attacks for CNN-based Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.00204v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.00204v1)
- **Published**: 2019-06-01 11:26:22+00:00
- **Updated**: 2019-06-01 11:26:22+00:00
- **Authors**: Sid Ahmed Fezza, Yassine Bakhti, Wassim Hamidouche, Olivier Déforges
- **Comment**: Eleventh International Conference on Quality of Multimedia Experience
  (QoMEX 2019)
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have recently achieved state-of-the-art performance and provide significant progress in many machine learning tasks, such as image classification, speech processing, natural language processing, etc. However, recent studies have shown that DNNs are vulnerable to adversarial attacks. For instance, in the image classification domain, adding small imperceptible perturbations to the input image is sufficient to fool the DNN and to cause misclassification. The perturbed image, called \textit{adversarial example}, should be visually as close as possible to the original image. However, all the works proposed in the literature for generating adversarial examples have used the $L_{p}$ norms ($L_{0}$, $L_{2}$ and $L_{\infty}$) as distance metrics to quantify the similarity between the original image and the adversarial example. Nonetheless, the $L_{p}$ norms do not correlate with human judgment, making them not suitable to reliably assess the perceptual similarity/fidelity of adversarial examples. In this paper, we present a database for visual fidelity assessment of adversarial examples. We describe the creation of the database and evaluate the performance of fifteen state-of-the-art full-reference (FR) image fidelity assessment metrics that could substitute $L_{p}$ norms. The database as well as subjective scores are publicly available to help designing new metrics for adversarial examples and to facilitate future research works.



### RGB and LiDAR fusion based 3D Semantic Segmentation for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1906.00208v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.00208v2)
- **Published**: 2019-06-01 11:57:38+00:00
- **Updated**: 2019-07-17 16:28:00+00:00
- **Authors**: Khaled El Madawy, Hazem Rashed, Ahmad El Sallab, Omar Nasr, Hanan Kamel, Senthil Yogamani
- **Comment**: Accepted for Oral Presentation at IEEE Intelligent Transportation
  Systems Conference (ITSC) 2019
- **Journal**: None
- **Summary**: LiDAR has become a standard sensor for autonomous driving applications as they provide highly precise 3D point clouds. LiDAR is also robust for low-light scenarios at night-time or due to shadows where the performance of cameras is degraded. LiDAR perception is gradually becoming mature for algorithms including object detection and SLAM. However, semantic segmentation algorithm remains to be relatively less explored. Motivated by the fact that semantic segmentation is a mature algorithm on image data, we explore sensor fusion based 3D segmentation. Our main contribution is to convert the RGB image to a polar-grid mapping representation used for LiDAR and design early and mid-level fusion architectures. Additionally, we design a hybrid fusion architecture that combines both fusion algorithms. We evaluate our algorithm on KITTI dataset which provides segmentation annotation for cars, pedestrians and cyclists. We evaluate two state-of-the-art architectures namely SqueezeSeg and PointSeg and improve the mIoU score by 10 % in both cases relative to the LiDAR only baseline.



### Robust Learning Under Label Noise With Iterative Noise-Filtering
- **Arxiv ID**: http://arxiv.org/abs/1906.00216v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.00216v1)
- **Published**: 2019-06-01 12:34:41+00:00
- **Updated**: 2019-06-01 12:34:41+00:00
- **Authors**: Duc Tam Nguyen, Thi-Phuong-Nhung Ngo, Zhongyu Lou, Michael Klar, Laura Beggel, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of training a model under the presence of label noise. Current approaches identify samples with potentially incorrect labels and reduce their influence on the learning process by either assigning lower weights to them or completely removing them from the training set. In the first case the model however still learns from noisy labels; in the latter approach, good training data can be lost. In this paper, we propose an iterative semi-supervised mechanism for robust learning which excludes noisy labels but is still able to learn from the corresponding samples. To this end, we add an unsupervised loss term that also serves as a regularizer against the remaining label noise. We evaluate our approach on common classification tasks with different noise ratios. Our robust models outperform the state-of-the-art methods by a large margin. Especially for very large noise ratios, we achieve up to 20 % absolute improvement compared to the previous best model.



### A Semantic-based Medical Image Fusion Approach
- **Arxiv ID**: http://arxiv.org/abs/1906.00225v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00225v2)
- **Published**: 2019-06-01 14:13:02+00:00
- **Updated**: 2019-12-11 06:44:13+00:00
- **Authors**: Fanda Fan, Yunyou Huang, Lei Wang, Xingwang Xiong, Zihan Jiang, Zhifei Zhang, Jianfeng Zhan
- **Comment**: None
- **Journal**: None
- **Summary**: It is necessary for clinicians to comprehensively analyze patient information from different sources. Medical image fusion is a promising approach to providing overall information from medical images of different modalities. However, existing medical image fusion approaches ignore the semantics of images, making the fused image difficult to understand. In this work, we propose a new evaluation index to measure the semantic loss of fused image, and put forward a Fusion W-Net (FW-Net) for multimodal medical image fusion. The experimental results are promising: the fused image generated by our approach greatly reduces the semantic information loss, and has better visual effects in contrast to five state-of-art approaches. Our approach and tool have great potential to be applied in the clinical setting.



### Lung cancer screening with low-dose CT scans using a deep learning approach
- **Arxiv ID**: http://arxiv.org/abs/1906.00240v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00240v1)
- **Published**: 2019-06-01 15:19:34+00:00
- **Updated**: 2019-06-01 15:19:34+00:00
- **Authors**: Jason L. Causey, Yuanfang Guan, Wei Dong, Karl Walker, Jake A. Qualls, Fred Prior, Xiuzhen Huang
- **Comment**: 6 figures
- **Journal**: None
- **Summary**: Lung cancer is the leading cause of cancer deaths. Early detection through low-dose computed tomography (CT) screening has been shown to significantly reduce mortality but suffers from a high false positive rate that leads to unnecessary diagnostic procedures. Quantitative image analysis coupled to deep learning techniques has the potential to reduce this false positive rate. We conducted a computational analysis of 1449 low-dose CT studies drawn from the National Lung Screening Trial (NLST) cohort. We applied to this cohort our newly developed algorithm, DeepScreener, which is based on a novel deep learning approach. The algorithm, after the training process using about 3000 CT studies, does not require lung nodule annotations to conduct cancer prediction. The algorithm uses consecutive slices and multi-task features to determine whether a nodule is likely to be cancer, and a spatial pyramid to detect nodules at different scales. We find that the algorithm can predict a patient's cancer status from a volumetric lung CT image with high accuracy (78.2%, with area under the Receiver Operating Characteristic curve (AUC) of 0.858). Our preliminary framework ranked 16th of 1972 teams (top 1%) in the Data Science Bowl 2017 (DSB2017) competition, based on the challenge datasets. We report here the application of DeepScreener on an independent NLST test set. This study indicates that the deep learning approach has the potential to significantly reduce the false positive rate in lung cancer screening with low-dose CT scans.



### Super-resolution of Time-series Labels for Bootstrapped Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.00254v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SD, eess.AS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.00254v1)
- **Published**: 2019-06-01 16:29:50+00:00
- **Updated**: 2019-06-01 16:29:50+00:00
- **Authors**: Ivan Kiskin, Udeepa Meepegama, Steven Roberts
- **Comment**: Accepted at the Time-series workshop at ICML 2019, Long Beach
- **Journal**: None
- **Summary**: Solving real-world problems, particularly with deep learning, relies on the availability of abundant, quality data. In this paper we develop a novel framework that maximises the utility of time-series datasets that contain only small quantities of expertly-labelled data, larger quantities of weakly (or coarsely) labelled data and a large volume of unlabelled data. This represents scenarios commonly encountered in the real world, such as in crowd-sourcing applications. In our work, we use a nested loop using a Kernel Density Estimator (KDE) to super-resolve the abundant low-quality data labels, thereby enabling effective training of a Convolutional Neural Network (CNN). We demonstrate two key results: a) The KDE is able to super-resolve labels more accurately, and with better calibrated probabilities, than well-established classifiers acting as baselines; b) Our CNN, trained on super-resolved labels from the KDE, achieves an improvement in F1 score of 22.1% over the next best baseline system in our candidate problem domain.



### Enhancing Transformation-based Defenses using a Distribution Classifier
- **Arxiv ID**: http://arxiv.org/abs/1906.00258v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00258v2)
- **Published**: 2019-06-01 16:59:17+00:00
- **Updated**: 2020-01-30 05:34:00+00:00
- **Authors**: Connie Kou, Hwee Kuan Lee, Ee-Chien Chang, Teck Khim Ng
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks on convolutional neural networks (CNN) have gained significant attention and there have been active research efforts on defense mechanisms. Stochastic input transformation methods have been proposed, where the idea is to recover the image from adversarial attack by random transformation, and to take the majority vote as consensus among the random samples. However, the transformation improves the accuracy on adversarial images at the expense of the accuracy on clean images. While it is intuitive that the accuracy on clean images would deteriorate, the exact mechanism in which how this occurs is unclear. In this paper, we study the distribution of softmax induced by stochastic transformations. We observe that with random transformations on the clean images, although the mass of the softmax distribution could shift to the wrong class, the resulting distribution of softmax could be used to correct the prediction. Furthermore, on the adversarial counterparts, with the image transformation, the resulting shapes of the distribution of softmax are similar to the distributions from the clean images. With these observations, we propose a method to improve existing transformation-based defenses. We train a separate lightweight distribution classifier to recognize distinct features in the distributions of softmax outputs of transformed images. Our empirical studies show that our distribution classifier, by training on distributions obtained from clean images only, outperforms majority voting for both clean and adversarial images. Our method is generic and can be integrated with existing transformation-based defenses.



### Parametric Shape Modeling and Skeleton Extraction with Radial Basis Functions using Similarity Domains Network
- **Arxiv ID**: http://arxiv.org/abs/1906.00265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.00265v1)
- **Published**: 2019-06-01 18:34:10+00:00
- **Updated**: 2019-06-01 18:34:10+00:00
- **Authors**: Sedat Ozer
- **Comment**: This is the pre-print for the CVPR submission
- **Journal**: None
- **Summary**: We demonstrate the use of similarity domains (SDs) for shape modeling and skeleton extraction. SDs are recently proposed and they can be utilized in a neural network framework to help us analyze shapes. SDs are modeled with radial basis functions with varying shape parameters in Similarity Domains Networks (SDNs). In this paper, we demonstrate how using SDN can first help us model a pixel-based image in terms of SDs and then demonstrate how those learned SDs can be used to extract the skeleton of a shape.



### Natural Image Noise Dataset
- **Arxiv ID**: http://arxiv.org/abs/1906.00270v1
- **DOI**: 10.1109/CVPRW.2019.00228
- **Categories**: **eess.IV**, cs.CV, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1906.00270v1)
- **Published**: 2019-06-01 18:53:29+00:00
- **Updated**: 2019-06-01 18:53:29+00:00
- **Authors**: Benoit Brummer, Christophe De Vleeschouwer
- **Comment**: NTIRE at CVPR 2019
- **Journal**: None
- **Summary**: Convolutional neural networks have been the focus of research aiming to solve image denoising problems, but their performance remains unsatisfactory for most applications. These networks are trained with synthetic noise distributions that do not accurately reflect the noise captured by image sensors. Some datasets of clean-noisy image pairs have been introduced but they are usually meant for benchmarking or specific applications. We introduce the Natural Image Noise Dataset (NIND), a dataset of DSLR-like images with varying levels of ISO noise which is large enough to train models for blind denoising over a wide range of noise. We demonstrate a denoising model trained with the NIND and show that it significantly outperforms BM3D on ISO noise from unseen images, even when generalizing to images from a different type of camera. The Natural Image Noise Dataset is published on Wikimedia Commons such that it remains open for curation and contributions. We expect that this dataset will prove useful for future image denoising applications.



### Learning to Generate Grounded Visual Captions without Localization Supervision
- **Arxiv ID**: http://arxiv.org/abs/1906.00283v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.00283v3)
- **Published**: 2019-06-01 20:21:24+00:00
- **Updated**: 2020-07-17 23:56:28+00:00
- **Authors**: Chih-Yao Ma, Yannis Kalantidis, Ghassan AlRegib, Peter Vajda, Marcus Rohrbach, Zsolt Kira
- **Comment**: ECCV 2020. Code is available at
  https://github.com/chihyaoma/cyclical-visual-captioning
- **Journal**: None
- **Summary**: When automatically generating a sentence description for an image or video, it often remains unclear how well the generated caption is grounded, that is whether the model uses the correct image regions to output particular words, or if the model is hallucinating based on priors in the dataset and/or the language model. The most common way of relating image regions with words in caption models is through an attention mechanism over the regions that are used as input to predict the next word. The model must therefore learn to predict the attentional weights without knowing the word it should localize. This is difficult to train without grounding supervision since recurrent models can propagate past information and there is no explicit signal to force the captioning model to properly ground the individual decoded words. In this work, we help the model to achieve this via a novel cyclical training regimen that forces the model to localize each word in the image after the sentence decoder generates it, and then reconstruct the sentence from the localized image region(s) to match the ground-truth. Our proposed framework only requires learning one extra fully-connected layer (the localizer), a layer that can be removed at test time. We show that our model significantly improves grounding accuracy without relying on grounding supervision or introducing extra computation during inference, for both image and video captioning tasks. Code is available at https://github.com/chihyaoma/cyclical-visual-captioning .




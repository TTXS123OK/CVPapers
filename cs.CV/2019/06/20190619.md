# Arxiv Papers in cs.CV on 2019-06-19
### Event-based Star Tracking via Multiresolution Progressive Hough Transforms
- **Arxiv ID**: http://arxiv.org/abs/1906.07866v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07866v2)
- **Published**: 2019-06-19 01:17:05+00:00
- **Updated**: 2019-10-21 10:34:55+00:00
- **Authors**: Samya Bagchi, Tat-Jun Chin
- **Comment**: None
- **Journal**: None
- **Summary**: Star trackers are state-of-the-art attitude estimation devices which function by recognising and tracking star patterns. Most commercial star trackers use conventional optical sensors. A recent alternative is to use event sensors, which could enable more energy efficient and faster star trackers. However, this demands new algorithms that can efficiently cope with high-speed asynchronous data, and are feasible on resource-constrained computing platforms. To this end, we propose an event-based processing approach for star tracking. Our technique operates on the event stream from a star field, by using multiresolution Hough Transforms to time-progressively integrate event data and produce accurate relative rotations. Optimisation via rotation averaging is then used to fuse the relative rotations and jointly refine the absolute orientations. Our technique is designed to be feasible for asynchronous operation on standard hardware. Moreover, compared to state-of-the-art event-based motion estimation schemes, our technique is much more efficient and accurate.



### Analytical Derivatives for Differentiable Renderer: 3D Pose Estimation by Silhouette Consistency
- **Arxiv ID**: http://arxiv.org/abs/1906.07870v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1906.07870v1)
- **Published**: 2019-06-19 01:34:12+00:00
- **Updated**: 2019-06-19 01:34:12+00:00
- **Authors**: Zaiqiang Wu, Wei Jiang
- **Comment**: 19 pages, 8 figures
- **Journal**: None
- **Summary**: Differentiable render is widely used in optimization-based 3D reconstruction which requires gradients from differentiable operations for gradient-based optimization. The existing differentiable renderers obtain the gradients of rendering via numerical technique which is of low accuracy and efficiency. Motivated by this fact, a differentiable mesh renderer with analytical gradients is proposed. The main obstacle of rasterization based rendering being differentiable is the discrete sampling operation. To make the rasterization differentiable, the pixel intensity is defined as a double integral over the pixel area and the integral is approximated by anti-aliasing with an average filter. Then the analytical gradients with respect to the vertices coordinates can be derived from the continuous definition of pixel intensity. To demonstrate the effectiveness and efficiency of the proposed differentiable renderer, experiments of 3D pose estimation by only multi-viewpoint silhouettes were conducted. The experimental results show that 3D pose estimation without 3D and 2D joints supervision is capable of producing competitive results both qualitatively and quantitatively. The experimental results also show that the proposed differentiable renderer is of higher accuracy and efficiency compared with previous method of differentiable renderer.



### Unsupervised Learning of Object Structure and Dynamics from Videos
- **Arxiv ID**: http://arxiv.org/abs/1906.07889v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07889v3)
- **Published**: 2019-06-19 02:56:51+00:00
- **Updated**: 2020-03-02 17:32:13+00:00
- **Authors**: Matthias Minderer, Chen Sun, Ruben Villegas, Forrester Cole, Kevin Murphy, Honglak Lee
- **Comment**: 33rd Conference on Neural Information Processing Systems (NeurIPS
  2019), Vancouver, Canada
- **Journal**: None
- **Summary**: Extracting and predicting object structure and dynamics from videos without supervision is a major challenge in machine learning. To address this challenge, we adopt a keypoint-based image representation and learn a stochastic dynamics model of the keypoints. Future frames are reconstructed from the keypoints and a reference frame. By modeling dynamics in the keypoint coordinate space, we achieve stable learning and avoid compounding of errors in pixel space. Our method improves upon unstructured representations both for pixel-level video prediction and for downstream tasks requiring object-level understanding of motion dynamics. We evaluate our model on diverse datasets: a multi-agent sports dataset, the Human3.6M dataset, and datasets based on continuous control tasks from the DeepMind Control Suite. The spatially structured representation outperforms unstructured representations on a range of motion-related tasks such as object tracking, action recognition and reward prediction.



### Learning to Reconstruct and Understand Indoor Scenes from Sparse Views
- **Arxiv ID**: http://arxiv.org/abs/1906.07892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07892v1)
- **Published**: 2019-06-19 03:10:06+00:00
- **Updated**: 2019-06-19 03:10:06+00:00
- **Authors**: Jingyu Yang, Ji Xu, Kun Li, Yu-Kun Lai, Huanjing Yue, Jianzhi Lu, Hao Wu, Yebin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new method for simultaneous 3D reconstruction and semantic segmentation of indoor scenes. Unlike existing methods that require recording a video using a color camera and/or a depth camera, our method only needs a small number of (e.g., 3-5) color images from uncalibrated sparse views as input, which greatly simplifies data acquisition and extends applicable scenarios. Since different views have limited overlaps, our method allows a single image as input to discern the depth and semantic information of the scene. The key issue is how to recover relatively accurate depth from single images and reconstruct a 3D scene by fusing very few depth maps. To address this problem, we first design an iterative deep architecture, IterNet, that estimates depth and semantic segmentation alternately, so that they benefit each other. To deal with the little overlap and non-rigid transformation between views, we further propose a joint global and local registration method to reconstruct a 3D scene with semantic information from sparse views. We also make available a new indoor synthetic dataset simultaneously providing photorealistic high-resolution RGB images, accurate depth maps and pixel-level semantic labels for thousands of complex layouts, useful for training and evaluation. Experimental results on public datasets and our dataset demonstrate that our method achieves more accurate depth estimation, smaller semantic segmentation errors and better 3D reconstruction results, compared with state-of-the-art methods.



### Extended probabilistic Rand index and the adjustable moving window-based pixel-pair sampling method
- **Arxiv ID**: http://arxiv.org/abs/1906.07893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07893v1)
- **Published**: 2019-06-19 03:14:55+00:00
- **Updated**: 2019-06-19 03:14:55+00:00
- **Authors**: Hisashi Shimodaira
- **Comment**: 9 pages, 9 figures and 9tables
- **Journal**: None
- **Summary**: The probabilistic Rand (PR) index has the following three problems: It lacks variations in its value over images; the normalized probabilistic Rand (NPR) index to address this is theoretically unclear, and the sampling method of pixel-pairs was not proposed concretely. In this paper, we propose methods for solving these problems. First, we propose extended probabilistic Rand (EPR) index that considers not only similarity but also dissimilarity between segmentations. The EPR index provides twice as wide effective range as the PR index does. Second, we propose an adjustable moving window-based pixel-pair sampling (AWPS) method in which each pixel-pair is sampled adjustably by considering granularities of ground truth segmentations. Results of experiments show that the proposed methods work effectively and efficiently.



### Multimodal Abstractive Summarization for How2 Videos
- **Arxiv ID**: http://arxiv.org/abs/1906.07901v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1906.07901v1)
- **Published**: 2019-06-19 03:52:42+00:00
- **Updated**: 2019-06-19 03:52:42+00:00
- **Authors**: Shruti Palaskar, Jindrich Libovický, Spandana Gella, Florian Metze
- **Comment**: To appear in ACL 2019
- **Journal**: None
- **Summary**: In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to "compress" text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU.



### ViP: Virtual Pooling for Accelerating CNN-based Image Classification and Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.07912v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07912v2)
- **Published**: 2019-06-19 04:44:54+00:00
- **Updated**: 2020-02-20 06:25:43+00:00
- **Authors**: Zhuo Chen, Jiyuan Zhang, Ruizhou Ding, Diana Marculescu
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: In recent years, Convolutional Neural Networks (CNNs) have shown superior capability in visual learning tasks. While accuracy-wise CNNs provide unprecedented performance, they are also known to be computationally intensive and energy demanding for modern computer systems. In this paper, we propose Virtual Pooling (ViP), a model-level approach to improve speed and energy consumption of CNN-based image classification and object detection tasks, with a provable error bound. We show the efficacy of ViP through experiments on four CNN models, three representative datasets, both desktop and mobile platforms, and two visual learning tasks, i.e., image classification and object detection. For example, ViP delivers 2.1x speedup with less than 1.5% accuracy degradation in ImageNet classification on VGG-16, and 1.8x speedup with 0.025 mAP degradation in PASCAL VOC object detection with Faster-RCNN. ViP also reduces mobile GPU and CPU energy consumption by up to 55% and 70%, respectively. As a complementary method to existing acceleration approaches, ViP achieves 1.9x speedup on ThiNet leading to a combined speedup of 5.23x on VGG-16. Furthermore, ViP provides a knob for machine learning practitioners to generate a set of CNN models with varying trade-offs between system speed/energy consumption and accuracy to better accommodate the requirements of their tasks. Code is available at https://github.com/cmu-enyac/VirtualPooling.



### Predicting Confusion from Eye-Tracking Data with Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.11211v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.11211v1)
- **Published**: 2019-06-19 04:47:16+00:00
- **Updated**: 2019-06-19 04:47:16+00:00
- **Authors**: Shane D. Sims, Vanessa Putnam, Cristina Conati
- **Comment**: This work was presented at the 2nd Workshop on Humanizing AI (HAI) at
  IJCAI'19 in Macau, China
- **Journal**: None
- **Summary**: Encouraged by the success of deep learning in a variety of domains, we investigate the suitability and effectiveness of Recurrent Neural Networks (RNNs) in a domain where deep learning has not yet been used; namely detecting confusion from eye-tracking data. Through experiments with a dataset of user interactions with ValueChart (an interactive visualization tool), we found that RNNs learn a feature representation from the raw data that allows for a more powerful classifier than previous methods that use engineered features. This is evidenced by the stronger performance of the RNN (0.74/0.71 sensitivity/specificity), as compared to a Random Forest classifier (0.51/0.70 sensitivity/specificity), when both are trained on an un-augmented dataset. However, using engineered features allows for simple data augmentation methods to be used. These same methods are not as effective at augmentation for the feature representation learned from the raw data, likely due to an inability to match the temporal dynamics of the data.



### Automatic estimation of heading date of paddy rice using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1906.07917v1
- **DOI**: 10.1186/s13007-019-0457-1
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07917v1)
- **Published**: 2019-06-19 05:02:43+00:00
- **Updated**: 2019-06-19 05:02:43+00:00
- **Authors**: Sai Vikas Desai, Vineeth N Balasubramanian, Tokihiro Fukatsu, Seishi Ninomiya, Wei Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate estimation of heading date of paddy rice greatly helps the breeders to understand the adaptability of different crop varieties in a given location. The heading date also plays a vital role in determining grain yield for research experiments. Visual examination of the crop is laborious and time consuming. Therefore, quick and precise estimation of heading date of paddy rice is highly essential. In this work, we propose a simple pipeline to detect regions containing flowering panicles from ground level RGB images of paddy rice. Given a fixed region size for an image, the number of regions containing flowering panicles is directly proportional to the number of flowering panicles present. Consequently, we use the flowering panicle region counts to estimate the heading date of the crop. The method is based on image classification using Convolutional Neural Networks (CNNs). We evaluated the performance of our algorithm on five time series image sequences of three different varieties of rice crops. When compared to the previous work on this dataset, the accuracy and general versatility of the method has been improved and heading date has been estimated with a mean absolute error of less than 1 day.



### Imbalanced Learning-based Automatic SAR Images Change Detection by Morphologically Supervised PCA-Net
- **Arxiv ID**: http://arxiv.org/abs/1906.07923v1
- **DOI**: 10.1109/LGRS.2018.2878420
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.07923v1)
- **Published**: 2019-06-19 05:34:50+00:00
- **Updated**: 2019-06-19 05:34:50+00:00
- **Authors**: Rongfang Wang, Jie Zhang, Jia-Wei Chen, Licheng Jiao, Mi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Change detection is a quite challenging task due to the imbalance between unchanged and changed class. In addition, the traditional difference map generated by log-ratio is subject to the speckle, which will reduce the accuracy. In this letter, an imbalanced learning-based change detection is proposed based on PCA network (PCA-Net), where a supervised PCA-Net is designed to obtain the robust features directly from given multitemporal SAR images instead of a difference map. Furthermore, to tackle with the imbalance between changed and unchanged classes, we propose a morphologically supervised learning method, where the knowledge in the pixels near the boundary between two classes are exploited to guide network training. Finally, our proposed PCA-Net can be trained by the datasets with available reference maps and applied to a new dataset, which is quite practical in change detection projects. Our proposed method is verified on five sets of multiple temporal SAR images. It is demonstrated from the experiment results that with the knowledge in training samples from the boundary, the learned features benefit for change detection and make the proposed method outperforms than supervised methods trained by randomly drawing samples.



### SemanticAdv: Generating Adversarial Examples via Attribute-conditional Image Editing
- **Arxiv ID**: http://arxiv.org/abs/1906.07927v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07927v4)
- **Published**: 2019-06-19 05:55:16+00:00
- **Updated**: 2020-07-02 19:47:47+00:00
- **Authors**: Haonan Qiu, Chaowei Xiao, Lei Yang, Xinchen Yan, Honglak Lee, Bo Li
- **Comment**: To appear at ECCV 2020
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee "subtle perturbation" by limiting the $L_p$ norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate "unrestricted adversarial examples".   In particular, we propose an algorithm \emph{SemanticAdv} which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various "adversarial" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against \emph{real-world black-box} services such as Azure face verification service based on transferability.   To further demonstrate the applicability of \emph{SemanticAdv} beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.



### SAR Image Change Detection via Spatial Metric Learning with an Improved Mahalanobis Distance
- **Arxiv ID**: http://arxiv.org/abs/1906.07930v1
- **DOI**: 10.1109/LGRS.2019.2915251
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.07930v1)
- **Published**: 2019-06-19 06:10:58+00:00
- **Updated**: 2019-06-19 06:10:58+00:00
- **Authors**: Rongfang Wang, Jia-Wei Chen, Yule Wang, Licheng Jiao, Mi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The log-ratio (LR) operator has been widely employed to generate the difference image for synthetic aperture radar (SAR) image change detection. However, the difference image generated by this pixel-wise operator can be subject to SAR images speckle and unavoidable registration errors between bitemporal SAR images. In this letter, we proposed a spatial metric learning method to obtain a difference image more robust to the speckle by learning a metric from a set of constraint pairs. In the proposed method, spatial context is considered in constructing constraint pairs, each of which consists of patches in the same location of bitemporal SAR images. Then, a semi-definite positive metric matrix $\bf M$ can be obtained by the optimization with the max-margin criterion. Finally, we verify our proposed method on four challenging datasets of bitemporal SAR images. Experimental results demonstrate that the difference map obtained by our proposed method outperforms than other state-of-art methods.



### Learning Generalized Transformation Equivariant Representations via Autoencoding Transformations
- **Arxiv ID**: http://arxiv.org/abs/1906.08628v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08628v3)
- **Published**: 2019-06-19 06:17:56+00:00
- **Updated**: 2019-11-17 16:27:59+00:00
- **Authors**: Guo-Jun Qi, Liheng Zhang, Xiao Wang
- **Comment**: arXiv admin note: text overlap with arXiv:1903.10863
- **Journal**: None
- **Summary**: Transformation Equivariant Representations (TERs) aim to capture the intrinsic visual structures that equivary to various transformations by expanding the notion of {\em translation} equivariance underlying the success of Convolutional Neural Networks (CNNs). For this purpose, we present both deterministic AutoEncoding Transformations (AET) and probabilistic AutoEncoding Variational Transformations (AVT) models to learn visual representations from generic groups of transformations. While the AET is trained by directly decoding the transformations from the learned representations, the AVT is trained by maximizing the joint mutual information between the learned representation and transformations. This results in Generalized TERs (GTERs) equivariant against transformations in a more general fashion by capturing complex patterns of visual structures beyond the conventional linear equivariance under a transformation group. The presented approach can be extended to (semi-)supervised models by jointly maximizing the mutual information of the learned representation with both labels and transformations. Experiments demonstrate the proposed models outperform the state-of-the-art models in both unsupervised and (semi-)supervised tasks.



### A simple and effective postprocessing method for image classification
- **Arxiv ID**: http://arxiv.org/abs/1906.07934v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07934v1)
- **Published**: 2019-06-19 06:28:33+00:00
- **Updated**: 2019-06-19 06:28:33+00:00
- **Authors**: Yan Liu, Yun Li, Yunhao Yuan, jipeng qiang
- **Comment**: None
- **Journal**: None
- **Summary**: Whether it is computer vision, natural language processing or speech recognition, the essence of these applications is to obtain powerful feature representations that make downstream applications completion more efficient. Taking image recognition as an example, whether it is hand-crafted low-level feature representation or feature representation extracted by a convolutional neural networks(CNNs), the goal is to extract features that better represent image features, thereby improving classification accuracy. However, we observed that image feature representations share a large common vector and a few top dominating directions. To address this problems, we propose a simple but effective postprocessing method to render off-the-shelf feature representations even stronger by eliminating the common mean vector from off-the-shelf feature representations. The postprocessing is empirically validated on a variety of datasets and feature extraction methods.such as VGG, LBP, and HOG. Some experiments show that the features that have been post-processed by postprocessing algorithm can get better results than original ones.



### An Action Recognition network for specific target based on rMC and RPN
- **Arxiv ID**: http://arxiv.org/abs/1906.07944v1
- **DOI**: 10.1088/1742-6596/1325/1/012073
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07944v1)
- **Published**: 2019-06-19 07:09:41+00:00
- **Updated**: 2019-06-19 07:09:41+00:00
- **Authors**: Mingjie Li, Youqian Feng, Zhonghai Yin, Cheng Zhou, Fanghao Dong, Yuan Lin, Yuhao Dong
- **Comment**: 10 pages, 14 figures
- **Journal**: None
- **Summary**: The traditional methods of action recognition are not specific for the operator, thus results are easy to be disturbed when other actions are operated in videos. The network based on mixed convolutional resnet and RPN is proposed in this paper. The rMC is tested in the data set of UCF-101 to compare with the method of R3D. The result shows that its correct rate reaches 71.07%. Meanwhile, the action recognition network is tested in our gesture and body posture data sets for specific target. The simulation achieves a good performance in which the running speed reaches 200 FPS. Finally, our model is improved by introducing the regression block and performs better, which shows the great potential of this model.



### Generative approach to unsupervised deep local learning
- **Arxiv ID**: http://arxiv.org/abs/1906.07947v2
- **DOI**: 10.1117/1.JEI.28.4.043005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07947v2)
- **Published**: 2019-06-19 07:19:06+00:00
- **Updated**: 2019-06-28 07:54:52+00:00
- **Authors**: Changlu Chen, Chaoxi Niu, Xia Zhan, Kun Zhan
- **Comment**: None
- **Journal**: Journal of Electronic Imaging, 2019
- **Summary**: Most existing feature learning methods optimize inflexible handcrafted features and the affinity matrix is constructed by shallow linear embedding methods. Different from these conventional methods, we pretrain a generative neural network by stacking convolutional autoencoders to learn the latent data representation and then construct an affinity graph with them as a prior. Based on the pretrained model and the constructed graph, we add a self-expressive layer to complete the generative model and then fine-tune it with a new loss function, including the reconstruction loss and a deliberately defined locality-preserving loss. The locality-preserving loss designed by the constructed affinity graph serves as prior to preserve the local structure during the fine-tuning stage, which in turn improves the quality of feature representation effectively. Furthermore, the self-expressive layer between the encoder and decoder is based on the assumption that each latent feature is a linear combination of other latent features, so the weighted combination coefficients of the self-expressive layer are used to construct a new refined affinity graph for representing the data structure. We conduct experiments on four datasets to demonstrate the superiority of the representation ability of our proposed model over the state-of-the-art methods.



### Artistic Enhancement and Style Transfer of Image Edges using Directional Pseudo-coloring
- **Arxiv ID**: http://arxiv.org/abs/1906.07981v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07981v1)
- **Published**: 2019-06-19 09:12:06+00:00
- **Updated**: 2019-06-19 09:12:06+00:00
- **Authors**: Shouvik Mani
- **Comment**: None
- **Journal**: 2nd Workshop on Humanizing AI (HAI), IJCAI 2019
- **Summary**: Computing the gradient of an image is a common step in computer vision pipelines. The image gradient quantifies the magnitude and direction of edges in an image and is used in creating features for downstream machine learning tasks. Typically, the image gradient is represented as a grayscale image. This paper introduces directional pseudo-coloring, an approach to color the image gradient in a deliberate and coherent manner. By pseudo-coloring the image gradient magnitude with the image gradient direction, we can enhance the visual quality of image edges and achieve an artistic transformation of the original image. Additionally, we present a simple style transfer pipeline which learns a color map from a style image and then applies that color map to color the edges of a content image through the directional pseudo-coloring technique. Code for the algorithms and experiments is available at https://github.com/shouvikmani/edge-colorizer.



### Cloud-based Image Classification Service Is Not Robust To Simple Transformations: A Forgotten Battlefield
- **Arxiv ID**: http://arxiv.org/abs/1906.07997v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07997v2)
- **Published**: 2019-06-19 09:36:17+00:00
- **Updated**: 2020-01-09 06:03:09+00:00
- **Authors**: Dou Goodman, Tao Wei
- **Comment**: arXiv admin note: text overlap with arXiv:1901.01223,
  arXiv:1704.05051, arXiv:1801.02612 by other authors
- **Journal**: None
- **Summary**: Many recent works demonstrated that Deep Learning models are vulnerable to adversarial examples.Fortunately, generating adversarial examples usually requires white-box access to the victim model, and the attacker can only access the APIs opened by cloud platforms. Thus, keeping models in the cloud can usually give a (false) sense of security.Unfortunately, cloud-based image classification service is not robust to simple transformations such as Gaussian Noise, Salt-and-Pepper Noise, Rotation and Monochromatization. In this paper,(1) we propose one novel attack method called Image Fusion(IF) attack, which achieve a high bypass rate,can be implemented only with OpenCV and is difficult to defend; and (2) we make the first attempt to conduct an extensive empirical study of Simple Transformation (ST) attacks against real-world cloud-based classification services. Through evaluations on four popular cloud platforms including Amazon, Google, Microsoft, Clarifai, we demonstrate that ST attack has a success rate of approximately 100% except Amazon approximately 50%, IF attack have a success rate over 98% among different classification services. (3) We discuss the possible defenses to address these security challenges.Experiments show that our defense technology can effectively defend known ST attacks.



### Back to Simplicity: How to Train Accurate BNNs from Scratch?
- **Arxiv ID**: http://arxiv.org/abs/1906.08637v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08637v1)
- **Published**: 2019-06-19 10:32:45+00:00
- **Updated**: 2019-06-19 10:32:45+00:00
- **Authors**: Joseph Bethge, Haojin Yang, Marvin Bornstein, Christoph Meinel
- **Comment**: Supplementary Material can be found
  https://owncloud.hpi.de/s/1jrAUnqRAfg0TXH. arXiv admin note: substantial text
  overlap with arXiv:1812.01965
- **Journal**: None
- **Summary**: Binary Neural Networks (BNNs) show promising progress in reducing computational and memory costs but suffer from substantial accuracy degradation compared to their real-valued counterparts on large-scale datasets, e.g., ImageNet. Previous work mainly focused on reducing quantization errors of weights and activations, whereby a series of approximation methods and sophisticated training tricks have been proposed. In this work, we make several observations that challenge conventional wisdom. We revisit some commonly used techniques, such as scaling factors and custom gradients, and show that these methods are not crucial in training well-performing BNNs. On the contrary, we suggest several design principles for BNNs based on the insights learned and demonstrate that highly accurate BNNs can be trained from scratch with a simple training strategy. We propose a new BNN architecture BinaryDenseNet, which significantly surpasses all existing 1-bit CNNs on ImageNet without tricks. In our experiments, BinaryDenseNet achieves 18.6% and 7.6% relative improvement over the well-known XNOR-Network and the current state-of-the-art Bi-Real Net in terms of top-1 accuracy on ImageNet, respectively.



### Automatic Scale Estimation of Structure from Motion based 3D Models using Laser Scalers
- **Arxiv ID**: http://arxiv.org/abs/1906.08019v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08019v1)
- **Published**: 2019-06-19 10:52:54+00:00
- **Updated**: 2019-06-19 10:52:54+00:00
- **Authors**: Klemen Istenic, Nuno Gracias, Aurelien Arnaubec, Javier Escartin, Rafael Garcia
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in structure-from-motion techniques are enabling many scientific fields to benefit from the routine creation of detailed 3D models. However, for a large number of applications, only a single camera is available, due to cost or space constraints in the survey platforms. Monocular structure-from-motion raises the issue of properly estimating the scale of the 3D models, in order to later use those models for metrology. The scale can be determined from the presence of visible objects of known dimensions, or from information on the magnitude of the camera motion provided by other sensors, such as GPS.   This paper addresses the problem of accurately scaling 3D models created from monocular cameras in GPS-denied environments, such as in underwater applications. Motivated by the common availability of underwater laser scalers, we present two novel approaches. A fully-calibrated method enables the use of arbitrary laser setups, while a partially-calibrated method reduces the need for calibration by only assuming parallelism on the laser beams, with no constraints on the camera. The proposed methods have several advantages with respect to the existing methods. The need for laser alignment with the optical axis of the camera is removed, together with the extremely error-prone manual identification of image points on the 3D model.   The performance of the methods and their applicability was evaluated on both data generated from a realistic 3D model and data collected during an oceanographic cruise in 2017. Three separate laser configurations have been tested, encompassing nearly all possible laser setups, to evaluate the effects of terrain roughness, noise, camera perspective angle and camera-scene distance. In the real scenario, the computation of 6 independent model scale estimates using our fully-calibrated approach, produced values with standard deviation of 0.3%.



### XNAS: Neural Architecture Search with Expert Advice
- **Arxiv ID**: http://arxiv.org/abs/1906.08031v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08031v1)
- **Published**: 2019-06-19 12:00:00+00:00
- **Updated**: 2019-06-19 12:00:00+00:00
- **Authors**: Niv Nayman, Asaf Noy, Tal Ridnik, Itamar Friedman, Rong Jin, Lihi Zelnik-Manor
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel optimization method for differential neural architecture search, based on the theory of prediction with expert advice. Its optimization criterion is well fitted for an architecture-selection, i.e., it minimizes the regret incurred by a sub-optimal selection of operations. Unlike previous search relaxations, that require hard pruning of architectures, our method is designed to dynamically wipe out inferior architectures and enhance superior ones. It achieves an optimal worst-case regret bound and suggests the use of multiple learning-rates, based on the amount of information carried by the backward gradients. Experiments show that our algorithm achieves a strong performance over several image classification datasets. Specifically, it obtains an error rate of 1.6% for CIFAR-10, 24% for ImageNet under mobile settings, and achieves state-of-the-art results on three additional datasets.



### Performance Evaluation Methodology for Long-Term Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1906.08675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08675v1)
- **Published**: 2019-06-19 12:38:21+00:00
- **Updated**: 2019-06-19 12:38:21+00:00
- **Authors**: Alan Lukežič, Luka Čehovin Zajc, Tomáš Vojíř, Jiří Matas, Matej Kristan
- **Comment**: Submitted to a journal on June 2018. arXiv admin note: substantial
  text overlap with arXiv:1804.07056
- **Journal**: None
- **Summary**: A long-term visual object tracking performance evaluation methodology and a benchmark are proposed. Performance measures are designed by following a long-term tracking definition to maximize the analysis probing strength. The new measures outperform existing ones in interpretation potential and in better distinguishing between different tracking behaviors. We show that these measures generalize the short-term performance measures, thus linking the two tracking problems. Furthermore, the new measures are highly robust to temporal annotation sparsity and allow annotation of sequences hundreds of times longer than in the current datasets without increasing manual annotation labor. A new challenging dataset of carefully selected sequences with many target disappearances is proposed. A new tracking taxonomy is proposed to position trackers on the short-term/long-term spectrum. The benchmark contains an extensive evaluation of the largest number of long-term tackers and comparison to state-of-the-art short-term trackers. We analyze the influence of tracking architecture implementations to long-term performance and explore various re-detection strategies as well as influence of visual model update strategies to long-term tracking drift. The methodology is integrated in the VOT toolkit to automate experimental analysis and benchmarking and to facilitate future development of long-term trackers.



### Monocular 3D Object Detection and Box Fitting Trained End-to-End Using Intersection-over-Union Loss
- **Arxiv ID**: http://arxiv.org/abs/1906.08070v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.08070v2)
- **Published**: 2019-06-19 12:39:16+00:00
- **Updated**: 2019-06-20 09:26:37+00:00
- **Authors**: Eskil Jörgensen, Christopher Zach, Fredrik Kahl
- **Comment**: For associated videos file, see http://tinyurl.com/SS3D-YouTube ;
  without supplementary material
- **Journal**: None
- **Summary**: Three-dimensional object detection from a single view is a challenging task which, if performed with good accuracy, is an important enabler of low-cost mobile robot perception. Previous approaches to this problem suffer either from an overly complex inference engine or from an insufficient detection accuracy. To deal with these issues, we present SS3D, a single-stage monocular 3D object detector. The framework consists of (i) a CNN, which outputs a redundant representation of each relevant object in the image with corresponding uncertainty estimates, and (ii) a 3D bounding box optimizer. We show how modeling heteroscedastic uncertainty improves performance upon our baseline, and furthermore, how back-propagation can be done through the optimizer in order to train the pipeline end-to-end for additional accuracy. Our method achieves SOTA accuracy on monocular 3D object detection, while running at 20 fps in a straightforward implementation. We argue that the SS3D architecture provides a solid framework upon which high performing detection systems can be built, with autonomous driving being the main application in mind.



### PoseConvGRU: A Monocular Approach for Visual Ego-motion Estimation by Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.08095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.08095v1)
- **Published**: 2019-06-19 13:42:35+00:00
- **Updated**: 2019-06-19 13:42:35+00:00
- **Authors**: Guangyao Zhai, Liang Liu, Linjian Zhang, Yong Liu
- **Comment**: 33 pages,12 figures
- **Journal**: None
- **Summary**: While many visual ego-motion algorithm variants have been proposed in the past decade, learning based ego-motion estimation methods have seen an increasing attention because of its desirable properties of robustness to image noise and camera calibration independence. In this work, we propose a data-driven approach of fully trainable visual ego-motion estimation for a monocular camera. We use an end-to-end learning approach in allowing the model to map directly from input image pairs to an estimate of ego-motion (parameterized as 6-DoF transformation matrices). We introduce a novel two-module Long-term Recurrent Convolutional Neural Networks called PoseConvGRU, with an explicit sequence pose estimation loss to achieve this. The feature-encoding module encodes the short-term motion feature in an image pair, while the memory-propagating module captures the long-term motion feature in the consecutive image pairs. The visual memory is implemented with convolutional gated recurrent units, which allows propagating information over time. At each time step, two consecutive RGB images are stacked together to form a 6 channels tensor for module-1 to learn how to extract motion information and estimate poses. The sequence of output maps is then passed through a stacked ConvGRU module to generate the relative transformation pose of each image pair. We also augment the training data by randomly skipping frames to simulate the velocity variation which results in a better performance in turning and high-velocity situations. We evaluate the performance of our proposed approach on the KITTI Visual Odometry benchmark. The experiments show a competitive performance of the proposed method to the geometric method and encourage further exploration of learning based methods for the purpose of estimating camera ego-motion even though geometrical methods demonstrate promising results.



### Model-based Deep Medical Imaging: the roadmap of generalizing iterative reconstruction model using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1906.08143v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP, math.OC, physics.med-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08143v4)
- **Published**: 2019-06-19 15:10:59+00:00
- **Updated**: 2019-09-05 15:42:47+00:00
- **Authors**: Jing Cheng, Haifeng Wang, Yanjie Zhu, Qiegen Liu, Qiyang Zhang, Ting Su, Jianwei Chen, Yongshuai Ge, Zhanli Hu, Xin Liu, Hairong Zheng, Leslie Ying, Dong Liang
- **Comment**: part of the preliminary work will be presented at MICCAI2019
- **Journal**: None
- **Summary**: Medical imaging is playing a more and more important role in clinics. However, there are several issues in different imaging modalities such as slow imaging speed in MRI, radiation injury in CT and PET. Therefore, accelerating MRI, reducing radiation dose in CT and PET have been ongoing research topics since their invention. Usually, acquiring less data is a direct but important strategy to address these issues. However, less acquisition usually results in aliasing artifacts in reconstructions. Recently, deep learning (DL) has been introduced in medical image reconstruction and shown potential on significantly speeding up MR reconstruction and reducing radiation dose. In this paper, we propose a general framework on combining the reconstruction model with deep learning to maximize the potential of deep learning and model-based reconstruction, and give the examples to demonstrate the performance and requirements of unrolling different algorithms using deep learning.



### Automated Definition of Skeletal Disease Burden in Metastatic Prostate Carcinoma: a 3D analysis of SPECT/CT images
- **Arxiv ID**: http://arxiv.org/abs/1906.08200v1
- **DOI**: None
- **Categories**: **q-bio.TO**, cs.CV, 92C55, 92C50, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1906.08200v1)
- **Published**: 2019-06-19 16:23:09+00:00
- **Updated**: 2019-06-19 16:23:09+00:00
- **Authors**: Francesco Fiz, Helmut Dittmann, Cristina Campi, Matthias Weissinger, Samine Sahbai, Matthias Reimold, Arnulf Stenzl, Michele Piana, Gianmario Sambuceti, Christian la Fougère
- **Comment**: None
- **Journal**: None
- **Summary**: To meet the current need for skeletal tumor-load estimation in prostate cancer (mCRPC), we developed a novel approach, based on adaptive bone segmentation. In this study, we compared the program output with existing estimates and with the radiological outcome. Seventy-six whole-body 99mTc-DPD-SPECT/CT from mCRPC patients were analyzed. The software identified the whole skeletal volume (SVol) and classified it voxels metastases (MVol) or normal bone (BVol). SVol was compared with the estimation of a commercial software. MVol was compared with manual assessment and with PSA-level. Counts/voxel were extracted from MVol and BVol. After six cycles of 223RaCl2-therapy every patient was re-evaluated as progressing (PD), stabilized (SD) or responsive (PR). SVol correlated with the one of the commercial software (R=0,99, p<0,001). MVol correlated with manually-counted lesions (R=0,61, p<0,001) and PSA (R=0,46, p<0.01). PD had a lower counts/voxel in MVol than PR/SD (715 \pm 190 Vs. 975 \pm 215 and 1058 \pm 255, p<0,05 and p<0,01) and in BVol (PD 275 \pm 60, PR 515 \pm 188 and SD 528 \pm 162 counts/voxel, p<0,001). Segmentation-based tumor load correlated with radiological/laboratory indices. Uptake was linked with the clinical outcome, suggesting that metastases in PD patients have a lower affinity for bone-seeking radionuclides and might benefit less from bone-targeted radioisotope therapies.



### PyRobot: An Open-source Robotics Framework for Research and Benchmarking
- **Arxiv ID**: http://arxiv.org/abs/1906.08236v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.08236v1)
- **Published**: 2019-06-19 17:35:43+00:00
- **Updated**: 2019-06-19 17:35:43+00:00
- **Authors**: Adithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces PyRobot, an open-source robotics framework for research and benchmarking. PyRobot is a light-weight, high-level interface on top of ROS that provides a consistent set of hardware independent mid-level APIs to control different robots. PyRobot abstracts away details about low-level controllers and inter-process communication, and allows non-robotics researchers (ML, CV researchers) to focus on building high-level AI applications. PyRobot aims to provide a research ecosystem with convenient access to robotics datasets, algorithm implementations and models that can be used to quickly create a state-of-the-art baseline. We believe PyRobot, when paired up with low-cost robot platforms such as LoCoBot, will reduce the entry barrier into robotics, and democratize robotics. PyRobot is open-source, and can be accessed via https://pyrobot.org.



### Neural Point-Based Graphics
- **Arxiv ID**: http://arxiv.org/abs/1906.08240v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08240v3)
- **Published**: 2019-06-19 17:38:45+00:00
- **Updated**: 2020-04-05 21:38:47+00:00
- **Authors**: Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new point-based approach for modeling the appearance of real scenes. The approach uses a raw point cloud as the geometric representation of a scene, and augments each point with a learnable neural descriptor that encodes local geometry and appearance. A deep rendering network is learned in parallel with the descriptors, so that new views of the scene can be obtained by passing the rasterizations of a point cloud from new viewpoints through this network. The input rasterizations use the learned descriptors as point pseudo-colors. We show that the proposed approach can be used for modeling complex scenes and obtaining their photorealistic views, while avoiding explicit surface estimation and meshing. In particular, compelling results are obtained for scene scanned using hand-held commodity RGB-D sensors as well as standard RGB cameras even in the presence of objects that are challenging for standard mesh-based modeling.



### SwiftNet: Using Graph Propagation as Meta-knowledge to Search Highly Representative Neural Architectures
- **Arxiv ID**: http://arxiv.org/abs/1906.08305v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.08305v2)
- **Published**: 2019-06-19 19:00:12+00:00
- **Updated**: 2019-06-25 23:33:24+00:00
- **Authors**: Hsin-Pai Cheng, Tunhou Zhang, Yukun Yang, Feng Yan, Shiyu Li, Harris Teague, Hai Li, Yiran Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Designing neural architectures for edge devices is subject to constraints of accuracy, inference latency, and computational cost. Traditionally, researchers manually craft deep neural networks to meet the needs of mobile devices. Neural Architecture Search (NAS) was proposed to automate the neural architecture design without requiring extensive domain expertise and significant manual efforts. Recent works utilized NAS to design mobile models by taking into account hardware constraints and achieved state-of-the-art accuracy with fewer parameters and less computational cost measured in Multiply-accumulates (MACs). To find highly compact neural architectures, existing works relies on predefined cells and directly applying width multiplier, which may potentially limit the model flexibility, reduce the useful feature map information, and cause accuracy drop. To conquer this issue, we propose GRAM(GRAph propagation as Meta-knowledge) that adopts fine-grained (node-wise) search method and accumulates the knowledge learned in updates into a meta-graph. As a result, GRAM can enable more flexible search space and achieve higher search efficiency. Without the constraints of predefined cell or blocks, we propose a new structure-level pruning method to remove redundant operations in neural architectures. SwiftNet, which is a set of models discovered by GRAM, outperforms MobileNet-V2 by 2.15x higher accuracy density and 2.42x faster with similar accuracy. Compared with FBNet, SwiftNet reduces the search cost by 26x and achieves 2.35x higher accuracy density and 1.47x speedup while preserving similar accuracy. SwiftNetcan obtain 63.28% top-1 accuracy on ImageNet-1K with only 53M MACs and 2.07M parameters. The corresponding inference latency is only 19.09 ms on Google Pixel 1.



### Light Field Saliency Detection with Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.08331v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.08331v2)
- **Published**: 2019-06-19 20:03:49+00:00
- **Updated**: 2019-10-29 09:17:08+00:00
- **Authors**: Jun Zhang, Yamei Liu, Shengping Zhang, Ronald Poppe, Meng Wang
- **Comment**: 14 pages, 14 figures
- **Journal**: None
- **Summary**: Light field imaging presents an attractive alternative to RGB imaging because of the recording of the direction of the incoming light. The detection of salient regions in a light field image benefits from the additional modeling of angular patterns. For RGB imaging, methods using CNNs have achieved excellent results on a range of tasks, including saliency detection. However, it is not trivial to use CNN-based methods for saliency detection on light field images because these methods are not specifically designed for processing light field inputs. In addition, current light field datasets are not sufficiently large to train CNNs. To overcome these issues, we present a new Lytro Illum dataset, which contains 640 light fields and their corresponding ground-truth saliency maps. Compared to current light field saliency datasets [1], [2], our new dataset is larger, of higher quality, contains more variation and more types of light field inputs. This makes our dataset suitable for training deeper networks and benchmarking. Furthermore, we propose a novel end-to-end CNN-based framework for light field saliency detection. Specifically, we propose three novel MAC (Model Angular Changes) blocks to process light field micro-lens images. We systematically study the impact of different architecture variants and compare light field saliency with regular 2D saliency. Our extensive comparisons indicate that our novel network significantly outperforms state-of-the-art methods on the proposed dataset and has desired generalization abilities on other existing datasets.



### A Strong Baseline and Batch Normalization Neck for Deep Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1906.08332v2
- **DOI**: 10.1109/TMM.2019.2958756
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08332v2)
- **Published**: 2019-06-19 20:12:20+00:00
- **Updated**: 2020-01-07 10:25:19+00:00
- **Authors**: Hao Luo, Wei Jiang, Youzhi Gu, Fuxu Liu, Xingyu Liao, Shenqi Lai, Jianyang Gu
- **Comment**: Accepted by IEEE Transactions on Multimedia. This is the submitted
  journal version of the oral paper [arXiv:1903.07071] in CVPRW'19. Code are
  avaliable at: https://github.com/michuanhaohao/reid-strong-baseline
- **Journal**: None
- **Summary**: This study explores a simple but strong baseline for person re-identification (ReID). Person ReID with deep neural networks has progressed and achieved high performance in recent years. However, many state-of-the-art methods design complex network structures and concatenate multi-branch features. In the literature, some effective training tricks briefly appear in several papers or source codes. The present study collects and evaluates these effective training tricks in person ReID. By combining these tricks, the model achieves 94.5% rank-1 and 85.9% mean average precision on Market1501 with only using the global features of ResNet50. The performance surpasses all existing global- and part-based baselines in person ReID. We propose a novel neck structure named as batch normalization neck (BNNeck). BNNeck adds a batch normalization layer after global pooling layer to separate metric and classification losses into two different feature spaces because we observe they are inconsistent in one embedding space. Extended experiments show that BNNeck can boost the baseline, and our baseline can improve the performance of existing state-of-the-art methods. Our codes and models are available at: https://github.com/michuanhaohao/reid-strong-baseline.



### 2D Linear Time-Variant Controller for Human's Intention Detection for Reach-to-Grasp Trajectories in Novel Scenes
- **Arxiv ID**: http://arxiv.org/abs/1906.08380v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.08380v2)
- **Published**: 2019-06-19 22:01:36+00:00
- **Updated**: 2019-07-16 19:04:15+00:00
- **Authors**: Claudio Zito, Tomasz Deregowski, Rustam Stolkin
- **Comment**: None
- **Journal**: In Proc. of the Workshop on Task-Informed Grasping (TIG-II): From
  Perception to Physical Interaction, Robotics: Science and Systems (RSS), 2019
- **Summary**: Designing robotic assistance devices for manipulation tasks is challenging. This work is concerned with improving accuracy and usability of semi-autonomous robots, such as human operated manipulators or exoskeletons. The key insight is to develop a system that takes into account context- and user-awareness to take better decisions in how to assist the user. The context-awareness is implemented by enabling the system to automatically generate a set of candidate grasps and reach-to-grasp trajectories in novel, cluttered scenes. The user-awareness is implemented as a linear time-variant feedback controller to facilitate the motion towards the most promising grasp. Our approach is demonstrated in a simple 2D example in which participants are asked to grasp a specific object in a clutter scene. Our approach also reduce the number of controllable dimensions for the user by providing only control on x- and y-axis, while orientation of the end-effector and the pose of its fingers are inferred by the system. The experimental results show the benefits of our approach in terms of accuracy and execution time with respect to a pure manual control.



### Training on test data: Removing near duplicates in Fashion-MNIST
- **Arxiv ID**: http://arxiv.org/abs/1906.08255v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08255v1)
- **Published**: 2019-06-19 22:09:47+00:00
- **Updated**: 2019-06-19 22:09:47+00:00
- **Authors**: Christopher Geier
- **Comment**: None
- **Journal**: None
- **Summary**: MNIST and Fashion MNIST are extremely popular for testing in the machine learning space. Fashion MNIST improves on MNIST by introducing a harder problem, increasing the diversity of testing sets, and more accurately representing a modern computer vision task. In order to increase the data quality of FashionMNIST, this paper investigates near duplicate images between training and testing sets. Near-duplicates between testing and training sets artificially increase the testing accuracy of machine learning models. This paper identifies near-duplicate images in Fashion MNIST and proposes a dataset with near-duplicates removed.



### Unsupervised Learning of Object Keypoints for Perception and Control
- **Arxiv ID**: http://arxiv.org/abs/1906.11883v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.11883v2)
- **Published**: 2019-06-19 23:58:54+00:00
- **Updated**: 2019-11-20 00:27:24+00:00
- **Authors**: Tejas Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew Zisserman, Volodymyr Mnih
- **Comment**: In NeurIPS 2019. Code
  https://github.com/deepmind/deepmind-research/tree/master/transporter
- **Journal**: None
- **Summary**: The study of object representations in computer vision has primarily focused on developing representations that are useful for image classification, object detection, or semantic segmentation as downstream tasks. In this work we aim to learn object representations that are useful for control and reinforcement learning (RL). To this end, we introduce Transporter, a neural network architecture for discovering concise geometric object representations in terms of keypoints or image-space coordinates. Our method learns from raw video frames in a fully unsupervised manner, by transporting learnt image features between video frames using a keypoint bottleneck. The discovered keypoints track objects and object parts across long time-horizons more accurately than recent similar methods. Furthermore, consistent long-term tracking enables two notable results in control domains -- (1) using the keypoint co-ordinates and corresponding image features as inputs enables highly sample-efficient reinforcement learning; (2) learning to explore by controlling keypoint locations drastically reduces the search space, enabling deep exploration (leading to states unreachable through random action exploration) without any extrinsic rewards.




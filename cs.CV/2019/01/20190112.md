# Arxiv Papers in cs.CV on 2019-01-12
### DeepSpline: Data-Driven Reconstruction of Parametric Curves and Surfaces
- **Arxiv ID**: http://arxiv.org/abs/1901.03781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03781v1)
- **Published**: 2019-01-12 02:20:09+00:00
- **Updated**: 2019-01-12 02:20:09+00:00
- **Authors**: Jun Gao, Chengcheng Tang, Vignesh Ganapathi-Subramanian, Jiahui Huang, Hao Su, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstruction of geometry based on different input modes, such as images or point clouds, has been instrumental in the development of computer aided design and computer graphics. Optimal implementations of these applications have traditionally involved the use of spline-based representations at their core. Most such methods attempt to solve optimization problems that minimize an output-target mismatch. However, these optimization techniques require an initialization that is close enough, as they are local methods by nature. We propose a deep learning architecture that adapts to perform spline fitting tasks accordingly, providing complementary results to the aforementioned traditional methods. We showcase the performance of our approach, by reconstructing spline curves and surfaces based on input images or point clouds.



### UPSNet: A Unified Panoptic Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/1901.03784v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03784v2)
- **Published**: 2019-01-12 02:39:03+00:00
- **Updated**: 2019-04-03 22:49:57+00:00
- **Authors**: Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min Bai, Ersin Yumer, Raquel Urtasun
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we propose a unified panoptic segmentation network (UPSNet) for tackling the newly proposed panoptic segmentation task. On top of a single backbone residual network, we first design a deformable convolution based semantic segmentation head and a Mask R-CNN style instance segmentation head which solve these two subtasks simultaneously. More importantly, we introduce a parameter-free panoptic head which solves the panoptic segmentation via pixel-wise classification. It first leverages the logits from the previous two heads and then innovatively expands the representation for enabling prediction of an extra unknown class which helps better resolve the conflicts between semantic and instance segmentation. Additionally, it handles the challenge caused by the varying number of instances and permits back propagation to the bottom modules in an end-to-end manner. Extensive experimental results on Cityscapes, COCO and our internal dataset demonstrate that our UPSNet achieves state-of-the-art performance with much faster inference. Code has been made available at: https://github.com/uber-research/UPSNet



### Automatic classification of geologic units in seismic images using partially interpreted examples
- **Arxiv ID**: http://arxiv.org/abs/1901.03786v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1901.03786v1)
- **Published**: 2019-01-12 02:45:32+00:00
- **Updated**: 2019-01-12 02:45:32+00:00
- **Authors**: Bas Peters, Justin Granek, Eldad Haber
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Geologic interpretation of large seismic stacked or migrated seismic images can be a time-consuming task for seismic interpreters. Neural network based semantic segmentation provides fast and automatic interpretations, provided a sufficient number of example interpretations are available. Networks that map from image-to-image emerged recently as powerful tools for automatic segmentation, but standard implementations require fully interpreted examples. Generating training labels for large images manually is time consuming. We introduce a partial loss-function and labeling strategies such that networks can learn from partially interpreted seismic images. This strategy requires only a small number of annotated pixels per seismic image. Tests on seismic images and interpretation information from the Sea of Ireland show that we obtain high-quality predicted interpretations from a small number of large seismic images. The combination of a partial-loss function, a multi-resolution network that explicitly takes small and large-scale geological features into account, and new labeling strategies make neural networks a more practical tool for automatic seismic interpretation.



### Learning Pairwise Relationship for Multi-object Detection in Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/1901.03796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03796v1)
- **Published**: 2019-01-12 04:43:17+00:00
- **Updated**: 2019-01-12 04:43:17+00:00
- **Authors**: Yu Liu, Lingqiao Liu, Hamid Rezatofighi, Thanh-Toan Do, Qinfeng Shi, Ian Reid
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: As the post-processing step for object detection, non-maximum suppression (GreedyNMS) is widely used in most of the detectors for many years. It is efficient and accurate for sparse scenes, but suffers an inevitable trade-off between precision and recall in crowded scenes. To overcome this drawback, we propose a Pairwise-NMS to cure GreedyNMS. Specifically, a pairwise-relationship network that is based on deep learning is learned to predict if two overlapping proposal boxes contain two objects or zero/one object, which can handle multiple overlapping objects effectively. Through neatly coupling with GreedyNMS without losing efficiency, consistent improvements have been achieved in heavily occluded datasets including MOT15, TUD-Crossing and PETS. In addition, Pairwise-NMS can be integrated into any learning based detectors (Both of Faster-RCNN and DPM detectors are tested in this paper), thus building a bridge between GreedyNMS and end-to-end learning detectors.



### 3D Human Pose Machines with Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.03798v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03798v2)
- **Published**: 2019-01-12 04:55:26+00:00
- **Updated**: 2019-01-15 01:21:59+00:00
- **Authors**: Keze Wang, Liang Lin, Chenhan Jiang, Chen Qian, Pengxu Wei
- **Comment**: To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI), 2019. Our simple yet effective self-supervised
  correction mechanism to incorporate 3D pose geometric structural information
  is innovative in the literature, and may also inspire other 3D vision tasks.
  Please find the code of this project at: http://www.sysu-hcp.net/3d_pose_ssl/
- **Journal**: None
- **Summary**: Driven by recent computer vision and robotic applications, recovering 3D human poses has become increasingly important and attracted growing interests. In fact, completing this task is quite challenging due to the diverse appearances, viewpoints, occlusions and inherently geometric ambiguities inside monocular images. Most of the existing methods focus on designing some elaborate priors /constraints to directly regress 3D human poses based on the corresponding 2D human pose-aware features or 2D pose predictions. However, due to the insufficient 3D pose data for training and the domain gap between 2D space and 3D space, these methods have limited scalabilities for all practical scenarios (e.g., outdoor scene). Attempt to address this issue, this paper proposes a simple yet effective self-supervised correction mechanism to learn all intrinsic structures of human poses from abundant images. Specifically, the proposed mechanism involves two dual learning tasks, i.e., the 2D-to-3D pose transformation and 3D-to-2D pose projection, to serve as a bridge between 3D and 2D human poses in a type of "free" self-supervision for accurate 3D human pose estimation. The 2D-to-3D pose implies to sequentially regress intermediate 3D poses by transforming the pose representation from the 2D domain to the 3D domain under the sequence-dependent temporal context, while the 3D-to-2D pose projection contributes to refining the intermediate 3D poses by maintaining geometric consistency between the 2D projections of 3D poses and the estimated 2D poses. We further apply our self-supervised correction mechanism to develop a 3D human pose machine, which jointly integrates the 2D spatial relationship, temporal smoothness of predictions and 3D geometric knowledge. Extensive evaluations demonstrate the superior performance and efficiency of our framework over all the compared competing methods.



### Boundary-Aware Network for Fast and High-Accuracy Portrait Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.03814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03814v1)
- **Published**: 2019-01-12 07:19:27+00:00
- **Updated**: 2019-01-12 07:19:27+00:00
- **Authors**: Xi Chen, Donglian Qi, Jianxin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Compared with other semantic segmentation tasks, portrait segmentation requires both higher precision and faster inference speed. However, this problem has not been well studied in previous works. In this paper, we propose a lightweight network architecture, called Boundary-Aware Network (BANet) which selectively extracts detail information in boundary area to make high-quality segmentation output with real-time( >25FPS) speed. In addition, we design a new loss function called refine loss which supervises the network with image level gradient information. Our model is able to produce finer segmentation results which has richer details than annotations.



### Summarization and Visualization of Large Volumes of Broadcast Video Data
- **Arxiv ID**: http://arxiv.org/abs/1901.03842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03842v1)
- **Published**: 2019-01-12 10:43:48+00:00
- **Updated**: 2019-01-12 10:43:48+00:00
- **Authors**: Kumar Abhishek, Ashok Yogi
- **Comment**: Undergraduate Thesis from April 2015, 55 pages, 25 figures
- **Journal**: None
- **Summary**: Over the past few years, there has been an astounding growth in the number of news channels as well as the amount of broadcast news video data. As a result, it is imperative that automated methods need to be developed in order to effectively summarize and store this voluminous data. Format detection of news videos plays an important role in news video analysis. Our problem involves building a robust and versatile news format detector, which identifies the different band elements in a news frame. Probabilistic progressive Hough transform has been used for the detection of band edges. The detected bands are classified as natural images, computer generated graphics (non-text) and text bands. A contrast based text detector has been used to identify the text regions from news frames. Two classifers have been trained and evaluated for the labeling of the detected bands as natural or artificial - Support Vector Machine (SVM) Classifer with RBF kernel, and Extreme Learning Machine (ELM) classifier. The classifiers have been trained on a dataset of 6000 images (3000 images of each class). The ELM classifier reports a balanced accuracy of 77.38%, while the SVM classifier outperforms it with a balanced accuracy of 96.5% using 10-fold cross-validation. The detected bands which have been fragmented due to the presence of gradients in the image have been merged using a three-tier hierarchical reasoning model. The bands were detected with a Jaccard Index of 0.8138, when compared to manually marked ground truth data. We have also presented an extensive literature review of previous work done towards news videos format detection, element band classification, and associative reasoning.



### One-view occlusion detection for stereo matching with a fully connected CRF model
- **Arxiv ID**: http://arxiv.org/abs/1901.03852v1
- **DOI**: 10.1109/TIP.2019.2892668
- **Categories**: **cs.CV**, 65-05
- **Links**: [PDF](http://arxiv.org/pdf/1901.03852v1)
- **Published**: 2019-01-12 11:24:36+00:00
- **Updated**: 2019-01-12 11:24:36+00:00
- **Authors**: Mikhail G. Mozerov, Joost van de Weijer
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing 2019
- **Summary**: In this paper, we extend the standard belief propagation (BP) sequential technique proposed in the tree-reweighted sequential method to the fully connected CRF models with the geodesic distance affinity. The proposed method has been applied to the stereo matching problem. Also a new approach to the BP marginal solution is proposed that we call one-view occlusion detection (OVOD). In contrast to the standard winner takes all (WTA) estimation, the proposed OVOD solution allows to find occluded regions in the disparity map and simultaneously improve the matching result. As a result we can perform only one energy minimization process and avoid the cost calculation for the second view and the left-right check procedure. We show that the OVOD approach considerably improves results for cost augmentation and energy minimization techniques in comparison with the standard one-view affinity space implementation. We apply our method to the Middlebury data set and reach state-of-the-art especially for median, average and mean squared error metrics.



### Deep-learning-based identification of odontogenic keratocysts in hematoxylin- and eosin-stained jaw cyst specimens
- **Arxiv ID**: http://arxiv.org/abs/1901.03857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03857v1)
- **Published**: 2019-01-12 12:10:52+00:00
- **Updated**: 2019-01-12 12:10:52+00:00
- **Authors**: Kei Sakamoto, Kei-ichi Morita, Tohru Ikeda, Kou Kayamori
- **Comment**: None
- **Journal**: None
- **Summary**: The aim of this study was to develop a digital histopathology system for identifying odontogenic keratocysts in hematoxylin- and eosin-stained tissue specimens of jaw cysts. Approximately 5000 microscopy images with 400$\times$ magnification were obtained from 199 odontogenic keratocysts, 208 dentigerous cysts, and 55 radicular cysts. A proportion of these images were used to make training patches, which were annotated as belonging to one of the following three classes: keratocysts, non-keratocysts, and stroma. The patches for the cysts contained the complete lining epithelium, with the cyst cavity being present on the upper side. The convolutional neural network (CNN) VGG16 was finetuned to this dataset. The trained CNN could recognize the basal cell palisading pattern, which is the definitive criterion for diagnosing keratocysts. Some of the remaining images were scanned and analyzed by the trained CNN, whose output was then used to train another CNN for binary classification (keratocyst or not). The area under the receiver operating characteristics curve for the entire algorithm was 0.997 for the test dataset. Thus, the proposed patch classification strategy is usable for automated keratocyst diagnosis. However, further optimization must be performed to make it suitable for practical use.



### HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.03861v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03861v2)
- **Published**: 2019-01-12 13:57:29+00:00
- **Updated**: 2019-04-06 07:16:05+00:00
- **Authors**: Cheng Sun, Chi-Wei Hsiao, Min Sun, Hwann-Tzong Chen
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present a new approach to the problem of estimating the 3D room layout from a single panoramic image. We represent room layout as three 1D vectors that encode, at each image column, the boundary positions of floor-wall and ceiling-wall, and the existence of wall-wall boundary. The proposed network, HorizonNet, trained for predicting 1D layout, outperforms previous state-of-the-art approaches. The designed post-processing procedure for recovering 3D room layouts from 1D predictions can automatically infer the room shape with low computation cost - it takes less than 20ms for a panorama image while prior works might need dozens of seconds. We also propose Pano Stretch Data Augmentation, which can diversify panorama data and be applied to other panorama-related learning tasks. Due to the limited data available for non-cuboid layout, we relabel 65 general layout from the current dataset for finetuning. Our approach shows good performance on general layouts by qualitative results and cross-validation.



### SteganoGAN: High Capacity Image Steganography with GANs
- **Arxiv ID**: http://arxiv.org/abs/1901.03892v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.03892v2)
- **Published**: 2019-01-12 18:47:38+00:00
- **Updated**: 2019-01-30 03:39:54+00:00
- **Authors**: Kevin Alex Zhang, Alfredo Cuesta-Infante, Lei Xu, Kalyan Veeramachaneni
- **Comment**: None
- **Journal**: None
- **Summary**: Image steganography is a procedure for hiding messages inside pictures. While other techniques such as cryptography aim to prevent adversaries from reading the secret message, steganography aims to hide the presence of the message itself. In this paper, we propose a novel technique for hiding arbitrary binary data in images using generative adversarial networks which allow us to optimize the perceptual quality of the images produced by our model. We show that our approach achieves state-of-the-art payloads of 4.4 bits per pixel, evades detection by steganalysis tools, and is effective on images from multiple datasets. To enable fair comparisons, we have released an open source library that is available online at https://github.com/DAI-Lab/SteganoGAN.



### A Machine Learning Benchmark for Facies Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.07659v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/1901.07659v2)
- **Published**: 2019-01-12 19:04:58+00:00
- **Updated**: 2019-04-10 07:09:35+00:00
- **Authors**: Yazeed Alaudah, Patrycja Michalowicz, Motaz Alfarraj, Ghassan AlRegib
- **Comment**: Submitted to the SEG Interpretation journal
- **Journal**: None
- **Summary**: The recent interest in using deep learning for seismic interpretation tasks, such as facies classification, has been facing a significant obstacle, namely the absence of large publicly available annotated datasets for training and testing models. As a result, researchers have often resorted to annotating their own training and testing data. However, different researchers may annotate different classes, or use different train and test splits. In addition, it is common for papers that apply machine learning for facies classification to not contain quantitative results, and rather rely solely on visual inspection of the results. All of these practices have lead to subjective results and have greatly hindered the ability to compare different machine learning models against each other and understand the advantages and disadvantages of each approach.   To address these issues, we open-source a fully-annotated 3D geological model of the Netherlands F3 Block. This model is based on the study of the 3D seismic data in addition to 26 well logs, and is grounded on the careful study of the geology of the region. Furthermore, we propose two baseline models for facies classification based on a deconvolution network architecture and make their codes publicly available. Finally, we propose a scheme for evaluating different models on this dataset, and we share the results of our baseline models. In addition to making the dataset and the code publicly available, this work helps advance research in this area by creating an objective benchmark for comparing the results of different machine learning approaches for facies classification.



### NRMVS: Non-Rigid Multi-View Stereo
- **Arxiv ID**: http://arxiv.org/abs/1901.03910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03910v1)
- **Published**: 2019-01-12 22:19:26+00:00
- **Updated**: 2019-01-12 22:19:26+00:00
- **Authors**: Matthias Innmann, Kihwan Kim, Jinwei Gu, Matthias Niessner, Charles Loop, Marc Stamminger, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: Scene reconstruction from unorganized RGB images is an important task in many computer vision applications. Multi-view Stereo (MVS) is a common solution in photogrammetry applications for the dense reconstruction of a static scene. The static scene assumption, however, limits the general applicability of MVS algorithms, as many day-to-day scenes undergo non-rigid motion, e.g., clothes, faces, or human bodies. In this paper, we open up a new challenging direction: dense 3D reconstruction of scenes with non-rigid changes observed from arbitrary, sparse, and wide-baseline views. We formulate the problem as a joint optimization of deformation and depth estimation, using deformation graphs as the underlying representation. We propose a new sparse 3D to 2D matching technique, together with a dense patch-match evaluation scheme to estimate deformation and depth with photometric consistency. We show that creating a dense 4D structure from a few RGB images with non-rigid changes is possible, and demonstrate that our method can be used to interpolate novel deformed scenes from various combinations of these deformation estimates derived from the sparse views.



### Real-time Joint Object Detection and Semantic Segmentation Network for Automated Driving
- **Arxiv ID**: http://arxiv.org/abs/1901.03912v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.03912v1)
- **Published**: 2019-01-12 22:25:06+00:00
- **Updated**: 2019-01-12 22:25:06+00:00
- **Authors**: Ganesh Sistu, Isabelle Leang, Senthil Yogamani
- **Comment**: Presented at NeurIPS 2018 Workshop on Machine Learning on the Phone
  and other Consumer Devices (MLPCD 2)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) are successfully used for various visual perception tasks including bounding box object detection, semantic segmentation, optical flow, depth estimation and visual SLAM. Generally these tasks are independently explored and modeled. In this paper, we present a joint multi-task network design for learning object detection and semantic segmentation simultaneously. The main motivation is to achieve real-time performance on a low power embedded SOC by sharing of encoder for both the tasks. We construct an efficient architecture using a small ResNet10 like encoder which is shared for both decoders. Object detection uses YOLO v2 like decoder and semantic segmentation uses FCN8 like decoder. We evaluate the proposed network in two public datasets (KITTI, Cityscapes) and in our private fisheye camera dataset, and demonstrate that joint network provides the same accuracy as that of separate networks. We further optimize the network to achieve 30 fps for 1280x384 resolution image.



### Automated Deep Photo Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1901.03915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03915v1)
- **Published**: 2019-01-12 23:41:28+00:00
- **Updated**: 2019-01-12 23:41:28+00:00
- **Authors**: Sebastian PenhouÃ«t, Paul Sanzenbacher
- **Comment**: None
- **Journal**: None
- **Summary**: Photorealism is a complex concept that cannot easily be formulated mathematically. Deep Photo Style Transfer is an attempt to transfer the style of a reference image to a content image while preserving its photorealism. This is achieved by introducing a constraint that prevents distortions in the content image and by applying the style transfer independently for semantically different parts of the images. In addition, an automated segmentation process is presented that consists of a neural network based segmentation method followed by a semantic grouping step. To further improve the results a measure for image aesthetics is used and elaborated. If the content and the style image are sufficiently similar, the result images look very realistic. With the automation of the image segmentation the pipeline becomes completely independent from any user interaction, which allows for new applications.




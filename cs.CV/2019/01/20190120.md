# Arxiv Papers in cs.CV on 2019-01-20
### Modeling the Biological Pathology Continuum with HSIC-regularized Wasserstein Auto-encoders
- **Arxiv ID**: http://arxiv.org/abs/1901.06618v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.06618v1)
- **Published**: 2019-01-20 03:40:55+00:00
- **Updated**: 2019-01-20 03:40:55+00:00
- **Authors**: Denny Wu, Hirofumi Kobayashi, Charles Ding, Lei Cheng, Keisuke Goda Marzyeh Ghassemi
- **Comment**: None
- **Journal**: None
- **Summary**: A crucial challenge in image-based modeling of biomedical data is to identify trends and features that separate normality and pathology. In many cases, the morphology of the imaged object exhibits continuous change as it deviates from normality, and thus a generative model can be trained to model this morphological continuum. Moreover, given side information that correlates to certain trend in morphological change, a latent variable model can be regularized such that its latent representation reflects this side information. In this work, we use the Wasserstein Auto-encoder to model this pathology continuum, and apply the Hilbert-Schmitt Independence Criterion (HSIC) to enforce dependency between certain latent features and the provided side information. We experimentally show that the model can provide disentangled and interpretable latent representations and also generate a continuum of morphological changes that corresponds to change in the side information.



### Improved Selective Refinement Network for Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.06651v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06651v3)
- **Published**: 2019-01-20 10:01:34+00:00
- **Updated**: 2019-01-30 01:18:31+00:00
- **Authors**: Shifeng Zhang, Rui Zhu, Xiaobo Wang, Hailin Shi, Tianyu Fu, Shuo Wang, Tao Mei, Stan Z. Li
- **Comment**: Technical report, 8 pages, 6 figures
- **Journal**: None
- **Summary**: As a long-standing problem in computer vision, face detection has attracted much attention in recent decades for its practical applications. With the availability of face detection benchmark WIDER FACE dataset, much of the progresses have been made by various algorithms in recent years. Among them, the Selective Refinement Network (SRN) face detector introduces the two-step classification and regression operations selectively into an anchor-based face detector to reduce false positives and improve location accuracy simultaneously. Moreover, it designs a receptive field enhancement block to provide more diverse receptive field. In this report, to further improve the performance of SRN, we exploit some existing techniques via extensive experiments, including new data augmentation strategy, improved backbone network, MS COCO pretraining, decoupled classification module, segmentation branch and Squeeze-and-Excitation block. Some of these techniques bring performance improvements, while few of them do not well adapt to our baseline. As a consequence, we present an improved SRN face detector by combining these useful techniques together and obtain the best performance on widely used face detection benchmark WIDER FACE dataset.



### Training Neural Networks with Local Error Signals
- **Arxiv ID**: http://arxiv.org/abs/1901.06656v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.06656v2)
- **Published**: 2019-01-20 10:59:53+00:00
- **Updated**: 2019-05-07 18:49:45+00:00
- **Authors**: Arild Nøkland, Lars Hiller Eidnes
- **Comment**: Accepted to ICML 2019
- **Journal**: None
- **Summary**: Supervised training of neural networks for classification is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is back-propagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss functions. In this paper we demonstrate, for the first time, that layer-wise training can approach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses help with optimization in the context of local learning. Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be transported back to hidden layers. A completely backprop free variant outperforms previously reported results among methods aiming for higher biological plausibility. Code is available https://github.com/anokland/local-loss



### Localizing dexterous surgical tools in X-ray for image-based navigation
- **Arxiv ID**: http://arxiv.org/abs/1901.06672v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06672v2)
- **Published**: 2019-01-20 12:48:29+00:00
- **Updated**: 2019-07-04 16:13:34+00:00
- **Authors**: Cong Gao, Mathias Unberath, Russell Taylor, Mehran Armand
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: X-ray image based surgical tool navigation is fast and supplies accurate images of deep seated structures. Typically, recovering the 6 DOF rigid pose and deformation of tools with respect to the X-ray camera can be accurately achieved through intensity-based 2D/3D registration of 3D images or models to 2D X-rays. However, the capture range of image-based 2D/3D registration is inconveniently small suggesting that automatic and robust initialization strategies are of critical importance. This manuscript describes a first step towards leveraging semantic information of the imaged object to initialize 2D/3D registration within the capture range of image-based registration by performing concurrent segmentation and localization of dexterous surgical tools in X-ray images.   We presented a learning-based strategy to simultaneously localize and segment dexterous surgical tools in X-ray images and demonstrate promising performance on synthetic and ex vivo data. We currently investigate methods to use semantic information extracted by the proposed network to reliably and robustly initialize image-based 2D/3D registration.   While image-based 2D/3D registration has been an obvious focus of the CAI community, robust initialization thereof (albeit critical) has largely been neglected. This manuscript discusses learning-based retrieval of semantic information on imaged-objects as a stepping stone for such initialization and may therefore be of interest to the IPCAI community. Since results are still preliminary and only focus on localization, we target the Long Abstract category.



### Visual Entailment: A Novel Task for Fine-Grained Image Understanding
- **Arxiv ID**: http://arxiv.org/abs/1901.06706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06706v1)
- **Published**: 2019-01-20 17:55:05+00:00
- **Updated**: 2019-01-20 17:55:05+00:00
- **Authors**: Ning Xie, Farley Lai, Derek Doran, Asim Kadav
- **Comment**: None
- **Journal**: None
- **Summary**: Existing visual reasoning datasets such as Visual Question Answering (VQA), often suffer from biases conditioned on the question, image or answer distributions. The recently proposed CLEVR dataset addresses these limitations and requires fine-grained reasoning but the dataset is synthetic and consists of similar objects and sentence structures across the dataset.   In this paper, we introduce a new inference task, Visual Entailment (VE) - consisting of image-sentence pairs whereby a premise is defined by an image, rather than a natural language sentence as in traditional Textual Entailment tasks. The goal of a trained VE model is to predict whether the image semantically entails the text. To realize this task, we build a dataset SNLI-VE based on the Stanford Natural Language Inference corpus and Flickr30k dataset. We evaluate various existing VQA baselines and build a model called Explainable Visual Entailment (EVE) system to address the VE task. EVE achieves up to 71% accuracy and outperforms several other state-of-the-art VQA based models. Finally, we demonstrate the explainability of EVE through cross-modal attention visualizations. The SNLI-VE dataset is publicly available at https://github.com/ necla-ml/SNLI-VE.



### Deep Features Analysis with Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.10042v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.10042v1)
- **Published**: 2019-01-20 18:44:43+00:00
- **Updated**: 2019-01-20 18:44:43+00:00
- **Authors**: Shipeng Xie, Da Chen, Rong Zhang, Hui Xue
- **Comment**: In AAAI-19 Workshop on Network Interpretability for Deep Learning
- **Journal**: None
- **Summary**: Deep neural network models have recently draw lots of attention, as it consistently produce impressive results in many computer vision tasks such as image classification, object detection, etc. However, interpreting such model and show the reason why it performs quite well becomes a challenging question. In this paper, we propose a novel method to interpret the neural network models with attention mechanism. Inspired by the heatmap visualization, we analyze the relation between classification accuracy with the attention based heatmap. An improved attention based method is also included and illustrate that a better classifier can be interpreted by the attention based heatmap.



### Inducing Sparse Coding and And-Or Grammar from Generator Network
- **Arxiv ID**: http://arxiv.org/abs/1901.11494v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.11494v1)
- **Published**: 2019-01-20 18:47:18+00:00
- **Updated**: 2019-01-20 18:47:18+00:00
- **Authors**: Xianglei Xing, Song-Chun Zhu, Ying Nian Wu
- **Comment**: In AAAI-19 Workshop on Network Interpretability for Deep Learning
- **Journal**: None
- **Summary**: We introduce an explainable generative model by applying sparse operation on the feature maps of the generator network. Meaningful hierarchical representations are obtained using the proposed generative model with sparse activations. The convolutional kernels from the bottom layer to the top layer of the generator network can learn primitives such as edges and colors, object parts, and whole objects layer by layer. From the perspective of the generator network, we propose a method for inducing both sparse coding and the AND-OR grammar for images. Experiments show that our method is capable of learning meaningful and explainable hierarchical representations.



### Understanding the Importance of Single Directions via Representative Substitution
- **Arxiv ID**: http://arxiv.org/abs/1911.05586v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05586v1)
- **Published**: 2019-01-20 18:49:17+00:00
- **Updated**: 2019-01-20 18:49:17+00:00
- **Authors**: Li Chen, Hailun Ding, Qi Li, Zhuo Li, Jian Peng, Haifeng Li
- **Comment**: In AAAI-19 Workshop on Network Interpretability for Deep Learning.
  Published version of arXiv:1811.11053
- **Journal**: None
- **Summary**: Understanding the internal representations of deep neural networks (DNNs) is crucal to explain their behavior. The interpretation of individual units, which are neurons in MLPs or convolution kernels in convolutional networks, has been paid much attention given their fundamental role. However, recent research (Morcos et al. 2018) presented a counterintuitive phenomenon, which suggests that an individual unit with high class selectivity, called interpretable units, has poor contributions to generalization of DNNs. In this work, we provide a new perspective to understand this counterintuitive phenomenon, which makes sense when we introduce Representative Substitution (RS). Instead of individually selective units with classes, the RS refers to the independence of a unit's representations in the same layer without any annotation. Our experiments demonstrate that interpretable units have high RS which are not critical to network's generalization. The RS provides new insights into the interpretation of DNNs and suggests that we need to focus on the independence and relationship of the representations.



### Fitting 3D Shapes from Partial and Noisy Point Clouds with Evolutionary Computing
- **Arxiv ID**: http://arxiv.org/abs/1901.06722v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1901.06722v1)
- **Published**: 2019-01-20 20:12:34+00:00
- **Updated**: 2019-01-20 20:12:34+00:00
- **Authors**: Jean F. Liénard
- **Comment**: None
- **Journal**: None
- **Summary**: Point clouds obtained from photogrammetry are noisy and incomplete models of reality. We propose an evolutionary optimization methodology that is able to approximate the underlying object geometry on such point clouds. This approach assumes a priori knowledge on the 3D structure modeled and enables the identification of a collection of primitive shapes approximating the scene. Built-in mechanisms that enforce high shape diversity and adaptive population size make this method suitable to modeling both simple and complex scenes. We focus here on the case of cylinder approximations and we describe, test, and compare a set of mutation operators designed for optimal exploration of their search space. We assess the robustness and limitations of this algorithm through a series of synthetic examples, and we finally demonstrate its general applicability on two real-life cases in vegetation and industrial settings.




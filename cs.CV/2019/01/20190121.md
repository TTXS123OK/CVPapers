# Arxiv Papers in cs.CV on 2019-01-21
### Pattern Generation Strategies for Improving Recognition of Handwritten Mathematical Expressions
- **Arxiv ID**: http://arxiv.org/abs/1901.06763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06763v1)
- **Published**: 2019-01-21 01:41:33+00:00
- **Updated**: 2019-01-21 01:41:33+00:00
- **Authors**: Anh Duc Le, Bipin Indurkhya, Masaki Nakagawa
- **Comment**: None
- **Journal**: None
- **Summary**: Recognition of Handwritten Mathematical Expressions (HMEs) is a challenging problem because of the ambiguity and complexity of two-dimensional handwriting. Moreover, the lack of large training data is a serious issue, especially for academic recognition systems. In this paper, we propose pattern generation strategies that generate shape and structural variations to improve the performance of recognition systems based on a small training set. For data generation, we employ the public databases: CROHME 2014 and 2016 of online HMEs. The first strategy employs local and global distortions to generate shape variations. The second strategy decomposes an online HME into sub-online HMEs to get more structural variations. The hybrid strategy combines both these strategies to maximize shape and structural variations. The generated online HMEs are converted to images for offline HME recognition. We tested our strategies in an end-to-end recognition system constructed from a recent deep learning model: Convolutional Neural Network and attention-based encoder-decoder. The results of experiments on the CROHME 2014 and 2016 databases demonstrate the superiority and effectiveness of our strategies: our hybrid strategy achieved classification rates of 48.78% and 45.60%, respectively, on these databases. These results are competitive compared to others reported in recent literature. Our generated datasets are openly available for research community and constitute a useful resource for the HME recognition research in future.



### Real-time 3D Face-Eye Performance Capture of a Person Wearing VR Headset
- **Arxiv ID**: http://arxiv.org/abs/1901.06765v1
- **DOI**: 10.1145/3240508.3240570
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06765v1)
- **Published**: 2019-01-21 01:58:15+00:00
- **Updated**: 2019-01-21 01:58:15+00:00
- **Authors**: Guoxian Song, Jianfei Cai, Tat-Jen Cham, Jianmin Zheng, Juyong Zhang, Henry Fuchs
- **Comment**: ACM Multimedia Conference 2018
- **Journal**: None
- **Summary**: Teleconference or telepresence based on virtual reality (VR) headmount display (HMD) device is a very interesting and promising application since HMD can provide immersive feelings for users. However, in order to facilitate face-to-face communications for HMD users, real-time 3D facial performance capture of a person wearing HMD is needed, which is a very challenging task due to the large occlusion caused by HMD. The existing limited solutions are very complex either in setting or in approach as well as lacking the performance capture of 3D eye gaze movement. In this paper, we propose a convolutional neural network (CNN) based solution for real-time 3D face-eye performance capture of HMD users without complex modification to devices. To address the issue of lacking training data, we generate massive pairs of HMD face-label dataset by data synthesis as well as collecting VR-IR eye dataset from multiple subjects. Then, we train a dense-fitting network for facial region and an eye gaze network to regress 3D eye model parameters. Extensive experimental results demonstrate that our system can efficiently and effectively produce in real time a vivid personalized 3D avatar with the correct identity, pose, expression and eye motion corresponding to the HMD user.



### LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators
- **Arxiv ID**: http://arxiv.org/abs/1901.06767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06767v1)
- **Published**: 2019-01-21 02:14:14+00:00
- **Updated**: 2019-01-21 02:14:14+00:00
- **Authors**: Jianan Li, Jimei Yang, Aaron Hertzmann, Jianming Zhang, Tingfa Xu
- **Comment**: Accepted as a conference paper at ICLR 2019
- **Journal**: None
- **Summary**: Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.



### Hybrid coarse-fine classification for head pose estimation
- **Arxiv ID**: http://arxiv.org/abs/1901.06778v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06778v2)
- **Published**: 2019-01-21 03:07:05+00:00
- **Updated**: 2019-10-02 23:25:54+00:00
- **Authors**: Haofan Wang, Zhenghua Chen, Yi Zhou
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Head pose estimation, which computes the intrinsic Euler angles (yaw, pitch, roll) from the human, is crucial for gaze estimation, face alignment, and 3D reconstruction. Traditional approaches heavily relies on the accuracy of facial landmarks. It limits their performances, especially when the visibility of the face is not in good condition. In this paper, to do the estimation without facial landmarks, we combine the coarse and fine regression output together for a deep network. Utilizing more quantization units for the angles, a fine classifier is trained with the help of other auxiliary coarse units. Integrating regression is adopted to get the final prediction. The proposed approach is evaluated on three challenging benchmarks. It achieves the state-of-the-art on AFLW2000, BIWI and performs favorably on AFLW. The code has been released on Github.



### Generating Text Sequence Images for Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.06782v1
- **DOI**: 10.1007/s11063-019-10166-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06782v1)
- **Published**: 2019-01-21 03:44:23+00:00
- **Updated**: 2019-01-21 03:44:23+00:00
- **Authors**: Yanxiang Gong, Linjie Deng, Zheng Ma, Mei Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, methods based on deep learning have dominated the field of text recognition. With a large number of training data, most of them can achieve the state-of-the-art performances. However, it is hard to harvest and label sufficient text sequence images from the real scenes. To mitigate this issue, several methods to synthesize text sequence images were proposed, yet they usually need complicated preceding or follow-up steps. In this work, we present a method which is able to generate infinite training data without any auxiliary pre/post-process. We tackle the generation task as an image-to-image translation one and utilize conditional adversarial networks to produce realistic text sequence images in the light of the semantic ones. Some evaluation metrics are involved to assess our method and the results demonstrate that the caliber of the data is satisfactory. The code and dataset will be publicly available soon.



### Dynamic Curriculum Learning for Imbalanced Data Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.06783v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06783v2)
- **Published**: 2019-01-21 03:48:10+00:00
- **Updated**: 2019-08-15 05:58:59+00:00
- **Authors**: Yiru Wang, Weihao Gan, Jie Yang, Wei Wu, Junjie Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Human attribute analysis is a challenging task in the field of computer vision, since the data is largely imbalance-distributed. Common techniques such as re-sampling and cost-sensitive learning require prior-knowledge to train the system. To address this problem, we propose a unified framework called Dynamic Curriculum Learning (DCL) to online adaptively adjust the sampling strategy and loss learning in single batch, which resulting in better generalization and discrimination. Inspired by the curriculum learning, DCL consists of two level curriculum schedulers: (1) sampling scheduler not only manages the data distribution from imbalanced to balanced but also from easy to hard; (2) loss scheduler controls the learning importance between classification and metric learning loss. Learning from these two schedulers, we demonstrate our DCL framework with the new state-of-the-art performance on the widely used face attribute dataset CelebA and pedestrian attribute dataset RAP.



### Semantic Image Networks for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.06792v1
- **DOI**: 10.1007/s11263-019-01248-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06792v1)
- **Published**: 2019-01-21 05:27:24+00:00
- **Updated**: 2019-01-21 05:27:24+00:00
- **Authors**: Sunder Ali Khowaja, Seok-Lyong Lee
- **Comment**: 30 pages
- **Journal**: International Journal of Computer Vision-2019
- **Summary**: In this paper, we propose the use of a semantic image, an improved representation for video analysis, principally in combination with Inception networks. The semantic image is obtained by applying localized sparse segmentation using global clustering (LSSGC) prior to the approximate rank pooling which summarizes the motion characteristics in single or multiple images. It incorporates the background information by overlaying a static background from the window onto the subsequent segmented frames. The idea is to improve the action-motion dynamics by focusing on the region which is important for action recognition and encoding the temporal variances using the frame ranking method. We also propose the sequential combination of Inception-ResNetv2 and long-short-term memory network (LSTM) to leverage the temporal variances for improved recognition performance. Extensive analysis has been carried out on UCF101 and HMDB51 datasets which are widely used in action recognition studies. We show that (i) the semantic image generates better activations and converges faster than its original variant, (ii) using segmentation prior to approximate rank pooling yields better recognition performance, (iii) The use of LSTM leverages the temporal variance information from approximate rank pooling to model the action behavior better than the base network, (iv) the proposed representations can be adaptive as they can be used with existing methods such as temporal segment networks to improve the recognition performance, and (v) our proposed four-stream network architecture comprising of semantic images and semantic optical flows achieves state-of-the-art performance, 95.9% and 73.5% recognition accuracy on UCF101 and HMDB51, respectively.



### Deep Level Sets: Implicit Surface Representations for 3D Shape Inference
- **Arxiv ID**: http://arxiv.org/abs/1901.06802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06802v1)
- **Published**: 2019-01-21 06:32:31+00:00
- **Updated**: 2019-01-21 06:32:31+00:00
- **Authors**: Mateusz Michalkiewicz, Jhony K. Pontes, Dominic Jack, Mahsa Baktashmotlagh, Anders Eriksson
- **Comment**: None
- **Journal**: None
- **Summary**: Existing 3D surface representation approaches are unable to accurately classify pixels and their orientation lying on the boundary of an object. Thus resulting in coarse representations which usually require post-processing steps to extract 3D surface meshes. To overcome this limitation, we propose an end-to-end trainable model that directly predicts implicit surface representations of arbitrary topology by optimising a novel geometric loss function. Specifically, we propose to represent the output as an oriented level set of a continuous embedding function, and incorporate this in a deep end-to-end learning framework by introducing a variational shape inference formulation. We investigate the benefits of our approach on the task of 3D surface prediction and demonstrate its ability to produce a more accurate reconstruction compared to voxel-based representations. We further show that our model is flexible and can be applied to a variety of shape inference problems.



### Impact of Fully Connected Layers on Performance of Convolutional Neural Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.02771v3
- **DOI**: 10.1016/j.neucom.2019.10.008
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.02771v3)
- **Published**: 2019-01-21 07:42:26+00:00
- **Updated**: 2019-11-19 05:28:01+00:00
- **Authors**: S. H. Shabbeer Basha, Shiv Ram Dubey, Viswanath Pulabaigari, Snehasis Mukherjee
- **Comment**: This paper is accepted for publication in Neurocomputing Journal
- **Journal**: None
- **Summary**: The Convolutional Neural Networks (CNNs), in domains like computer vision, mostly reduced the need for handcrafted features due to its ability to learn the problem-specific features from the raw input data. However, the selection of dataset-specific CNN architecture, which mostly performed by either experience or expertise is a time-consuming and error-prone process. To automate the process of learning a CNN architecture, this paper attempts at finding the relationship between Fully Connected (FC) layers with some of the characteristics of the datasets. The CNN architectures, and recently datasets also, are categorized as deep, shallow, wide, etc. This paper tries to formalize these terms along with answering the following questions. (i) What is the impact of deeper/shallow architectures on the performance of the CNN w.r.t. FC layers?, (ii) How the deeper/wider datasets influence the performance of CNN w.r.t. FC layers?, and (iii) Which kind of architecture (deeper/ shallower) is better suitable for which kind of (deeper/ wider) datasets. To address these findings, we have performed experiments with three CNN architectures having different depths. The experiments are conducted by varying the number of FC layers. We used four widely used datasets including CIFAR-10, CIFAR-100, Tiny ImageNet, and CRCHistoPhenotypes to justify our findings in the context of the image classification problem. The source code of this research is available at https://github.com/shabbeersh/Impact-of-FC-layers.



### Salient Object Detection with Lossless Feature Reflection and Weighted Structural Loss
- **Arxiv ID**: http://arxiv.org/abs/1901.06823v1
- **DOI**: 10.1109/TIP.2019.2893535
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06823v1)
- **Published**: 2019-01-21 08:45:34+00:00
- **Updated**: 2019-01-21 08:45:34+00:00
- **Authors**: Pingping Zhang, Wei Liu, Huchuan Lu, Chunhua Shen
- **Comment**: To appear in IEEE Transaction on Image Processing. This paper is
  extended from arXiv:1802.06527
- **Journal**: None
- **Summary**: Salient object detection (SOD), which aims to identify and locate the most salient pixels or regions in images, has been attracting more and more interest due to its various real-world applications. However, this vision task is quite challenging, especially under complex image scenes. Inspired by the intrinsic reflection of natural images, in this paper we propose a novel feature learning framework for large-scale salient object detection. Specifically, we design a symmetrical fully convolutional network (SFCN) to effectively learn complementary saliency features under the guidance of lossless feature reflection. The location information, together with contextual and semantic information, of salient objects are jointly utilized to supervise the proposed network for more accurate saliency predictions. In addition, to overcome the blurry boundary problem, we propose a new weighted structural loss function to ensure clear object boundaries and spatially consistent saliency. The coarse prediction results are effectively refined by these structural information for performance improvements. Extensive experiments on seven saliency detection datasets demonstrate that our approach achieves consistently superior performance and outperforms the very recent state-of-the-art methods with a large margin.



### Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos
- **Arxiv ID**: http://arxiv.org/abs/1901.06829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06829v1)
- **Published**: 2019-01-21 09:00:11+00:00
- **Updated**: 2019-01-21 09:00:11+00:00
- **Authors**: Dongliang He, Xiang Zhao, Jizhou Huang, Fu Li, Xiao Liu, Shilei Wen
- **Comment**: AAAI 2019
- **Journal**: None
- **Summary**: The task of video grounding, which temporally localizes a natural language description in a video, plays an important role in understanding videos. Existing studies have adopted strategies of sliding window over the entire video or exhaustively ranking all possible clip-sentence pairs in a pre-segmented video, which inevitably suffer from exhaustively enumerated candidates. To alleviate this problem, we formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy. Specifically, we propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training. Our proposed framework achieves state-of-the-art performance on ActivityNet'18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video.



### Skeleton-based Action Recognition of People Handling Objects
- **Arxiv ID**: http://arxiv.org/abs/1901.06882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06882v1)
- **Published**: 2019-01-21 11:20:11+00:00
- **Updated**: 2019-01-21 11:20:11+00:00
- **Authors**: Sunoh Kim, Kimin Yun, Jongyoul Park, Jin Young Choi
- **Comment**: Accepted in WACV 2019
- **Journal**: None
- **Summary**: In visual surveillance systems, it is necessary to recognize the behavior of people handling objects such as a phone, a cup, or a plastic bag. In this paper, to address this problem, we propose a new framework for recognizing object-related human actions by graph convolutional networks using human and object poses. In this framework, we construct skeletal graphs of reliable human poses by selectively sampling the informative frames in a video, which include human joints with high confidence scores obtained in pose estimation. The skeletal graphs generated from the sampled frames represent human poses related to the object position in both the spatial and temporal domains, and these graphs are used as inputs to the graph convolutional networks. Through experiments over an open benchmark and our own data sets, we verify the validity of our framework in that our method outperforms the state-of-the-art method for skeleton-based action recognition.



### A Fourier Disparity Layer representation for Light Fields
- **Arxiv ID**: http://arxiv.org/abs/1901.06919v1
- **DOI**: 10.1109/TIP.2019.2922099
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1901.06919v1)
- **Published**: 2019-01-21 13:11:11+00:00
- **Updated**: 2019-01-21 13:11:11+00:00
- **Authors**: Mikael Le Pendu, Christine Guillemot, Aljosa Smolic
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: In this paper, we present a new Light Field representation for efficient Light Field processing and rendering called Fourier Disparity Layers (FDL). The proposed FDL representation samples the Light Field in the depth (or equivalently the disparity) dimension by decomposing the scene as a discrete sum of layers. The layers can be constructed from various types of Light Field inputs including a set of sub-aperture images, a focal stack, or even a combination of both. From our derivations in the Fourier domain, the layers are simply obtained by a regularized least square regression performed independently at each spatial frequency, which is efficiently parallelized in a GPU implementation. Our model is also used to derive a gradient descent based calibration step that estimates the input view positions and an optimal set of disparity values required for the layer construction. Once the layers are known, they can be simply shifted and filtered to produce different viewpoints of the scene while controlling the focus and simulating a camera aperture of arbitrary shape and size. Our implementation in the Fourier domain allows real time Light Field rendering. Finally, direct applications such as view interpolation or extrapolation and denoising are presented and evaluated.



### SUMNet: Fully Convolutional Model for Fast Segmentation of Anatomical Structures in Ultrasound Volumes
- **Arxiv ID**: http://arxiv.org/abs/1901.06920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06920v1)
- **Published**: 2019-01-21 13:16:18+00:00
- **Updated**: 2019-01-21 13:16:18+00:00
- **Authors**: Sumanth Nandamuri, Debarghya China, Pabitra Mitra, Debdoot Sheet
- **Comment**: Accepted in ISBI 2019
- **Journal**: None
- **Summary**: Ultrasound imaging is generally employed for real-time investigation of internal anatomy of the human body for disease identification. Delineation of the anatomical boundary of organs and pathological lesions is quite challenging due to the stochastic nature of speckle intensity in the images, which also introduces visual fatigue for the observer. This paper introduces a fully convolutional neural network based method to segment organ and pathologies in ultrasound volume by learning the spatial-relationship between closely related classes in the presence of stochastically varying speckle intensity. We propose a convolutional encoder-decoder like framework with (i) feature concatenation across matched layers in encoder and decoder and (ii) index passing based unpooling at the decoder for semantic segmentation of ultrasound volumes. We have experimentally evaluated the performance on publicly available datasets consisting of $10$ intravascular ultrasound pullback acquired at $20$ MHz and $16$ freehand thyroid ultrasound volumes acquired $11 - 16$ MHz. We have obtained a dice score of $0.93 \pm 0.08$ and $0.92 \pm 0.06$ respectively, following a $10$-fold cross-validation experiment while processing frame of $256 \times 384$ pixel in $0.035$s and a volume of $256 \times 384 \times 384$ voxel in $13.44$s.



### Segmentation of Lumen and External Elastic Laminae in Intravascular Ultrasound Images using Ultrasonic Backscattering Physics Initialized Multiscale Random Walks
- **Arxiv ID**: http://arxiv.org/abs/1901.06926v1
- **DOI**: 10.1007/978-3-319-68124-5_34
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06926v1)
- **Published**: 2019-01-21 13:29:46+00:00
- **Updated**: 2019-01-21 13:29:46+00:00
- **Authors**: Debarghya China, Pabitra Mitra, Debdoot Sheet
- **Comment**: ICVGIP MedImage 2016
- **Journal**: None
- **Summary**: Coronary artery disease accounts for a large number of deaths across the world and clinicians generally prefer using x-ray computed tomography or magnetic resonance imaging for localizing vascular pathologies. Interventional imaging modalities like intravascular ultrasound (IVUS) are used to adjunct diagnosis of atherosclerotic plaques in vessels, and help assess morphological state of the vessel and plaque, which play a significant role for treatment planning. Since speckle intensity in IVUS images are inherently stochastic in nature and challenge clinicians with accurate visibility of the vessel wall boundaries, it requires automation. In this paper we present a method for segmenting the lumen and external elastic laminae of the artery wall in IVUS images using random walks over a multiscale pyramid of Gaussian decomposed frames. The seeds for the random walker are initialized by supervised learning of ultrasonic backscattering and attenuation statistical mechanics from labelled training samples. We have experimentally evaluated the performance using $77$ IVUS images acquired at $40$ MHz that are available in the IVUS segmentation challenge dataset\footnote{http://www.cvc.uab.es/IVUSchallenge2011/dataset.html} to obtain a Jaccard score of $0.89 \pm 0.14$ for lumen and $0.85 \pm 0.12$ for external elastic laminae segmentation over a $10$-fold cross-validation study.



### Deep Neural Network Approximation for Custom Hardware: Where We've Been, Where We're Going
- **Arxiv ID**: http://arxiv.org/abs/1901.06955v4
- **DOI**: 10.1145/3309551
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06955v4)
- **Published**: 2019-01-21 15:14:31+00:00
- **Updated**: 2019-07-08 14:30:02+00:00
- **Authors**: Erwei Wang, James J. Davis, Ruizhe Zhao, Ho-Cheung Ng, Xinyu Niu, Wayne Luk, Peter Y. K. Cheung, George A. Constantinides
- **Comment**: Accepted manuscript uploaded 21/01/19. DOA 15/01/19
- **Journal**: ACM Comput. Surv. 52, 2, Article 40 (May 2019), 39 pages
- **Summary**: Deep neural networks have proven to be particularly effective in visual and audio recognition tasks. Existing models tend to be computationally expensive and memory intensive, however, and so methods for hardware-oriented approximation have become a hot topic. Research has shown that custom hardware-based neural network accelerators can surpass their general-purpose processor equivalents in terms of both throughput and energy efficiency. Application-tailored accelerators, when co-designed with approximation-based network training methods, transform large, dense and computationally expensive networks into small, sparse and hardware-efficient alternatives, increasing the feasibility of network deployment. In this article, we provide a comprehensive evaluation of approximation methods for high-performance network inference along with in-depth discussion of their effectiveness for custom hardware implementation. We also include proposals for future research based on a thorough analysis of current trends. This article represents the first survey providing detailed comparisons of custom hardware accelerators featuring approximation for both convolutional and recurrent neural networks, through which we hope to inspire exciting new developments in the field.



### Adversarial training with cycle consistency for unsupervised super-resolution in endomicroscopy
- **Arxiv ID**: http://arxiv.org/abs/1901.06988v2
- **DOI**: 10.1016/j.media.2019.01.011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06988v2)
- **Published**: 2019-01-21 16:23:32+00:00
- **Updated**: 2019-02-06 18:31:03+00:00
- **Authors**: Daniele Rav√¨, Agnieszka Barbara Szczotka, Stephen P Pereira, Tom Vercauteren
- **Comment**: Accepted for publication on Medical Image Analysis journal
- **Journal**: None
- **Summary**: In recent years, endomicroscopy has become increasingly used for diagnostic purposes and interventional guidance. It can provide intraoperative aids for real-time tissue characterization and can help to perform visual investigations aimed for example to discover epithelial cancers. Due to physical constraints on the acquisition process, endomicroscopy images, still today have a low number of informative pixels which hampers their quality. Post-processing techniques, such as Super-Resolution (SR), are a potential solution to increase the quality of these images. SR techniques are often supervised, requiring aligned pairs of low-resolution (LR) and high-resolution (HR) images patches to train a model. However, in our domain, the lack of HR images hinders the collection of such pairs and makes supervised training unsuitable. For this reason, we propose an unsupervised SR framework based on an adversarial deep neural network with a physically-inspired cycle consistency, designed to impose some acquisition properties on the super-resolved images. Our framework can exploit HR images, regardless of the domain where they are coming from, to transfer the quality of the HR images to the initial LR images. This property can be particularly useful in all situations where pairs of LR/HR are not available during the training. Our quantitative analysis, validated using a database of 238 endomicroscopy video sequences from 143 patients, shows the ability of the pipeline to produce convincing super-resolved images. A Mean Opinion Score (MOS) study also confirms this quantitative image quality assessment.



### Understanding the Impact of Label Granularity on CNN-based Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.07012v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07012v1)
- **Published**: 2019-01-21 17:53:43+00:00
- **Updated**: 2019-01-21 17:53:43+00:00
- **Authors**: Zhuo Chen, Ruizhou Ding, Ting-Wu Chin, Diana Marculescu
- **Comment**: 10 pages. Accepted at DSBDA with ICDM
- **Journal**: None
- **Summary**: In recent years, supervised learning using Convolutional Neural Networks (CNNs) has achieved great success in image classification tasks, and large scale labeled datasets have contributed significantly to this achievement. However, the definition of a label is often application dependent. For example, an image of a cat can be labeled as "cat" or perhaps more specifically "Persian cat." We refer to this as label granularity. In this paper, we conduct extensive experiments using various datasets to demonstrate and analyze how and why training based on fine-grain labeling, such as "Persian cat" can improve CNN accuracy on classifying coarse-grain classes, in this case "cat." The experimental results show that training CNNs with fine-grain labels improves both network's optimization and generalization capabilities, as intuitively it encourages the network to learn more features, and hence increases classification accuracy on coarse-grain classes under all datasets considered. Moreover, fine-grain labels enhance data efficiency in CNN training. For example, a CNN trained with fine-grain labels and only 40% of the total training data can achieve higher accuracy than a CNN trained with the full training dataset and coarse-grain labels. These results point to two possible applications of this work: (i) with sufficient human resources, one can improve CNN performance by re-labeling the dataset with fine-grain labels, and (ii) with limited human resources, to improve CNN performance, rather than collecting more training data, one may instead use fine-grain labels for the dataset. We further propose a metric called Average Confusion Ratio to characterize the effectiveness of fine-grain labeling, and show its use through extensive experimentation. Code is available at https://github.com/cmu-enyac/Label-Granularity.



### Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs
- **Arxiv ID**: http://arxiv.org/abs/1901.07017v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.07017v2)
- **Published**: 2019-01-21 18:08:49+00:00
- **Updated**: 2019-08-14 10:02:46+00:00
- **Authors**: Nicholas Watters, Loic Matthey, Christopher P. Burgess, Alexander Lerchner
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple neural rendering architecture that helps variational autoencoders (VAEs) learn disentangled representations. Instead of the deconvolutional network typically used in the decoder of VAEs, we tile (broadcast) the latent vector across space, concatenate fixed X- and Y-"coordinate" channels, and apply a fully convolutional network with 1x1 stride. This provides an architectural prior for dissociating positional from non-positional features in the latent distribution of VAEs, yet without providing any explicit supervision to this effect. We show that this architecture, which we term the Spatial Broadcast decoder, improves disentangling, reconstruction accuracy, and generalization to held-out regions in data space. It provides a particularly dramatic benefit when applied to datasets with small objects. We also emphasize a method for visualizing learned latent spaces that helped us diagnose our models and may prove useful for others aiming to assess data representations. Finally, we show the Spatial Broadcast Decoder is complementary to state-of-the-art (SOTA) disentangling techniques and when incorporated improves their performance.



### CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison
- **Arxiv ID**: http://arxiv.org/abs/1901.07031v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1901.07031v1)
- **Published**: 2019-01-21 18:41:59+00:00
- **Updated**: 2019-01-21 18:41:59+00:00
- **Authors**: Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David A. Mong, Safwan S. Halabi, Jesse K. Sandberg, Ricky Jones, David B. Larson, Curtis P. Langlotz, Bhavik N. Patel, Matthew P. Lungren, Andrew Y. Ng
- **Comment**: Published in AAAI 2019
- **Journal**: None
- **Summary**: Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models.   The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .



### MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs
- **Arxiv ID**: http://arxiv.org/abs/1901.07042v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1901.07042v5)
- **Published**: 2019-01-21 19:01:00+00:00
- **Updated**: 2019-11-14 17:34:51+00:00
- **Authors**: Alistair E. W. Johnson, Tom J. Pollard, Nathaniel R. Greenbaum, Matthew P. Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G. Mark, Seth J. Berkowitz, Steven Horng
- **Comment**: None
- **Journal**: None
- **Summary**: Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's thorax, but requiring specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. However, a key challenge in the development of these techniques is the lack of sufficient data. Here we describe MIMIC-CXR-JPG v2.0.0, a large dataset of 377,110 chest x-rays associated with 227,827 imaging studies sourced from the Beth Israel Deaconess Medical Center between 2011 - 2016. Images are provided with 14 labels derived from two natural language processing tools applied to the corresponding free-text radiology reports. MIMIC-CXR-JPG is derived entirely from the MIMIC-CXR database, and aims to provide a convenient processed version of MIMIC-CXR, as well as to provide a standard reference for data splits and image labels. All images have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in medical computer vision.



### Partition Pruning: Parallelization-Aware Pruning for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.11391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11391v2)
- **Published**: 2019-01-21 19:34:21+00:00
- **Updated**: 2019-02-27 07:13:53+00:00
- **Authors**: Sina Shahhosseini, Ahmad Albaqsami, Masoomeh Jasemi, Nader Bagherzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Parameters of recent neural networks require a huge amount of memory. These parameters are used by neural networks to perform machine learning tasks when processing inputs. To speed up inference, we develop Partition Pruning, an innovative scheme to reduce the parameters used while taking into consideration parallelization. We evaluated the performance and energy consumption of parallel inference of partitioned models, which showed a 7.72x speed up of performance and a 2.73x reduction in the energy used for computing pruned layers of TinyVGG16 in comparison to running the unpruned model on a single accelerator. In addition, our method showed a limited reduction some numbers in accuracy while partitioning fully connected layers.



### On Compression of Unsupervised Neural Nets by Pruning Weak Connections
- **Arxiv ID**: http://arxiv.org/abs/1901.07066v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1901.07066v3)
- **Published**: 2019-01-21 20:19:27+00:00
- **Updated**: 2021-05-08 14:19:42+00:00
- **Authors**: Zhiwen Zuo, Lei Zhao, Liwen Zuo, Feng Jiang, Wei Xing, Dongming Lu
- **Comment**: This paper needs to be further revised
- **Journal**: None
- **Summary**: Unsupervised neural nets such as Restricted Boltzmann Machines(RBMs) and Deep Belif Networks(DBNs), are powerful in automatic feature extraction,unsupervised weight initialization and density estimation. In this paper,we demonstrate that the parameters of these neural nets can be dramatically reduced without affecting their performance. We describe a method to reduce the parameters required by RBM which is the basic building block for deep architectures. Further we propose an unsupervised sparse deep architectures selection algorithm to form sparse deep neural networks.Experimental results show that there is virtually no loss in either generative or discriminative performance.



### Robust Angular Local Descriptor Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.07076v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07076v2)
- **Published**: 2019-01-21 20:55:53+00:00
- **Updated**: 2019-01-26 19:38:59+00:00
- **Authors**: Yanwu Xu, Mingming Gong, Tongliang Liu, Kayhan Batmanghelich, Chaohui Wang
- **Comment**: Accepted by ACCV2018
- **Journal**: None
- **Summary**: In recent years, the learned local descriptors have outperformed handcrafted ones by a large margin, due to the powerful deep convolutional neural network architectures such as L2-Net [1] and triplet based metric learning [2]. However, there are two problems in the current methods, which hinders the overall performance. Firstly, the widely-used margin loss is sensitive to incorrect correspondences, which are prevalent in the existing local descriptor learning datasets. Second, the L2 distance ignores the fact that the feature vectors have been normalized to unit norm. To tackle these two problems and further boost the performance, we propose a robust angular loss which 1) uses cosine similarity instead of L2 distance to compare descriptors and 2) relies on a robust loss function that gives smaller penalty to triplets with negative relative similarity. The resulting descriptor shows robustness on different datasets, reaching the state-of-the-art result on Brown dataset , as well as demonstrating excellent generalization ability on the Hpatches dataset and a Wide Baseline Stereo dataset.



### Modeling Human Motion with Quaternion-based Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.07677v2
- **DOI**: 10.1007/s11263-019-01245-6
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1901.07677v2)
- **Published**: 2019-01-21 23:56:54+00:00
- **Updated**: 2019-10-26 14:49:53+00:00
- **Authors**: Dario Pavllo, Christoph Feichtenhofer, Michael Auli, David Grangier
- **Comment**: Follow-up work of arXiv:1805.06485. This is a pre-print of an article
  published in IJCV. The final authenticated version is available online at
  https://doi.org/10.1007/s11263-019-01245-6
- **Journal**: International Journal of Computer Vision (Special Issue on Machine
  Vision with Deep Learning), 2019. Online ISSN: 1573-1405
- **Summary**: Previous work on predicting or generating 3D human pose sequences regresses either joint rotations or joint positions. The former strategy is prone to error accumulation along the kinematic chain, as well as discontinuities when using Euler angles or exponential maps as parameterizations. The latter requires re-projection onto skeleton constraints to avoid bone stretching and invalid configurations. This work addresses both limitations. QuaterNet represents rotations with quaternions and our loss function performs forward kinematics on a skeleton to penalize absolute position errors instead of angle errors. We investigate both recurrent and convolutional architectures and evaluate on short-term prediction and long-term generation. For the latter, our approach is qualitatively judged as realistic as recent neural strategies from the graphics literature. Our experiments compare quaternions to Euler angles as well as exponential maps and show that only a very short context is required to make reliable future predictions. Finally, we show that the standard evaluation protocol for Human3.6M produces high variance results and we propose a simple solution.




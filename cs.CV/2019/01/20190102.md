# Arxiv Papers in cs.CV on 2019-01-02
### Ancient Painting to Natural Image: A New Solution for Painting Processing
- **Arxiv ID**: http://arxiv.org/abs/1901.00224v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00224v2)
- **Published**: 2019-01-02 00:35:19+00:00
- **Updated**: 2019-03-29 02:59:47+00:00
- **Authors**: Tingting Qiao, Weijing Zhang, Miao Zhang, Zixuan Ma, Duanqing Xu
- **Comment**: 10 pages, 6 figures, published in WACV 2019
- **Journal**: None
- **Summary**: Collecting a large-scale and well-annotated dataset for image processing has become a common practice in computer vision. However, in the ancient painting area, this task is not practical as the number of paintings is limited and their style is greatly diverse. We, therefore, propose a novel solution for the problems that come with ancient painting processing. This is to use domain transfer to convert ancient paintings to photo-realistic natural images. By doing so, the ancient painting processing problems become natural image processing problems and models trained on natural images can be directly applied to the transferred paintings. Specifically, we focus on Chinese ancient flower, bird and landscape paintings in this work. A novel Domain Style Transfer Network (DSTN) is proposed to transfer ancient paintings to natural images which employ a compound loss to ensure that the transferred paintings still maintain the color composition and content of the input paintings. The experiment results show that the transferred paintings generated by the DSTN have a better performance in both the human perceptual test and other image processing tasks than other state-of-art methods, indicating the authenticity of the transferred paintings and the superiority of the proposed method.



### A Survey on Multi-output Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.00248v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.00248v2)
- **Published**: 2019-01-02 03:10:24+00:00
- **Updated**: 2019-10-13 10:59:28+00:00
- **Authors**: Donna Xu, Yaxin Shi, Ivor W. Tsang, Yew-Soon Ong, Chen Gong, Xiaobo Shen
- **Comment**: Paper accepted by IEEE Transactions on Neural Networks and Learning
  Systems
- **Journal**: None
- **Summary**: Multi-output learning aims to simultaneously predict multiple outputs given an input. It is an important learning problem due to the pressing need for sophisticated decision making in real-world applications. Inspired by big data, the 4Vs characteristics of multi-output imposes a set of challenges to multi-output learning, in terms of the volume, velocity, variety and veracity of the outputs. Increasing number of works in the literature have been devoted to the study of multi-output learning and the development of novel approaches for addressing the challenges encountered. However, it lacks a comprehensive overview on different types of challenges of multi-output learning brought by the characteristics of the multiple outputs and the techniques proposed to overcome the challenges. This paper thus attempts to fill in this gap to provide a comprehensive review on this area. We first introduce different stages of the life cycle of the output labels. Then we present the paradigm on multi-output learning, including its myriads of output structures, definitions of its different sub-problems, model evaluation metrics and popular data repositories used in the study. Subsequently, we review a number of state-of-the-art multi-output learning methods, which are categorized based on the challenges.



### Vector and Line Quantization for Billion-scale Similarity Search on GPUs
- **Arxiv ID**: http://arxiv.org/abs/1901.00275v2
- **DOI**: 10.1016/j.future.2019.04.033
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00275v2)
- **Published**: 2019-01-02 06:09:12+00:00
- **Updated**: 2019-04-18 11:06:20+00:00
- **Authors**: Wei Chen, Jincai Chen, Fuhao Zou, Yuan-Fang Li, Ping Lu, Qiang Wang, Wei Zhao
- **Comment**: Accepted by Future Generation Computer Systems (FGCS)
- **Journal**: None
- **Summary**: Billion-scale high-dimensional approximate nearest neighbour (ANN) search has become an important problem for searching similar objects among the vast amount of images and videos available online. The existing ANN methods are usually characterized by their specific indexing structures, including the inverted index and the inverted multi-index structure. The inverted index structure is amenable to GPU-based implementations, and the state-of-the-art systems such as Faiss are able to exploit the massive parallelism offered by GPUs. However, the inverted index requires high memory overhead to index the dataset effectively. The inverted multi-index structure is difficult to implement for GPUs, and also ineffective in dealing with database with different data distributions. In this paper we propose a novel hierarchical inverted index structure generated by vector and line quantization methods. Our quantization method improves both search efficiency and accuracy, while maintaining comparable memory consumption. This is achieved by reducing search space and increasing the number of indexed regions. We introduce a new ANN search system, VLQ-ADC, that is based on the proposed inverted index, and perform extensive evaluation on two public billion-scale benchmark datasets SIFT1B and DEEP1B. Our evaluation shows that VLQ-ADC significantly outperforms the state-of-the-art GPU- and CPU-based systems in terms of both accuracy and search speed. The source code of VLQ-ADC is available at https://github.com/zjuchenwei/vector-line-quantization.



### On Minimum Discrepancy Estimation for Deep Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1901.00282v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00282v1)
- **Published**: 2019-01-02 07:17:58+00:00
- **Updated**: 2019-01-02 07:17:58+00:00
- **Authors**: Mohammad Mahfujur Rahman, Clinton Fookes, Mahsa Baktashmotlagh, Sridha Sridharan
- **Comment**: Accepted in Joint IJCAI/ECAI/AAMAS/ICML 2018 Workshop
- **Journal**: None
- **Summary**: In the presence of large sets of labeled data, Deep Learning (DL) has accomplished extraordinary triumphs in the avenue of computer vision, particularly in object classification and recognition tasks. However, DL cannot always perform well when the training and testing images come from different distributions or in the presence of domain shift between training and testing images. They also suffer in the absence of labeled input data. Domain adaptation (DA) methods have been proposed to make up the poor performance due to domain shift. In this paper, we present a new unsupervised deep domain adaptation method based on the alignment of second order statistics (covariances) as well as maximum mean discrepancy of the source and target data with a two stream Convolutional Neural Network (CNN). We demonstrate the ability of the proposed approach to achieve state-of the-art performance for image classification on three benchmark domain adaptation datasets: Office-31 [27], Office-Home [37] and Office-Caltech [8].



### SIXray : A Large-scale Security Inspection X-ray Benchmark for Prohibited Item Discovery in Overlapping Images
- **Arxiv ID**: http://arxiv.org/abs/1901.00303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00303v1)
- **Published**: 2019-01-02 09:23:42+00:00
- **Updated**: 2019-01-02 09:23:42+00:00
- **Authors**: Caijing Miao, Lingxi Xie, Fang Wan, Chi Su, Hongye Liu, Jianbin Jiao, Qixiang Ye
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a large-scale dataset and establish a baseline for prohibited item discovery in Security Inspection X-ray images. Our dataset, named SIXray, consists of 1,059,231 X-ray images, in which 6 classes of 8,929 prohibited items are manually annotated. It raises a brand new challenge of overlapping image data, meanwhile shares the same properties with existing datasets, including complex yet meaningless contexts and class imbalance. We propose an approach named class-balanced hierarchical refinement (CHR) to deal with these difficulties. CHR assumes that each input image is sampled from a mixture distribution, and that deep networks require an iterative process to infer image contents accurately. To accelerate, we insert reversed connections to different network backbones, delivering high-level visual cues to assist mid-level features. In addition, a class-balanced loss function is designed to maximally alleviate the noise introduced by easy negative samples. We evaluate CHR on SIXray with different ratios of positive/negative samples. Compared to the baselines, CHR enjoys a better ability of discriminating objects especially using mid-level features, which offers the possibility of using a weakly-supervised approach towards accurate object localization. In particular, the advantage of CHR is more significant in the scenarios with fewer positive training samples, which demonstrates its potential application in real-world security inspection.



### Plugin Networks for Inference under Partial Evidence
- **Arxiv ID**: http://arxiv.org/abs/1901.00326v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00326v3)
- **Published**: 2019-01-02 11:30:19+00:00
- **Updated**: 2020-03-05 05:51:02+00:00
- **Authors**: Michal Koperski, Tomasz Konopczynski, Rafał Nowak, Piotr Semberecki, Tomasz Trzcinski
- **Comment**: Accepted to WACV 2020
- **Journal**: None
- **Summary**: In this paper, we propose a novel method to incorporate partial evidence in the inference of deep convolutional neural networks. Contrary to the existing, top performing methods, which either iteratively modify the input of the network or exploit external label taxonomy to take the partial evidence into account, we add separate network modules ("Plugin Networks") to the intermediate layers of a pre-trained convolutional network. The goal of these modules is to incorporate additional signal, ie information about known labels, into the inference procedure and adjust the predicted output accordingly. Since the attached plugins have a simple structure, consisting of only fully connected layers, we drastically reduced the computational cost of training and inference. At the same time, the proposed architecture allows to propagate information about known labels directly to the intermediate layers to improve the final representation. Extensive evaluation of the proposed method confirms that our Plugin Networks outperform the state-of-the-art in a variety of tasks, including scene categorization, multi-label image annotation, and semantic segmentation.



### Optical Fringe Patterns Filtering Based on Multi-Stage Convolution Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1901.00361v3
- **DOI**: 10.1016/j.optlaseng.2019.105853
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00361v3)
- **Published**: 2019-01-02 13:55:16+00:00
- **Updated**: 2020-07-02 05:44:47+00:00
- **Authors**: Bowen Lin, Shujun Fu, Caiming Zhang, Fengling Wang, Yuliang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Optical fringe patterns are often contaminated by speckle noise, making it difficult to accurately and robustly extract their phase fields. To deal with this problem, we propose a filtering method based on deep learning, called optical fringe patterns denoising convolutional neural network (FPD-CNN), for directly removing speckle from the input noisy fringe patterns. Regularization technology is integrated into the design of deep architecture. Specifically, the FPD-CNN method is divided into multiple stages, each stage consists of a set of convolutional layers along with batch normalization and leaky rectified linear unit (Leaky ReLU) activation function. The end-to-end joint training is carried out using the Euclidean loss. Extensive experiments on simulated and experimental optical fringe patterns,especially finer ones with high-density regions, show that the proposed method is competitive with some state-of-the-art denoising techniques in spatial or transform domains, efficiently preserving main features of fringe at a fairly fast speed.



### Detecting Text in the Wild with Deep Character Embedding Network
- **Arxiv ID**: http://arxiv.org/abs/1901.00363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00363v1)
- **Published**: 2019-01-02 14:00:33+00:00
- **Updated**: 2019-01-02 14:00:33+00:00
- **Authors**: Jiaming Liu, Chengquan Zhang, Yipeng Sun, Junyu Han, Errui Ding
- **Comment**: Asian Conference on Computer Vision 2018
- **Journal**: None
- **Summary**: Most text detection methods hypothesize texts are horizontal or multi-oriented and thus define quadrangles as the basic detection unit. However, text in the wild is usually perspectively distorted or curved, which can not be easily tackled by existing approaches. In this paper, we propose a deep character embedding network (CENet) which simultaneously predicts the bounding boxes of characters and their embedding vectors, thus making text detection a simple clustering task in the character embedding space. The proposed method does not require strong assumptions of forming a straight line on general text detection, which provides flexibility on arbitrarily curved or perspectively distorted text. For character detection task, a dense prediction subnetwork is designed to obtain the confidence score and bounding boxes of characters. For character embedding task, a subnet is trained with contrastive loss to project detected characters into embedding space. The two tasks share a backbone CNN from which the multi-scale feature maps are extracted. The final text regions can be easily achieved by a thresholding process on character confidence and embedding distance of character pairs. We evaluated our method on ICDAR13, ICDAR15, MSRA-TD500, and Total-Text. The proposed method achieves state-of-the-art or comparable performance on all these datasets, and shows substantial improvement in the irregular-text datasets, i.e. Total-Text.



### Learning Efficient Detector with Semi-supervised Adaptive Distillation
- **Arxiv ID**: http://arxiv.org/abs/1901.00366v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00366v2)
- **Published**: 2019-01-02 14:03:32+00:00
- **Updated**: 2019-01-14 05:50:37+00:00
- **Authors**: Shitao Tang, Litong Feng, Wenqi Shao, Zhanghui Kuang, Wei Zhang, Yimin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) has been used in image classification for model compression. However, rare studies apply this technology on single-stage object detectors. Focal loss shows that the accumulated errors of easily-classified samples dominate the overall loss in the training process. This problem is also encountered when applying KD in the detection task. For KD, the teacher-defined hard samples are far more important than any others. We propose ADL to address this issue by adaptively mimicking the teacher's logits, with more attention paid on two types of hard samples: hard-to-learn samples predicted by teacher with low certainty and hard-to-mimic samples with a large gap between the teacher's and the student's prediction. ADL enlarges the distillation loss for hard-to-learn and hard-to-mimic samples and reduces distillation loss for the dominant easy samples, enabling distillation to work on the single-stage detector first time, even if the student and the teacher are identical. Besides, ADL is effective in both the supervised setting and the semi-supervised setting, even when the labeled data and unlabeled data are from different distributions. For distillation on unlabeled data, ADL achieves better performance than existing data distillation which simply utilizes hard targets, making the student detector surpass its teacher. On the COCO database, semi-supervised adaptive distillation (SAD) makes a student detector with a backbone of ResNet-50 surpasses its teacher with a backbone of ResNet-101, while the student has half of the teacher's computation complexity. The code is avaiable at https://github.com/Tangshitao/Semi-supervised-Adaptive-Distillation



### Attribute-Aware Attention Model for Fine-grained Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.00392v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00392v2)
- **Published**: 2019-01-02 14:22:59+00:00
- **Updated**: 2019-12-20 12:29:29+00:00
- **Authors**: Kai Han, Jianyuan Guo, Chao Zhang, Mingjian Zhu
- **Comment**: Accepted by ACM Multimedia 2018 (Oral). Code is available at
  https://github.com/iamhankai/attribute-aware-attention
- **Journal**: None
- **Summary**: How to learn a discriminative fine-grained representation is a key point in many computer vision applications, such as person re-identification, fine-grained classification, fine-grained image retrieval, etc. Most of the previous methods focus on learning metrics or ensemble to derive better global representation, which are usually lack of local information. Based on the considerations above, we propose a novel Attribute-Aware Attention Model ($A^3M$), which can learn local attribute representation and global category representation simultaneously in an end-to-end manner. The proposed model contains two attention models: attribute-guided attention module uses attribute information to help select category features in different regions, at the same time, category-guided attention module selects local features of different attributes with the help of category cues. Through this attribute-category reciprocal process, local and global features benefit from each other. Finally, the resulting feature contains more intrinsic information for image recognition instead of the noisy and irrelevant features. Extensive experiments conducted on Market-1501, CompCars, CUB-200-2011 and CARS196 demonstrate the effectiveness of our $A^3M$. Code is available at https://github.com/iamhankai/attribute-aware-attention.



### Application of image processing in optical method, Moire deflectometry for investigating the optical properties of zinc oxide nanoparticle
- **Arxiv ID**: http://arxiv.org/abs/1902.01196v1
- **DOI**: 10.13140/RG.2.2.23059.32804
- **Categories**: **physics.ins-det**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.01196v1)
- **Published**: 2019-01-02 14:48:05+00:00
- **Updated**: 2019-01-02 14:48:05+00:00
- **Authors**: Fatemeh Jamal, Fatemeh Ahmadi, Mohammad Khanzadeh, Saber Malekzadeh
- **Comment**: Submitted to "Optic and laser technology" journal
- **Journal**: None
- **Summary**: Nowadays for measurement of refractive index of nanomaterials usually spectro-photometric and mechanical methods are used which are expensive and indirect. In this paper for measuring these parameters of zinc oxide nanomaterial with two different stabilizers, a simple optical method, Moire deflectometry, which is based on wave front analysis and geometric optics is used. In the Moire deflectometry method, the beam of light from the laser diode passes through the sample. As a result of that, a change in the sample environment is observed as deflections of the fringes. By recording of these deflections using CCD and image processing with MATLAB, the nanomaterials refractive indices can be calculated. Due to the high accuracy of this method and improved the image processing code in Matlab, the smallest changes of the refractive index in the sample can be measured. Digital Image processing is used for processing images in a way that features can be selected and being shown. The results obtained in this method show a good improvement over the other used methods.



### Lipi Gnani - A Versatile OCR for Documents in any Language Printed in Kannada Script
- **Arxiv ID**: http://arxiv.org/abs/1901.00413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00413v1)
- **Published**: 2019-01-02 15:22:41+00:00
- **Updated**: 2019-01-02 15:22:41+00:00
- **Authors**: Shiva Kumar H R, Ramakrishnan A G
- **Comment**: 21 pages, 16 figures, 12 tables, submitted to ACM Transactions on
  Asian and Low-Resource Language Information Processing
- **Journal**: None
- **Summary**: A Kannada OCR, named Lipi Gnani, has been designed and developed from scratch, with the motivation of it being able to convert printed text or poetry in Kannada script, without any restriction on vocabulary. The training and test sets have been collected from over 35 books published between the period 1970 to 2002, and this includes books written in Halegannada and pages containing Sanskrit slokas written in Kannada script. The coverage of the OCR is nearly complete in the sense that it recognizes all the punctuation marks, special symbols, Indo-Arabic and Kannada numerals and also the interspersed English words. Several minor and major original contributions have been done in developing this OCR at the different processing stages such as binarization, line and character segmentation, recognition and Unicode mapping. This has created a Kannada OCR that performs as good as, and in some cases, better than the Google's Tesseract OCR, as shown by the results. To the knowledge of the authors, this is the maiden report of a complete Kannada OCR, handling all the issues involved. Currently, there is no dictionary based postprocessing, and the obtained results are due solely to the recognition process. Four benchmark test databases containing scanned pages from books in Kannada, Sanskrit, Konkani and Tulu languages, but all of them printed in Kannada script, have been created. The word level recognition accuracy of Lipi Gnani is 4% higher on the Kannada dataset than that of Google's Tesseract OCR, 8% higher on the datasets of Tulu and Sanskrit, and 25% higher on the Konkani dataset.



### Improved Hyperspectral Unmixing With Endmember Variability Parametrized Using an Interpolated Scaling Tensor
- **Arxiv ID**: http://arxiv.org/abs/1901.00463v1
- **DOI**: 10.1109/ICASSP.2019.8683155
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00463v1)
- **Published**: 2019-01-02 17:51:12+00:00
- **Updated**: 2019-01-02 17:51:12+00:00
- **Authors**: Ricardo Augusto Borsoi, Tales Imbiriba, José Carlos Moreira Bermudez
- **Comment**: None
- **Journal**: None
- **Summary**: Endmember (EM) variability has an important impact on the performance of hyperspectral image (HI) analysis algorithms. Recently, extended linear mixing models have been proposed to account for EM variability in the spectral unmixing (SU) problem. The direct use of these models has led to severely ill-posed optimization problems. Different regularization strategies have been considered to deal with this issue, but none so far has consistently exploited the information provided by the existence of multiple pure pixels often present in HIs. In this work, we propose to break the SU problem into a sequence of two problems. First, we use pure pixel information to estimate an interpolated tensor of scaling factors representing spectral variability. This is done by considering the spectral variability to be a smooth function over the HI and confining the energy of the scaling tensor to a low-rank structure. Afterwards, we solve a matrix-factorization problem to estimate the fractional abundances using the variability scaling factors estimated in the previous step, what leads to a significantly more well-posed problem. Simulation swith synthetic and real data attest the effectiveness of the proposed strategy.



### Learning Generalizable Physical Dynamics of 3D Rigid Objects
- **Arxiv ID**: http://arxiv.org/abs/1901.00466v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.00466v1)
- **Published**: 2019-01-02 17:57:50+00:00
- **Updated**: 2019-01-02 17:57:50+00:00
- **Authors**: Davis Rempe, Srinath Sridhar, He Wang, Leonidas J. Guibas
- **Comment**: 13 pages, 12 figures, 1 table
- **Journal**: None
- **Summary**: Humans have a remarkable ability to predict the effect of physical interactions on the dynamics of objects. Endowing machines with this ability would allow important applications in areas like robotics and autonomous vehicles. In this work, we focus on predicting the dynamics of 3D rigid objects, in particular an object's final resting position and total rotation when subjected to an impulsive force. Different from previous work, our approach is capable of generalizing to unseen object shapes - an important requirement for real-world applications. To achieve this, we represent object shape as a 3D point cloud that is used as input to a neural network, making our approach agnostic to appearance variation. The design of our network is informed by an understanding of physical laws. We train our model with data from a physics engine that simulates the dynamics of a large number of shapes. Experiments show that we can accurately predict the resting position and total rotation for unseen object geometries.



### Action2Vec: A Crossmodal Embedding Approach to Action Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.00484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00484v1)
- **Published**: 2019-01-02 18:35:32+00:00
- **Updated**: 2019-01-02 18:35:32+00:00
- **Authors**: Meera Hahn, Andrew Silva, James M. Rehg
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a novel cross-modal embedding space for actions, named Action2Vec, which combines linguistic cues from class labels with spatio-temporal features derived from video clips. Our approach uses a hierarchical recurrent network to capture the temporal structure of video features. We train our embedding using a joint loss that combines classification accuracy with similarity to Word2Vec semantics. We evaluate Action2Vec by performing zero shot action recognition and obtain state of the art results on three standard datasets. In addition, we present two novel analogy tests which quantify the extent to which our joint embedding captures distributional semantics. This is the first joint embedding space to combine verbs and action videos, and the first to be thoroughly evaluated with respect to its distributional semantics.



### Improving Face Anti-Spoofing by 3D Virtual Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1901.00488v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00488v3)
- **Published**: 2019-01-02 18:40:33+00:00
- **Updated**: 2021-02-10 15:37:37+00:00
- **Authors**: Jianzhu Guo, Xiangyu Zhu, Jinchuan Xiao, Zhen Lei, Genxun Wan, Stan Z. Li
- **Comment**: Accepted to ICB 2019 Oral
- **Journal**: None
- **Summary**: Face anti-spoofing is crucial for the security of face recognition systems. Learning based methods especially deep learning based methods need large-scale training samples to reduce overfitting. However, acquiring spoof data is very expensive since the live faces should be re-printed and re-captured in many views. In this paper, we present a method to synthesize virtual spoof data in 3D space to alleviate this problem. Specifically, we consider a printed photo as a flat surface and mesh it into a 3D object, which is then randomly bent and rotated in 3D space. Afterward, the transformed 3D photo is rendered through perspective projection as a virtual sample. The synthetic virtual samples can significantly boost the anti-spoofing performance when combined with a proposed data balancing strategy. Our promising results open up new possibilities for advancing face anti-spoofing using cheap and large-scale synthetic data.



### Flow Based Self-supervised Pixel Embedding for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.00520v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00520v2)
- **Published**: 2019-01-02 20:24:41+00:00
- **Updated**: 2019-01-08 20:01:48+00:00
- **Authors**: Bin Ma, Shubao Liu, Yingxuan Zhi, Qi Song
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new self-supervised approach to image feature learning from motion cue. This new approach leverages recent advances in deep learning in two directions: 1) the success of training deep neural network in estimating optical flow in real data using synthetic flow data; and 2) emerging work in learning image features from motion cues, such as optical flow. Building on these, we demonstrate that image features can be learned in self-supervision by first training an optical flow estimator with synthetic flow data, and then learning image features from the estimated flows in real motion data. We demonstrate and evaluate this approach on an image segmentation task. Using the learned image feature representation, the network performs significantly better than the ones trained from scratch in few-shot segmentation tasks.



### Linear colour segmentation revisited
- **Arxiv ID**: http://arxiv.org/abs/1901.00534v1
- **DOI**: 10.1117/12.2523007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00534v1)
- **Published**: 2019-01-02 21:06:55+00:00
- **Updated**: 2019-01-02 21:06:55+00:00
- **Authors**: Anna Smagina, Valentina Bozhkova, Sergey Gladilin, Dmitry Nikolaev
- **Comment**: None
- **Journal**: Proc. SPIE 11041, Eleventh International Conference on Machine
  Vision (ICMV 2018)
- **Summary**: In this work we discuss the known algorithms for linear colour segmentation based on a physical approach and propose a new modification of segmentation algorithm. This algorithm is based on a region adjacency graph framework without a pre-segmentation stage. Proposed edge weight functions are defined from linear image model with normal noise. The colour space projective transform is introduced as a novel pre-processing technique for better handling of shadow and highlight areas. The resulting algorithm is tested on a benchmark dataset consisting of the images of 19 natural scenes selected from the Barnard's DXC-930 SFU dataset and 12 natural scene images newly published for common use. The dataset is provided with pixel-by-pixel ground truth colour segmentation for every image. Using this dataset, we show that the proposed algorithm modifications lead to qualitative advantages over other model-based segmentation algorithms, and also show the positive effect of each proposed modification. The source code and datasets for this work are available for free access at http://github.com/visillect/segmentation.



### Visualizing Deep Similarity Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.00536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.00536v1)
- **Published**: 2019-01-02 21:24:11+00:00
- **Updated**: 2019-01-02 21:24:11+00:00
- **Authors**: Abby Stylianou, Richard Souvenir, Robert Pless
- **Comment**: None
- **Journal**: None
- **Summary**: For convolutional neural network models that optimize an image embedding, we propose a method to highlight the regions of images that contribute most to pairwise similarity. This work is a corollary to the visualization tools developed for classification networks, but applicable to the problem domains better suited to similarity learning. The visualization shows how similarity networks that are fine-tuned learn to focus on different features. We also generalize our approach to embedding networks that use different pooling strategies and provide a simple mechanism to support image similarity searches on objects or sub-regions in the query image.



### Photo-Sketching: Inferring Contour Drawings from Images
- **Arxiv ID**: http://arxiv.org/abs/1901.00542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.00542v1)
- **Published**: 2019-01-02 21:59:44+00:00
- **Updated**: 2019-01-02 21:59:44+00:00
- **Authors**: Mengtian Li, Zhe Lin, Radomir Mech, Ersin Yumer, Deva Ramanan
- **Comment**: WACV 2019. For code and dataset, see
  http://www.cs.cmu.edu/~mengtial/proj/sketch/
- **Journal**: None
- **Summary**: Edges, boundaries and contours are important subjects of study in both computer graphics and computer vision. On one hand, they are the 2D elements that convey 3D shapes, on the other hand, they are indicative of occlusion events and thus separation of objects or semantic concepts. In this paper, we aim to generate contour drawings, boundary-like drawings that capture the outline of the visual scene. Prior art often cast this problem as boundary detection. However, the set of visual cues presented in the boundary detection output are different from the ones in contour drawings, and also the artistic style is ignored. We address these issues by collecting a new dataset of contour drawings and proposing a learning-based method that resolves diversity in the annotation and, unlike boundary detectors, can work with imperfect alignment of the annotation and the actual ground truth. Our method surpasses previous methods quantitatively and qualitatively. Surprisingly, when our model fine-tunes on BSDS500, we achieve the state-of-the-art performance in salient boundary detection, suggesting contour drawing might be a scalable alternative to boundary annotation, which at the same time is easier and more interesting for annotators to draw.



### Multi-class Classification without Multi-class Labels
- **Arxiv ID**: http://arxiv.org/abs/1901.00544v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.00544v1)
- **Published**: 2019-01-02 22:09:12+00:00
- **Updated**: 2019-01-02 22:09:12+00:00
- **Authors**: Yen-Chang Hsu, Zhaoyang Lv, Joel Schlosser, Phillip Odom, Zsolt Kira
- **Comment**: International Conference on Learning Representations (ICLR 2019)
- **Journal**: None
- **Summary**: This work presents a new strategy for multi-class classification that requires no class-specific labels, but instead leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. We then demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings. Our method is evaluated against state of the art in all three learning paradigms and shows a superior or comparable accuracy, providing evidence that learning multi-class classification without multi-class labels is a viable learning option.




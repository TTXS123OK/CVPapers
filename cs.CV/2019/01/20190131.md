# Arxiv Papers in cs.CV on 2019-01-31
### Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images
- **Arxiv ID**: http://arxiv.org/abs/1901.11153v2
- **DOI**: 10.1109/ICCV.2019.00278
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11153v2)
- **Published**: 2019-01-31 00:01:25+00:00
- **Updated**: 2019-07-29 01:50:59+00:00
- **Authors**: Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, Shengping Zhang
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method.



### Spatial-Temporal Graph Convolutional Networks for Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.11164v2
- **DOI**: 10.1007/978-3-030-30493-5_59
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.11164v2)
- **Published**: 2019-01-31 01:25:47+00:00
- **Updated**: 2020-05-20 01:19:54+00:00
- **Authors**: Cleison Correia de Amorim, David Macêdo, Cleber Zanchettin
- **Comment**: None
- **Journal**: 2019 International Conference on Artificial Neural Networks
  (ICANN)
- **Summary**: The recognition of sign language is a challenging task with an important role in society to facilitate the communication of deaf persons. We propose a new approach of Spatial-Temporal Graph Convolutional Network to sign language recognition based on the human skeletal movements. The method uses graphs to capture the signs dynamics in two dimensions, spatial and temporal, considering the complex aspects of the language. Additionally, we present a new dataset of human skeletons for sign language based on ASLLVD to contribute to future related studies.



### Human Face Expressions from Images - 2D Face Geometry and 3D Face Local Motion versus Deep Neural Features
- **Arxiv ID**: http://arxiv.org/abs/1901.11179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11179v1)
- **Published**: 2019-01-31 02:32:47+00:00
- **Updated**: 2019-01-31 02:32:47+00:00
- **Authors**: Rafal Pilarczyk, Xin Chang, Wladyslaw Skarbek
- **Comment**: None
- **Journal**: None
- **Summary**: Several computer algorithms for recognition of visible human emotions are compared at the web camera scenario using CNN/MMOD face detector. The recognition refers to four face expressions: smile, surprise, anger, and neutral. At the feature extraction stage, the following three concepts of face description are confronted: (a) static 2D face geometry represented by its 68 characteristic landmarks (FP68); (b) dynamic 3D geometry defined by motion parameters for eight distinguished face parts (denoted as AU8) of personalized Candide-3 model; (c) static 2D visual description as 2D array of gray scale pixels (known as facial raw image). At the classification stage, the performance of two major models are analyzed: (a) support vector machine (SVM) with kernel options; (b) convolutional neural network (CNN) with variety of relevant tensor processing layers and blocks of them. The models are trained for frontal views of human faces while they are tested for arbitrary head poses. For geometric features, the success rate (accuracy) indicate nearly triple increase of performance of CNN with respect to SVM classifiers. For raw images, CNN outperforms in accuracy its best geometric counterpart (AU/CNN) by about 30 percent while the best SVM solutions are inferior nearly four times. For F-score the high advantage of raw/CNN over geometric/CNN and geometric/SVM is observed, as well. We conclude that contrary to CNN based emotion classifiers, the generalization capability wrt human head pose is for SVM based emotion classifiers poor.



### On Intra-Class Variance for Deep Learning of Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1901.11186v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11186v2)
- **Published**: 2019-01-31 02:54:14+00:00
- **Updated**: 2019-04-22 08:45:36+00:00
- **Authors**: Rafal Pilarczyk, Wladyslaw Skarbek
- **Comment**: changed abstract, character-level errors have been fixed
- **Journal**: None
- **Summary**: A novel technique for deep learning of image classifiers is presented. The learned CNN models offer better separation of deep features (also known as embedded vectors) measured by Euclidean proximity and also no deterioration of the classification results by class membership probability. The latter feature can be used for enhancing image classifiers having the classes at the model's exploiting stage different from from classes during the training stage. While the Shannon information of SoftMax probability for target class is extended for mini-batch by the intra-class variance, the trained network itself is extended by the Hadamard layer with the parameters representing the class centers. Contrary to the existing solutions, this extra neural layer enables interfacing of the training algorithm to the standard stochastic gradient optimizers, e.g. AdaM algorithm. Moreover, this approach makes the computed centroids immediately adapting to the updating embedded vectors and finally getting the comparable accuracy in less epochs.



### Augmenting Model Robustness with Transformation-Invariant Attacks
- **Arxiv ID**: http://arxiv.org/abs/1901.11188v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11188v2)
- **Published**: 2019-01-31 02:56:28+00:00
- **Updated**: 2019-06-13 22:32:45+00:00
- **Authors**: Houpu Yao, Zhe Wang, Guangyu Nie, Yassine Mazboudi, Yezhou Yang, Yi Ren
- **Comment**: None
- **Journal**: None
- **Summary**: The vulnerability of neural networks under adversarial attacks has raised serious concerns and motivated extensive research. It has been shown that both neural networks and adversarial attacks against them can be sensitive to input transformations such as linear translation and rotation, and that human vision, which is robust against adversarial attacks, is invariant to natural input transformations. Based on these, this paper tests the hypothesis that model robustness can be further improved when it is adversarially trained against transformed attacks and transformation-invariant attacks. Experiments on MNIST, CIFAR-10, and restricted ImageNet show that while transformations of attacks alone do not affect robustness, transformation-invariant attacks can improve model robustness by 2.5\% on MNIST, 3.7\% on CIFAR-10, and 1.1\% on restricted ImageNet. We discuss the intuition behind this phenomenon.



### Joint Iris Segmentation and Localization Using Deep Multi-task Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/1901.11195v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11195v2)
- **Published**: 2019-01-31 03:16:15+00:00
- **Updated**: 2019-06-19 03:54:03+00:00
- **Authors**: Caiyong Wang, Yuhao Zhu, Yunfan Liu, Ran He, Zhenan Sun
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Iris segmentation and localization in non-cooperative environment is challenging due to illumination variations, long distances, moving subjects and limited user cooperation, etc. Traditional methods often suffer from poor performance when confronted with iris images captured in these conditions. Recent studies have shown that deep learning methods could achieve impressive performance on iris segmentation task. In addition, as iris is defined as an annular region between pupil and sclera, geometric constraints could be imposed to help locating the iris more accurately and improve the segmentation results. In this paper, we propose a deep multi-task learning framework, named as IrisParseNet, to exploit the inherent correlations between pupil, iris and sclera to boost up the performance of iris segmentation and localization in a unified model. In particular, IrisParseNet firstly applies a Fully Convolutional Encoder-Decoder Attention Network to simultaneously estimate pupil center, iris segmentation mask and iris inner/outer boundary. Then, an effective post-processing method is adopted for iris inner/outer circle localization.To train and evaluate the proposed method, we manually label three challenging iris datasets, namely CASIA-Iris-Distance, UBIRIS.v2, and MICHE-I, which cover various types of noises. Extensive experiments are conducted on these newly annotated datasets, and results show that our method outperforms state-of-the-art methods on various benchmarks. All the ground-truth annotations, annotation codes and evaluation protocols are publicly available at https://github.com/xiamenwcy/IrisParseNet.



### A lossless data hiding scheme in JPEG images with segment coding
- **Arxiv ID**: http://arxiv.org/abs/1901.11203v1
- **DOI**: 10.1117/1.JEI.28.5.053015
- **Categories**: **cs.CV**, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1901.11203v1)
- **Published**: 2019-01-31 04:01:52+00:00
- **Updated**: 2019-01-31 04:01:52+00:00
- **Authors**: Mingming Zhang, Quan Zhou, Yanlang Hu
- **Comment**: 14 pages, 5 figures, 8 tables
- **Journal**: None
- **Summary**: In this paper, we propose a lossless data hiding scheme in JPEG images. After quantified DCT transform, coefficients have characteristics that distribution in high frequencies is relatively sparse and absolute values are small. To improve encoding efficiency, we put forward an encoding algorithm that searches for a high frequency as terminate point and recode the coefficients above, so spare space is reserved to embed secret data and appended data with no file expansion. Receiver can obtain terminate point through data analysis, extract additional data and recover original JPEG images lossless. Experimental results show that the proposed method has a larger capacity than state-of-the-art works.



### Chester: A Web Delivered Locally Computed Chest X-Ray Disease Prediction System
- **Arxiv ID**: http://arxiv.org/abs/1901.11210v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1901.11210v3)
- **Published**: 2019-01-31 04:53:53+00:00
- **Updated**: 2020-02-02 18:34:09+00:00
- **Authors**: Joseph Paul Cohen, Paul Bertin, Vincent Frappier
- **Comment**: Submitted to MIDL2020
- **Journal**: None
- **Summary**: In order to bridge the gap between Deep Learning researchers and medical professionals we develop a very accessible free prototype system which can be used by medical professionals to understand the reality of Deep Learning tools for chest X-ray diagnostics. The system is designed to be a second opinion where a user can process an image to confirm or aid in their diagnosis. Code and network weights are delivered via a URL to a web browser (including cell phones) but the patient data remains on the users machine and all processing occurs locally. This paper discusses the three main components in detail: out-of-distribution detection, disease prediction, and prediction explanation. The system open source and freely available here: https://mlmed.org/tools/xray



### Uncertainty Quantification in Deep MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1901.11228v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.11228v3)
- **Published**: 2019-01-31 06:33:48+00:00
- **Updated**: 2020-04-25 19:26:37+00:00
- **Authors**: Vineet Edupuganti, Morteza Mardani, Shreyas Vasanawala, John Pauly
- **Comment**: None
- **Journal**: None
- **Summary**: Reliable MRI is crucial for accurate interpretation in therapeutic and diagnostic tasks. However, undersampling during MRI acquisition as well as the overparameterized and non-transparent nature of deep learning (DL) leaves substantial uncertainty about the accuracy of DL reconstruction. With this in mind, this study aims to quantify the uncertainty in image recovery with DL models. To this end, we first leverage variational autoencoders (VAEs) to develop a probabilistic reconstruction scheme that maps out (low-quality) short scans with aliasing artifacts to the diagnostic-quality ones. The VAE encodes the acquisition uncertainty in a latent code and naturally offers a posterior of the image from which one can generate pixel variance maps using Monte-Carlo sampling. Accurately predicting risk requires knowledge of the bias as well, for which we leverage Stein's Unbiased Risk Estimator (SURE) as a proxy for mean-squared-error (MSE). Extensive empirical experiments are performed for Knee MRI reconstruction under different training losses (adversarial and pixel-wise) and unrolled recurrent network architectures. Our key observations indicate that: 1) adversarial losses introduce more uncertainty; and 2) recurrent unrolled nets reduce the prediction uncertainty and risk.



### Three-dimensional virtual refocusing of fluorescence microscopy images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1901.11252v2
- **DOI**: 10.1038/s41592-019-0622-5
- **Categories**: **cs.CV**, cs.LG, physics.app-ph, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1901.11252v2)
- **Published**: 2019-01-31 07:56:34+00:00
- **Updated**: 2019-11-02 07:29:25+00:00
- **Authors**: Yichen Wu, Yair Rivenson, Hongda Wang, Yilin Luo, Eyal Ben-David, Laurent A. Bentolila, Christian Pritz, Aydogan Ozcan
- **Comment**: 47 pages, 5 figures (main text)
- **Journal**: Nature Methods (2019)
- **Summary**: Three-dimensional (3D) fluorescence microscopy in general requires axial scanning to capture images of a sample at different planes. Here we demonstrate that a deep convolutional neural network can be trained to virtually refocus a 2D fluorescence image onto user-defined 3D surfaces within the sample volume. With this data-driven computational microscopy framework, we imaged the neuron activity of a Caenorhabditis elegans worm in 3D using a time-sequence of fluorescence images acquired at a single focal plane, digitally increasing the depth-of-field of the microscope by 20-fold without any axial scanning, additional hardware, or a trade-off of imaging resolution or speed. Furthermore, we demonstrate that this learning-based approach can correct for sample drift, tilt, and other image aberrations, all digitally performed after the acquisition of a single fluorescence image. This unique framework also cross-connects different imaging modalities to each other, enabling 3D refocusing of a single wide-field fluorescence image to match confocal microscopy images acquired at different sample planes. This deep learning-based 3D image refocusing method might be transformative for imaging and tracking of 3D biological samples, especially over extended periods of time, mitigating photo-toxicity, sample drift, aberration and defocusing related challenges associated with standard 3D fluorescence microscopy techniques.



### Semantic Hierarchy Preserving Deep Hashing for Large-scale Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1901.11259v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11259v3)
- **Published**: 2019-01-31 08:19:24+00:00
- **Updated**: 2021-06-22 11:05:31+00:00
- **Authors**: Ming Zhang, Xuefei Zhe, Le Ou-Yang, Shifeng Chen, Hong Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep hashing models have been proposed as an efficient method for large-scale similarity search. However, most existing deep hashing methods only utilize fine-level labels for training while ignoring the natural semantic hierarchy structure. This paper presents an effective method that preserves the classwise similarity of full-level semantic hierarchy for large-scale image retrieval. Experiments on two benchmark datasets show that our method helps improve the fine-level retrieval performance. Moreover, with the help of the semantic hierarchy, it can produce significantly better binary codes for hierarchical retrieval, which indicates its potential of providing more user-desired retrieval results.



### Self-Supervised Visual Representations for Cross-Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1902.00378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00378v1)
- **Published**: 2019-01-31 09:17:07+00:00
- **Updated**: 2019-01-31 09:17:07+00:00
- **Authors**: Yash Patel, Lluis Gomez, Marçal Rusiñol, Dimosthenis Karatzas, C. V. Jawahar
- **Comment**: arXiv admin note: text overlap with arXiv:1807.02110
- **Journal**: None
- **Summary**: Cross-modal retrieval methods have been significantly improved in last years with the use of deep neural networks and large-scale annotated datasets such as ImageNet and Places. However, collecting and annotating such datasets requires a tremendous amount of human effort and, besides, their annotations are usually limited to discrete sets of popular visual classes that may not be representative of the richer semantics found on large-scale cross-modal retrieval datasets. In this paper, we present a self-supervised cross-modal retrieval framework that leverages as training data the correlations between images and text on the entire set of Wikipedia articles. Our method consists in training a CNN to predict: (1) the semantic context of the article in which an image is more probable to appear as an illustration (global context), and (2) the semantic context of its caption (local context). Our experiments demonstrate that the proposed method is not only capable of learning discriminative visual representations for solving vision tasks like image classification and object detection, but that the learned representations are better for cross-modal retrieval when compared to supervised pre-training of the network on the ImageNet dataset.



### Capturing Object Detection Uncertainty in Multi-Layer Grid Maps
- **Arxiv ID**: http://arxiv.org/abs/1901.11284v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.11284v1)
- **Published**: 2019-01-31 09:33:26+00:00
- **Updated**: 2019-01-31 09:33:26+00:00
- **Authors**: Sascha Wirges, Marcel Reith-Braun, Martin Lauer, Christoph Stiller
- **Comment**: 8 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: We propose a deep convolutional object detector for automated driving applications that also estimates classification, pose and shape uncertainty of each detected object. The input consists of a multi-layer grid map which is well-suited for sensor fusion, free-space estimation and machine learning. Based on the estimated pose and shape uncertainty we approximate object hulls with bounded collision probability which we find helpful for subsequent trajectory planning tasks. We train our models based on the KITTI object detection data set. In a quantitative and qualitative evaluation some models show a similar performance and superior robustness compared to previously developed object detectors. However, our evaluation also points to undesired data set properties which should be addressed when training data-driven models or creating new data sets.



### Generalising Deep Learning MRI Reconstruction across Different Domains
- **Arxiv ID**: http://arxiv.org/abs/1902.10815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10815v1)
- **Published**: 2019-01-31 12:08:33+00:00
- **Updated**: 2019-01-31 12:08:33+00:00
- **Authors**: Cheng Ouyang, Jo Schlemper, Carlo Biffi, Gavin Seegoolam, Jose Caballero, Anthony N. Price, Joseph V. Hajnal, Daniel Rueckert
- **Comment**: Accepted for ISBI2019 as a 1-page abstract
- **Journal**: None
- **Summary**: We look into robustness of deep learning based MRI reconstruction when tested on unseen contrasts and organs. We then propose to generalise the network by training with large publicly-available natural image datasets with synthesised phase information to achieve high cross-domain reconstruction performance which is competitive with domain-specific training. To explain its generalisation mechanism, we have also analysed patch sets for different training datasets.



### Automated brain extraction of multi-sequence MRI using artificial neural networks
- **Arxiv ID**: http://arxiv.org/abs/1901.11341v2
- **DOI**: 10.1002/hbm.24750
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11341v2)
- **Published**: 2019-01-31 13:10:39+00:00
- **Updated**: 2019-08-13 13:52:57+00:00
- **Authors**: Fabian Isensee, Marianne Schell, Irada Tursunova, Gianluca Brugnara, David Bonekamp, Ulf Neuberger, Antje Wick, Heinz-Peter Schlemmer, Sabine Heiland, Wolfgang Wick, Martin Bendszus, Klaus Hermann Maier-Hein, Philipp Kickingereder
- **Comment**: Fabian Isensee, Marianne Schell and Irada Tursunova share the first
  authorship
- **Journal**: Hum Brain Mapp. 2019; 1-13
- **Summary**: Brain extraction is a critical preprocessing step in the analysis of MRI neuroimaging studies and influences the accuracy of downstream analyses. The majority of brain extraction algorithms are, however, optimized for processing healthy brains and thus frequently fail in the presence of pathologically altered brain or when applied to heterogeneous MRI datasets. Here we introduce a new, rigorously validated algorithm (termed HD-BET) relying on artificial neural networks that aims to overcome these limitations. We demonstrate that HD-BET outperforms six popular, publicly available brain extraction algorithms in several large-scale neuroimaging datasets, including one from a prospective multicentric trial in neuro-oncology, yielding state-of-the-art performance with median improvements of +1.16 to +2.11 points for the DICE coefficient and -0.66 to -2.51 mm for the Hausdorff distance. Importantly, the HD-BET algorithm shows robust performance in the presence of pathology or treatment-induced tissue alterations, is applicable to a broad range of MRI sequence types and is not influenced by variations in MRI hardware and acquisition parameters encountered in both research and clinical practice. For broader accessibility our HD-BET prediction algorithm is made freely available (http://www.neuroAI-HD.org) and may become an essential component for robust, automated, high-throughput processing of MRI neuroimaging data.



### Efficient Relative Pose Estimation for Cameras and Generalized Cameras in Case of Known Relative Rotation Angle
- **Arxiv ID**: http://arxiv.org/abs/1901.11357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11357v1)
- **Published**: 2019-01-31 13:52:50+00:00
- **Updated**: 2019-01-31 13:52:50+00:00
- **Authors**: Evgeniy Martyushev, Bo Li
- **Comment**: 15 pages, 9 eps-figures
- **Journal**: None
- **Summary**: We propose two minimal solutions to the problem of relative pose estimation of (i) a calibrated camera from four points in two views and (ii) a calibrated generalized camera from five points in two views. In both cases, the relative rotation angle between the views is assumed to be known. In practice, such angle can be derived from the readings of a 3d gyroscope. We represent the rotation part of the motion in terms of unit quaternions in order to construct polynomial equations encoding the epipolar constraints. The Gr\"{o}bner basis technique is then used to efficiently derive the solutions. Our first solver for regular cameras significantly improves the existing state-of-the-art solution. The second solver for generalized cameras is novel.   The presented minimal solvers can be used in a hypothesize-and-test architecture such as RANSAC for reliable pose estimation. Experiments on synthetic and real datasets confirm that our algorithms are numerically stable, fast and robust.



### Cross-modality (CT-MRI) prior augmented deep learning for robust lung tumor segmentation from small MR datasets
- **Arxiv ID**: http://arxiv.org/abs/1901.11369v2
- **DOI**: 10.1002/mp.13695
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11369v2)
- **Published**: 2019-01-31 14:17:36+00:00
- **Updated**: 2019-02-27 18:27:47+00:00
- **Authors**: Jue Jiang, Yu-Chi Hu, Neelam Tyagi, Pengpeng Zhang, Andreas Rimner, Joseph O. Deasy, Harini Veeraraghavan
- **Comment**: Submitted to Medical Physics
- **Journal**: None
- **Summary**: Lack of large expert annotated MR datasets makes training deep learning models difficult. Therefore, a cross-modality (MR-CT) deep learning segmentation approach that augments training data using pseudo MR images produced by transforming expert-segmented CT images was developed. Eighty-One T2-weighted MRI scans from 28 patients with non-small cell lung cancers were analyzed. Cross-modality prior encoding the transformation of CT to pseudo MR images resembling T2w MRI was learned as a generative adversarial deep learning model. This model augmented training data arising from 6 expert-segmented T2w MR patient scans with 377 pseudo MRI from non-small cell lung cancer CT patient scans with obtained from the Cancer Imaging Archive. A two-dimensional Unet implemented with batch normalization was trained to segment the tumors from T2w MRI. This method was benchmarked against (a) standard data augmentation and two state-of-the art cross-modality pseudo MR-based augmentation and (b) two segmentation networks. Segmentation accuracy was computed using Dice similarity coefficient (DSC), Hausdroff distance metrics, and volume ratio. The proposed approach produced the lowest statistical variability in the intensity distribution between pseudo and T2w MR images measured as Kullback-Leibler divergence of 0.069. This method produced the highest segmentation accuracy with a DSC of 0.75 and the lowest Hausdroff distance on the test dataset. This approach produced highly similar estimations of tumor growth as an expert (P = 0.37). A novel deep learning MR segmentation was developed that overcomes the limitation of learning robust models from small datasets by leveraging learned cross-modality priors to augment training. The results show the feasibility of the approach and the corresponding improvement over the state-of-the-art methods.



### Is Image Memorability Prediction Solved?
- **Arxiv ID**: http://arxiv.org/abs/1901.11420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11420v1)
- **Published**: 2019-01-31 15:15:27+00:00
- **Updated**: 2019-01-31 15:15:27+00:00
- **Authors**: Shay Perera, Ayellet Tal, Lihi Zelnik-Manor
- **Comment**: None
- **Journal**: None
- **Summary**: This paper deals with the prediction of the memorability of a given image. We start by proposing an algorithm that reaches human-level performance on the LaMem dataset - the only large scale benchmark for memorability prediction. The suggested algorithm is based on three observations we make regarding convolutional neural networks (CNNs) that affect memorability prediction. Having reached human-level performance we were humbled, and asked ourselves whether indeed we have resolved memorability prediction - and answered this question in the negative. We studied a few factors and made some recommendations that should be taken into account when designing the next benchmark.



### Automated detection of celiac disease on duodenal biopsy slides: a deep learning approach
- **Arxiv ID**: http://arxiv.org/abs/1901.11447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11447v1)
- **Published**: 2019-01-31 16:06:53+00:00
- **Updated**: 2019-01-31 16:06:53+00:00
- **Authors**: Jason W. Wei, Jerry W. Wei, Christopher R. Jackson, Bing Ren, Arief A. Suriawinata, Saeed Hassanpour
- **Comment**: Accepted in Journal of Pathology Informatics
- **Journal**: None
- **Summary**: Celiac disease prevalence and diagnosis have increased substantially in recent years. The current gold standard for celiac disease confirmation is visual examination of duodenal mucosal biopsies. An accurate computer-aided biopsy analysis system using deep learning can help pathologists diagnose celiac disease more efficiently. In this study, we trained a deep learning model to detect celiac disease on duodenal biopsy images. Our model uses a state-of-the-art residual convolutional neural network to evaluate patches of duodenal tissue and then aggregates those predictions for whole-slide classification. We tested the model on an independent set of 212 images and evaluated its classification results against reference standards established by pathologists. Our model identified celiac disease, normal tissue, and nonspecific duodenitis with accuracies of 95.3%, 91.0%, and 89.2%, respectively. The area under the receiver operating characteristic curve was greater than 0.95 for all classes. We have developed an automated biopsy analysis system that achieves high performance in detecting celiac disease on biopsy slides. Our system can highlight areas of interest and provide preliminary classification of duodenal biopsies prior to review by pathologists. This technology has great potential for improving the accuracy and efficiency of celiac disease diagnosis.



### GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects
- **Arxiv ID**: http://arxiv.org/abs/1901.11461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11461v1)
- **Published**: 2019-01-31 16:37:38+00:00
- **Updated**: 2019-01-31 16:37:38+00:00
- **Authors**: Edward J. Smith, Scott Fujimoto, Adriana Romero, David Meger
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: Mesh models are a promising approach for encoding the structure of 3D objects. Current mesh reconstruction systems predict uniformly distributed vertex locations of a predetermined graph through a series of graph convolutions, leading to compromises with respect to performance or resolution. In this paper, we argue that the graph representation of geometric objects allows for additional structure, which should be leveraged for enhanced reconstruction. Thus, we propose a system which properly benefits from the advantages of the geometric structure of graph encoded objects by introducing (1) a graph convolutional update preserving vertex information; (2) an adaptive splitting heuristic allowing detail to emerge; and (3) a training objective operating both on the local surfaces defined by vertices as well as the global structure defined by the mesh. Our proposed method is evaluated on the task of 3D object reconstruction from images with the ShapeNet dataset, where we demonstrate state of the art performance, both visually and numerically, while having far smaller space requirements by generating adaptive meshes



### Pathologist-level classification of histologic patterns on resected lung adenocarcinoma slides with deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1901.11489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11489v1)
- **Published**: 2019-01-31 17:34:46+00:00
- **Updated**: 2019-01-31 17:34:46+00:00
- **Authors**: Jason W. Wei, Laura J. Tafe, Yevgeniy A. Linnik, Louis J. Vaickus, Naofumi Tomita, Saeed Hassanpour
- **Comment**: None
- **Journal**: None
- **Summary**: Classification of histologic patterns in lung adenocarcinoma is critical for determining tumor grade and treatment for patients. However, this task is often challenging due to the heterogeneous nature of lung adenocarcinoma and the subjective criteria for evaluation. In this study, we propose a deep learning model that automatically classifies the histologic patterns of lung adenocarcinoma on surgical resection slides. Our model uses a convolutional neural network to identify regions of neoplastic cells, then aggregates those classifications to infer predominant and minor histologic patterns for any given whole-slide image. We evaluated our model on an independent set of 143 whole-slide images. It achieved a kappa score of 0.525 and an agreement of 66.6% with three pathologists for classifying the predominant patterns, slightly higher than the inter-pathologist kappa score of 0.485 and agreement of 62.7% on this test set. All evaluation metrics for our model and the three pathologists were within 95% confidence intervals of agreement. If confirmed in clinical practice, our model can assist pathologists in improving classification of lung adenocarcinoma patterns by automatically pre-screening and highlighting cancerous regions prior to review. Our approach can be generalized to any whole-slide image classification task, and code is made publicly available at https://github.com/BMIRDS/deepslide.



### BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/1902.00038v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00038v2)
- **Published**: 2019-01-31 19:10:14+00:00
- **Updated**: 2019-02-12 10:54:56+00:00
- **Authors**: Hedi Ben-younes, Rémi Cadene, Nicolas Thome, Matthieu Cord
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal representation learning is gaining more and more interest within the deep learning community. While bilinear models provide an interesting framework to find subtle combination of modalities, their number of parameters grows quadratically with the input dimensions, making their practical implementation within classical deep learning pipelines challenging. In this paper, we introduce BLOCK, a new multimodal fusion based on the block-superdiagonal tensor decomposition. It leverages the notion of block-term ranks, which generalizes both concepts of rank and mode ranks for tensors, already used for multimodal fusion. It allows to define new ways for optimizing the tradeoff between the expressiveness and complexity of the fusion model, and is able to represent very fine interactions between modalities while maintaining powerful mono-modal representations. We demonstrate the practical interest of our fusion model by using BLOCK for two challenging tasks: Visual Question Answering (VQA) and Visual Relationship Detection (VRD), where we design end-to-end learnable architectures for representing relevant interactions between modalities. Through extensive experiments, we show that BLOCK compares favorably with respect to state-of-the-art multimodal fusion models for both VQA and VRD tasks. Our code is available at https://github.com/Cadene/block.bootstrap.pytorch.



### Improving Dense Crowd Counting Convolutional Neural Networks using Inverse k-Nearest Neighbor Maps and Multiscale Upsampling
- **Arxiv ID**: http://arxiv.org/abs/1902.05379v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05379v3)
- **Published**: 2019-01-31 22:05:47+00:00
- **Updated**: 2019-03-29 20:59:03+00:00
- **Authors**: Greg Olmschenk, Hao Tang, Zhigang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Gatherings of thousands to millions of people frequently occur for an enormous variety of events, and automated counting of these high-density crowds is useful for safety, management, and measuring significance of an event. In this work, we show that the regularly accepted labeling scheme of crowd density maps for training deep neural networks is less effective than our alternative inverse k-nearest neighbor (i$k$NN) maps, even when used directly in existing state-of-the-art network structures. We also provide a new network architecture MUD-i$k$NN, which uses multi-scale upsampling via transposed convolutions to take full advantage of the provided i$k$NN labeling. This upsampling combined with the i$k$NN maps further improves crowd counting accuracy. Our new network architecture performs favorably in comparison with the state-of-the-art. However, our labeling and upsampling techniques are generally applicable to existing crowd counting architectures.



### Learning Metric Graphs for Neuron Segmentation In Electron Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1902.00100v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00100v1)
- **Published**: 2019-01-31 22:16:58+00:00
- **Updated**: 2019-01-31 22:16:58+00:00
- **Authors**: Kyle Luther, H. Sebastian Seung
- **Comment**: 5 pages, Accepted at IEEE ISBI 2019
- **Journal**: None
- **Summary**: In the deep metric learning approach to image segmentation, a convolutional net densely generates feature vectors at the pixels of an image. Pairs of feature vectors are trained to be similar or different, depending on whether the corresponding pixels belong to same or different ground truth segments. To segment a new image, the feature vectors are computed and clustered. Both empirically and theoretically, it is unclear whether or when deep metric learning is superior to the more conventional approach of directly predicting an affinity graph with a convolutional net. We compare the two approaches using brain images from serial section electron microscopy images, which constitute an especially challenging example of instance segmentation. We first show that seed-based postprocessing of the feature vectors, as originally proposed, produces inferior accuracy because it is difficult for the convolutional net to predict feature vectors that remain uniform across large objects. Then we consider postprocessing by thresholding a nearest neighbor graph followed by connected components. In this case, segmentations from a "metric graph" turn out to be competitive or even superior to segmentations from a directly predicted affinity graph. To explain these findings theoretically, we invoke the property that the metric function satisfies the triangle inequality. Then we show with an example where this constraint suppresses noise, causing connected components to more robustly segment a metric graph than an unconstrained affinity graph.



### Episodic Training for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/1902.00113v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00113v3)
- **Published**: 2019-01-31 22:45:51+00:00
- **Updated**: 2019-12-06 12:31:57+00:00
- **Authors**: Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, Timothy M. Hospedales
- **Comment**: ICCV'19 CR version and fix Table 5. Code is now available at
  https://github.com/HAHA-DL/Episodic-DG
- **Journal**: None
- **Summary**: Domain generalization (DG) is the challenging and topical problem of learning models that generalize to novel testing domains with different statistics than a set of known training domains. The simple approach of aggregating data from all source domains and training a single deep neural network end-to-end on all the data provides a surprisingly strong baseline that surpasses many prior published methods. In this paper, we build on this strong baseline by designing an episodic training procedure that trains a single deep network in a way that exposes it to the domain shift that characterises a novel domain at runtime. Specifically, we decompose a deep network into feature extractor and classifier components, and then train each component by simulating it interacting with a partner who is badly tuned for the current domain. This makes both components more robust, ultimately leading to our networks producing state-of-the-art performance on three DG benchmarks. Furthermore, we consider the pervasive workflow of using an ImageNet trained CNN as a fixed feature extractor for downstream recognition tasks. Using the Visual Decathlon benchmark, we demonstrate that our episodic-DG training improves the performance of such a general-purpose feature extractor by explicitly training a feature for robustness to novel problems. This shows that DG training can benefit standard practice in computer vision.



### US-net for robust and efficient nuclei instance segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.00125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00125v1)
- **Published**: 2019-01-31 23:27:52+00:00
- **Updated**: 2019-01-31 23:27:52+00:00
- **Authors**: Zhaoyang Xu, Faranak Sobhani, Carlos Fernandez Moro, Qianni Zhang
- **Comment**: To appear in ISBI 2019
- **Journal**: None
- **Summary**: We present a novel neural network architecture, US-Net, for robust nuclei instance segmentation in histopathology images. The proposed framework integrates the nuclei detection and segmentation networks by sharing their outputs through the same foundation network, and thus enhancing the performance of both. The detection network takes into account the high-level semantic cues with contextual information, while the segmentation network focuses more on the low-level details like the edges. Extensive experiments reveal that our proposed framework can strengthen the performance of both branch networks in an integrated architecture and outperforms most of the state-of-the-art nuclei detection and segmentation networks.




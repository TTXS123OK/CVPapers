# Arxiv Papers in cs.CV on 2019-04-10
### Dynamic Gesture Recognition by Using CNNs and Star RGB: a Temporal Information Condensation
- **Arxiv ID**: http://arxiv.org/abs/1904.08505v2
- **DOI**: 10.1016/j.neucom.2020.03.038
- **Categories**: **cs.CV**, cs.HC, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.08505v2)
- **Published**: 2019-04-10 00:39:32+00:00
- **Updated**: 2019-09-08 15:57:22+00:00
- **Authors**: Clebeson Canuto dos Santos, Jorge Leonid Aching Samatelo, Raquel Frizera Vassallo
- **Comment**: 19 pages, 12 figures, submitted to Neurocomputing Journal
- **Journal**: None
- **Summary**: Due to the advance of technologies, machines are increasingly present in people's daily lives. Thus, there has been more and more effort to develop interfaces, such as dynamic gestures, that provide an intuitive way of interaction. Currently, the most common trend is to use multimodal data, as depth and skeleton information, to enable dynamic gesture recognition. However, using only color information would be more interesting, since RGB cameras are usually available in almost every public place, and could be used for gesture recognition without the need of installing other equipment. The main problem with such approach is the difficulty of representing spatio-temporal information using just color. With this in mind, we propose a technique capable of condensing a dynamic gesture, shown in a video, in just one RGB image. We call this technique star RGB. This image is then passed to a classifier formed by two Resnet CNNs, a soft-attention ensemble, and a fully connected layer, which indicates the class of the gesture present in the input video. Experiments were carried out using both Montalbano and GRIT datasets. For Montalbano dataset, the proposed approach achieved an accuracy of 94.58%. Such result reaches the state-of-the-art when considering this dataset and only color information. Regarding the GRIT dataset, our proposal achieves more than 98% of accuracy, recall, precision, and F1-score, outperforming the reference approach by more than 6%.



### On zero-shot recognition of generic objects
- **Arxiv ID**: http://arxiv.org/abs/1904.04957v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04957v1)
- **Published**: 2019-04-10 01:04:35+00:00
- **Updated**: 2019-04-10 01:04:35+00:00
- **Authors**: Tristan Hascoet, Yasuo Ariki, Tetsuya Takiguchi
- **Comment**: None
- **Journal**: None
- **Summary**: Many recent advances in computer vision are the result of a healthy competition among researchers on high quality, task-specific, benchmarks. After a decade of active research, zero-shot learning (ZSL) models accuracy on the Imagenet benchmark remains far too low to be considered for practical object recognition applications. In this paper, we argue that the main reason behind this apparent lack of progress is the poor quality of this benchmark. We highlight major structural flaws of the current benchmark and analyze different factors impacting the accuracy of ZSL models. We show that the actual classification accuracy of existing ZSL models is significantly higher than was previously thought as we account for these flaws. We then introduce the notion of structural bias specific to ZSL datasets. We discuss how the presence of this new form of bias allows for a trivial solution to the standard benchmark and conclude on the need for a new benchmark. We then detail the semi-automated construction of a new benchmark to address these flaws.



### Image Quality Assessment for Omnidirectional Cross-reference Stitching
- **Arxiv ID**: http://arxiv.org/abs/1904.04960v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04960v2)
- **Published**: 2019-04-10 01:11:07+00:00
- **Updated**: 2019-04-23 01:03:02+00:00
- **Authors**: Kaiwen Yu, Jia Li, Yu Zhang, Yifan Zhao, Long Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Along with the development of virtual reality (VR), omnidirectional images play an important role in producing multimedia content with immersive experience. However, despite various existing approaches for omnidirectional image stitching, how to quantitatively assess the quality of stitched images is still insufficiently explored. To address this problem, we establish a novel omnidirectional image dataset containing stitched images as well as dual-fisheye images captured from standard quarters of 0$^\circ$, 90$^\circ$, 180$^\circ$ and 270$^\circ$. In this manner, when evaluating the quality of an image stitched from a pair of fisheye images (e.g., 0$^\circ$ and 180$^\circ$), the other pair of fisheye images (e.g., 90$^\circ$ and 270$^\circ$) can be used as the cross-reference to provide ground-truth observations of the stitching regions. Based on this dataset, we further benchmark six widely used stitching models with seven evaluation metrics for IQA. To the best of our knowledge, it is the first dataset that focuses on assessing the stitching quality of omnidirectional images.



### CondConv: Conditionally Parameterized Convolutions for Efficient Inference
- **Arxiv ID**: http://arxiv.org/abs/1904.04971v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04971v3)
- **Published**: 2019-04-10 01:46:48+00:00
- **Updated**: 2020-09-04 00:53:44+00:00
- **Authors**: Brandon Yang, Gabriel Bender, Quoc V. Le, Jiquan Ngiam
- **Comment**: None
- **Journal**: NeurIPS 2019
- **Summary**: Convolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efficient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classification and detection tasks. On ImageNet classification, our CondConv approach applied to EfficientNet-B0 achieves state-of-the-art performance of 78.3% accuracy with only 413M multiply-adds. Code and checkpoints for the CondConv Tensorflow layer and CondConv-EfficientNet models are available at: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv.



### Decorrelated Adversarial Learning for Age-Invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.04972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04972v1)
- **Published**: 2019-04-10 01:47:09+00:00
- **Updated**: 2019-04-10 01:47:09+00:00
- **Authors**: Hao Wang, Dihong Gong, Zhifeng Li, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: There has been an increasing research interest in age-invariant face recognition. However, matching faces with big age gaps remains a challenging problem, primarily due to the significant discrepancy of face appearances caused by aging. To reduce such a discrepancy, in this paper we propose a novel algorithm to remove age-related components from features mixed with both identity and age information. Specifically, we factorize a mixed face feature into two uncorrelated components: identity-dependent component and age-dependent component, where the identity-dependent component includes information that is useful for face recognition. To implement this idea, we propose the Decorrelated Adversarial Learning (DAL) algorithm, where a Canonical Mapping Module (CMM) is introduced to find the maximum correlation between the paired features generated by a backbone network, while the backbone network and the factorization module are trained to generate features reducing the correlation. Thus, the proposed model learns the decomposed features of age and identity whose correlation is significantly reduced. Simultaneously, the identity-dependent feature and the age-dependent feature are respectively supervised by ID and age preserving signals to ensure that they both contain the correct information. Extensive experiments are conducted on popular public-domain face aging datasets (FG-NET, MORPH Album 2, and CACD-VS) to demonstrate the effectiveness of the proposed approach.



### Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1904.04975v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04975v2)
- **Published**: 2019-04-10 02:04:24+00:00
- **Updated**: 2019-04-11 03:46:27+00:00
- **Authors**: Lingxiao He, Yinggang Wang, Wu Liu, Xingyu Liao, He Zhao, Zhenan Sun, Jiashi Feng
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Re-identifying a person across multiple disjoint camera views is important for intelligent video surveillance, smart retailing and many other applications. However, existing person re-identification (ReID) methods are challenged by the ubiquitous occlusion over persons and suffer from performance degradation. This paper proposes a novel occlusion-robust and alignment-free model for occluded person ReID and extends its application to realistic and crowded scenarios. The proposed model first leverages the full convolution network (FCN) and pyramid pooling to extract spatial pyramid features. Then an alignment-free matching approach, namely Foreground-aware Pyramid Reconstruction (FPR), is developed to accurately compute matching scores between occluded persons, despite their different scales and sizes. FPR uses the error from robust reconstruction over spatial pyramid features to measure similarities between two persons. More importantly, we design an occlusion-sensitive foreground probability generator that focuses more on clean human body parts to refine the similarity computation with less contamination from occlusion. The FPR is easily embedded into any end-to-end person ReID models. The effectiveness of the proposed method is clearly demonstrated by the experimental results (Rank-1 accuracy) on three occluded person datasets: Partial REID (78.30\%), Partial iLIDS (68.08\%) and Occluded REID (81.00\%); and three benchmark person datasets: Market1501 (95.42\%), DukeMTMC (88.64\%) and CUHK03 (76.08\%)



### Data Priming Network for Automatic Check-Out
- **Arxiv ID**: http://arxiv.org/abs/1904.04978v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04978v3)
- **Published**: 2019-04-10 02:12:48+00:00
- **Updated**: 2019-08-07 03:04:32+00:00
- **Authors**: Congcong Li, Dawei Du, Libo Zhang, Tiejian Luo, Yanjun Wu, Qi Tian, Longyin Wen, Siwei Lyu
- **Comment**: Accepted to ACM MM 2019
- **Journal**: None
- **Summary**: Automatic Check-Out (ACO) receives increased interests in recent years. An important component of the ACO system is the visual item counting, which recognizes the categories and counts of the items chosen by the customers. However, the training of such a system is challenged by the domain adaptation problem, in which the training data are images from isolated items while the testing images are for collections of items. Existing methods solve this problem with data augmentation using synthesized images, but the image synthesis leads to unreal images that affect the training process. In this paper, we propose a new data priming method to solve the domain adaptation problem. Specifically, we first use pre-augmentation data priming, in which we remove distracting background from the training images using the coarse-to-fine strategy and select images with realistic view angles by the pose pruning method. In the post-augmentation step, we train a data priming network using detection and counting collaborative learning, and select more reliable images from testing data to fine-tune the final visual item tallying network. Experiments on the large scale Retail Product Checkout (RPC) dataset demonstrate the superiority of the proposed method, i.e., we achieve 80.51% checkout accuracy compared with 56.68% of the baseline methods. The source codes can be found in https://isrc.iscas.ac.cn/gitlab/research/acm-mm-2019-ACO.



### Context-Aware Embeddings for Automatic Art Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.04985v1
- **DOI**: 10.1145/3323873.3325028
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04985v1)
- **Published**: 2019-04-10 02:37:49+00:00
- **Updated**: 2019-04-10 02:37:49+00:00
- **Authors**: Noa Garcia, Benjamin Renoust, Yuta Nakashima
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic art analysis aims to classify and retrieve artistic representations from a collection of images by using computer vision and machine learning techniques. In this work, we propose to enhance visual representations from neural networks with contextual artistic information. Whereas visual representations are able to capture information about the content and the style of an artwork, our proposed context-aware embeddings additionally encode relationships between different artistic attributes, such as author, school, or historical period. We design two different approaches for using context in automatic art analysis. In the first one, contextual data is obtained through a multi-task learning model, in which several attributes are trained together to find visual relationships between elements. In the second approach, context is obtained through an art-specific knowledge graph, which encodes relationships between artistic attributes. An exhaustive evaluation of both of our models in several art analysis problems, such as author identification, type classification, or cross-modal retrieval, show that performance is improved by up to 7.3% in art classification and 37.24% in retrieval when context-aware embeddings are used.



### A Data Fusion Platform for Supporting Bridge Deck Condition Monitoring by Merging Aerial and Ground Inspection Imagery
- **Arxiv ID**: http://arxiv.org/abs/1904.04986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04986v1)
- **Published**: 2019-04-10 02:40:45+00:00
- **Updated**: 2019-04-10 02:40:45+00:00
- **Authors**: Zhexiong Shang, Chongsheng Cheng, Zhigang Shen
- **Comment**: 8 pages, 5 figures, submitted to i3ce 2019
- **Journal**: None
- **Summary**: UAVs showed great efficiency on scanning bridge decks surface by taking a single shot or through stitching a couple of overlaid still images. If potential surface deficits are identified through aerial images, subsequent ground inspections can be scheduled. This two-phase inspection procedure showed great potentials on increasing field inspection productivity. Since aerial and ground inspection images are taken at different scales, a tool to properly fuse these multi-scale images is needed for improving the current bridge deck condition monitoring practice. In response to this need a data fusion platform is introduced in this study. Using this proposed platform multi-scale images taken by different inspection devices can be fused through geo-referencing. As part of the platform, a web-based user interface is developed to organize and visualize those images with inspection notes under users queries. For illustration purpose, a case study involving multi-scale optical and infrared images from UAV and ground inspector, and its implementation using the proposed platform is presented.



### Vision-model-based Real-time Localization of Unmanned Aerial Vehicle for Autonomous Structure Inspection under GPS-denied Environment
- **Arxiv ID**: http://arxiv.org/abs/1904.04987v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.04987v1)
- **Published**: 2019-04-10 02:43:52+00:00
- **Updated**: 2019-04-10 02:43:52+00:00
- **Authors**: Zhexiong Shang, Zhigang Shen
- **Comment**: 8 pages, 5 figures, submitted to i3ce 2019
- **Journal**: None
- **Summary**: UAVs have been widely used in visual inspections of buildings, bridges and other structures. In either outdoor autonomous or semi-autonomous flights missions strong GPS signal is vital for UAV to locate its own positions. However, strong GPS signal is not always available, and it can degrade or fully loss underneath large structures or close to power lines, which can cause serious control issues or even UAV crashes. Such limitations highly restricted the applications of UAV as a routine inspection tool in various domains. In this paper a vision-model-based real-time self-positioning method is proposed to support autonomous aerial inspection without the need of GPS support. Compared to other localization methods that requires additional onboard sensors, the proposed method uses a single camera to continuously estimate the inflight poses of UAV. Each step of the proposed method is discussed in detail, and its performance is tested through an indoor test case.



### FAMNet: Joint Learning of Feature, Affinity and Multi-dimensional Assignment for Online Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1904.04989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04989v1)
- **Published**: 2019-04-10 03:07:00+00:00
- **Updated**: 2019-04-10 03:07:00+00:00
- **Authors**: Peng Chu, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Data association-based multiple object tracking (MOT) involves multiple separated modules processed or optimized differently, which results in complex method design and requires non-trivial tuning of parameters. In this paper, we present an end-to-end model, named FAMNet, where Feature extraction, Affinity estimation and Multi-dimensional assignment are refined in a single network. All layers in FAMNet are designed differentiable thus can be optimized jointly to learn the discriminative features and higher-order affinity model for robust MOT, which is supervised by the loss directly from the assignment ground truth. We also integrate single object tracking technique and a dedicated target management scheme into the FAMNet-based tracking system to further recover false negatives and inhibit noisy target candidates generated by the external detector. The proposed method is evaluated on a diverse set of benchmarks including MOT2015, MOT2017, KITTI-Car and UA-DETRAC, and achieves promising performance on all of them in comparison with state-of-the-arts.



### Spatiotemporal Knowledge Distillation for Efficient Estimation of Aerial Video Saliency
- **Arxiv ID**: http://arxiv.org/abs/1904.04992v1
- **DOI**: 10.1109/TIP.2019.2946102
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04992v1)
- **Published**: 2019-04-10 03:41:10+00:00
- **Updated**: 2019-04-10 03:41:10+00:00
- **Authors**: Jia Li, Kui Fu, Shengwei Zhao, Shiming Ge
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of video saliency estimation techniques has achieved significant advances along with the rapid development of Convolutional Neural Networks (CNNs). However, devices like cameras and drones may have limited computational capability and storage space so that the direct deployment of complex deep saliency models becomes infeasible. To address this problem, this paper proposes a dynamic saliency estimation approach for aerial videos via spatiotemporal knowledge distillation. In this approach, five components are involved, including two teachers, two students and the desired spatiotemporal model. The knowledge of spatial and temporal saliency is first separately transferred from the two complex and redundant teachers to their simple and compact students, and the input scenes are also degraded from high-resolution to low-resolution to remove the probable data redundancy so as to greatly speed up the feature extraction process. After that, the desired spatiotemporal model is further trained by distilling and encoding the spatial and temporal saliency knowledge of two students into a unified network. In this manner, the inter-model redundancy can be further removed for the effective estimation of dynamic saliency on aerial videos. Experimental results show that the proposed approach outperforms ten state-of-the-art models in estimating visual saliency on aerial videos, while its speed reaches up to 28,738 FPS on the GPU platform.



### Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras
- **Arxiv ID**: http://arxiv.org/abs/1904.04998v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.04998v1)
- **Published**: 2019-04-10 04:16:30+00:00
- **Updated**: 2019-04-10 04:16:30+00:00
- **Authors**: Ariel Gordon, Hanhan Li, Rico Jonschkowski, Anelia Angelova
- **Comment**: None
- **Journal**: The IEEE International Conference on Computer Vision (ICCV), 2019,
  pp. 8977-8986
- **Summary**: We present a novel method for simultaneous learning of depth, egomotion, object motion, and camera intrinsics from monocular videos, using only consistency across neighboring video frames as supervision signal. Similarly to prior work, our method learns by applying differentiable warping to frames and comparing the result to adjacent ones, but it provides several improvements: We address occlusions geometrically and differentiably, directly using the depth maps as predicted during training. We introduce randomized layer normalization, a novel powerful regularizer, and we account for object motion relative to the scene. To the best of our knowledge, our work is the first to learn the camera intrinsic parameters, including lens distortion, from video in an unsupervised manner, thereby allowing us to extract accurate depth and motion from arbitrary videos of unknown origin at scale. We evaluate our results on the Cityscapes, KITTI and EuRoC datasets, establishing new state of the art on depth prediction and odometry, and demonstrate qualitatively that depth prediction can be learned from a collection of YouTube videos.



### Semi-Supervised Graph Classification: A Hierarchical Graph Perspective
- **Arxiv ID**: http://arxiv.org/abs/1904.05003v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05003v1)
- **Published**: 2019-04-10 04:53:20+00:00
- **Updated**: 2019-04-10 04:53:20+00:00
- **Authors**: Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, Junzhou Huang
- **Comment**: 12 pages, WWW-2019
- **Journal**: None
- **Summary**: Node classification and graph classification are two graph learning problems that predict the class label of a node and the class label of a graph respectively. A node of a graph usually represents a real-world entity, e.g., a user in a social network, or a protein in a protein-protein interaction network. In this work, we consider a more challenging but practically useful setting, in which a node itself is a graph instance. This leads to a hierarchical graph perspective which arises in many domains such as social network, biological network and document collection. For example, in a social network, a group of people with shared interests forms a user group, whereas a number of user groups are interconnected via interactions or common members. We study the node classification problem in the hierarchical graph where a `node' is a graph instance, e.g., a user group in the above example. As labels are usually limited in real-world data, we design two novel semi-supervised solutions named \underline{SE}mi-supervised gr\underline{A}ph c\underline{L}assification via \underline{C}autious/\underline{A}ctive \underline{I}teration (or SEAL-C/AI in short). SEAL-C/AI adopt an iterative framework that takes turns to build or update two classifiers, one working at the graph instance level and the other at the hierarchical graph level. To simplify the representation of the hierarchical graph, we propose a novel supervised, self-attentive graph embedding method called SAGE, which embeds graph instances of arbitrary size into fixed-length vectors. Through experiments on synthetic data and Tencent QQ group data, we demonstrate that SEAL-C/AI not only outperform competing methods by a significant margin in terms of accuracy/Macro-F1, but also generate meaningful interpretations of the learned representations.



### Person Re-identification with Metric Learning using Privileged Information
- **Arxiv ID**: http://arxiv.org/abs/1904.05005v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05005v1)
- **Published**: 2019-04-10 05:01:28+00:00
- **Updated**: 2019-04-10 05:01:28+00:00
- **Authors**: Xun Yang, Meng Wang, Dacheng Tao
- **Comment**: Accepted for IEEE TIP
- **Journal**: None
- **Summary**: Despite the promising progress made in recent years, person re-identification remains a challenging task due to complex variations in human appearances from different camera views. This paper presents a logistic discriminant metric learning method for this challenging problem. Different with most existing metric learning algorithms, it exploits both original data and auxiliary data during training, which is motivated by the new machine learning paradigm - Learning Using Privileged Information. Such privileged information is a kind of auxiliary knowledge which is only available during training. Our goal is to learn an optimal distance function by constructing a locally adaptive decision rule with the help of privileged information. We jointly learn two distance metrics by minimizing the empirical loss penalizing the difference between the distance in the original space and that in the privileged space. In our setting, the distance in the privileged space functions as a local decision threshold, which guides the decision making in the original space like a teacher. The metric learned from the original space is used to compute the distance between a probe image and a gallery image during testing. In addition, we extend the proposed approach to a multi-view setting which is able to explore the complementation of multiple feature representations. In the multi-view setting, multiple metrics corresponding to different original features are jointly learned, guided by the same privileged information. Besides, an effective iterative optimization scheme is introduced to simultaneously optimize the metrics and the assigned metric weights. Experiment results on several widely-used datasets demonstrate that the proposed approach is superior to global decision threshold based methods and outperforms most state-of-the-art results.



### Efficient Retrieval of Logos Using Rough Set Reducts
- **Arxiv ID**: http://arxiv.org/abs/1904.05008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05008v1)
- **Published**: 2019-04-10 05:34:04+00:00
- **Updated**: 2019-04-10 05:34:04+00:00
- **Authors**: Ushasi Chaudhuri, Partha Bhowmick, Jayanta Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Searching for similar logos in the registered logo database is a very important and tedious task at the trademark office. Speed and accuracy are two aspects that one must attend to while developing a system for retrieval of logos. In this paper, we propose a rough-set based method to quantify the structural information in a logo image that can be used to efficiently index an image. A logo is split into a number of polygons, and for each polygon, we compute the tight upper and lower approximations based on the principles of a rough set. This representation is used for forming feature vectors for retrieval of logos. Experimentation on a standard data set shows the usefulness of the proposed technique. It is computationally efficient and also provides retrieval results at high accuracy.



### SOSNet: Second Order Similarity Regularization for Local Descriptor Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.05019v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05019v2)
- **Published**: 2019-04-10 06:33:28+00:00
- **Updated**: 2019-12-16 23:37:59+00:00
- **Authors**: Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen, Vassileios Balntas
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the fact that Second Order Similarity (SOS) has been used with significant success in tasks such as graph matching and clustering, it has not been exploited for learning local descriptors. In this work, we explore the potential of SOS in the field of descriptor learning by building upon the intuition that a positive pair of matching points should exhibit similar distances with respect to other points in the embedding space. Thus, we propose a novel regularization term, named Second Order Similarity Regularization (SOSR), that follows this principle. By incorporating SOSR into training, our learned descriptor achieves state-of-the-art performance on several challenging benchmarks containing distinct tasks ranging from local patch retrieval to structure from motion. Furthermore, by designing a von Mises-Fischer distribution based evaluation method, we link the utilization of the descriptor space to the matching performance, thus demonstrating the effectiveness of our proposed SOSR. Extensive experimental results, empirical evidence, and in-depth analysis are provided, indicating that SOSR can significantly boost the matching performance of the learned descriptor.



### Imitating Targets from all sides: An Unsupervised Transfer Learning method for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1904.05020v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05020v2)
- **Published**: 2019-04-10 06:33:39+00:00
- **Updated**: 2021-04-28 02:59:40+00:00
- **Authors**: Jiajie Tian, Zhu Teng, Rui Li, Yan Li, Baopeng Zhang, Jianping Fan
- **Comment**: The author and result of model have changed
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) models usually show a limited performance when they are trained on one dataset and tested on another dataset due to the inter-dataset bias (e.g. completely different identities and backgrounds) and the intra-dataset difference (e.g. camera invariance). In terms of this issue, given a labelled source training set and an unlabelled target training set, we propose an unsupervised transfer learning method characterized by 1) bridging inter-dataset bias and intra-dataset difference via a proposed ImitateModel simultaneously; 2) regarding the unsupervised person Re-ID problem as a semi-supervised learning problem formulated by a dual classification loss to learn a discriminative representation across domains; 3) exploiting the underlying commonality across different domains from the class-style space to improve the generalization ability of re-ID models. Extensive experiments are conducted on two widely employed benchmarks, including Market-1501 and DukeMTMC-reID, and experimental results demonstrate that the proposed method can achieve a competitive performance against other state-of-the-art unsupervised Re-ID approaches.



### DSNet: An Efficient CNN for Road Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.05022v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05022v1)
- **Published**: 2019-04-10 06:46:04+00:00
- **Updated**: 2019-04-10 06:46:04+00:00
- **Authors**: Ping-Rong Chen, Hsueh-Ming Hang, Sheng-Wei Chan, Jing-Jhih Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Road scene understanding is a critical component in an autonomous driving system. Although the deep learning-based road scene segmentation can achieve very high accuracy, its complexity is also very high for developing real-time applications. It is challenging to design a neural net with high accuracy and low computational complexity. To address this issue, we investigate the advantages and disadvantages of several popular CNN architectures in terms of speed, storage and segmentation accuracy. We start from the Fully Convolutional Network (FCN) with VGG, and then we study ResNet and DenseNet. Through detailed experiments, we pick up the favorable components from the existing architectures and at the end, we construct a light-weight network architecture based on the DenseNet. Our proposed network, called DSNet, demonstrates a real-time testing (inferencing) ability (on the popular GPU platform) and it maintains an accuracy comparable with most previous systems. We test our system on several datasets including the challenging Cityscapes dataset (resolution of 1024x512) with an mIoU of about 69.1 % and runtime of 0.0147 second per image on a single GTX 1080Ti. We also design a more accurate model but at the price of a slower speed, which has an mIoU of about 72.6 % on the CamVid dataset.



### ThumbNet: One Thumbnail Image Contains All You Need for Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.05034v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05034v3)
- **Published**: 2019-04-10 07:44:09+00:00
- **Updated**: 2020-12-03 12:32:20+00:00
- **Authors**: Chen Zhao, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep convolutional neural networks (CNNs) have achieved great success in computer vision tasks, its real-world application is still impeded by its voracious demand of computational resources. Current works mostly seek to compress the network by reducing its parameters or parameter-incurred computation, neglecting the influence of the input image on the system complexity. Based on the fact that input images of a CNN contain substantial redundancy, in this paper, we propose a unified framework, dubbed as ThumbNet, to simultaneously accelerate and compress CNN models by enabling them to infer on one thumbnail image. We provide three effective strategies to train ThumbNet. In doing so, ThumbNet learns an inference network that performs equally well on small images as the original-input network on large images. With ThumbNet, not only do we obtain the thumbnail-input inference network that can drastically reduce computation and memory requirements, but also we obtain an image downscaler that can generate thumbnail images for generic classification tasks. Extensive experiments show the effectiveness of ThumbNet, and demonstrate that the thumbnail-input inference network learned by ThumbNet can adequately retain the accuracy of the original-input network even when the input images are downscaled 16 times.



### Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations
- **Arxiv ID**: http://arxiv.org/abs/1904.05044v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05044v3)
- **Published**: 2019-04-10 08:02:35+00:00
- **Updated**: 2019-05-10 01:46:17+00:00
- **Authors**: Jiwoon Ahn, Sunghyun Cho, Suha Kwak
- **Comment**: Accepted to CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: This paper presents a novel approach for learning instance segmentation with image-level class labels as supervision. Our approach generates pseudo instance segmentation labels of training images, which are used to train a fully supervised model. For generating the pseudo labels, we first identify confident seed areas of object classes from attention maps of an image classification model, and propagate them to discover the entire instance areas with accurate boundaries. To this end, we propose IRNet, which estimates rough areas of individual instances and detects boundaries between different object classes. It thus enables to assign instance labels to the seeds and to propagate them within the boundaries so that the entire areas of instances can be estimated accurately. Furthermore, IRNet is trained with inter-pixel relations on the attention maps, thus no extra supervision is required. Our method with IRNet achieves an outstanding performance on the PASCAL VOC 2012 dataset, surpassing not only previous state-of-the-art trained with the same level of supervision, but also some of previous models relying on stronger supervision.



### Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution
- **Arxiv ID**: http://arxiv.org/abs/1904.05049v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05049v3)
- **Published**: 2019-04-10 08:15:00+00:00
- **Updated**: 2019-08-18 08:21:46+00:00
- **Authors**: Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus Rohrbach, Shuicheng Yan, Jiashi Feng
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: In natural images, information is conveyed at different frequencies where higher frequencies are usually encoded with fine details and lower frequencies are usually encoded with global structures. Similarly, the output feature maps of a convolution layer can also be seen as a mixture of information at different frequencies. In this work, we propose to factorize the mixed feature maps by their frequencies, and design a novel Octave Convolution (OctConv) operation to store and process feature maps that vary spatially "slower" at a lower spatial resolution reducing both memory and computation cost. Unlike existing multi-scale methods, OctConv is formulated as a single, generic, plug-and-play convolutional unit that can be used as a direct replacement of (vanilla) convolutions without any adjustments in the network architecture. It is also orthogonal and complementary to methods that suggest better topologies or reduce channel-wise redundancy like group or depth-wise convolutions. We experimentally show that by simply replacing convolutions with OctConv, we can consistently boost accuracy for both image and video recognition tasks, while reducing memory and computational cost. An OctConv-equipped ResNet-152 can achieve 82.9% top-1 classification accuracy on ImageNet with merely 22.2 GFLOPs.



### Heavy Rain Image Restoration: Integrating Physics Model and Conditional Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.05050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05050v1)
- **Published**: 2019-04-10 08:15:59+00:00
- **Updated**: 2019-04-10 08:15:59+00:00
- **Authors**: Ruotent Li, Loong Fah Cheong, Robby T. Tan
- **Comment**: CVPR19
- **Journal**: None
- **Summary**: Most deraining works focus on rain streaks removal but they cannot deal adequately with heavy rain images. In heavy rain, streaks are strongly visible, dense rain accumulation or rain veiling effect significantly washes out the image, further scenes are relatively more blurry, etc. In this paper, we propose a novel method to address these problems. We put forth a 2-stage network: a physics-based backbone followed by a depth-guided GAN refinement. The first stage estimates the rain streaks, the transmission, and the atmospheric light governed by the underlying physics. To tease out these components more reliably, a guided filtering framework is used to decompose the image into its low- and high-frequency components. This filtering is guided by a rain-free residue image --- its content is used to set the passbands for the two channels in a spatially-variant manner so that the background details do not get mixed up with the rain-streaks. For the second stage, the refinement stage, we put forth a depth-guided GAN to recover the background details failed to be retrieved by the first stage, as well as correcting artefacts introduced by that stage. We have evaluated our method against the state of the art methods. Extensive experiments show that our method outperforms them on real rain image data, recovering visually clean images with good details.



### C3AE: Exploring the Limits of Compact Model for Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.05059v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05059v2)
- **Published**: 2019-04-10 08:33:14+00:00
- **Updated**: 2019-04-11 15:24:36+00:00
- **Authors**: Chao Zhang, Shuaicheng Liu, Xun Xu, Ce Zhu
- **Comment**: accepted by cvpr2019
- **Journal**: None
- **Summary**: Age estimation is a classic learning problem in computer vision. Many larger and deeper CNNs have been proposed with promising performance, such as AlexNet, VggNet, GoogLeNet and ResNet. However, these models are not practical for the embedded/mobile devices. Recently, MobileNets and ShuffleNets have been proposed to reduce the number of parameters, yielding lightweight models. However, their representation has been weakened because of the adoption of depth-wise separable convolution. In this work, we investigate the limits of compact model for small-scale image and propose an extremely Compact yet efficient Cascade Context-based Age Estimation model(C3AE). This model possesses only 1/9 and 1/2000 parameters compared with MobileNets/ShuffleNets and VggNet, while achieves competitive performance. In particular, we re-define age estimation problem by two-points representation, which is implemented by a cascade model. Moreover, to fully utilize the facial context information, multi-branch CNN network is proposed to aggregate multi-scale context. Experiments are carried out on three age estimation datasets. The state-of-the-art performance on compact model has been achieved with a relatively large margin.



### DAVANet: Stereo Deblurring with View Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1904.05065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05065v1)
- **Published**: 2019-04-10 08:47:56+00:00
- **Updated**: 2019-04-10 08:47:56+00:00
- **Authors**: Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, Haozhe Xie, Jinshan Pan, Jimmy Ren
- **Comment**: CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: Nowadays stereo cameras are more commonly adopted in emerging devices such as dual-lens smartphones and unmanned aerial vehicles. However, they also suffer from blurry images in dynamic scenes which leads to visual discomfort and hampers further image processing. Previous works have succeeded in monocular deblurring, yet there are few studies on deblurring for stereoscopic images. By exploiting the two-view nature of stereo images, we propose a novel stereo image deblurring network with Depth Awareness and View Aggregation, named DAVANet. In our proposed network, 3D scene cues from the depth and varying information from two views are incorporated, which help to remove complex spatially-varying blur in dynamic scenes. Specifically, with our proposed fusion network, we integrate the bidirectional disparities estimation and deblurring into a unified framework. Moreover, we present a large-scale multi-scene dataset for stereo deblurring, containing 20,637 blurry-sharp stereo image pairs from 135 diverse sequences and their corresponding bidirectional disparities. The experimental results on our dataset demonstrate that DAVANet outperforms state-of-the-art methods in terms of accuracy, speed, and model size.



### Relational Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1904.05068v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05068v2)
- **Published**: 2019-04-10 08:52:14+00:00
- **Updated**: 2019-05-01 09:36:42+00:00
- **Authors**: Wonpyo Park, Dongju Kim, Yan Lu, Minsu Cho
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Knowledge distillation aims at transferring knowledge acquired in one model (a teacher) to another model (a student) that is typically smaller. Previous approaches can be expressed as a form of training the student to mimic output activations of individual data examples represented by the teacher. We introduce a novel approach, dubbed relational knowledge distillation (RKD), that transfers mutual relations of data examples instead. For concrete realizations of RKD, we propose distance-wise and angle-wise distillation losses that penalize structural differences in relations. Experiments conducted on different tasks show that the proposed method improves educated student models with a significant margin. In particular for metric learning, it allows students to outperform their teachers' performance, achieving the state of the arts on standard benchmark datasets.



### Cross-lingual Visual Verb Sense Disambiguation
- **Arxiv ID**: http://arxiv.org/abs/1904.05092v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.05092v2)
- **Published**: 2019-04-10 09:43:06+00:00
- **Updated**: 2019-04-17 12:52:07+00:00
- **Authors**: Spandana Gella, Desmond Elliott, Frank Keller
- **Comment**: NAACL 2019; fix typo in author name
- **Journal**: None
- **Summary**: Recent work has shown that visual context improves cross-lingual sense disambiguation for nouns. We extend this line of work to the more challenging task of cross-lingual verb sense disambiguation, introducing the MultiSense dataset of 9,504 images annotated with English, German, and Spanish verbs. Each image in MultiSense is annotated with an English verb and its translation in German or Spanish. We show that cross-lingual verb sense disambiguation models benefit from visual context, compared to unimodal baselines. We also show that the verb sense predicted by our best disambiguation model can improve the results of a text-only machine translation system when used for a multimodal translation task.



### Text Guided Person Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1904.05118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05118v1)
- **Published**: 2019-04-10 11:40:15+00:00
- **Updated**: 2019-04-10 11:40:15+00:00
- **Authors**: Xingran Zhou, Siyu Huang, Bin Li, Yingming Li, Jiachen Li, Zhongfei Zhang
- **Comment**: To appear at CVPR 2019
- **Journal**: None
- **Summary**: This paper presents a novel method to manipulate the visual appearance (pose and attribute) of a person image according to natural language descriptions. Our method can be boiled down to two stages: 1) text guided pose generation and 2) visual appearance transferred image synthesis. In the first stage, our method infers a reasonable target human pose based on the text. In the second stage, our method synthesizes a realistic and appearance transferred person image according to the text in conjunction with the target pose. Our method extracts sufficient information from the text and establishes a mapping between the image space and the language space, making generating and editing images corresponding to the description possible. We conduct extensive experiments to reveal the effectiveness of our method, as well as using the VQA Perceptual Score as a metric for evaluating the method. It shows for the first time that we can automatically edit the person image from the natural language descriptions.



### Predicting Novel Views Using Generative Adversarial Query Network
- **Arxiv ID**: http://arxiv.org/abs/1904.05124v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1904.05124v1)
- **Published**: 2019-04-10 11:49:25+00:00
- **Updated**: 2019-04-10 11:49:25+00:00
- **Authors**: Phong Nguyen-Ha, Lam Huynh, Esa Rahtu, Janne Heikkila
- **Comment**: 12 pages, 4 figures, accepted for presentation at the Scandinavian
  Conference on Image Analysis 2019
- **Journal**: None
- **Summary**: The problem of predicting a novel view of the scene using an arbitrary number of observations is a challenging problem for computers as well as for humans. This paper introduces the Generative Adversarial Query Network (GAQN), a general learning framework for novel view synthesis that combines Generative Query Network (GQN) and Generative Adversarial Networks (GANs). The conventional GQN encodes input views into a latent representation that is used to generate a new view through a recurrent variational decoder. The proposed GAQN builds on this work by adding two novel aspects: First, we extend the current GQN architecture with an adversarial loss function for improving the visual quality and convergence speed. Second, we introduce a feature-matching loss function for stabilizing the training procedure. The experiments demonstrate that GAQN is able to produce high-quality results and faster convergence compared to the conventional approach.



### Actor-Critic Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.05126v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05126v1)
- **Published**: 2019-04-10 12:08:13+00:00
- **Updated**: 2019-04-10 12:08:13+00:00
- **Authors**: Nikita Araslanov, Constantin Rothkopf, Stefan Roth
- **Comment**: To appear at CVPR 2019
- **Journal**: None
- **Summary**: Most approaches to visual scene analysis have emphasised parallel processing of the image elements. However, one area in which the sequential nature of vision is apparent, is that of segmenting multiple, potentially similar and partially occluded objects in a scene. In this work, we revisit the recurrent formulation of this challenging problem in the context of reinforcement learning. Motivated by the limitations of the global max-matching assignment of the ground-truth segments to the recurrent states, we develop an actor-critic approach in which the actor recurrently predicts one instance mask at a time and utilises the gradient from a concurrently trained critic network. We formulate the state, action, and the reward such as to let the critic model long-term effects of the current prediction and incorporate this information into the gradient signal. Furthermore, to enable effective exploration in the inherently high-dimensional action space of instance masks, we learn a compact representation using a conditional variational auto-encoder. We show that our actor-critic model consistently provides accuracy benefits over the recurrent baseline on standard instance segmentation benchmarks.



### Joint Manifold Diffusion for Combining Predictions on Decoupled Observations
- **Arxiv ID**: http://arxiv.org/abs/1904.05159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05159v1)
- **Published**: 2019-04-10 13:07:48+00:00
- **Updated**: 2019-04-10 13:07:48+00:00
- **Authors**: Kwang In Kim, Hyung Jin Chang
- **Comment**: Published at CVPR 2019
- **Journal**: None
- **Summary**: We present a new predictor combination algorithm that improves a given task predictor based on potentially relevant reference predictors. Existing approaches are limited in that, to discover the underlying task dependence, they either require known parametric forms of all predictors or access to a single fixed dataset on which all predictors are jointly evaluated. To overcome these limitations, we design a new non-parametric task dependence estimation procedure that automatically aligns evaluations of heterogeneous predictors across disjoint feature sets. Our algorithm is instantiated as a robust manifold diffusion process that jointly refines the estimated predictor alignments and the corresponding task dependence. We apply this algorithm to the relative attributes ranking problem and demonstrate that it not only broadens the application range of predictor combination approaches but also outperforms existing methods even when applied to classical predictor combination settings.



### Large-Scale Long-Tailed Recognition in an Open World
- **Arxiv ID**: http://arxiv.org/abs/1904.05160v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05160v2)
- **Published**: 2019-04-10 13:09:42+00:00
- **Updated**: 2019-04-16 14:17:39+00:00
- **Authors**: Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, Stella X. Yu
- **Comment**: To appear in CVPR 2019 as an oral presentation. Code, datasets and
  models are available at https://liuziwei7.github.io/projects/LongTail.html
- **Journal**: None
- **Summary**: Real world data often have a long-tailed and open-ended distribution. A practical recognition system must classify among majority and minority classes, generalize from a few known instances, and acknowledge novelty upon a never seen instance. We define Open Long-Tailed Recognition (OLTR) as learning from such naturally distributed data and optimizing the classification accuracy over a balanced test set which include head, tail, and open classes. OLTR must handle imbalanced classification, few-shot learning, and open-set recognition in one integrated algorithm, whereas existing classification approaches focus only on one aspect and deliver poorly over the entire class spectrum. The key challenges are how to share visual knowledge between head and tail classes and how to reduce confusion between tail and open classes. We develop an integrated OLTR algorithm that maps an image to a feature space such that visual concepts can easily relate to each other based on a learned metric that respects the closed-world classification while acknowledging the novelty of the open world. Our so-called dynamic meta-embedding combines a direct image feature and an associated memory feature, with the feature norm indicating the familiarity to known classes. On three large-scale OLTR datasets we curate from object-centric ImageNet, scene-centric Places, and face-centric MS1M data, our method consistently outperforms the state-of-the-art. Our code, datasets, and models enable future OLTR research and are publicly available at https://liuziwei7.github.io/projects/LongTail.html.



### Black-box Adversarial Attacks on Video Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/1904.05181v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.05181v2)
- **Published**: 2019-04-10 13:41:02+00:00
- **Updated**: 2019-06-28 06:22:54+00:00
- **Authors**: Linxi Jiang, Xingjun Ma, Shaoxiang Chen, James Bailey, Yu-Gang Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are known for their vulnerability to adversarial examples. These are examples that have undergone small, carefully crafted perturbations, and which can easily fool a DNN into making misclassifications at test time. Thus far, the field of adversarial research has mainly focused on image models, under either a white-box setting, where an adversary has full access to model parameters, or a black-box setting where an adversary can only query the target model for probabilities or labels. Whilst several white-box attacks have been proposed for video models, black-box video attacks are still unexplored. To close this gap, we propose the first black-box video attack framework, called V-BAD. V-BAD utilizes tentative perturbations transferred from image models, and partition-based rectifications found by the NES on partitions (patches) of tentative perturbations, to obtain good adversarial gradient estimates with fewer queries to the target model. V-BAD is equivalent to estimating the projection of an adversarial gradient on a selected subspace. Using three benchmark video datasets, we demonstrate that V-BAD can craft both untargeted and targeted attacks to fool two state-of-the-art deep video recognition models. For the targeted attack, it achieves $>$93\% success rate using only an average of $3.4 \sim 8.4 \times 10^4$ queries, a similar number of queries to state-of-the-art black-box image attacks. This is despite the fact that videos often have two orders of magnitude higher dimensionality than static images. We believe that V-BAD is a promising new tool to evaluate and improve the robustness of video recognition models to black-box adversarial attacks.



### Weakly-Supervised White and Grey Matter Segmentation in 3D Brain Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1904.05191v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05191v3)
- **Published**: 2019-04-10 13:55:10+00:00
- **Updated**: 2019-04-16 07:53:34+00:00
- **Authors**: Beatrice Demiray, Julia Rackerseder, Stevica Bozhinoski, Nassir Navab
- **Comment**: * Beatrice Demiray and Julia Rackerseder contributed equally to this
  work
- **Journal**: None
- **Summary**: Although the segmentation of brain structures in ultrasound helps initialize image based registration, assist brain shift compensation, and provides interventional decision support, the task of segmenting grey and white matter in cranial ultrasound is very challenging and has not been addressed yet. We train a multi-scale fully convolutional neural network simultaneously for two classes in order to segment real clinical 3D ultrasound data. Parallel pathways working at different levels of resolution account for high frequency speckle noise and global 3D image features. To ensure reproducibility, the publicly available RESECT dataset is utilized for training and cross-validation. Due to the absence of a ground truth, we train with weakly annotated label. We implement label transfer from MRI to US, which is prone to a residual but inevitable registration error. To further improve results, we perform transfer learning using synthetic US data. The resulting method leads to excellent Dice scores of 0.7080, 0.8402 and 0.9315 for grey matter, white matter and background. Our proposed methodology sets an unparalleled standard for white and grey matter segmentation in 3D intracranial ultrasound.



### Active Multi-Kernel Domain Adaptation for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.05200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05200v1)
- **Published**: 2019-04-10 14:07:25+00:00
- **Updated**: 2019-04-10 14:07:25+00:00
- **Authors**: Cheng Deng, Xianglong Liu, Chao Li, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed the quick progress of the hyperspectral images (HSI) classification. Most of existing studies either heavily rely on the expensive label information using the supervised learning or can hardly exploit the discriminative information borrowed from related domains. To address this issues, in this paper we show a novel framework addressing HSI classification based on the domain adaptation (DA) with active learning (AL). The main idea of our method is to retrain the multi-kernel classifier by utilizing the available labeled samples from source domain, and adding minimum number of the most informative samples with active queries in the target domain. The proposed method adaptively combines multiple kernels, forming a DA classifier that minimizes the bias between the source and target domains. Further equipped with the nested actively updating process, it sequentially expands the training set and gradually converges to a satisfying level of classification performance. We study this active adaptation framework with the Margin Sampling (MS) strategy in the HSI classification task. Our experimental results on two popular HSI datasets demonstrate its effectiveness.



### Curriculum semi-supervised segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.05236v2
- **DOI**: 10.1007/978-3-030-32245-8_63
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05236v2)
- **Published**: 2019-04-10 15:14:47+00:00
- **Updated**: 2019-07-26 13:29:51+00:00
- **Authors**: Hoel Kervadec, Jose Dolz, Eric Granger, Ismail Ben Ayed
- **Comment**: Accepted as paper as MICCAI 2O19
- **Journal**: None
- **Summary**: This study investigates a curriculum-style strategy for semi-supervised CNN segmentation, which devises a regression network to learn image-level information such as the size of a target region. These regressions are used to effectively regularize the segmentation network, constraining softmax predictions of the unlabeled images to match the inferred label distributions. Our framework is based on inequality constraints that tolerate uncertainties with inferred knowledge, e.g., regressed region size, and can be employed for a large variety of region attributes. We evaluated our proposed strategy for left ventricle segmentation in magnetic resonance images (MRI), and compared it to standard proposal-based semi-supervision strategies. Our strategy leverages unlabeled data in more efficiently, and achieves very competitive results, approaching the performance of full-supervision.



### Diagnosis of Celiac Disease and Environmental Enteropathy on Biopsy Images Using Color Balancing on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.05773v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.05773v5)
- **Published**: 2019-04-10 15:24:32+00:00
- **Updated**: 2019-10-09 16:24:12+00:00
- **Authors**: Kamran Kowsari, Rasoul Sali, Marium N. Khan, William Adorno, S. Asad Ali, Sean R. Moore, Beatrice C. Amadi, Paul Kelly, Sana Syed, Donald E. Brown
- **Comment**: None
- **Journal**: None
- **Summary**: Celiac Disease (CD) and Environmental Enteropathy (EE) are common causes of malnutrition and adversely impact normal childhood development. CD is an autoimmune disorder that is prevalent worldwide and is caused by an increased sensitivity to gluten. Gluten exposure destructs the small intestinal epithelial barrier, resulting in nutrient mal-absorption and childhood under-nutrition. EE also results in barrier dysfunction but is thought to be caused by an increased vulnerability to infections. EE has been implicated as the predominant cause of under-nutrition, oral vaccine failure, and impaired cognitive development in low-and-middle-income countries. Both conditions require a tissue biopsy for diagnosis, and a major challenge of interpreting clinical biopsy images to differentiate between these gastrointestinal diseases is striking histopathologic overlap between them. In the current study, we propose a convolutional neural network (CNN) to classify duodenal biopsy images from subjects with CD, EE, and healthy controls. We evaluated the performance of our proposed model using a large cohort containing 1000 biopsy images. Our evaluations show that the proposed model achieves an area under ROC of 0.99, 1.00, and 0.97 for CD, EE, and healthy controls, respectively. These results demonstrate the discriminative power of the proposed model in duodenal biopsies classification.



### Localized Trajectories for 2D and 3D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.05244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05244v1)
- **Published**: 2019-04-10 15:34:25+00:00
- **Updated**: 2019-04-10 15:34:25+00:00
- **Authors**: Konstantinos Papadopoulos, Girum Demisse, Enjie Ghorbel, Michel Antunes, Djamila Aouada, Bjrn Ottersten
- **Comment**: 36 pages, 2 figures
- **Journal**: None
- **Summary**: The Dense Trajectories concept is one of the most successful approaches in action recognition, suitable for scenarios involving a significant amount of motion. However, due to noise and background motion, many generated trajectories are irrelevant to the actual human activity and can potentially lead to performance degradation. In this paper, we propose Localized Trajectories as an improved version of Dense Trajectories where motion trajectories are clustered around human body joints provided by RGB-D cameras and then encoded by local Bag-of-Words. As a result, the Localized Trajectories concept provides a more discriminative representation of actions as compared to Dense Trajectories. Moreover, we generalize Localized Trajectories to 3D by using the modalities offered by RGB-D cameras. One of the main advantages of using RGB-D data to generate trajectories is that they include radial displacements that are perpendicular to the image plane. Extensive experiments and analysis are carried out on five different datasets.



### Next-Active-Object prediction from Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1904.05250v1
- **DOI**: 10.1016/j.jvcir.2017.10.004
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05250v1)
- **Published**: 2019-04-10 15:39:19+00:00
- **Updated**: 2019-04-10 15:39:19+00:00
- **Authors**: Antonino Furnari, Sebastiano Battiato, Kristen Grauman, Giovanni Maria Farinella
- **Comment**: None
- **Journal**: Journal of Visual Communication and Image Representation, Volume
  49, 2017, Pages 401-411, ISSN 1047-3203
- **Summary**: Although First Person Vision systems can sense the environment from the user's perspective, they are generally unable to predict his intentions and goals. Since human activities can be decomposed in terms of atomic actions and interactions with objects, intelligent wearable systems would benefit from the ability to anticipate user-object interactions. Even if this task is not trivial, the First Person Vision paradigm can provide important cues to address this challenge. We propose to exploit the dynamics of the scene to recognize next-active-objects before an object interaction begins. We train a classifier to discriminate trajectories leading to an object activation from all others and forecast next-active-objects by analyzing fixed-length trajectory segments within a temporal sliding window. The proposed method compares favorably with respect to several baselines on the Activity of Daily Living (ADL) egocentric dataset comprising 10 hours of videos acquired by 20 subjects while performing unconstrained interactions with several objects.



### Instance Segmentation of Biological Images Using Harmonic Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1904.05257v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05257v2)
- **Published**: 2019-04-10 15:56:16+00:00
- **Updated**: 2020-04-23 15:26:52+00:00
- **Authors**: Victor Kulikov, Victor Lempitsky
- **Comment**: Accepted as oral to CVPR 2020
- **Journal**: None
- **Summary**: We present a new instance segmentation approach tailored to biological images, where instances may correspond to individual cells, organisms or plant parts. Unlike instance segmentation for user photographs or road scenes, in biological data object instances may be particularly densely packed, the appearance variation may be particularly low, the processing power may be restricted, while, on the other hand, the variability of sizes of individual instances may be limited. The proposed approach successfully addresses these peculiarities.   Our approach describes each object instance using an expectation of a limited number of sine waves with frequencies and phases adjusted to particular object sizes and densities. At train time, a fully-convolutional network is learned to predict the object embeddings at each pixel using a simple pixelwise regression loss, while at test time the instances are recovered using clustering in the embedding space. In the experiments, we show that our approach outperforms previous embedding-based instance segmentation approaches on a number of biological datasets, achieving state-of-the-art on a popular CVPPP benchmark. This excellent performance is combined with computational efficiency that is needed for deployment to domain specialists.   The source code of the approach is available at https://github.com/kulikovv/harmonic



### Egocentric Visitors Localization in Cultural Sites
- **Arxiv ID**: http://arxiv.org/abs/1904.05264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05264v1)
- **Published**: 2019-04-10 16:05:43+00:00
- **Updated**: 2019-04-10 16:05:43+00:00
- **Authors**: Francesco Ragusa, Antonino Furnari, Sebastiano Battiato, Giovanni Signorello, Giovanni Maria Farinella
- **Comment**: To appear in ACM Journal on Computing and Cultural Heritage (JOCCH),
  2019
- **Journal**: ACM Journal on Computing and Cultural Heritage (JOCCH), 2019
- **Summary**: We consider the problem of localizing visitors in a cultural site from egocentric (first person) images. Localization information can be useful both to assist the user during his visit (e.g., by suggesting where to go and what to see next) and to provide behavioral information to the manager of the cultural site (e.g., how much time has been spent by visitors at a given location? What has been liked most?). To tackle the problem, we collected a large dataset of egocentric videos using two cameras: a head-mounted HoloLens device and a chest-mounted GoPro. Each frame has been labeled according to the location of the visitor and to what he was looking at. The dataset is freely available in order to encourage research in this domain. The dataset is complemented with baseline experiments performed considering a state-of-the-art method for location-based temporal segmentation of egocentric videos. Experiments show that compelling results can be achieved to extract useful information for both the visitor and the site-manager.



### Deep Learning Inversion of Electrical Resistivity Data
- **Arxiv ID**: http://arxiv.org/abs/1904.05265v2
- **DOI**: 10.1109/TGRS.2020.2969040
- **Categories**: **cs.CV**, cs.AI, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/1904.05265v2)
- **Published**: 2019-04-10 16:06:29+00:00
- **Updated**: 2020-06-26 08:14:16+00:00
- **Authors**: Bin Liu, Qian Guo, Shucai Li, Benchao Liu, Yuxiao Ren, Yonghao Pang, Xu Guo, Lanbo Liu, Peng Jiang
- **Comment**: IEEE Transactions on Geoscience and Remote Sensing, 2020
- **Journal**: None
- **Summary**: The inverse problem of electrical resistivity surveys (ERSs) is difficult because of its nonlinear and ill-posed nature. For this task, traditional linear inversion methods still face challenges such as suboptimal approximation and initial model selection. Inspired by the remarkable nonlinear mapping ability of deep learning approaches, in this article, we propose to build the mapping from apparent resistivity data (input) to resistivity model (output) directly by convolutional neural networks (CNNs). However, the vertically varying characteristic of patterns in the apparent resistivity data may cause ambiguity when using CNNs with the weight sharing and effective receptive field properties. To address the potential issue, we supply an additional tier feature map to CNNs to help those aware of the relationship between input and output. Based on the prevalent U-Net architecture, we design our network (ERSInvNet) that can be trained end-to-end and can reach a very fast inference speed during testing. We further introduce a depth weighting function and a smooth constraint into loss function to improve inversion accuracy for the deep region and suppress false anomalies. Six groups of experiments are considered to demonstrate the feasibility and efficiency of the proposed methods. According to the comprehensive qualitative analysis and quantitative comparison, ERSInvNet with tier feature map, smooth constraints, and depth weighting function together achieve the best performance.



### Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.05290v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05290v1)
- **Published**: 2019-04-10 16:50:38+00:00
- **Updated**: 2019-04-10 16:50:38+00:00
- **Authors**: Junhwa Hur, Stefan Roth
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: Deep learning approaches to optical flow estimation have seen rapid progress over the recent years. One common trait of many networks is that they refine an initial flow estimate either through multiple stages or across the levels of a coarse-to-fine representation. While leading to more accurate results, the downside of this is an increased number of parameters. Taking inspiration from both classical energy minimization approaches as well as residual networks, we propose an iterative residual refinement (IRR) scheme based on weight sharing that can be combined with several backbone networks. It reduces the number of parameters, improves the accuracy, or even achieves both. Moreover, we show that integrating occlusion prediction and bi-directional flow estimation into our IRR scheme can further boost the accuracy. Our full network achieves state-of-the-art results for both optical flow and occlusion estimation across several standard datasets.



### Evaluation of a Dual Convolutional Neural Network Architecture for Object-wise Anomaly Detection in Cluttered X-ray Security Imagery
- **Arxiv ID**: http://arxiv.org/abs/1904.05304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05304v1)
- **Published**: 2019-04-10 17:11:50+00:00
- **Updated**: 2019-04-10 17:11:50+00:00
- **Authors**: Yona Falinie A. Gaus, Neelanjan Bhowmik, Samet Akay, Paolo M. Guillen-Garcia, Jack W. Barker, Toby P. Breckon
- **Comment**: IJCNN 2019
- **Journal**: None
- **Summary**: X-ray baggage security screening is widely used to maintain aviation and transport security. Of particular interest is the focus on automated security X-ray analysis for particular classes of object such as electronics, electrical items, and liquids. However, manual inspection of such items is challenging when dealing with potentially anomalous items. Here we present a dual convolutional neural network (CNN) architecture for automatic anomaly detection within complex security X-ray imagery. We leverage recent advances in region-based (R-CNN), mask-based CNN (Mask R-CNN) and detection architectures such as RetinaNet to provide object localisation variants for specific object classes of interest. Subsequently, leveraging a range of established CNN object and fine-grained category classification approaches we formulate within object anomaly detection as a two-class problem (anomalous or benign). While the best performing object localisation method is able to perform with 97.9% mean average precision (mAP) over a six-class X-ray object detection problem, subsequent two-class anomaly/benign classification is able to achieve 66% performance for within object anomaly detection. Overall, this performance illustrates both the challenge and promise of object-wise anomaly detection within the context of cluttered X-ray security imagery.



### StegaStamp: Invisible Hyperlinks in Physical Photographs
- **Arxiv ID**: http://arxiv.org/abs/1904.05343v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05343v2)
- **Published**: 2019-04-10 17:53:38+00:00
- **Updated**: 2020-03-26 02:51:10+00:00
- **Authors**: Matthew Tancik, Ben Mildenhall, Ren Ng
- **Comment**: CVPR 2020, Project page: http://www.matthewtancik.com/stegastamp
- **Journal**: None
- **Summary**: Printed and digitally displayed photos have the ability to hide imperceptible digital data that can be accessed through internet-connected imaging systems. Another way to think about this is physical photographs that have unique QR codes invisibly embedded within them. This paper presents an architecture, algorithms, and a prototype implementation addressing this vision. Our key technical contribution is StegaStamp, a learned steganographic algorithm to enable robust encoding and decoding of arbitrary hyperlink bitstrings into photos in a manner that approaches perceptual invisibility. StegaStamp comprises a deep neural network that learns an encoding/decoding algorithm robust to image perturbations approximating the space of distortions resulting from real printing and photography. We demonstrates real-time decoding of hyperlinks in photos from in-the-wild videos that contain variation in lighting, shadows, perspective, occlusion and viewing distance. Our prototype system robustly retrieves 56 bit hyperlinks after error correction - sufficient to embed a unique code within every photo on the internet.



### H+O: Unified Egocentric Recognition of 3D Hand-Object Poses and Interactions
- **Arxiv ID**: http://arxiv.org/abs/1904.05349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05349v1)
- **Published**: 2019-04-10 17:58:32+00:00
- **Updated**: 2019-04-10 17:58:32+00:00
- **Authors**: Bugra Tekin, Federica Bogo, Marc Pollefeys
- **Comment**: CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: We present a unified framework for understanding 3D hand and object interactions in raw image sequences from egocentric RGB cameras. Given a single RGB image, our model jointly estimates the 3D hand and object poses, models their interactions, and recognizes the object and action classes with a single feed-forward pass through a neural network. We propose a single architecture that does not rely on external detection algorithms but rather is trained end-to-end on single images. We further merge and propagate information in the temporal domain to infer interactions between hand and object trajectories and recognize actions. The complete model takes as input a sequence of frames and outputs per-frame 3D hand and object pose predictions along with the estimates of object and action categories for the entire sequence. We demonstrate state-of-the-art performance of our algorithm even in comparison to the approaches that work on depth data and ground-truth annotations.



### Pixel-Adaptive Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.05373v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05373v1)
- **Published**: 2019-04-10 18:02:54+00:00
- **Updated**: 2019-04-10 18:02:54+00:00
- **Authors**: Hang Su, Varun Jampani, Deqing Sun, Orazio Gallo, Erik Learned-Miller, Jan Kautz
- **Comment**: CVPR 2019. Video introduction: https://youtu.be/gsQZbHuR64o
- **Journal**: None
- **Summary**: Convolutions are the fundamental building block of CNNs. The fact that their weights are spatially shared is one of the main reasons for their widespread use, but it also is a major limitation, as it makes convolutions content agnostic. We propose a pixel-adaptive convolution (PAC) operation, a simple yet effective modification of standard convolutions, in which the filter weights are multiplied with a spatially-varying kernel that depends on learnable, local pixel features. PAC is a generalization of several popular filtering techniques and thus can be used for a wide range of use cases. Specifically, we demonstrate state-of-the-art performance when PAC is used for deep joint image upsampling. PAC also offers an effective alternative to fully-connected CRF (Full-CRF), called PAC-CRF, which performs competitively, while being considerably faster. In addition, we also demonstrate that PAC can be used as a drop-in replacement for convolution layers in pre-trained networks, resulting in consistent performance improvements.



### Spherical Regression: Learning Viewpoints, Surface Normals and 3D Rotations on n-Spheres
- **Arxiv ID**: http://arxiv.org/abs/1904.05404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05404v1)
- **Published**: 2019-04-10 19:33:59+00:00
- **Updated**: 2019-04-10 19:33:59+00:00
- **Authors**: Shuai Liao, Efstratios Gavves, Cees G. M. Snoek
- **Comment**: CVPR 2019 camera ready
- **Journal**: None
- **Summary**: Many computer vision challenges require continuous outputs, but tend to be solved by discrete classification. The reason is classification's natural containment within a probability $n$-simplex, as defined by the popular softmax activation function. Regular regression lacks such a closed geometry, leading to unstable training and convergence to suboptimal local minima. Starting from this insight we revisit regression in convolutional neural networks. We observe many continuous output problems in computer vision are naturally contained in closed geometrical manifolds, like the Euler angles in viewpoint estimation or the normals in surface normal estimation. A natural framework for posing such continuous output problems are $n$-spheres, which are naturally closed geometric manifolds defined in the $\mathbb{R}^{(n+1)}$ space. By introducing a spherical exponential mapping on $n$-spheres at the regression output, we obtain well-behaved gradients, leading to stable training. We show how our spherical regression can be utilized for several computer vision challenges, specifically viewpoint estimation, surface normal estimation and 3D rotation estimation. For all these problems our experiments demonstrate the benefit of spherical regression. All paper resources are available at https://github.com/leoshine/Spherical_Regression.



### Sliced Wasserstein Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1904.05408v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05408v2)
- **Published**: 2019-04-10 19:49:43+00:00
- **Updated**: 2019-04-13 06:53:02+00:00
- **Authors**: Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, Luc Van Gool
- **Comment**: This paper is submitted to arxiv twice, thus withdraw one of the
  versions. See arXiv:1706.02631 instead
- **Journal**: None
- **Summary**: In generative modeling, the Wasserstein distance (WD) has emerged as a useful metric to measure the discrepancy between generated and real data distributions. Unfortunately, it is challenging to approximate the WD of high-dimensional distributions. In contrast, the sliced Wasserstein distance (SWD) factorizes high-dimensional distributions into their multiple one-dimensional marginal distributions and is thus easier to approximate. In this paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by conventional SWD approximation methods, we propose to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion. As concrete applications of our SWD approximations, we design two types of differentiable SWD blocks to equip modern generative frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In the experiments, we not only show the superiority of the proposed generative models on standard image synthesis benchmarks, but also demonstrate the state-of-the-art performance on challenging high resolution image and video generation in an unsupervised manner.



### Attentive Action and Context Factorization
- **Arxiv ID**: http://arxiv.org/abs/1904.05410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05410v1)
- **Published**: 2019-04-10 19:50:45+00:00
- **Updated**: 2019-04-10 19:50:45+00:00
- **Authors**: Yang Wang, Vinh Tran, Gedas Bertasius, Lorenzo Torresani, Minh Hoai
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: We propose a method for human action recognition, one that can localize the spatiotemporal regions that `define' the actions. This is a challenging task due to the subtlety of human actions in video and the co-occurrence of contextual elements. To address this challenge, we utilize conjugate samples of human actions, which are video clips that are contextually similar to human action samples but do not contain the action. We introduce a novel attentional mechanism that can spatially and temporally separate human actions from the co-occurring contextual factors. The separation of the action and context factors is weakly supervised, eliminating the need for laboriously detailed annotation of these two factors in training samples. Our method can be used to build human action classifiers with higher accuracy and better interpretability. Experiments on several human action recognition datasets demonstrate the quantitative and qualitative benefits of our approach.



### BAOD: Budget-Aware Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.05443v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05443v2)
- **Published**: 2019-04-10 21:13:08+00:00
- **Updated**: 2021-08-09 08:14:02+00:00
- **Authors**: Alejandro Pardo, Mengmeng Xu, Ali Thabet, Pablo Arbelaez, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of object detection from a novel perspective in which annotation budget constraints are taken into consideration, appropriately coined Budget Aware Object Detection (BAOD). When provided with a fixed budget, we propose a strategy for building a diverse and informative dataset that can be used to optimally train a robust detector. We investigate both optimization and learning-based methods to sample which images to annotate and what type of annotation (strongly or weakly supervised) to annotate them with. We adopt a hybrid supervised learning framework to train the object detector from both these types of annotation. We conduct a comprehensive empirical study showing that a handcrafted optimization method outperforms other selection techniques including random sampling, uncertainty sampling and active learning. By combining an optimal image/annotation selection scheme with hybrid supervised learning to solve the BAOD problem, we show that one can achieve the performance of a strongly supervised detector on PASCAL-VOC 2007 while saving 12.8% of its original annotation budget. Furthermore, when $100\%$ of the budget is used, it surpasses this performance by 2.0 mAP percentage points.



### Predicting Future Pedestrian Motion in Video Sequences using Crowd Simulation
- **Arxiv ID**: http://arxiv.org/abs/1904.05448v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1904.05448v1)
- **Published**: 2019-04-10 21:25:24+00:00
- **Updated**: 2019-04-10 21:25:24+00:00
- **Authors**: Cliceres dal Bianco, Soraia Raupp Musse
- **Comment**: None
- **Journal**: None
- **Summary**: While human and group analysis have become an important area in last decades, some current and relevant applications involve to estimate future motion of pedestrians in real video sequences. This paper presents a method to provide motion estimation of real pedestrians in next seconds, using crowd simulation. Our method is based on Physics and heuristics and use BioCrowds as crowd simulation methodology to estimate future positions of people in video sequences. Results show that our method for estimation works well even for complex videos where events can happen. The maximum achieved average error is $2.72$cm when estimating the future motion of 32 pedestrians with more than 2 seconds in advance. This paper discusses this and other results.



### Analyzing Dynamical Brain Functional Connectivity As Trajectories on Space of Covariance Matrices
- **Arxiv ID**: http://arxiv.org/abs/1904.05449v2
- **DOI**: 10.1109/TMI.2019.2931708
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05449v2)
- **Published**: 2019-04-10 21:27:42+00:00
- **Updated**: 2019-05-15 18:49:33+00:00
- **Authors**: Mengyu Dai, Zhengwu Zhang, Anuj Srivastava
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: Published in IEEE Transactions on Medical Imaging, 2019
- **Summary**: Human brain functional connectivity (FC) is often measured as the similarity of functional MRI responses across brain regions when a brain is either resting or performing a task. This paper aims to statistically analyze the dynamic nature of FC by representing the collective time-series data, over a set of brain regions, as a trajectory on the space of covariance matrices, or symmetric-positive definite matrices (SPDMs). We use a recently developed metric on the space of SPDMs for quantifying differences across FC observations, and for clustering and classification of FC trajectories. To facilitate large scale and high-dimensional data analysis, we propose a novel, metric-based dimensionality reduction technique to reduce data from large SPDMs to small SPDMs. We illustrate this comprehensive framework using data from the Human Connectome Project (HCP) database for multiple subjects and tasks, with task classification rates that match or outperform state-of-the-art techniques.



### Instance Segmentation based Semantic Matting for Compositing Applications
- **Arxiv ID**: http://arxiv.org/abs/1904.05457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05457v1)
- **Published**: 2019-04-10 21:48:34+00:00
- **Updated**: 2019-04-10 21:48:34+00:00
- **Authors**: Guanqing Hu, James J. Clark
- **Comment**: 16th Conference on Computer and Robot Vision (CRV 2019)
- **Journal**: None
- **Summary**: Image compositing is a key step in film making and image editing that aims to segment a foreground object and combine it with a new background. Automatic image compositing can be done easily in a studio using chroma-keying when the background is pure blue or green. However, image compositing in natural scenes with complex backgrounds remains a tedious task, requiring experienced artists to hand-segment. In order to achieve automatic compositing in natural scenes, we propose a fully automated method that integrates instance segmentation and image matting processes to generate high-quality semantic mattes that can be used for image editing task. Our approach can be seen both as a refinement of existing instance segmentation algorithms and as a fully automated semantic image matting method. It extends automatic image compositing techniques such as chroma-keying to scenes with complex natural backgrounds without the need for any kind of user interaction. The output of our approach can be considered as both refined instance segmentations and alpha mattes with semantic meanings. We provide experimental results which show improved performance results as compared to existing approaches.



### Learning to Generate Synthetic Data via Compositing
- **Arxiv ID**: http://arxiv.org/abs/1904.05475v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05475v2)
- **Published**: 2019-04-10 23:22:09+00:00
- **Updated**: 2019-07-08 22:15:12+00:00
- **Authors**: Shashank Tripathi, Siddhartha Chandra, Amit Agrawal, Ambrish Tyagi, James M. Rehg, Visesh Chari
- **Comment**: Accepted to CVPR 2019, supplementary material included
- **Journal**: None
- **Summary**: We present a task-aware approach to synthetic data generation. Our framework employs a trainable synthesizer network that is optimized to produce meaningful training samples by assessing the strengths and weaknesses of a `target' network. The synthesizer and target networks are trained in an adversarial manner wherein each network is updated with a goal to outdo the other. Additionally, we ensure the synthesizer generates realistic data by pairing it with a discriminator trained on real-world images. Further, to make the target classifier invariant to blending artefacts, we introduce these artefacts to background regions of the training images so the target does not over-fit to them.   We demonstrate the efficacy of our approach by applying it to different target networks including a classification network on AffNIST, and two object detection networks (SSD, Faster-RCNN) on different datasets. On the AffNIST benchmark, our approach is able to surpass the baseline results with just half the training examples. On the VOC person detection benchmark, we show improvements of up to 2.7% as a result of our data augmentation. Similarly on the GMU detection benchmark, we report a performance boost of 3.5% in mAP over the baseline method, outperforming the previous state of the art approaches by up to 7.5% on specific categories.



### Predicting Progression of Age-related Macular Degeneration from Fundus Images using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.05478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05478v1)
- **Published**: 2019-04-10 23:31:01+00:00
- **Updated**: 2019-04-10 23:31:01+00:00
- **Authors**: Boris Babenko, Siva Balasubramanian, Katy E. Blumer, Greg S. Corrado, Lily Peng, Dale R. Webster, Naama Hammel, Avinash V. Varadarajan
- **Comment**: 27 pages, 7 figures
- **Journal**: None
- **Summary**: Background: Patients with neovascular age-related macular degeneration (AMD) can avoid vision loss via certain therapy. However, methods to predict the progression to neovascular age-related macular degeneration (nvAMD) are lacking. Purpose: To develop and validate a deep learning (DL) algorithm to predict 1-year progression of eyes with no, early, or intermediate AMD to nvAMD, using color fundus photographs (CFP). Design: Development and validation of a DL algorithm. Methods: We trained a DL algorithm to predict 1-year progression to nvAMD, and used 10-fold cross-validation to evaluate this approach on two groups of eyes in the Age-Related Eye Disease Study (AREDS): none/early/intermediate AMD, and intermediate AMD (iAMD) only. We compared the DL algorithm to the manually graded 4-category and 9-step scales in the AREDS dataset. Main outcome measures: Performance of the DL algorithm was evaluated using the sensitivity at 80% specificity for progression to nvAMD. Results: The DL algorithm's sensitivity for predicting progression to nvAMD from none/early/iAMD (78+/-6%) was higher than manual grades from the 9-step scale (67+/-8%) or the 4-category scale (48+/-3%). For predicting progression specifically from iAMD, the DL algorithm's sensitivity (57+/-6%) was also higher compared to the 9-step grades (36+/-8%) and the 4-category grades (20+/-0%). Conclusions: Our DL algorithm performed better in predicting progression to nvAMD than manual grades. Future investigations are required to test the application of this DL algorithm in a real-world clinical setting.




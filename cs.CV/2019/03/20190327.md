# Arxiv Papers in cs.CV on 2019-03-27
### Information Maximizing Visual Question Generation
- **Arxiv ID**: http://arxiv.org/abs/1903.11207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11207v1)
- **Published**: 2019-03-27 00:57:25+00:00
- **Updated**: 2019-03-27 00:57:25+00:00
- **Authors**: Ranjay Krishna, Michael Bernstein, Li Fei-Fei
- **Comment**: CVPR 2019
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition, 2019
- **Summary**: Though image-to-sequence generation models have become overwhelmingly popular in human-computer communications, they suffer from strongly favoring safe generic questions ("What is in this picture?"). Generating uninformative but relevant questions is not sufficient or useful. We argue that a good question is one that has a tightly focused purpose --- one that is aimed at expecting a specific type of response. We build a model that maximizes mutual information between the image, the expected answer and the generated question. To overcome the non-differentiability of discrete natural language tokens, we introduce a variational continuous latent space onto which the expected answers project. We regularize this latent space with a second latent space that ensures clustering of similar answers. Even when we don't know the expected answer, this second latent space can generate goal-driven questions specifically aimed at extracting objects ("what is the person throwing"), attributes, ("What kind of shirt is the person wearing?"), color ("what color is the frisbee?"), material ("What material is the frisbee?"), etc. We quantitatively show that our model is able to retain information about an expected answer category, resulting in more diverse, goal-driven questions. We launch our model on a set of real world images and extract previously unseen visual concepts.



### Colorectal cancer diagnosis from histology images: A comparative study
- **Arxiv ID**: http://arxiv.org/abs/1903.11210v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.11210v2)
- **Published**: 2019-03-27 01:02:44+00:00
- **Updated**: 2019-03-28 03:56:18+00:00
- **Authors**: Junaid Malik, Serkan Kiranyaz, Suchitra Kunhoth, Turker Ince, Somaya Al-Maadeed, Ridha Hamila, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-aided diagnosis (CAD) based on histopathological imaging has progressed rapidly in recent years with the rise of machine learning based methodologies. Traditional approaches consist of training a classification model using features extracted from the images, based on textures or morphological properties. Recently, deep-learning based methods have been applied directly to the raw (unprocessed) data. However, their usability is impacted by the paucity of annotated data in the biomedical sector. In order to leverage the learning capabilities of deep Convolutional Neural Nets (CNNs) within the confines of limited labelled data, in this study we shall investigate the transfer learning approaches that aim to apply the knowledge gained from solving a source (e.g., non-medical) problem, to learn better predictive models for the target (e.g., biomedical) task. As an alternative, we shall further propose a new adaptive and compact CNN based architecture that can be trained from scratch even on scarce and low-resolution data. Moreover, we conduct quantitative comparative evaluations among the traditional methods, transfer learning-based methods and the proposed adaptive approach for the particular task of cancer detection and identification from scarce and low-resolution histology images. Over the largest benchmark dataset formed for this purpose, the proposed adaptive approach achieved a higher cancer detection accuracy with a significant gap, whereas the deep CNNs with transfer learning achieved a superior cancer identification.



### Neural-networks for geophysicists and their application to seismic data interpretation
- **Arxiv ID**: http://arxiv.org/abs/1903.11215v1
- **DOI**: None
- **Categories**: **physics.geo-ph**, cs.CV, 86A04
- **Links**: [PDF](http://arxiv.org/pdf/1903.11215v1)
- **Published**: 2019-03-27 01:26:41+00:00
- **Updated**: 2019-03-27 01:26:41+00:00
- **Authors**: Bas Peters, Eldad Haber, Justin Granek
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Neural-networks have seen a surge of interest for the interpretation of seismic images during the last few years. Network-based learning methods can provide fast and accurate automatic interpretation, provided there are sufficiently many training labels. We provide an introduction to the field aimed at geophysicists that are familiar with the framework of forward modeling and inversion. We explain the similarities and differences between deep networks to other geophysical inverse problems and show their utility in solving problems such as lithology interpolation between wells, horizon tracking and segmentation of seismic images. The benefits of our approach are demonstrated on field data from the Sea of Ireland and the North Sea.



### BAE-NET: Branched Autoencoder for Shape Co-Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.11228v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.11228v2)
- **Published**: 2019-03-27 02:33:20+00:00
- **Updated**: 2019-08-13 20:38:10+00:00
- **Authors**: Zhiqin Chen, Kangxue Yin, Matthew Fisher, Siddhartha Chaudhuri, Hao Zhang
- **Comment**: Accepted to ICCV 2019. Code: https://github.com/czq142857/BAE-NET
  Supplementary material: https://www.sfu.ca/~zhiqinc/imseg/sup.pdf
- **Journal**: None
- **Summary**: We treat shape co-segmentation as a representation learning problem and introduce BAE-NET, a branched autoencoder network, for the task. The unsupervised BAE-NET is trained with a collection of un-segmented shapes, using a shape reconstruction loss, without any ground-truth labels. Specifically, the network takes an input shape and encodes it using a convolutional neural network, whereas the decoder concatenates the resulting feature code with a point coordinate and outputs a value indicating whether the point is inside/outside the shape. Importantly, the decoder is branched: each branch learns a compact representation for one commonly recurring part of the shape collection, e.g., airplane wings. By complementing the shape reconstruction loss with a label loss, BAE-NET is easily tuned for one-shot learning. We show unsupervised, weakly supervised, and one-shot learning results by BAE-NET, demonstrating that using only a couple of exemplars, our network can generally outperform state-of-the-art supervised methods trained on hundreds of segmented shapes. Code is available at https://github.com/czq142857/BAE-NET.



### Deep Co-Training for Semi-Supervised Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.11233v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11233v3)
- **Published**: 2019-03-27 03:10:36+00:00
- **Updated**: 2019-10-30 17:48:03+00:00
- **Authors**: Jizong Peng, Guillermo Estrada, Marco Pedersoli, Christian Desrosiers
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim to improve the performance of semantic image segmentation in a semi-supervised setting in which training is effectuated with a reduced set of annotated images and additional non-annotated images. We present a method based on an ensemble of deep segmentation models. Each model is trained on a subset of the annotated data, and uses the non-annotated images to exchange information with the other models, similar to co-training. Even if each model learns on the same non-annotated images, diversity is preserved with the use of adversarial samples. Our results show that this ability to simultaneously train models, which exchange knowledge while preserving diversity, leads to state-of-the-art results on two challenging medical image datasets.



### Training Quantized Neural Networks with a Full-precision Auxiliary Module
- **Arxiv ID**: http://arxiv.org/abs/1903.11236v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11236v3)
- **Published**: 2019-03-27 03:42:31+00:00
- **Updated**: 2020-03-18 08:06:50+00:00
- **Authors**: Bohan Zhuang, Lingqiao Liu, Mingkui Tan, Chunhua Shen, Ian Reid
- **Comment**: Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition
  (CVPR 2020)
- **Journal**: None
- **Summary**: In this paper, we seek to tackle a challenge in training low-precision networks: the notorious difficulty in propagating gradient through a low-precision network due to the non-differentiable quantization function. We propose a solution by training the low-precision network with a fullprecision auxiliary module. Specifically, during training, we construct a mix-precision network by augmenting the original low-precision network with the full precision auxiliary module. Then the augmented mix-precision network and the low-precision network are jointly optimized. This strategy creates additional full-precision routes to update the parameters of the low-precision model, thus making the gradient back-propagates more easily. At the inference time, we discard the auxiliary module without introducing any computational complexity to the low-precision network. We evaluate the proposed method on image classification and object detection over various quantization approaches and show consistent performance increase. In particular, we achieve near lossless performance to the full-precision model by using a 4-bit detector, which is of great practical value.



### TossingBot: Learning to Throw Arbitrary Objects with Residual Physics
- **Arxiv ID**: http://arxiv.org/abs/1903.11239v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.11239v3)
- **Published**: 2019-03-27 04:04:28+00:00
- **Updated**: 2020-05-30 15:59:12+00:00
- **Authors**: Andy Zeng, Shuran Song, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser
- **Comment**: Summary Video: https://youtu.be/f5Zn2Up2RjQ Project webpage:
  https://tossingbot.cs.princeton.edu
- **Journal**: None
- **Summary**: We investigate whether a robot arm can learn to pick and throw arbitrary objects into selected boxes quickly and accurately. Throwing has the potential to increase the physical reachability and picking speed of a robot arm. However, precisely throwing arbitrary objects in unstructured settings presents many challenges: from acquiring reliable pre-throw conditions (e.g. initial pose of object in manipulator) to handling varying object-centric properties (e.g. mass distribution, friction, shape) and dynamics (e.g. aerodynamics). In this work, we propose an end-to-end formulation that jointly learns to infer control parameters for grasping and throwing motion primitives from visual observations (images of arbitrary objects in a bin) through trial and error. Within this formulation, we investigate the synergies between grasping and throwing (i.e., learning grasps that enable more accurate throws) and between simulation and deep learning (i.e., using deep networks to predict residuals on top of control parameters predicted by a physics simulator). The resulting system, TossingBot, is able to grasp and throw arbitrary objects into boxes located outside its maximum reach range at 500+ mean picks per hour (600+ grasps per hour with 85% throwing accuracy); and generalizes to new objects and target locations. Videos are available at https://tossingbot.cs.princeton.edu



### Mimicking the In-Camera Color Pipeline for Camera-Aware Object Compositing
- **Arxiv ID**: http://arxiv.org/abs/1903.11248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11248v1)
- **Published**: 2019-03-27 04:41:19+00:00
- **Updated**: 2019-03-27 04:41:19+00:00
- **Authors**: Jun Gao, Xiao Li, Liwei Wang, Sanja Fidler, Stephen Lin
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for compositing virtual objects into a photograph such that the object colors appear to have been processed by the photo's camera imaging pipeline. Compositing in such a camera-aware manner is essential for high realism, and it requires the color transformation in the photo's pipeline to be inferred, which is challenging due to the inherent one-to-many mapping that exists from a scene to a photo. To address this problem for the case of a single photo taken from an unknown camera, we propose a dual-learning approach in which the reverse color transformation (from the photo to the scene) is jointly estimated. Learning of the reverse transformation is used to facilitate learning of the forward mapping, by enforcing cycle consistency of the two processes. We additionally employ a feature sharing schema to extract evidence from the target photo in the reverse mapping to guide the forward color transformation. Our dual-learning approach achieves object compositing results that surpass those of alternative techniques.



### W-Net: Reinforced U-Net for Density Map Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.11249v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11249v2)
- **Published**: 2019-03-27 04:46:23+00:00
- **Updated**: 2019-03-29 09:19:46+00:00
- **Authors**: Varun Kannadi Valloli, Kinal Mehta
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd management is of paramount importance when it comes to preventing stampedes and saving lives, especially in a countries like China and India where the combined population is a third of the global population. Millions of people convene annually all around the nation to celebrate a myriad of events and crowd count estimation is the linchpin of the crowd management system that could prevent stampedes and save lives. We present a network for crowd counting which reports state of the art results on crowd counting benchmarks. Our contributions are, first, a U-Net inspired model which affords us to report state of the art results. Second, we propose an independent decoding Reinforcement branch which helps the network converge much earlier and also enables the network to estimate density maps with high Structural Similarity Index (SSIM). Third, we discuss the drawbacks of the contemporary architectures and empirically show that even though our architecture achieves state of the art results, the merit may be due to the encoder-decoder pipeline instead. Finally, we report the error analysis which shows that the contemporary line of work is at saturation and leaves certain prominent problems unsolved.



### Auto-Embedding Generative Adversarial Networks for High Resolution Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1903.11250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11250v2)
- **Published**: 2019-03-27 05:13:29+00:00
- **Updated**: 2019-03-29 01:35:14+00:00
- **Authors**: Yong Guo, Qi Chen, Jian Chen, Qingyao Wu, Qinfeng Shi, Mingkui Tan
- **Comment**: Accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Generating images via the generative adversarial network (GAN) has attracted much attention recently. However, most of the existing GAN-based methods can only produce low-resolution images of limited quality. Directly generating high-resolution images using GANs is nontrivial, and often produces problematic images with incomplete objects. To address this issue, we develop a novel GAN called Auto-Embedding Generative Adversarial Network (AEGAN), which simultaneously encodes the global structure features and captures the fine-grained details. In our network, we use an autoencoder to learn the intrinsic high-level structure of real images and design a novel denoiser network to provide photo-realistic details for the generated images. In the experiments, we are able to produce 512x512 images of promising quality directly from the input noise. The resultant images exhibit better perceptual photo-realism, i.e., with sharper structure and richer details, than other baselines on several datasets, including Oxford-102 Flowers, Caltech-UCSD Birds (CUB), High-Quality Large-scale CelebFaces Attributes (CelebA-HQ), Large-scale Scene Understanding (LSUN) and ImageNet.



### Small Data Challenges in Big Data Era: A Survey of Recent Progress on Unsupervised and Semi-Supervised Methods
- **Arxiv ID**: http://arxiv.org/abs/1903.11260v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11260v2)
- **Published**: 2019-03-27 05:50:28+00:00
- **Updated**: 2021-01-02 16:11:08+00:00
- **Authors**: Guo-Jun Qi, Jiebo Luo
- **Comment**: published in IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Representation learning with small labeled data have emerged in many problems, since the success of deep neural networks often relies on the availability of a huge amount of labeled data that is expensive to collect. To address it, many efforts have been made on training sophisticated models with few labeled data in an unsupervised and semi-supervised fashion. In this paper, we will review the recent progresses on these two major categories of methods. A wide spectrum of models will be categorized in a big picture, where we will show how they interplay with each other to motivate explorations of new ideas. We will review the principles of learning the transformation equivariant, disentangled, self-supervised and semi-supervised representations, all of which underpin the foundation of recent progresses. Many implementations of unsupervised and semi-supervised generative models have been developed on the basis of these criteria, greatly expanding the territory of existing autoencoders, generative adversarial nets (GANs) and other deep networks by exploring the distribution of unlabeled data for more powerful representations. We will discuss emerging topics by revealing the intrinsic connections between unsupervised and semi-supervised learning, and propose in future directions to bridge the algorithmic and theoretical gap between transformation equivariance for unsupervised learning and supervised invariance for supervised learning, and unify unsupervised pretraining and supervised finetuning. We will also provide a broader outlook of future directions to unify transformation and instance equivariances for representation learning, connect unsupervised and semi-supervised augmentations, and explore the role of the self-supervised regularization for many learning problems.



### Graph Convolution for Multimodal Information Extraction from Visually Rich Documents
- **Arxiv ID**: http://arxiv.org/abs/1903.11279v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.11279v1)
- **Published**: 2019-03-27 07:47:12+00:00
- **Updated**: 2019-03-27 07:47:12+00:00
- **Authors**: Xiaojing Liu, Feiyu Gao, Qiong Zhang, Huasha Zhao
- **Comment**: naacl'19 accepted paper
- **Journal**: None
- **Summary**: Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model.



### Deformable kernel networks for guided depth map upsampling
- **Arxiv ID**: http://arxiv.org/abs/1903.11286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11286v1)
- **Published**: 2019-03-27 08:21:09+00:00
- **Updated**: 2019-03-27 08:21:09+00:00
- **Authors**: Beomjun Kim, Jean Ponce, Bumsub Ham
- **Comment**: conference submission
- **Journal**: None
- **Summary**: We address the problem of upsampling a low-resolution (LR) depth map using a registered high-resolution (HR) color image of the same scene. Previous methods based on convolutional neural networks (CNNs) combine nonlinear activations of spatially-invariant kernels to estimate structural details from LR depth and HR color images, and regress upsampling results directly from the networks. In this paper, we revisit the weighted averaging process that has been widely used to transfer structural details from hand-crafted visual features to LR depth maps. We instead learn explicitly sparse and spatially-variant kernels for this task. To this end, we propose a CNN architecture and its efficient implementation, called the deformable kernel network (DKN), that outputs sparse sets of neighbors and the corresponding weights adaptively for each pixel. We also propose a fast version of DKN (FDKN) that runs about 17 times faster (0.01 seconds for a HR image of size 640 x 480). Experimental results on standard benchmarks demonstrate the effectiveness of our approach. In particular, we show that the weighted averaging process with 3 x 3 kernels (i.e., aggregating 9 samples sparsely chosen) outperforms the state of the art by a significant margin.



### Image search using multilingual texts: a cross-modal learning approach between image and text
- **Arxiv ID**: http://arxiv.org/abs/1903.11299v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1903.11299v3)
- **Published**: 2019-03-27 09:02:41+00:00
- **Updated**: 2019-05-14 09:34:25+00:00
- **Authors**: Maxime Portaz, Hicham Randrianarivo, Adrien Nivaggioli, Estelle Maudet, Christophe Servan, Sylvain Peyronnet
- **Comment**: None
- **Journal**: None
- **Summary**: Multilingual (or cross-lingual) embeddings represent several languages in a unique vector space. Using a common embedding space enables for a shared semantic between words from different languages. In this paper, we propose to embed images and texts into a unique distributional vector space, enabling to search images by using text queries expressing information needs related to the (visual) content of images, as well as using image similarity. Our framework forces the representation of an image to be similar to the representation of the text that describes it. Moreover, by using multilingual embeddings we ensure that words from two different languages have close descriptors and thus are attached to similar images. We provide experimental evidence of the efficiency of our approach by experimenting it on two datasets: Common Objects in COntext (COCO) [19] and Multi30K [7].



### 3D Face Mask Presentation Attack Detection Based on Intrinsic Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1903.11303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11303v1)
- **Published**: 2019-03-27 09:13:08+00:00
- **Updated**: 2019-03-27 09:13:08+00:00
- **Authors**: Lei Li, Zhaoqiang Xia, Xiaoyue Jiang, Yupeng Ma, Fabio Roli, Xiaoyi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Face presentation attacks have become a major threat to face recognition systems and many countermeasures have been proposed in the past decade. However, most of them are devoted to 2D face presentation attacks, rather than 3D face masks. Unlike the real face, the 3D face mask is usually made of resin materials and has a smooth surface, resulting in reflectance differences. So, we propose a novel detection method for 3D face mask presentation attack by modeling reflectance differences based on intrinsic image analysis. In the proposed method, the face image is first processed with intrinsic image decomposition to compute its reflectance image. Then, the intensity distribution histograms are extracted from three orthogonal planes to represent the intensity differences of reflectance images between the real face and 3D face mask. After that, the 1D convolutional network is further used to capture the information for describing different materials or surfaces react differently to changes in illumination. Extensive experiments on the 3DMAD database demonstrate the effectiveness of our proposed method in distinguishing a face mask from the real one and show that the detection performance outperforms other state-of-the-art methods.



### Linkage Based Face Clustering via Graph Convolution Network
- **Arxiv ID**: http://arxiv.org/abs/1903.11306v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11306v3)
- **Published**: 2019-03-27 09:36:23+00:00
- **Updated**: 2019-04-08 05:35:15+00:00
- **Authors**: Zhongdao Wang, Liang Zheng, Yali Li, Shengjin Wang
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we present an accurate and scalable approach to the face clustering task. We aim at grouping a set of faces by their potential identities. We formulate this task as a link prediction problem: a link exists between two faces if they are of the same identity. The key idea is that we find the local context in the feature space around an instance (face) contains rich information about the linkage relationship between this instance and its neighbors. By constructing sub-graphs around each instance as input data, which depict the local context, we utilize the graph convolution network (GCN) to perform reasoning and infer the likelihood of linkage between pairs in the sub-graphs. Experiments show that our method is more robust to the complex distribution of faces than conventional methods, yielding favorably comparable results to state-of-the-art methods on standard face clustering benchmarks, and is scalable to large datasets. Furthermore, we show that the proposed method does not need the number of clusters as prior, is aware of noises and outliers, and can be extended to a multi-view version for more accurate clustering accuracy.



### A novel machine learning based framework for detection of Autism Spectrum Disorder (ASD)
- **Arxiv ID**: http://arxiv.org/abs/1903.11323v3
- **DOI**: 10.1080/08839514.2021.2004655
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1903.11323v3)
- **Published**: 2019-03-27 10:02:08+00:00
- **Updated**: 2020-01-02 14:58:23+00:00
- **Authors**: Hamza Sharif, Rizwan Ahmed Khan
- **Comment**: None
- **Journal**: Applied Artificial Intelligence 2021
- **Summary**: Computer vision and machine learning are the linchpin of field of automation. The medicine industry has adopted numerous methods to discover the root causes of many diseases in order to automate detection process. But, the biomarkers of Autism Spectrum Disorder (ASD) are still unknown, let alone automating its detection. Studies from the neuroscience domain highlighted the fact that corpus callosum and intracranial brain volume holds significant information for detection of ASD. Such results and studies are not tested and verified by scientists working in the domain of computer vision / machine learning. Thus, in this study we have proposed a machine learning based framework for automatic detection of ASD using features extracted from corpus callosum and intracranial brain volume from ABIDE dataset. Corpus callosum and intracranial brain volume data is obtained from T1-weighted MRI scans. Our proposed framework first calculates weights of features extracted from Corpus callosum and intracranial brain volume data. This step ensures to utilize discriminative capabilities of only those features that will help in robust recognition of ASD. Then, conventional machine learning algorithm (conventional refers to algorithms other than deep learning) is applied on features that are most significant in terms of discriminative capabilities for recognition of ASD. Finally, for benchmarking and to verify potential of deep learning on analyzing neuroimaging data i.e. T1-weighted MRI scans, we have done experiment with state of the art deep learning architecture i.e. VGG16 . We have used transfer learning approach to use already trained VGG16 model for detection of ASD. This is done to help readers understand benefits and bottlenecks of using deep learning approach for analyzing neuroimaging data which is difficult to record in large enough quantity for deep learning.



### Dense Intrinsic Appearance Flow for Human Pose Transfer
- **Arxiv ID**: http://arxiv.org/abs/1903.11326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11326v1)
- **Published**: 2019-03-27 10:11:09+00:00
- **Updated**: 2019-03-27 10:11:09+00:00
- **Authors**: Yining Li, Chen Huang, Chen Change Loy
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present a novel approach for the task of human pose transfer, which aims at synthesizing a new image of a person from an input image of that person and a target pose. We address the issues of limited correspondences identified between keypoints only and invisible pixels due to self-occlusion. Unlike existing methods, we propose to estimate dense and intrinsic 3D appearance flow to better guide the transfer of pixels between poses. In particular, we wish to generate the 3D flow from just the reference and target poses. Training a network for this purpose is non-trivial, especially when the annotations for 3D appearance flow are scarce by nature. We address this problem through a flow synthesis stage. This is achieved by fitting a 3D model to the given pose pair and project them back to the 2D plane to compute the dense appearance flow for training. The synthesized ground-truths are then used to train a feedforward network for efficient mapping from the input and target skeleton poses to the 3D appearance flow. With the appearance flow, we perform feature warping on the input image and generate a photorealistic image of the target pose. Extensive results on DeepFashion and Market-1501 datasets demonstrate the effectiveness of our approach over existing methods. Our code is available at http://mmlab.ie.cuhk.edu.hk/projects/pose-transfer



### Rethinking the Evaluation of Video Summaries
- **Arxiv ID**: http://arxiv.org/abs/1903.11328v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11328v2)
- **Published**: 2019-03-27 10:15:19+00:00
- **Updated**: 2019-04-11 07:52:09+00:00
- **Authors**: Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkil√§
- **Comment**: CVPR'19 poster
- **Journal**: None
- **Summary**: Video summarization is a technique to create a short skim of the original video while preserving the main stories/content. There exists a substantial interest in automatizing this process due to the rapid growth of the available material. The recent progress has been facilitated by public benchmark datasets, which enable easy and fair comparison of methods. Currently the established evaluation protocol is to compare the generated summary with respect to a set of reference summaries provided by the dataset. In this paper, we will provide in-depth assessment of this pipeline using two popular benchmark datasets. Surprisingly, we observe that randomly generated summaries achieve comparable or better performance to the state-of-the-art. In some cases, the random summaries outperform even the human generated summaries in leave-one-out experiments. Moreover, it turns out that the video segmentation, which is often considered as a fixed pre-processing method, has the most significant impact on the performance measure. Based on our observations, we propose alternative approaches for assessing the importance scores as well as an intuitive visualization of correlation between the estimated scoring and human annotations.



### Speed Invariant Time Surface for Learning to Detect Corner Points with Event-Based Cameras
- **Arxiv ID**: http://arxiv.org/abs/1903.11332v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11332v2)
- **Published**: 2019-03-27 10:19:40+00:00
- **Updated**: 2019-04-30 13:20:01+00:00
- **Authors**: Jacques Manderscheid, Amos Sironi, Nicolas Bourdis, Davide Migliore, Vincent Lepetit
- **Comment**: 8 pages, 7 figures, accepted at CVPR 2019
- **Journal**: None
- **Summary**: We propose a learning approach to corner detection for event-based cameras that is stable even under fast and abrupt motions. Event-based cameras offer high temporal resolution, power efficiency, and high dynamic range. However, the properties of event-based data are very different compared to standard intensity images, and simple extensions of corner detection methods designed for these images do not perform well on event-based data. We first introduce an efficient way to compute a time surface that is invariant to the speed of the objects. We then show that we can train a Random Forest to recognize events generated by a moving corner from our time surface. Random Forests are also extremely efficient, and therefore a good choice to deal with the high capture frequency of event-based cameras ---our implementation processes up to 1.6Mev/s on a single CPU. Thanks to our time surface formulation and this learning approach, our method is significantly more robust to abrupt changes of direction of the corners compared to previous ones. Our method also naturally assigns a confidence score for the corners, which can be useful for postprocessing. Moreover, we introduce a high-resolution dataset suitable for quantitative evaluation and comparison of corner detection methods for event-based cameras. We call our approach SILC, for Speed Invariant Learned Corners, and compare it to the state-of-the-art with extensive experiments, showing better performance.



### Diversity with Cooperation: Ensemble Methods for Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.11341v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1903.11341v2)
- **Published**: 2019-03-27 10:53:22+00:00
- **Updated**: 2019-08-30 09:34:59+00:00
- **Authors**: Nikita Dvornik, Cordelia Schmid, Julien Mairal
- **Comment**: Added experiments for different network architectures across
  different input image resolutions
- **Journal**: None
- **Summary**: Few-shot classification consists of learning a predictive model that is able to effectively adapt to a new class, given only a few annotated samples. To solve this challenging problem, meta-learning has become a popular paradigm that advocates the ability to "learn to adapt". Recent works have shown, however, that simple learning strategies without meta-learning could be competitive. In this paper, we go a step further and show that by addressing the fundamental high-variance issue of few-shot learning classifiers, it is possible to significantly outperform current meta-learning techniques. Our approach consists of designing an ensemble of deep networks to leverage the variance of the classifiers, and introducing new strategies to encourage the networks to cooperate, while encouraging prediction diversity. Evaluation is conducted on the mini-ImageNet and CUB datasets, where we show that even a single network obtained by distillation yields state-of-the-art results.



### Tightness-aware Evaluation Protocol for Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.00813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00813v1)
- **Published**: 2019-03-27 11:06:17+00:00
- **Updated**: 2019-03-27 11:06:17+00:00
- **Authors**: Yuliang Liu, Lianwen Jin, Zecheng Xie, Canjie Luo, Shuaitao Zhang, Lele Xie
- **Comment**: Accepted to appear in CVPR 2019
- **Journal**: None
- **Summary**: Evaluation protocols play key role in the developmental progress of text detection methods. There are strict requirements to ensure that the evaluation methods are fair, objective and reasonable. However, existing metrics exhibit some obvious drawbacks: 1) They are not goal-oriented; 2) they cannot recognize the tightness of detection methods; 3) existing one-to-many and many-to-one solutions involve inherent loopholes and deficiencies. Therefore, this paper proposes a novel evaluation protocol called Tightness-aware Intersect-over-Union (TIoU) metric that could quantify completeness of ground truth, compactness of detection, and tightness of matching degree. Specifically, instead of merely using the IoU value, two common detection behaviors are properly considered; meanwhile, directly using the score of TIoU to recognize the tightness. In addition, we further propose a straightforward method to address the annotation granularity issue, which can fairly evaluate word and text-line detections simultaneously. By adopting the detection results from published methods and general object detection frameworks, comprehensive experiments on ICDAR 2013 and ICDAR 2015 datasets are conducted to compare recent metrics and the proposed TIoU metric. The comparison demonstrated some promising new prospects, e.g., determining the methods and frameworks for which the detection is tighter and more beneficial to recognize. Our method is extremely simple; however, the novelty is none other than the proposed metric can utilize simplest but reasonable improvements to lead to many interesting and insightful prospects and solving most the issues of the previous metrics. The code is publicly available at https://github.com/Yuliang-Liu/TIoU-metric .



### Scaling up the randomized gradient-free adversarial attack reveals overestimation of robustness using established attacks
- **Arxiv ID**: http://arxiv.org/abs/1903.11359v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.11359v2)
- **Published**: 2019-03-27 11:41:27+00:00
- **Updated**: 2019-09-25 17:04:43+00:00
- **Authors**: Francesco Croce, Jonas Rauber, Matthias Hein
- **Comment**: Accepted at International Journal of Computer Vision
- **Journal**: None
- **Summary**: Modern neural networks are highly non-robust against adversarial manipulation. A significant amount of work has been invested in techniques to compute lower bounds on robustness through formal guarantees and to build provably robust models. However, it is still difficult to get guarantees for larger networks or robustness against larger perturbations. Thus attack strategies are needed to provide tight upper bounds on the actual robustness. We significantly improve the randomized gradient-free attack for ReLU networks [9], in particular by scaling it up to large networks. We show that our attack achieves similar or significantly smaller robust accuracy than state-of-the-art attacks like PGD or the one of Carlini and Wagner, thus revealing an overestimation of the robustness by these state-of-the-art methods. Our attack is not based on a gradient descent scheme and in this sense gradient-free, which makes it less sensitive to the choice of hyperparameters as no careful selection of the stepsize is required.



### Understanding Unconventional Preprocessors in Deep Convolutional Neural Networks for Face Identification
- **Arxiv ID**: http://arxiv.org/abs/1904.00815v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00815v2)
- **Published**: 2019-03-27 13:05:55+00:00
- **Updated**: 2019-05-02 10:54:14+00:00
- **Authors**: Chollette C. Olisah, Lyndon Smith
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Deep networks have achieved huge successes in application domains like object and face recognition. The performance gain is attributed to different facets of the network architecture such as: depth of the convolutional layers, activation function, pooling, batch normalization, forward and back propagation and many more. However, very little emphasis is made on the preprocessors. Therefore, in this paper, the network's preprocessing module is varied across different preprocessing approaches while keeping constant other facets of the network architecture, to investigate the contribution preprocessing makes to the network. Commonly used preprocessors are the data augmentation and normalization and are termed conventional preprocessors. Others are termed the unconventional preprocessors, they are: color space converters; HSV, CIE L*a*b* and YCBCR, grey-level resolution preprocessors; full-based and plane-based image quantization, illumination normalization and insensitive feature preprocessing using: histogram equalization (HE), local contrast normalization (LN) and complete face structural pattern (CFSP). To achieve fixed network parameters, CNNs with transfer learning is employed. Knowledge from the high-level feature vectors of the Inception-V3 network is transferred to offline preprocessed LFW target data; and features trained using the SoftMax classifier for face identification. The experiments show that the discriminative capability of the deep networks can be improved by preprocessing RGB data with HE, full-based and plane-based quantization, rgbGELog, and YCBCR, preprocessors before feeding it to CNNs. However, for best performance, the right setup of preprocessed data with augmentation and/or normalization is required. The plane-based image quantization is found to increase the homogeneity of neighborhood pixels and utilizes reduced bit depth for better storage efficiency.



### Self-Supervised Learning via Conditional Motion Propagation
- **Arxiv ID**: http://arxiv.org/abs/1903.11412v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.11412v3)
- **Published**: 2019-03-27 13:24:46+00:00
- **Updated**: 2019-04-25 03:52:57+00:00
- **Authors**: Xiaohang Zhan, Xingang Pan, Ziwei Liu, Dahua Lin, Chen Change Loy
- **Comment**: In CVPR 2019. More details at the project page:
  http://mmlab.ie.cuhk.edu.hk/projects/CMP/
- **Journal**: None
- **Summary**: Intelligent agent naturally learns from motion. Various self-supervised algorithms have leveraged motion cues to learn effective visual representations. The hurdle here is that motion is both ambiguous and complex, rendering previous works either suffer from degraded learning efficacy, or resort to strong assumptions on object motions. In this work, we design a new learning-from-motion paradigm to bridge these gaps. Instead of explicitly modeling the motion probabilities, we design the pretext task as a conditional motion propagation problem. Given an input image and several sparse flow guidance vectors on it, our framework seeks to recover the full-image motion. Compared to other alternatives, our framework has several appealing properties: (1) Using sparse flow guidance during training resolves the inherent motion ambiguity, and thus easing feature learning. (2) Solving the pretext task of conditional motion propagation encourages the emergence of kinematically-sound representations that poss greater expressive power. Extensive experiments demonstrate that our framework learns structural and coherent features; and achieves state-of-the-art self-supervision performance on several downstream tasks including semantic segmentation, instance segmentation, and human parsing. Furthermore, our framework is successfully extended to several useful applications such as semi-automatic pixel-level annotation. Project page: "http://mmlab.ie.cuhk.edu.hk/projects/CMP/".



### Social Behavioral Phenotyping of Drosophila with a2D-3D Hybrid CNN Framework
- **Arxiv ID**: http://arxiv.org/abs/1903.11421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.ET, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1903.11421v1)
- **Published**: 2019-03-27 13:41:17+00:00
- **Updated**: 2019-03-27 13:41:17+00:00
- **Authors**: Ziping Jiang, Paul L. Chazot, M. Emre Celebi, Danny Crookes, Richard Jiang
- **Comment**: None
- **Journal**: IEEE Access 2019
- **Summary**: Behavioural phenotyping of Drosophila is an important means in biological and medical research to identify genetic, pathologic or psychologic impact on animal behaviour.



### Spontaneous Facial Micro-Expression Recognition using 3D Spatiotemporal Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.01390v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1904.01390v1)
- **Published**: 2019-03-27 13:45:49+00:00
- **Updated**: 2019-03-27 13:45:49+00:00
- **Authors**: Sai Prasanna Teja Reddy, Surya Teja Karri, Shiv Ram Dubey, Snehasis Mukherjee
- **Comment**: Accepted in 2019 International Joint Conference on Neural Networks
  (IJCNN)
- **Journal**: None
- **Summary**: Facial expression recognition in videos is an active area of research in computer vision. However, fake facial expressions are difficult to be recognized even by humans. On the other hand, facial micro-expressions generally represent the actual emotion of a person, as it is a spontaneous reaction expressed through human face. Despite of a few attempts made for recognizing micro-expressions, still the problem is far from being a solved problem, which is depicted by the poor rate of accuracy shown by the state-of-the-art methods. A few CNN based approaches are found in the literature to recognize micro-facial expressions from still images. Whereas, a spontaneous micro-expression video contains multiple frames that have to be processed together to encode both spatial and temporal information. This paper proposes two 3D-CNN methods: MicroExpSTCNN and MicroExpFuseNet, for spontaneous facial micro-expression recognition by exploiting the spatiotemporal information in CNN framework. The MicroExpSTCNN considers the full spatial information, whereas the MicroExpFuseNet is based on the 3D-CNN feature fusion of the eyes and mouth regions. The experiments are performed over CAS(ME)^2 and SMIC micro-expression databases. The proposed MicroExpSTCNN model outperforms the state-of-the-art methods.



### Accurate Monocular Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1903.11444v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11444v4)
- **Published**: 2019-03-27 14:23:44+00:00
- **Updated**: 2021-03-30 09:14:19+00:00
- **Authors**: Xinzhu Ma, Zhihui Wang, Haojie Li, Pengbo Zhang, Xin Fan, Wanli Ouyang
- **Comment**: To appear in ICCV'19
- **Journal**: None
- **Summary**: In this paper, we propose a monocular 3D object detection framework in the domain of autonomous driving. Unlike previous image-based methods which focus on RGB feature extracted from 2D images, our method solves this problem in the reconstructed 3D space in order to exploit 3D contexts explicitly. To this end, we first leverage a stand-alone module to transform the input data from 2D image plane to 3D point clouds space for a better input representation, then we perform the 3D detection using PointNet backbone net to obtain objects 3D locations, dimensions and orientations. To enhance the discriminative capability of point clouds, we propose a multi-modal feature fusion module to embed the complementary RGB cue into the generated point clouds representation. We argue that it is more effective to infer the 3D bounding boxes from the generated 3D scene space (i.e., X,Y, Z space) compared to the image plane (i.e., R,G,B image plane). Evaluation on the challenging KITTI dataset shows that our approach boosts the performance of state-of-the-art monocular approach by a large margin.



### k-Same-Siamese-GAN: k-Same Algorithm with Generative Adversarial Network for Facial Image De-identification with Hyperparameter Tuning and Mixed Precision Training
- **Arxiv ID**: http://arxiv.org/abs/1904.00816v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00816v2)
- **Published**: 2019-03-27 14:27:07+00:00
- **Updated**: 2019-09-17 05:24:19+00:00
- **Authors**: Yi-Lun Pan, Min-Jhih Huang, Kuo-Teng Ding, Ja-Ling Wu, Jyh-Shing Jang
- **Comment**: None
- **Journal**: None
- **Summary**: For a data holder, such as a hospital or a government entity, who has a privately held collection of personal data, in which the revealing and/or processing of the personal identifiable data is restricted and prohibited by law. Then, "how can we ensure the data holder does conceal the identity of each individual in the imagery of personal data while still preserving certain useful aspects of the data after de-identification?" becomes a challenge issue. In this work, we propose an approach towards high-resolution facial image de-identification, called k-Same-Siamese-GAN, which leverages the k-Same-Anonymity mechanism, the Generative Adversarial Network, and the hyperparameter tuning methods. Moreover, to speed up model training and reduce memory consumption, the mixed precision training technique is also applied to make kSS-GAN provide guarantees regarding privacy protection on close-form identities and be trained much more efficiently as well. Finally, to validate its applicability, the proposed work has been applied to actual datasets - RafD and CelebA for performance testing. Besides protecting privacy of high-resolution facial images, the proposed system is also justified for its ability in automating parameter tuning and breaking through the limitation of the number of adjustable parameters.



### Learning 2D to 3D Lifting for Object Detection in 3D for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1904.08494v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08494v2)
- **Published**: 2019-03-27 14:59:40+00:00
- **Updated**: 2019-10-11 07:02:34+00:00
- **Authors**: Siddharth Srivastava, Frederic Jurie, Gaurav Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of 3D object detection from 2D monocular images in autonomous driving scenarios. We propose to lift the 2D images to 3D representations using learned neural networks and leverage existing networks working directly on 3D data to perform 3D object detection and localization. We show that, with carefully designed training mechanism and automatically selected minimally noisy data, such a method is not only feasible, but gives higher results than many methods working on actual 3D inputs acquired from physical sensors. On the challenging KITTI benchmark, we show that our 2D to 3D lifted method outperforms many recent competitive 3D networks while significantly outperforming previous state-of-the-art for 3D detection from monocular images. We also show that a late fusion of the output of the network trained on generated 3D images, with that trained on real 3D images, improves performance. We find the results very interesting and argue that such a method could serve as a highly reliable backup in case of malfunction of expensive 3D sensors, if not potentially making them redundant, at least in the case of low human injury risk autonomous navigation scenarios like warehouse automation.



### DeepPoint3D: Learning Discriminative Local Descriptors using Deep Metric Learning on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1904.00817v1
- **DOI**: 10.1016/j.patrec.2019.02.027
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00817v1)
- **Published**: 2019-03-27 15:47:07+00:00
- **Updated**: 2019-03-27 15:47:07+00:00
- **Authors**: Siddharth Srivastava, Brejesh Lall
- **Comment**: None
- **Journal**: None
- **Summary**: Learning local descriptors is an important problem in computer vision. While there are many techniques for learning local patch descriptors for 2D images, recently efforts have been made for learning local descriptors for 3D points. The recent progress towards solving this problem in 3D leverages the strong feature representation capability of image based convolutional neural networks by utilizing RGB-D or multi-view representations. However, in this paper, we propose to learn 3D local descriptors by directly processing unstructured 3D point clouds without needing any intermediate representation. The method constitutes a deep network for learning permutation invariant representation of 3D points. To learn the local descriptors, we use a multi-margin contrastive loss which discriminates between similar and dissimilar points on a surface while also leveraging the extent of dissimilarity among the negative samples at the time of training. With comprehensive evaluation against strong baselines, we show that the proposed method outperforms state-of-the-art methods for matching points in 3D point clouds. Further, we demonstrate the effectiveness of the proposed method on various applications achieving state-of-the-art results.



### Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems
- **Arxiv ID**: http://arxiv.org/abs/1903.11508v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.11508v2)
- **Published**: 2019-03-27 16:01:18+00:00
- **Updated**: 2020-06-10 12:20:04+00:00
- **Authors**: Steffen Eger, G√∂zde G√ºl ≈ûahin, Andreas R√ºckl√©, Ji-Ung Lee, Claudia Schulz, Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson, Iryna Gurevych
- **Comment**: Accepted as long paper at NAACL-2019; fixed one ungrammatical
  sentence
- **Journal**: None
- **Summary**: Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., "!d10t") or as a writing style ("1337" in "leet speak"), among other scenarios. We consider this as a new type of adversarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual input perturbations demonstrate. We then investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82\%. We then explore three shielding methods---visual character embeddings, adversarial training, and rule-based recovery---which substantially improve the robustness of the models. However, the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks.



### Privacy Protection in Street-View Panoramas using Depth and Multi-View Imagery
- **Arxiv ID**: http://arxiv.org/abs/1903.11532v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11532v1)
- **Published**: 2019-03-27 16:34:16+00:00
- **Updated**: 2019-03-27 16:34:16+00:00
- **Authors**: Ries Uittenbogaard, Clint Sebastian, Julien Vijverberg, Bas Boom, Dariu M. Gavrila, Peter H. N. de With
- **Comment**: Accepted to CVPR 2019. Dataset (and provided link) will be made
  available before the CVPR
- **Journal**: None
- **Summary**: The current paradigm in privacy protection in street-view images is to detect and blur sensitive information. In this paper, we propose a framework that is an alternative to blurring, which automatically removes and inpaints moving objects (e.g. pedestrians, vehicles) in street-view imagery. We propose a novel moving object segmentation algorithm exploiting consistencies in depth across multiple street-view images that are later combined with the results of a segmentation network. The detected moving objects are removed and inpainted with information from other views, to obtain a realistic output image such that the moving object is not visible anymore. We evaluate our results on a dataset of 1000 images to obtain a peak noise-to-signal ratio (PSNR) and L1 loss of 27.2 dB and 2.5%, respectively. To ensure the subjective quality, To assess overall quality, we also report the results of a survey conducted on 35 professionals, asked to visually inspect the images whether object removal and inpainting had taken place. The inpainting dataset will be made publicly available for scientific benchmarking purposes at https://research.cyclomedia.com



### GAN-based Pose-aware Regulation for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1903.11552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11552v1)
- **Published**: 2019-03-27 17:14:51+00:00
- **Updated**: 2019-03-27 17:14:51+00:00
- **Authors**: Alessandro Borgia, Yang Hua, Elyor Kodirov, Neil M. Robertson
- **Comment**: None
- **Journal**: None
- **Summary**: Video-based person re-identification deals with the inherent difficulty of matching unregulated sequences with different length and with incomplete target pose/viewpoint structure. Common approaches operate either by reducing the problem to the still images case, facing a significant information loss, or by exploiting inter-sequence temporal dependencies as in Siamese Recurrent Neural Networks or in gait analysis. However, in all cases, the inter-sequences pose/viewpoint misalignment is not considered, and the existing spatial approaches are mostly limited to the still images context. To this end, we propose a novel approach that can exploit more effectively the rich video information, by accounting for the role that the changing pose/viewpoint factor plays in the sequences matching process. Specifically, our approach consists of two components. The first one attempts to complement the original pose-incomplete information carried by the sequences with synthetic GAN-generated images, and fuse their feature vectors into a more discriminative viewpoint-insensitive embedding, namely Weighted Fusion (WF). Another one performs an explicit pose-based alignment of sequence pairs to promote coherent feature matching, namely Weighted-Pose Regulation (WPR). Extensive experiments on two large video-based benchmark datasets show that our approach outperforms considerably existing methods.



### Laplace Landmark Localization
- **Arxiv ID**: http://arxiv.org/abs/1903.11633v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11633v2)
- **Published**: 2019-03-27 18:14:50+00:00
- **Updated**: 2019-08-14 20:16:35+00:00
- **Authors**: Joseph P Robinson, Yuncheng Li, Ning Zhang, Yun Fu, and Sergey Tulyakov
- **Comment**: International Conference on Computer Vision (ICCV), 2019
- **Journal**: None
- **Summary**: Landmark localization in images and videos is a classic problem solved in various ways. Nowadays, with deep networks prevailing throughout machine learning, there are revamped interests in pushing facial landmark detection technologies to handle more challenging data. Most efforts use network objectives based on L1 or L2 norms, which have several disadvantages. First of all, the locations of landmarks are determined from generated heatmaps (i.e., confidence maps) from which predicted landmark locations (i.e., the means) get penalized without accounting for the spread: a high scatter corresponds to low confidence and vice-versa. For this, we introduce a LaplaceKL objective that penalizes for a low confidence. Another issue is a dependency on labeled data, which are expensive to obtain and susceptible to error. To address both issues we propose an adversarial training framework that leverages unlabeled data to improve model performance. Our method claims state-of-the-art on all of the 300W benchmarks and ranks second-to-best on the Annotated Facial Landmarks in the Wild (AFLW) dataset. Furthermore, our model is robust with a reduced size: 1/8 the number of channels (i.e., 0.0398MB) is comparable to state-of-that-art in real-time on CPU. Thus, we show that our method is of high practical value to real-life application.



### Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment
- **Arxiv ID**: http://arxiv.org/abs/1903.11649v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11649v2)
- **Published**: 2019-03-27 18:55:32+00:00
- **Updated**: 2019-10-15 17:09:03+00:00
- **Authors**: Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja, Devi Parikh, Ajay Divakaran
- **Comment**: v2 contains phrase localization results on Flickr30k Entities.
  Accepted for publication at ICCV 2019
- **Journal**: None
- **Summary**: We address the problem of grounding free-form textual phrases by using weak supervision from image-caption pairs. We propose a novel end-to-end model that uses caption-to-image retrieval as a `downstream' task to guide the process of phrase localization. Our method, as a first step, infers the latent correspondences between regions-of-interest (RoIs) and phrases in the caption and creates a discriminative image representation using these matched RoIs. In a subsequent step, this (learned) representation is aligned with the caption. Our key contribution lies in building this `caption-conditioned' image encoding which tightly couples both the tasks and allows the weak supervision to effectively guide visual grounding. We provide an extensive empirical and qualitative analysis to investigate the different components of our proposed model and compare it with competitive baselines. For phrase localization, we report an improvement of 4.9% (absolute) over the prior state-of-the-art on the VisualGenome dataset. We also report results that are at par with the state-of-the-art on the downstream caption-to-image retrieval task on COCO and Flickr30k datasets.



### Outlier-Robust Spatial Perception: Hardness, General-Purpose Algorithms, and Guarantees
- **Arxiv ID**: http://arxiv.org/abs/1903.11683v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.RO, cs.SY, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1903.11683v2)
- **Published**: 2019-03-27 20:12:37+00:00
- **Updated**: 2019-07-29 19:46:50+00:00
- **Authors**: Vasileios Tzoumas, Pasquale Antonante, Luca Carlone
- **Comment**: None
- **Journal**: None
- **Summary**: Spatial perception is the backbone of many robotics applications, and spans a broad range of research problems, including localization and mapping, point cloud alignment, and relative pose estimation from camera images. Robust spatial perception is jeopardized by the presence of incorrect data association, and in general, outliers. Although techniques to handle outliers do exist, they can fail in unpredictable manners (e.g., RANSAC, robust estimators), or can have exponential runtime (e.g., branch-and-bound). In this paper, we advance the state of the art in outlier rejection by making three contributions. First, we show that even a simple linear instance of outlier rejection is inapproximable: in the worst-case one cannot design a quasi-polynomial time algorithm that computes an approximate solution efficiently. Our second contribution is to provide the first per-instance sub-optimality bounds to assess the approximation quality of a given outlier rejection outcome. Our third contribution is to propose a simple general-purpose algorithm, named adaptive trimming, to remove outliers. Our algorithm leverages recently-proposed global solvers that are able to solve outlier-free problems, and iteratively removes measurements with large errors. We demonstrate the proposed algorithm on three spatial perception problems: 3D registration, two-view geometry, and SLAM. The results show that our algorithm outperforms several state-of-the-art methods across applications while being a general-purpose method.



### Zero-shot Image Recognition Using Relational Matching, Adaptation and Calibration
- **Arxiv ID**: http://arxiv.org/abs/1903.11701v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.11701v1)
- **Published**: 2019-03-27 21:07:00+00:00
- **Updated**: 2019-03-27 21:07:00+00:00
- **Authors**: Debasmit Das, C. S. George Lee
- **Comment**: International Joint Conference on Neural Networks (IJCNN) 2019.
  Copyright 2019 IEEE
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) for image classification focuses on recognizing novel categories that have no labeled data available for training. The learning is generally carried out with the help of mid-level semantic descriptors associated with each class. This semantic-descriptor space is generally shared by both seen and unseen categories. However, ZSL suffers from hubness, domain discrepancy and biased-ness towards seen classes. To tackle these problems, we propose a three-step approach to zero-shot learning. Firstly, a mapping is learned from the semantic-descriptor space to the image-feature space. This mapping learns to minimize both one-to-one and pairwise distances between semantic embeddings and the image features of the corresponding classes. Secondly, we propose test-time domain adaptation to adapt the semantic embedding of the unseen classes to the test data. This is achieved by finding correspondences between the semantic descriptors and the image features. Thirdly, we propose scaled calibration on the classification scores of the seen classes. This is necessary because the ZSL model is biased towards seen classes as the unseen classes are not used in the training. Finally, to validate the proposed three-step approach, we performed experiments on four benchmark datasets where the proposed method outperformed previous results. We also studied and analyzed the performance of each component of our proposed ZSL framework.



### AutoSlim: Towards One-Shot Architecture Search for Channel Numbers
- **Arxiv ID**: http://arxiv.org/abs/1903.11728v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1903.11728v2)
- **Published**: 2019-03-27 23:17:28+00:00
- **Updated**: 2019-06-01 03:19:54+00:00
- **Authors**: Jiahui Yu, Thomas Huang
- **Comment**: tech report
- **Journal**: None
- **Summary**: We study how to set channel numbers in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot solution, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods.   Notably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs). Code and models will be available at: https://github.com/JiahuiYu/slimmable_networks




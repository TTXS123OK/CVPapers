# Arxiv Papers in cs.CV on 2019-03-01
### Video Summarization via Actionness Ranking
- **Arxiv ID**: http://arxiv.org/abs/1903.00110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00110v1)
- **Published**: 2019-03-01 00:03:14+00:00
- **Updated**: 2019-03-01 00:03:14+00:00
- **Authors**: Mohamed Elfeki, Ali Borji
- **Comment**: None
- **Journal**: Published in WACV-2019 as an Oral
- **Summary**: To automatically produce a brief yet expressive summary of a long video, an automatic algorithm should start by resembling the human process of summary generation. Prior work proposed supervised and unsupervised algorithms to train models for learning the underlying behavior of humans by increasing modeling complexity or craft-designing better heuristics to simulate human summary generation process. In this work, we take a different approach by analyzing a major cue that humans exploit for the summary generation; the nature and intensity of actions.   We empirically observed that a frame is more likely to be included in human-generated summaries if it contains a substantial amount of deliberate motion performed by an agent, which is referred to as actionness. Therefore, we hypothesize that learning to automatically generate summaries involves an implicit knowledge of actionness estimation and ranking. We validate our hypothesis by running a user study that explores the correlation between human-generated summaries and actionness ranks. We also run a consensus and behavioral analysis between human subjects to ensure reliable and consistent results. The analysis exhibits a considerable degree of agreement among subjects within obtained data and verifying our initial hypothesis.   Based on the study findings, we develop a method to incorporate actionness data to explicitly regulate a learning algorithm that is trained for summary generation. We assess the performance of our approach to four summarization benchmark datasets and demonstrate an evident advantage compared to state-of-the-art summarization methods.



### Self-supervised Learning for Single View Depth and Surface Normal Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.00112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00112v1)
- **Published**: 2019-03-01 00:07:12+00:00
- **Updated**: 2019-03-01 00:07:12+00:00
- **Authors**: Huangying Zhan, Chamara Saroj Weerasekera, Ravi Garg, Ian Reid
- **Comment**: 6 pages, 3 figures, ICRA 2019
- **Journal**: None
- **Summary**: In this work we present a self-supervised learning framework to simultaneously train two Convolutional Neural Networks (CNNs) to predict depth and surface normals from a single image. In contrast to most existing frameworks which represent outdoor scenes as fronto-parallel planes at piece-wise smooth depth, we propose to predict depth with surface orientation while assuming that natural scenes have piece-wise smooth normals. We show that a simple depth-normal consistency as a soft-constraint on the predictions is sufficient and effective for training both these networks simultaneously. The trained normal network provides state-of-the-art predictions while the depth network, relying on much realistic smooth normal assumption, outperforms the traditional self-supervised depth prediction network by a large margin on the KITTI benchmark. Demo video: https://youtu.be/ZD-ZRsw7hdM



### A Sketch Based 3D Shape Retrieval Approach Based on Efficient Deep Point-to-Subspace Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.00117v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00117v2)
- **Published**: 2019-03-01 01:13:30+00:00
- **Updated**: 2019-03-13 14:07:45+00:00
- **Authors**: Yinjie Lei, Ziqin Zhou, Pingping Zhang, Yulan Guo, Zijun Ma, Lingqiao Liu
- **Comment**: The first author wants to withdraw this paper. He has noticed several
  setting errors in experiment parts
- **Journal**: None
- **Summary**: A sketch based 3D shape retrieval



### Local Geometric Indexing of High Resolution Data for Facial Reconstruction from Sparse Markers
- **Arxiv ID**: http://arxiv.org/abs/1903.00119v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00119v2)
- **Published**: 2019-03-01 01:37:46+00:00
- **Updated**: 2019-09-04 23:38:29+00:00
- **Authors**: Matthew Cong, Lana Lan, Ronald Fedkiw
- **Comment**: 8 pages. Includes figures which were previously redacted. Added
  acknowledgements section and minor changes to text
- **Journal**: None
- **Summary**: When considering sparse motion capture marker data, one typically struggles to balance its overfitting via a high dimensional blendshape system versus underfitting caused by smoothness constraints. With the current trend towards using more and more data, our aim is not to fit the motion capture markers with a parameterized (blendshape) model or to smoothly interpolate a surface through the marker positions, but rather to find an instance in the high resolution dataset that contains local geometry to fit each marker. Just as is true for typical machine learning applications, this approach benefits from a plethora of data, and thus we also consider augmenting the dataset via specially designed physical simulations that target the high resolution dataset such that the simulation output lies on the same so-called manifold as the data targeted.



### Video Extrapolation with an Invertible Linear Embedding
- **Arxiv ID**: http://arxiv.org/abs/1903.00133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00133v1)
- **Published**: 2019-03-01 02:52:51+00:00
- **Updated**: 2019-03-01 02:52:51+00:00
- **Authors**: Robert Pottorff, Jared Nielsen, David Wingate
- **Comment**: None
- **Journal**: None
- **Summary**: We predict future video frames from complex dynamic scenes, using an invertible neural network as the encoder of a nonlinear dynamic system with latent linear state evolution. Our invertible linear embedding (ILE) demonstrates successful learning, prediction and latent state inference. In contrast to other approaches, ILE does not use any explicit reconstruction loss or simplistic pixel-space assumptions. Instead, it leverages invertibility to optimize the likelihood of image sequences exactly, albeit indirectly. Comparison with a state-of-the-art method demonstrates the viability of our approach.



### A Unified Neural Architecture for Instrumental Audio Tasks
- **Arxiv ID**: http://arxiv.org/abs/1903.00142v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.00142v1)
- **Published**: 2019-03-01 03:28:54+00:00
- **Updated**: 2019-03-01 03:28:54+00:00
- **Authors**: Steven Spratley, Daniel Beck, Trevor Cohn
- **Comment**: To appear in Proc. ICASSP 2019, May 12-17, Brighton, UK
- **Journal**: None
- **Summary**: Within Music Information Retrieval (MIR), prominent tasks -- including pitch-tracking, source-separation, super-resolution, and synthesis -- typically call for specialised methods, despite their similarities. Conditional Generative Adversarial Networks (cGANs) have been shown to be highly versatile in learning general image-to-image translations, but have not yet been adapted across MIR. In this work, we present an end-to-end supervisable architecture to perform all aforementioned audio tasks, consisting of a WaveNet synthesiser conditioned on the output of a jointly-trained cGAN spectrogram translator. In doing so, we demonstrate the potential of such flexible techniques to unify MIR tasks, promote efficient transfer learning, and converge research to the improvement of powerful, general methods. Finally, to the best of our knowledge, we present the first application of GANs to guided instrument synthesis.



### Image-Based Geo-Localization Using Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1903.00159v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00159v3)
- **Published**: 2019-03-01 05:20:10+00:00
- **Updated**: 2019-06-02 02:54:20+00:00
- **Authors**: Sixing Hu, Gim Hee Lee
- **Comment**: IJCV preprint
- **Journal**: None
- **Summary**: The problem of localization on a geo-referenced satellite map given a query ground view image is useful yet remains challenging due to the drastic change in viewpoint. To this end, in this paper we work on the extension of our earlier work on the Cross-View Matching Network (CVM-Net) for the ground-to-aerial image matching task since the traditional image descriptors fail due to the drastic viewpoint change. In particular, we show more extensive experimental results and analyses of the network architecture on our CVM-Net. Furthermore, we propose a Markov localization framework that enforces the temporal consistency between image frames to enhance the geo-localization results in the case where a video stream of ground view images is available. Experimental results show that our proposed Markov localization framework can continuously localize the vehicle within a small error on our Singapore dataset.



### Pyramid Feature Attention Network for Saliency detection
- **Arxiv ID**: http://arxiv.org/abs/1903.00179v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00179v2)
- **Published**: 2019-03-01 06:58:09+00:00
- **Updated**: 2019-04-04 13:25:55+00:00
- **Authors**: Ting Zhao, Xiangqian Wu
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: Saliency detection is one of the basic challenges in computer vision. How to extract effective features is a critical point for saliency detection. Recent methods mainly adopt integrating multi-scale convolutional features indiscriminately. However, not all features are useful for saliency detection and some even cause interferences. To solve this problem, we propose Pyramid Feature Attention network to focus on effective high-level context features and low-level spatial structural features. First, we design Context-aware Pyramid Feature Extraction (CPFE) module for multi-scale high-level feature maps to capture rich context features. Second, we adopt channel-wise attention (CA) after CPFE feature maps and spatial attention (SA) after low-level feature maps, then fuse outputs of CA & SA together. Finally, we propose an edge preservation loss to guide network to learn more detailed information in boundary localization. Extensive evaluations on five benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art approaches under different evaluation metrics.



### Lung CT Imaging Sign Classification through Deep Learning on Small Data
- **Arxiv ID**: http://arxiv.org/abs/1903.00183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00183v1)
- **Published**: 2019-03-01 07:34:35+00:00
- **Updated**: 2019-03-01 07:34:35+00:00
- **Authors**: Guocai He
- **Comment**: None
- **Journal**: None
- **Summary**: The annotated medical images are usually expensive to be collected. This paper proposes a deep learning method on small data to classify Common Imaging Signs of Lung diseases (CISL) in computed tomography (CT) images. We explore both the real data and the data generated by Generative Adversarial Network (GAN) to improve the reliability and the generalization of learning. First, we use GAN to generate a large number of CISLs from small annotated data, which are difficult to be distinguished from real counterparts. These generated samples are used to pre-train a Convolutional Neural Network (CNN) for classifying CISLs. Second, we fine-tune the CNN classification model with real data. Experiments were conducted on the LISS database of CISLs. We successfully convinced radiologists that our generated CISLs samples were real for 56.7% of our experiments. The pre-trained CNN model achieves 88.4% of mean accuracy of binary classification, and after fine-tuning, the mean accuracy is significantly increased to 95.0%. For multi-classification of all types of CISLs and normal tissues, through the two stages of training, the mean accuracy, sensitivity and specificity are up to about 91.83%, 92.73% and 99.0%, respectively. To our knowledge, this is the best result achieved on the LISS database, which demonstrates that the proposed method is effective and promising for fulfilling deep learning on small data.



### Single Image Deblurring and Camera Motion Estimation with Depth Map
- **Arxiv ID**: http://arxiv.org/abs/1903.00231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00231v1)
- **Published**: 2019-03-01 10:08:08+00:00
- **Updated**: 2019-03-01 10:08:08+00:00
- **Authors**: Liyuan Pan, Yuchao Dai, Miaomiao Liu
- **Comment**: Accepted by WACV 2019
- **Journal**: None
- **Summary**: Camera shake during exposure is a major problem in hand-held photography, as it causes image blur that destroys details in the captured images.~In the real world, such blur is mainly caused by both the camera motion and the complex scene structure.~While considerable existing approaches have been proposed based on various assumptions regarding the scene structure or the camera motion, few existing methods could handle the real 6 DoF camera motion.~In this paper, we propose to jointly estimate the 6 DoF camera motion and remove the non-uniform blur caused by camera motion by exploiting their underlying geometric relationships, with a single blurry image and its depth map (either direct depth measurements, or a learned depth map) as input.~We formulate our joint deblurring and 6 DoF camera motion estimation as an energy minimization problem which is solved in an alternative manner. Our model enables the recovery of the 6 DoF camera motion and the latent clean image, which could also achieve the goal of generating a sharp sequence from a single blurry image. Experiments on challenging real-world and synthetic datasets demonstrate that image blur from camera shake can be well addressed within our proposed framework.



### Mask Scoring R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1903.00241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00241v1)
- **Published**: 2019-03-01 10:38:57+00:00
- **Updated**: 2019-03-01 10:38:57+00:00
- **Authors**: Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, Xinggang Wang
- **Comment**: Accepted in CVPR 2019
- **Journal**: None
- **Summary**: Letting a deep network be aware of the quality of its own predictions is an interesting yet important problem. In the task of instance segmentation, the confidence of instance classification is used as mask quality score in most instance segmentation frameworks. However, the mask quality, quantified as the IoU between the instance mask and its ground truth, is usually not well correlated with classification score. In this paper, we study this problem and propose Mask Scoring R-CNN which contains a network block to learn the quality of the predicted instance masks. The proposed network block takes the instance feature and the corresponding predicted mask together to regress the mask IoU. The mask scoring strategy calibrates the misalignment between mask quality and mask score, and improves instance segmentation performance by prioritizing more accurate mask predictions during COCO AP evaluation. By extensive evaluations on the COCO dataset, Mask Scoring R-CNN brings consistent and noticeable gain with different models, and outperforms the state-of-the-art Mask R-CNN. We hope our simple and effective approach will provide a new direction for improving instance segmentation. The source code of our method is available at \url{https://github.com/zjhuang22/maskscoring_rcnn}.



### Optimal Projection Guided Transfer Hashing for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1903.00252v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.00252v1)
- **Published**: 2019-03-01 11:43:31+00:00
- **Updated**: 2019-03-01 11:43:31+00:00
- **Authors**: Ji Liu, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, learning to hash has been widely studied for image retrieval thanks to the computation and storage efficiency of binary codes. For most existing learning to hash methods, sufficient training images are required and used to learn precise hashing codes. However, in some real-world applications, there are not always sufficient training images in the domain of interest. In addition, some existing supervised approaches need a amount of labeled data, which is an expensive process in term of time, label and human expertise. To handle such problems, inspired by transfer learning, we propose a simple yet effective unsupervised hashing method named Optimal Projection Guided Transfer Hashing (GTH) where we borrow the images of other different but related domain i.e., source domain to help learn precise hashing codes for the domain of interest i.e., target domain. Besides, we propose to seek for the maximum likelihood estimation (MLE) solution of the hashing functions of target and source domains due to the domain gap. Furthermore,an alternating optimization method is adopted to obtain the two projections of target and source domains such that the domain hashing disparity is reduced gradually. Extensive experiments on various benchmark databases verify that our method outperforms many state-of-the-art learning to hash methods. The implementation details are available at https://github.com/liuji93/GTH.



### A Deep DUAL-PATH Network for Improved Mammogram Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1903.00001v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.00001v1)
- **Published**: 2019-03-01 11:51:47+00:00
- **Updated**: 2019-03-01 11:51:47+00:00
- **Authors**: Heyi Li, Dongdong Chen, William H. Nailon, Mike E. Davies, Dave Laurenson
- **Comment**: To Appear in ICCASP 2019 May
- **Journal**: None
- **Summary**: We present, for the first time, a novel deep neural network architecture called \dcn with a dual-path connection between the input image and output class label for mammogram image processing. This architecture is built upon U-Net, which non-linearly maps the input data into a deep latent space. One path of the \dcnn, the locality preserving learner, is devoted to hierarchically extracting and exploiting intrinsic features of the input, while the other path, called the conditional graph learner, focuses on modeling the input-mask correlations. The learned mask is further used to improve classification results, and the two learning paths complement each other. By integrating the two learners our new architecture provides a simple but effective way to jointly learn the segmentation and predict the class label. Benefiting from the powerful expressive capacity of deep neural networks a more discriminative representation can be learned, in which both the semantics and structure are well preserved. Experimental results show that \dcn achieves the best mammography segmentation and classification simultaneously, outperforming recent state-of-the-art models.



### Crowding in humans is unlike that in convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1903.00258v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00258v2)
- **Published**: 2019-03-01 12:03:19+00:00
- **Updated**: 2019-11-25 12:43:09+00:00
- **Authors**: Ben Lonnqvist, Alasdair D. F. Clarke, Ramakrishna Chakravarthi
- **Comment**: 34 pages, 30 figures
- **Journal**: None
- **Summary**: Object recognition is a primary function of the human visual system. It has recently been claimed that the highly successful ability to recognise objects in a set of emergent computer vision systems---Deep Convolutional Neural Networks (DCNNs)---can form a useful guide to recognition in humans. To test this assertion, we systematically evaluated visual crowding, a dramatic breakdown of recognition in clutter, in DCNNs and compared their performance to extant research in humans. We examined crowding in three architectures of DCNNs with the same methodology as that used among humans. We manipulated multiple stimulus factors including inter-letter spacing, letter colour, size, and flanker location to assess the extent and shape of crowding in DCNNs. We found that crowding followed a predictable pattern across architectures that was different from that in humans. Some characteristic hallmarks of human crowding, such as invariance to size, the effect of target-flanker similarity, and confusions between target and flanker identities, were completely missing, minimised or even reversed. These data show that DCNNs, while proficient in object recognition, likely achieve this competence through a set of mechanisms that are distinct from those in humans. They are not necessarily equivalent models of human or primate object recognition and caution must be exercised when inferring mechanisms derived from their operation.



### Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery
- **Arxiv ID**: http://arxiv.org/abs/1903.00268v2
- **DOI**: 10.1109/LRA.2019.2923960
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.00268v2)
- **Published**: 2019-03-01 12:32:46+00:00
- **Updated**: 2019-07-10 14:39:33+00:00
- **Authors**: Margarita Grinvald, Fadri Furrer, Tonci Novkovic, Jen Jen Chung, Cesar Cadena, Roland Siegwart, Juan Nieto
- **Comment**: 8 pages, 4 figures. To be published in IEEE Robotics and Automation
  Letters (RA-L) and 2019 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS). Accompanying video material can be found at
  http://youtu.be/Jvl42VJmYxg
- **Journal**: IEEE Robotics and Automation Letters, vol. 4, no. 3, pp.
  3037-3044, July 2019
- **Summary**: To autonomously navigate and plan interactions in real-world environments, robots require the ability to robustly perceive and map complex, unstructured surrounding scenes. Besides building an internal representation of the observed scene geometry, the key insight toward a truly functional understanding of the environment is the usage of higher-level entities during mapping, such as individual object instances. We propose an approach to incrementally build volumetric object-centric maps during online scanning with a localized RGB-D camera. First, a per-frame segmentation scheme combines an unsupervised geometric approach with instance-aware semantic object predictions. This allows us to detect and segment elements both from the set of known classes and from other, previously unseen categories. Next, a data association step tracks the predicted instances across the different frames. Finally, a map integration strategy fuses information about their 3D shape, location, and, if available, semantic class into a global volume. Evaluation on a publicly available dataset shows that the proposed approach for building instance-level semantic maps is competitive with state-of-the-art methods, while additionally able to discover objects of unseen categories. The system is further evaluated within a real-world robotic mapping setup, for which qualitative results highlight the online nature of the method.



### Frequency Domain Transformer Networks for Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1903.00271v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.00271v1)
- **Published**: 2019-03-01 12:50:15+00:00
- **Updated**: 2019-03-01 12:50:15+00:00
- **Authors**: Hafez Farazi, Sven Behnke
- **Comment**: Accepted for European Symposium on Artificial Neural Networks,
  Computational Intelligence and Machine Learning (ESANN), Bruges, Belgium, to
  appear April 2019
- **Journal**: None
- **Summary**: The task of video prediction is forecasting the next frames given some previous frames. Despite much recent progress, this task is still challenging mainly due to high nonlinearity in the spatial domain. To address this issue, we propose a novel architecture, Frequency Domain Transformer Network (FDTN), which is an end-to-end learnable model that estimates and uses the transformations of the signal in the frequency domain. Experimental evaluations show that this approach can outperform some widely used video prediction methods like Video Ladder Network (VLN) and Predictive Gated Pyramids (PGP).



### Adversarial Generation of Handwritten Text Images Conditioned on Sequences
- **Arxiv ID**: http://arxiv.org/abs/1903.00277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00277v1)
- **Published**: 2019-03-01 13:11:44+00:00
- **Updated**: 2019-03-01 13:11:44+00:00
- **Authors**: Eloi Alonso, Bastien Moysset, Ronaldo Messina
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art offline handwriting text recognition systems tend to use neural networks and therefore require a large amount of annotated data to be trained. In order to partially satisfy this requirement, we propose a system based on Generative Adversarial Networks (GAN) to produce synthetic images of handwritten words. We use bidirectional LSTM recurrent layers to get an embedding of the word to be rendered, and we feed it to the generator network. We also modify the standard GAN by adding an auxiliary network for text recognition. The system is then trained with a balanced combination of an adversarial loss and a CTC loss. Together, these extensions to GAN enable to control the textual content of the generated word images. We obtain realistic images on both French and Arabic datasets, and we show that integrating these synthetic images into the existing training data of a text recognition system can slightly enhance its performance.



### Provably scale-covariant networks from oriented quasi quadrature measures in cascade
- **Arxiv ID**: http://arxiv.org/abs/1903.00289v2
- **DOI**: 10.1007/978-3-030-22368-7_26
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00289v2)
- **Published**: 2019-03-01 13:33:22+00:00
- **Updated**: 2019-06-27 12:42:26+00:00
- **Authors**: Tony Lindeberg
- **Comment**: 12 pages, 3 figures, 1 table
- **Journal**: In: Proc. SSVM 2019: Scale Space and Variational Methods in
  Computer Vision, Springer LNCS volume 11603, pages 328-240
- **Summary**: This article presents a continuous model for hierarchical networks based on a combination of mathematically derived models of receptive fields and biologically inspired computations. Based on a functional model of complex cells in terms of an oriented quasi quadrature combination of first- and second-order directional Gaussian derivatives, we couple such primitive computations in cascade over combinatorial expansions over image orientations. Scale-space properties of the computational primitives are analysed and it is shown that the resulting representation allows for provable scale and rotation covariance. A prototype application to texture analysis is developed and it is demonstrated that a simplified mean-reduced representation of the resulting QuasiQuadNet leads to promising experimental results on three texture datasets.



### Progress Regression RNN for Online Spatial-Temporal Action Localization in Unconstrained Videos
- **Arxiv ID**: http://arxiv.org/abs/1903.00304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00304v1)
- **Published**: 2019-03-01 14:08:21+00:00
- **Updated**: 2019-03-01 14:08:21+00:00
- **Authors**: Bo Hu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Previous spatial-temporal action localization methods commonly follow the pipeline of object detection to estimate bounding boxes and labels of actions. However, the temporal relation of an action has not been fully explored. In this paper, we propose an end-to-end Progress Regression Recurrent Neural Network (PR-RNN) for online spatial-temporal action localization, which learns to infer the action by temporal progress regression. Two new action attributes, called progression and progress rate, are introduced to describe the temporal engagement and relative temporal position of an action. In our method, frame-level features are first extracted by a Fully Convolutional Network (FCN). Subsequently, detection results and action progress attributes are regressed by the Convolutional Gated Recurrent Unit (ConvGRU) based on all the observed frames instead of a single frame or a short clip. Finally, a novel online linking method is designed to connect single-frame results to spatial-temporal tubes with the help of the estimated action progress attributes. Extensive experiments demonstrate that the progress attributes improve the localization accuracy by providing more precise temporal position of an action in unconstrained videos. Our proposed PR-RNN achieves the stateof-the-art performance for most of the IoU thresholds on two benchmark datasets.



### TamperNN: Efficient Tampering Detection of Deployed Neural Nets
- **Arxiv ID**: http://arxiv.org/abs/1903.00317v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.00317v2)
- **Published**: 2019-03-01 14:32:45+00:00
- **Updated**: 2019-09-05 11:42:19+00:00
- **Authors**: Erwan Le Merrer, Gilles Tredan
- **Comment**: In the 30th International Symposium on Software Reliability
  Engineering (ISSRE 2019)
- **Journal**: None
- **Summary**: Neural networks are powering the deployment of embedded devices and Internet of Things. Applications range from personal assistants to critical ones such as self-driving cars. It has been shown recently that models obtained from neural nets can be trojaned ; an attacker can then trigger an arbitrary model behavior facing crafted inputs. This has a critical impact on the security and reliability of those deployed devices. We introduce novel algorithms to detect the tampering with deployed models, classifiers in particular. In the remote interaction setup we consider, the proposed strategy is to identify markers of the model input space that are likely to change class if the model is attacked, allowing a user to detect a possible tampering. This setup makes our proposal compatible with a wide range of scenarios, such as embedded models, or models exposed through prediction APIs. We experiment those tampering detection algorithms on the canonical MNIST dataset, over three different types of neural nets, and facing five different attacks (trojaning, quantization, fine-tuning, compression and watermarking). We then validate over five large models (VGG16, VGG19, ResNet, MobileNet, DenseNet) with a state of the art dataset (VGGFace2), and report results demonstrating the possibility of an efficient detection of model tampering.



### Answer Them All! Toward Universal Visual Question Answering Models
- **Arxiv ID**: http://arxiv.org/abs/1903.00366v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00366v2)
- **Published**: 2019-03-01 15:25:24+00:00
- **Updated**: 2019-04-05 17:25:36+00:00
- **Authors**: Robik Shrestha, Kushal Kafle, Christopher Kanan
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) research is split into two camps: the first focuses on VQA datasets that require natural image understanding and the second focuses on synthetic datasets that test reasoning. A good VQA algorithm should be capable of both, but only a few VQA algorithms are tested in this manner. We compare five state-of-the-art VQA algorithms across eight VQA datasets covering both domains. To make the comparison fair, all of the models are standardized as much as possible, e.g., they use the same visual features, answer vocabularies, etc. We find that methods do not generalize across the two domains. To address this problem, we propose a new VQA algorithm that rivals or exceeds the state-of-the-art for both domains.



### PEA265: Perceptual Assessment of Video Compression Artifacts
- **Arxiv ID**: http://arxiv.org/abs/1903.00473v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.00473v1)
- **Published**: 2019-03-01 15:25:35+00:00
- **Updated**: 2019-03-01 15:25:35+00:00
- **Authors**: Liqun Lin, Shiqi Yu, Tiesong Zhao, Zhou Wang
- **Comment**: 10 pages,15 figures,4 tables
- **Journal**: None
- **Summary**: The most widely used video encoders share a common hybrid coding framework that includes block-based motion estimation/compensation and block-based transform coding. Despite their high coding efficiency, the encoded videos often exhibit visually annoying artifacts, denoted as Perceivable Encoding Artifacts (PEAs), which significantly degrade the visual Qualityof- Experience (QoE) of end users. To monitor and improve visual QoE, it is crucial to develop subjective and objective measures that can identify and quantify various types of PEAs. In this work, we make the first attempt to build a large-scale subjectlabelled database composed of H.265/HEVC compressed videos containing various PEAs. The database, namely the PEA265 database, includes 4 types of spatial PEAs (i.e. blurring, blocking, ringing and color bleeding) and 2 types of temporal PEAs (i.e. flickering and floating). Each containing at least 60,000 image or video patches with positive and negative labels. To objectively identify these PEAs, we train Convolutional Neural Networks (CNNs) using the PEA265 database. It appears that state-of-theart ResNeXt is capable of identifying each type of PEAs with high accuracy. Furthermore, we define PEA pattern and PEA intensity measures to quantify PEA levels of compressed video sequence. We believe that the PEA265 database and our findings will benefit the future development of video quality assessment methods and perceptually motivated video encoders.



### Automatic microscopic cell counting by use of unsupervised adversarial domain adaptation and supervised density regression
- **Arxiv ID**: http://arxiv.org/abs/1903.00388v2
- **DOI**: 10.1117/12.2513058
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00388v2)
- **Published**: 2019-03-01 16:15:56+00:00
- **Updated**: 2019-03-22 15:23:09+00:00
- **Authors**: Shenghua He, Kyaw Thu Minn, Lilianna Solnica-Krezel, Hua Li, Mark Anastasio
- **Comment**: SPIE Medical imaging 2019 oral presentation
- **Journal**: None
- **Summary**: Accurate cell counting in microscopic images is important for medical diagnoses and biological studies. However, manual cell counting is very time-consuming, tedious, and prone to subjective errors. We propose a new density regression-based method for automatic cell counting that reduces the need to manually annotate experimental images. A supervised learning-based density regression model (DRM) is trained with annotated synthetic images (the source domain) and their corresponding ground truth density maps. A domain adaptation model (DAM) is built to map experimental images (the target domain) to the feature space of the source domain. By use of the unsupervised learning-based DAM and supervised learning-based DRM, a cell density map of a given target image can be estimated, from which the number of cells can be counted. Results from experimental immunofluorescent microscopic images of human embryonic stem cells demonstrate the promising performance of the proposed counting method.



### Deep Neural Network and Data Augmentation Methodology for off-axis iris segmentation in wearable headsets
- **Arxiv ID**: http://arxiv.org/abs/1903.00389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00389v1)
- **Published**: 2019-03-01 16:17:00+00:00
- **Updated**: 2019-03-01 16:17:00+00:00
- **Authors**: Viktor Varkarakis, Shabab Bazrafkan, Peter Corcoran
- **Comment**: None
- **Journal**: None
- **Summary**: A data augmentation methodology is presented and applied to generate a large dataset of off-axis iris regions and train a low-complexity deep neural network. Although of low complexity the resulting network achieves a high level of accuracy in iris region segmentation for challenging off-axis eye-patches. Interestingly, this network is also shown to achieve high levels of performance for regular, frontal, segmentation of iris regions, comparing favorably with state-of-the-art techniques of significantly higher complexity. Due to its lower complexity, this network is well suited for deployment in embedded applications such as augmented and mixed reality headsets.



### Single Image Haze Removal Using Conditional Wasserstein Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.00395v1
- **DOI**: 10.23919/EUSIPCO.2019.8902992
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00395v1)
- **Published**: 2019-03-01 16:32:05+00:00
- **Updated**: 2019-03-01 16:32:05+00:00
- **Authors**: Joshua Peter Ebenezer, Bijaylaxmi Das, Sudipta Mukhopadhyay
- **Comment**: 5 pages
- **Journal**: 2019 27th European Signal Processing Conference (EUSIPCO), 1-5
- **Summary**: We present a method to restore a clear image from a haze-affected image using a Wasserstein generative adversarial network. As the problem is ill-conditioned, previous methods have required a prior on natural images or multiple images of the same scene. We train a generative adversarial network to learn the probability distribution of clear images conditioned on the haze-affected images using the Wasserstein loss function, using a gradient penalty to enforce the Lipschitz constraint. The method is data-adaptive, end-to-end, and requires no further processing or tuning of parameters. We also incorporate the use of a texture-based loss metric and the L1 loss to improve results, and show that our results are better than the current state-of-the-art.



### Learning To Follow Directions in Street View
- **Arxiv ID**: http://arxiv.org/abs/1903.00401v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.00401v2)
- **Published**: 2019-03-01 16:50:02+00:00
- **Updated**: 2019-11-21 22:38:35+00:00
- **Authors**: Karl Moritz Hermann, Mateusz Malinowski, Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Raia Hadsell
- **Comment**: None
- **Journal**: AAAI 2020
- **Summary**: Navigating and understanding the real world remains a key challenge in machine learning and inspires a great variety of research in areas such as language grounding, planning, navigation and computer vision. We propose an instruction-following task that requires all of the above, and which combines the practicality of simulated environments with the challenges of ambiguous, noisy real world data. StreetNav is built on top of Google Street View and provides visually accurate environments representing real places. Agents are given driving instructions which they must learn to interpret in order to successfully navigate in this environment. Since humans equipped with driving instructions can readily navigate in previously unseen cities, we set a high bar and test our trained agents for similar cognitive capabilities. Although deep reinforcement learning (RL) methods are frequently evaluated only on data that closely follow the training distribution, our dataset extends to multiple cities and has a clean train/test separation. This allows for thorough testing of generalisation ability. This paper presents the StreetNav environment and tasks, models that establish strong baselines, and extensive analysis of the task and the trained agents.



### Deep Learning for Multiple-Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1903.00440v1
- **DOI**: 10.1109/LGRS.2019.2940483
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00440v1)
- **Published**: 2019-03-01 17:59:59+00:00
- **Updated**: 2019-03-01 17:59:59+00:00
- **Authors**: Michal Kawulok, Pawel Benecki, Szymon Piechaczek, Krzysztof Hrynczenko, Daniel Kostrzewa, Jakub Nalepa
- **Comment**: Submitted to IEEE Geoscience and Remote Sensing Letters
- **Journal**: None
- **Summary**: Super-resolution reconstruction (SRR) is a process aimed at enhancing spatial resolution of images, either from a single observation, based on the learned relation between low and high resolution, or from multiple images presenting the same scene. SRR is particularly valuable, if it is infeasible to acquire images at desired resolution, but many images of the same scene are available at lower resolution---this is inherent to a variety of remote sensing scenarios. Recently, we have witnessed substantial improvement in single-image SRR attributed to the use of deep neural networks for learning the relation between low and high resolution. Importantly, deep learning has not been exploited for multiple-image SRR, which benefits from information fusion and in general allows for achieving higher reconstruction accuracy. In this letter, we introduce a new method which combines the advantages of multiple-image fusion with learning the low-to-high resolution mapping using deep networks. The reported experimental results indicate that our algorithm outperforms the state-of-the-art SRR methods, including these that operate from a single image, as well as those that perform multiple-image fusion.



### A Behavioral Approach to Visual Navigation with Graph Localization Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.00445v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.00445v1)
- **Published**: 2019-03-01 18:16:03+00:00
- **Updated**: 2019-03-01 18:16:03+00:00
- **Authors**: Kevin Chen, Juan Pablo de Vicente, Gabriel Sepulveda, Fei Xia, Alvaro Soto, Marynel Vazquez, Silvio Savarese
- **Comment**: Video: https://youtu.be/nN3B1F90CFM
- **Journal**: None
- **Summary**: Inspired by research in psychology, we introduce a behavioral approach for visual navigation using topological maps. Our goal is to enable a robot to navigate from one location to another, relying only on its visual input and the topological map of the environment. We propose using graph neural networks for localizing the agent in the map, and decompose the action space into primitive behaviors implemented as convolutional or recurrent neural networks. Using the Gibson simulator, we verify that our approach outperforms relevant baselines and is able to navigate in both seen and unseen environments.



### Multi-Object Representation Learning with Iterative Variational Inference
- **Arxiv ID**: http://arxiv.org/abs/1903.00450v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.00450v3)
- **Published**: 2019-03-01 18:21:02+00:00
- **Updated**: 2020-07-27 19:55:14+00:00
- **Authors**: Klaus Greff, RaphaÃ«l Lopez Kaufman, Rishabh Kabra, Nick Watters, Chris Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, Alexander Lerchner
- **Comment**: None
- **Journal**: ICML 2019 (PMLR 97:2424-2433)
- **Summary**: Human perception is structured around objects which form the basis for our higher-level cognition and impressive systematic generalization abilities. Yet most work on representation learning focuses on feature learning without even considering multiple objects, or treats segmentation as an (often supervised) preprocessing step. Instead, we argue for the importance of learning to segment and represent objects jointly. We demonstrate that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations. Our method learns -- without supervision -- to inpaint occluded parts, and extrapolates to scenes with more objects and to unseen objects with novel feature combinations. We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal posteriors for ambiguous inputs and extends naturally to sequences.



### Semantic-Guided Multi-Attention Localization for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.00502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00502v2)
- **Published**: 2019-03-01 19:25:24+00:00
- **Updated**: 2019-12-02 03:20:41+00:00
- **Authors**: Yizhe Zhu, Jianwen Xie, Zhiqiang Tang, Xi Peng, Ahmed Elgammal
- **Comment**: accepted to NeurIPS'19
- **Journal**: None
- **Summary**: Zero-shot learning extends the conventional object classification to the unseen class recognition by introducing semantic representations of classes. Existing approaches predominantly focus on learning the proper mapping function for visual-semantic embedding, while neglecting the effect of learning discriminative visual features. In this paper, we study the significance of the discriminative region localization. We propose a semantic-guided multi-attention localization model, which automatically discovers the most discriminative parts of objects for zero-shot learning without any human annotations. Our model jointly learns cooperative global and local features from the whole object as well as the detected parts to categorize objects based on semantic descriptions. Moreover, with the joint supervision of embedding softmax loss and class-center triplet loss, the model is encouraged to learn features with high inter-class dispersion and intra-class compactness. Through comprehensive experiments on three widely used zero-shot learning benchmarks, we show the efficacy of the multi-attention localization and our proposed approach improves the state-of-the-art results by a considerable margin.



### Unsupervised Tracklet Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1903.00535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00535v1)
- **Published**: 2019-03-01 20:40:33+00:00
- **Updated**: 2019-03-01 20:40:33+00:00
- **Authors**: Minxian Li, Xiatian Zhu, Shaogang Gong
- **Comment**: Accepted to appear in IEEE Transactions on Pattern Analysis and
  Machine Intelligence. The new dataset is publicly available at
  https://github.com/liminxian/DukeMTMC-SI-Tracklet
- **Journal**: None
- **Summary**: Most existing person re-identification (re-id) methods rely on supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in a practical re-id deployment, due to the lack of exhaustive identity labelling of positive and negative image pairs for every camera-pair. In this work, we present an unsupervised re-id deep learning approach. It is capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data end-to-end. We formulate an Unsupervised Tracklet Association Learning (UTAL) framework. This is by jointly learning within-camera tracklet discrimination and cross-camera tracklet association in order to maximise the discovery of tracklet identity matching both within and across camera views. Extensive experiments demonstrate the superiority of the proposed model over the state-of-the-art unsupervised learning and domain adaptation person re-id methods on eight benchmarking datasets.




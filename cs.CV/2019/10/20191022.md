# Arxiv Papers in cs.CV on 2019-10-22
### Discriminative Neural Clustering for Speaker Diarisation
- **Arxiv ID**: http://arxiv.org/abs/1910.09703v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CL, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1910.09703v2)
- **Published**: 2019-10-22 00:09:22+00:00
- **Updated**: 2020-11-23 15:32:03+00:00
- **Authors**: Qiujia Li, Florian L. Kreyssig, Chao Zhang, Philip C. Woodland
- **Comment**: Accepted as a conference paper at the 8th IEEE Spoken Language
  Technology Workshop (SLT 2021)
- **Journal**: None
- **Summary**: In this paper, we propose Discriminative Neural Clustering (DNC) that formulates data clustering with a maximum number of clusters as a supervised sequence-to-sequence learning problem. Compared to traditional unsupervised clustering algorithms, DNC learns clustering patterns from training data without requiring an explicit definition of a similarity measure. An implementation of DNC based on the Transformer architecture is shown to be effective on a speaker diarisation task using the challenging AMI dataset. Since AMI contains only 147 complete meetings as individual input sequences, data scarcity is a significant issue for training a Transformer model for DNC. Accordingly, this paper proposes three data augmentation schemes: sub-sequence randomisation, input vector randomisation, and Diaconis augmentation, which generates new data samples by rotating the entire input sequence of L2-normalised speaker embeddings. Experimental results on AMI show that DNC achieves a reduction in speaker error rate (SER) of 29.4% relative to spectral clustering.



### Mobile Recognition of Wikipedia Featured Sites using Deep Learning and Crowd-sourced Imagery
- **Arxiv ID**: http://arxiv.org/abs/1910.09705v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09705v2)
- **Published**: 2019-10-22 00:17:55+00:00
- **Updated**: 2019-11-04 18:15:39+00:00
- **Authors**: Jimin Tan, Anastasios Noulas, Diego Sáez, Rossano Schifanella
- **Comment**: None
- **Journal**: None
- **Summary**: Rendering Wikipedia content through mobile and augmented reality mediums can enable new forms of interaction in urban-focused user communities facilitating learning, communication and knowledge exchange. With this objective in mind, in this work we develop a mobile application that allows for the recognition of notable sites featured on Wikipedia. The application is powered by a deep neural network that has been trained on crowd-sourced imagery describing sites of interest, such as buildings, statues, museums or other physical entities that are present and visually accessible in an urban environment. We describe an end-to-end pipeline that describes data collection, model training and evaluation of our application considering online and real world scenarios. We identify a number of challenges in the site recognition task which arise due to visual similarities amongst the classified sites as well as due to noise introduce by the surrounding built environment. We demonstrate how using mobile contextual information, such as user location, orientation and attention patterns can significantly alleviate such challenges. Moreover, we present an unsupervised learning technique to de-noise crowd-sourced imagery which improves classification performance further.



### A blind Robust Image Watermarking Approach exploiting the DFT Magnitude
- **Arxiv ID**: http://arxiv.org/abs/1910.11185v1
- **DOI**: 10.1109/AICCSA.2015.7507124
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11185v1)
- **Published**: 2019-10-22 00:26:13+00:00
- **Updated**: 2019-10-22 00:26:13+00:00
- **Authors**: Mohamed Hamidi, Mohamed El Haziti, Hocine Cherifi, Driss Aboutajdine
- **Comment**: 6 pages, 4 Figures, published in : (2015) IEEE/ACS 12th International
  Conference of Computer Systems and Applications (AICCSA)
- **Journal**: None
- **Summary**: Due to the current progress in Internet, digital contents (video, audio and images) are widely used. Distribution of multimedia contents is now faster and it allows for easy unauthorized reproduction of information. Digital watermarking came up while trying to solve this problem. Its main idea is to embed a watermark into a host digital content without affecting its quality. Moreover, watermarking can be used in several applications such as authentication, copy control, indexation, Copyright protection, etc. In this paper, we propose a blind robust image watermarking approach as a solution to the problem of copyright protection of digital images. The underlying concept of our method is to apply a discrete cosine transform (DCT) to the magnitude resulting from a discrete Fourier transform (DFT) applied to the original image. Then, the watermark is embedded by modifying the coefficients of the DCT using a secret key to increase security. Experimental results show the robustness of the proposed technique to a wide range of common attacks, e.g., Low-Pass Gaussian Filtering, JPEG compression, Gaussian noise, salt & pepper noise, Gaussian Smoothing and Histogram equalization. The proposed method achieves a Peak signal-to-noise-ration (PSNR) value greater than 66 (dB) and ensures a perfect watermark extraction.



### A deep active learning system for species identification and counting in camera trap images
- **Arxiv ID**: http://arxiv.org/abs/1910.09716v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09716v1)
- **Published**: 2019-10-22 01:03:33+00:00
- **Updated**: 2019-10-22 01:03:33+00:00
- **Authors**: Mohammad Sadegh Norouzzadeh, Dan Morris, Sara Beery, Neel Joshi, Nebojsa Jojic, Jeff Clune
- **Comment**: 15 pages, 5 figures
- **Journal**: None
- **Summary**: Biodiversity conservation depends on accurate, up-to-date information about wildlife population distributions. Motion-activated cameras, also known as camera traps, are a critical tool for population surveys, as they are cheap and non-intrusive. However, extracting useful information from camera trap images is a cumbersome process: a typical camera trap survey may produce millions of images that require slow, expensive manual review. Consequently, critical information is often lost due to resource limitations, and critical conservation questions may be answered too slowly to support decision-making. Computer vision is poised to dramatically increase efficiency in image-based biodiversity surveys, and recent studies have harnessed deep learning techniques for automatic information extraction from camera trap images. However, the accuracy of results depends on the amount, quality, and diversity of the data available to train models, and the literature has focused on projects with millions of relevant, labeled training images. Many camera trap projects do not have a large set of labeled images and hence cannot benefit from existing machine learning techniques. Furthermore, even projects that do have labeled data from similar ecosystems have struggled to adopt deep learning methods because image classification models overfit to specific image backgrounds (i.e., camera locations). In this paper, we focus not on automating the labeling of camera trap images, but on accelerating this process. We combine the power of machine intelligence and human intelligence to build a scalable, fast, and accurate active learning system to minimize the manual work required to identify and count animals in camera trap images. Our proposed scheme can match the state of the art accuracy on a 3.2 million image dataset with as few as 14,100 manual labels, which means decreasing manual labeling effort by over 99.5%.



### Penalizing small errors using an Adaptive Logarithmic Loss
- **Arxiv ID**: http://arxiv.org/abs/1910.09717v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09717v2)
- **Published**: 2019-10-22 01:24:41+00:00
- **Updated**: 2021-04-07 22:58:36+00:00
- **Authors**: Chaitanya Kaul, Nick Pears, Hang Dai, Roderick Murray-Smith, Suresh Manandhar
- **Comment**: Published at AIHA 2020 (ICPR 2020 Workshop)
- **Journal**: None
- **Summary**: Loss functions are error metrics that quantify the difference between a prediction and its corresponding ground truth. Fundamentally, they define a functional landscape for traversal by gradient descent. Although numerous loss functions have been proposed to date in order to handle various machine learning problems, little attention has been given to enhancing these functions to better traverse the loss landscape. In this paper, we simultaneously and significantly mitigate two prominent problems in medical image segmentation namely: i) class imbalance between foreground and background pixels and ii) poor loss function convergence. To this end, we propose an adaptive logarithmic loss function. We compare this loss function with the existing state-of-the-art on the ISIC 2018 dataset, the nuclei segmentation dataset as well as the DRIVE retinal vessel segmentation dataset. We measure the performance of our methodology on benchmark metrics and demonstrate state-of-the-art performance. More generally, we show that our system can be used as a framework for better training of deep neural networks.



### Drivers Drowsiness Detection using Condition-Adaptive Representation Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/1910.09722v1
- **DOI**: 10.1109/TITS.2018.2883823
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09722v1)
- **Published**: 2019-10-22 01:51:43+00:00
- **Updated**: 2019-10-22 01:51:43+00:00
- **Authors**: Jongmin Yu, Sangwoo Park, Sangwook Lee, Moongu Jeon
- **Comment**: IEEE Transactions on Intelligent Transportation Systems publication
  information (2018)
- **Journal**: None
- **Summary**: We propose a condition-adaptive representation learning framework for the driver drowsiness detection based on 3D-deep convolutional neural network. The proposed framework consists of four models: spatio-temporal representation learning, scene condition understanding, feature fusion, and drowsiness detection. The spatio-temporal representation learning extracts features that can describe motions and appearances in video simultaneously. The scene condition understanding classifies the scene conditions related to various conditions about the drivers and driving situations such as statuses of wearing glasses, illumination condition of driving, and motion of facial elements such as head, eye, and mouth. The feature fusion generates a condition-adaptive representation using two features extracted from above models. The detection model recognizes drivers drowsiness status using the condition-adaptive representation. The condition-adaptive representation learning framework can extract more discriminative features focusing on each scene condition than the general representation so that the drowsiness detection method can provide more accurate results for the various driving situations. The proposed framework is evaluated with the NTHU Drowsy Driver Detection video dataset. The experimental results show that our framework outperforms the existing drowsiness detection methods based on visual analysis.



### Face Detection on Surveillance Images
- **Arxiv ID**: http://arxiv.org/abs/1910.11121v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11121v1)
- **Published**: 2019-10-22 02:10:58+00:00
- **Updated**: 2019-10-22 02:10:58+00:00
- **Authors**: Mohammad Iqbal Nouyed, Guodong Guo
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: In last few decades, a lot of progress has been made in the field of face detection. Various face detection methods have been proposed by numerous researchers working in this area. The two well-known benchmarking platform: the FDDB and WIDER face detection provide quite challenging scenarios to assess the efficacy of the detection methods. These benchmarking data sets are mostly created using images from the public network ie. the Internet. A recent, face detection and open-set recognition challenge has shown that those same face detection algorithms produce high false alarms for images taken in surveillance scenario. This shows the difficult nature of the surveillance environment. Our proposed body pose based face detection method was one of the top performers in this competition. In this paper, we perform a comparative performance analysis of some of the well known face detection methods including the few used in that competition, and, compare them to our proposed body pose based face detection method. Experiment results show that, our proposed method that leverages body information to detect faces, is the most realistic approach in terms of accuracy, false alarms and average detection time, when surveillance scenario is in consideration.



### Convolutional Prototype Learning for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.09728v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09728v3)
- **Published**: 2019-10-22 02:15:46+00:00
- **Updated**: 2020-02-11 02:56:56+00:00
- **Authors**: Zhizhe Liu, Xingxing Zhang, Zhenfeng Zhu, Shuai Zheng, Yao Zhao, Jian Cheng
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) has received increasing attention in recent years especially in areas of fine-grained object recognition, retrieval, and image captioning. The key to ZSL is to transfer knowledge from the seen to the unseen classes via auxiliary class attribute vectors. However, the popularly learned projection functions in previous works cannot generalize well since they assume the distribution consistency between seen and unseen domains at sample-level.Besides, the provided non-visual and unique class attributes can significantly degrade the recognition performance in semantic space. In this paper, we propose a simple yet effective convolutional prototype learning (CPL) framework for zero-shot recognition. By assuming distribution consistency at task-level, our CPL is capable of transferring knowledge smoothly to recognize unseen samples.Furthermore, inside each task, discriminative visual prototypes are learned via a distance based training mechanism. Consequently, we can perform recognition in visual space, instead of semantic space. An extensive group of experiments are then carefully designed and presented, demonstrating that CPL obtains more favorable effectiveness, over currently available alternatives under various settings.



### Single and Union Non-parallel Support Vector Machine Frameworks
- **Arxiv ID**: http://arxiv.org/abs/1910.09734v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09734v3)
- **Published**: 2019-10-22 02:34:34+00:00
- **Updated**: 2021-06-25 06:56:34+00:00
- **Authors**: Chun-Na Li, Yuan-Hai Shao, Huajun Wang, Yu-Ting Zhao, Ling-Wei Huang, Naihua Xiu, Nai-Yang Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Considering the classification problem, we summarize the nonparallel support vector machines with the nonparallel hyperplanes to two types of frameworks. The first type constructs the hyperplanes separately. It solves a series of small optimization problems to obtain a series of hyperplanes, but is hard to measure the loss of each sample. The other type constructs all the hyperplanes simultaneously, and it solves one big optimization problem with the ascertained loss of each sample. We give the characteristics of each framework and compare them carefully. In addition, based on the second framework, we construct a max-min distance-based nonparallel support vector machine for multiclass classification problem, called NSVM. It constructs hyperplanes with large distance margin by solving an optimization problem. Experimental results on benchmark data sets show the advantages of our NSVM.



### Assessment of the Local Tchebichef Moments Method for Texture Classification by Fine Tuning Extraction Parameters
- **Arxiv ID**: http://arxiv.org/abs/1910.09758v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1910.09758v1)
- **Published**: 2019-10-22 04:13:34+00:00
- **Updated**: 2019-10-22 04:13:34+00:00
- **Authors**: Andre Barczak, Napoleon Reyes, Teo Susnjak
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we use machine learning to study the application of Local Tchebichef Moments (LTM) to the problem of texture classification. The original LTM method was proposed by Mukundan (2014).   The LTM method can be used for texture analysis in many different ways, either using the moment values directly, or more simply creating a relationship between the moment values of different orders, producing a histogram similar to those of Local Binary Pattern (LBP) based methods. The original method was not fully tested with large datasets, and there are several parameters that should be characterised for performance. Among these parameters are the kernel size, the moment orders and the weights for each moment.   We implemented the LTM method in a flexible way in order to allow for the modification of the parameters that can affect its performance. Using four subsets from the Outex dataset (a popular benchmark for texture analysis), we used Random Forests to create models and to classify texture images, recording the standard metrics for each classifier. We repeated the process using several variations of the LBP method for comparison. This allowed us to find the best combination of orders and weights for the LTM method for texture classification.



### A Review of Visual Trackers and Analysis of its Application to Mobile Robot
- **Arxiv ID**: http://arxiv.org/abs/1910.09761v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09761v1)
- **Published**: 2019-10-22 04:30:48+00:00
- **Updated**: 2019-10-22 04:30:48+00:00
- **Authors**: Shaoze You, Hua Zhu, Menggang Li, Yutan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision has received a significant attention in recent year, which is one of the important parts for robots to obtain information about the external environment. Visual trackers can provide the necessary physical and environmental parameters for the mobile robot, and their performance is related to the actual application of the robot. This study provides a comprehensive survey on visual trackers. Following a brief introduction, we first analyzed the basic framework and difficulties of visual trackers. Then the structure of generative and discriminative methods is introduced, and summarized the feature descriptors, modeling methods, and learning methods which be used in tracker. Later we reviewed and evaluated the state-of-the-art progress on discriminative trackers from three directions: correlation filter, deep learning and convolutional features. Finally, we analyzed the research direction of visual tracker used in mobile robot, as well as outlined the future trends for visual tracker on mobile robot.



### Face representation by deep learning: a linear encoding in a parameter space?
- **Arxiv ID**: http://arxiv.org/abs/1910.09768v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09768v1)
- **Published**: 2019-10-22 05:01:28+00:00
- **Updated**: 2019-10-22 05:01:28+00:00
- **Authors**: Qiulei Dong, Jiayin Sun, Zhanyi Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Convolutional Neural Networks (CNNs) have achieved tremendous performances on face recognition, and one popular perspective regarding CNNs' success is that CNNs could learn discriminative face representations from face images with complex image feature encoding. However, it is still unclear what is the intrinsic mechanism of face representation in CNNs. In this work, we investigate this problem by formulating face images as points in a shape-appearance parameter space, and our results demonstrate that: (i) The encoding and decoding of the neuron responses (representations) to face images in CNNs could be achieved under a linear model in the parameter space, in agreement with the recent discovery in primate IT face neurons, but different from the aforementioned perspective on CNNs' face representation with complex image feature encoding; (ii) The linear model for face encoding and decoding in the parameter space could achieve close or even better performances on face recognition and verification than state-of-the-art CNNs, which might provide new lights on the design strategies for face recognition systems; (iii) The neuron responses to face images in CNNs could not be adequately modelled by the axis model, a model recently proposed on face modelling in primate IT cortex. All these results might shed some lights on the often complained blackbox nature behind CNNs' tremendous performances on face recognition.



### Trident Segmentation CNN: A Spatiotemporal Transformation CNN for Punctate White Matter Lesions Segmentation in Preterm Neonates
- **Arxiv ID**: http://arxiv.org/abs/1910.09773v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09773v1)
- **Published**: 2019-10-22 05:36:56+00:00
- **Updated**: 2019-10-22 05:36:56+00:00
- **Authors**: Yalong Liu, Jie Li, Miaomiao Wang, Zhicheng Jiao, Jian Yang, Xianjun Li
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of punctate white matter lesions (PWML) in preterm neonates by an automatic algorithm can better assist doctors in diagnosis. However, the existing algorithms have many limitations, such as low detection accuracy and large resource consumption. In this paper, a novel spatiotemporal transformation deep learning method called Trident Segmentation CNN (TS-CNN) is proposed to segment PWML in MR images. It can convert spatial information into temporal information, which reduces the consumption of computing resources. Furthermore, a new improved training loss called Self-balancing Focal Loss (SBFL) is proposed to balance the loss during the training process. The whole model is evaluated on a dataset of 704 MR images. Overall the method achieves median DSC, sensitivity, specificity, and Hausdorff distance of 0.6355, 0.7126, 0.9998, and 24.5836 mm which outperforms the state-of-the-art algorithm. (The code is now available on https://github.com/YalongLiu/Trident-Segmentation-CNN)



### Self-Correction for Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/1910.09777v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09777v1)
- **Published**: 2019-10-22 05:49:54+00:00
- **Updated**: 2019-10-22 05:49:54+00:00
- **Authors**: Peike Li, Yunqiu Xu, Yunchao Wei, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Labeling pixel-level masks for fine-grained semantic segmentation tasks, e.g. human parsing, remains a challenging task. The ambiguous boundary between different semantic parts and those categories with similar appearance usually are confusing, leading to unexpected noises in ground truth masks. To tackle the problem of learning with label noises, this work introduces a purification strategy, called Self-Correction for Human Parsing (SCHP), to progressively promote the reliability of the supervised labels as well as the learned models. In particular, starting from a model trained with inaccurate annotations as initialization, we design a cyclically learning scheduler to infer more reliable pseudo-masks by iteratively aggregating the current learned model with the former optimal one in an online manner. Besides, those correspondingly corrected labels can in turn to further boost the model performance. In this way, the models and the labels will reciprocally become more robust and accurate during the self-correction learning cycles. Benefiting from the superiority of SCHP, we achieve the best performance on two popular single-person human parsing benchmarks, including LIP and Pascal-Person-Part datasets. Our overall system ranks 1st in CVPR2019 LIP Challenge. Code is available at https://github.com/PeikeLi/Self-Correction-Human-Parsing.



### J Regularization Improves Imbalanced Multiclass Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.09783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09783v1)
- **Published**: 2019-10-22 06:25:06+00:00
- **Updated**: 2019-10-22 06:25:06+00:00
- **Authors**: Fidel A. Guerrero Peña, Pedro D. Marrero Fernandez, Paul T. Tarr, Tsang Ing Ren, Elliot M. Meyerowitz, Alexandre Cunha
- **Comment**: Submitted to ISBI 2020
- **Journal**: None
- **Summary**: We propose a new loss formulation to further advance the multiclass segmentation of cluttered cells under weakly supervised conditions.   We improve the separation of touching and immediate cells, obtaining sharp segmentation boundaries with high adequacy, when we add Youden's $J$ statistic regularization term to the cross entropy loss. This regularization intrinsically supports class imbalance thus eliminating the necessity of explicitly using weights to balance training. Simulations demonstrate this capability and show how the regularization leads to better results by helping advancing the optimization when cross entropy stalls.   We build upon our previous work on multiclass segmentation by adding yet another training class representing gaps between adjacent cells.   This addition helps the classifier identify narrow gaps as background and no longer as touching regions.   We present results of our methods for 2D and 3D images, from bright field to confocal stacks containing different types of cells, and we show that they accurately segment individual cells after training with a limited number of annotated images, some of which are poorly annotated.



### Robust Training with Ensemble Consensus
- **Arxiv ID**: http://arxiv.org/abs/1910.09792v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09792v3)
- **Published**: 2019-10-22 06:58:10+00:00
- **Updated**: 2020-11-11 09:59:16+00:00
- **Authors**: Jisoo Lee, Sae-Young Chung
- **Comment**: ICLR 2020
- **Journal**: None
- **Summary**: Since deep neural networks are over-parameterized, they can memorize noisy examples. We address such a memorization issue in the presence of label noise. From the fact that deep neural networks cannot generalize to neighborhoods of memorized features, we hypothesize that noisy examples do not consistently incur small losses on the network under a certain perturbation. Based on this, we propose a novel training method called Learning with Ensemble Consensus (LEC) that prevents overfitting to noisy examples by removing them based on the consensus of an ensemble of perturbed networks. One of the proposed LECs, LTEC outperforms the current state-of-the-art methods on noisy MNIST, CIFAR-10, and CIFAR-100 in an efficient manner.



### Improving Siamese Networks for One Shot Learning using Kernel Based Activation functions
- **Arxiv ID**: http://arxiv.org/abs/1910.09798v1
- **DOI**: 10.1007/978-981-15-5619-7_25
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09798v1)
- **Published**: 2019-10-22 07:17:07+00:00
- **Updated**: 2019-10-22 07:17:07+00:00
- **Authors**: Shruti Jadon, Aditya Acrot Srinivasan
- **Comment**: 15 pages, 8 figures
- **Journal**: Advances in Intelligent Systems and Computing book series (AISC,
  volume 1175) Springer 2020
- **Summary**: The lack of a large amount of training data has always been the constraining factor in solving a lot of problems in machine learning, making One Shot Learning one of the most intriguing ideas in machine learning. It aims to learn information about object categories from one, or only a few training examples. This process of learning in deep learning is usually accomplished by proper objective function, i.e; loss function and embeddings extraction i.e; architecture. In this paper, we discussed about metrics based deep learning architectures for one shot learning such as Siamese neural networks and present a method to improve on their accuracy using Kafnets (kernel-based non-parametric activation functions for neural networks) by learning proper embeddings with relatively less number of epochs. Using kernel activation functions, we are able to achieve strong results which exceed those of ReLU based deep learning models in terms of embeddings structure, loss convergence, and accuracy.



### A low-power end-to-end hybrid neuromorphic framework for surveillance applications
- **Arxiv ID**: http://arxiv.org/abs/1910.09806v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09806v3)
- **Published**: 2019-10-22 07:51:27+00:00
- **Updated**: 2020-01-29 08:44:37+00:00
- **Authors**: Andres Ussa, Luca Della Vedova, Vandana Reddy Padala, Deepak Singla, Jyotibdha Acharya, Charles Zhang Lei, Garrick Orchard, Arindam Basu, Bharath Ramesh
- **Comment**: 12 pages, 3 figures, pre-print to BMVC workshops 2018
- **Journal**: None
- **Summary**: With the success of deep learning, object recognition systems that can be deployed for real-world applications are becoming commonplace. However, inference that needs to largely take place on the `edge' (not processed on servers), is a highly computational and memory intensive workload, making it intractable for low-power mobile nodes and remote security applications. To address this challenge, this paper proposes a low-power (5W) end-to-end neuromorphic framework for object tracking and classification using event-based cameras that possess desirable properties such as low power consumption (5-14 mW) and high dynamic range (120 dB). Nonetheless, unlike traditional approaches of using event-by-event processing, this work uses a mixed frame and event approach to get energy savings with high performance. Using a frame-based region proposal method based on the density of foreground events, a hardware-friendly object tracking is implemented using the apparent object velocity while tackling occlusion scenarios. For low-power classification of the tracked objects, the event camera is interfaced to IBM TrueNorth, which is time-multiplexed to tackle up to eight instances for a traffic monitoring application. The frame-based object track input is converted back to spikes for Truenorth classification via the energy efficient deep network (EEDN) pipeline. Using originally collected datasets, we train the TrueNorth model on the hardware track outputs, instead of using ground truth object locations as commonly done, and demonstrate the efficacy of our system to handle practical surveillance scenarios. Finally, we compare the proposed methodologies to state-of-the-art event-based systems for object tracking and classification, and demonstrate the use case of our neuromorphic approach for low-power applications without sacrificing on performance.



### Structure Matters: Towards Generating Transferable Adversarial Images
- **Arxiv ID**: http://arxiv.org/abs/1910.09821v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09821v3)
- **Published**: 2019-10-22 08:20:00+00:00
- **Updated**: 2020-08-14 10:33:59+00:00
- **Authors**: Dan Peng, Zizhan Zheng, Linhao Luo, Xiaofeng Zhang
- **Comment**: accepted to ECAI 2020
- **Journal**: None
- **Summary**: Recent works on adversarial examples for image classification focus on directly modifying pixels with minor perturbations. The small perturbation requirement is imposed to ensure the generated adversarial examples being natural and realistic to humans, which, however, puts a curb on the attack space thus limiting the attack ability and transferability especially for systems protected by a defense mechanism. In this paper, we propose the novel concepts of structure patterns and structure-aware perturbations that relax the small perturbation constraint while still keeping images natural. The key idea of our approach is to allow perceptible deviation in adversarial examples while keeping structure patterns that are central to a human classifier. Built upon these concepts, we propose a \emph{structure-preserving attack (SPA)} for generating natural adversarial examples with extremely high transferability. Empirical results on the MNIST and the CIFAR10 datasets show that SPA exhibits strong attack ability in both the white-box and black-box setting even defenses are applied. Moreover, with the integration of PGD or CW attack, its attack ability escalates sharply under the white-box setting, without losing the outstanding transferability inherited from SPA.



### Hetero-Center Loss for Cross-Modality Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1910.09830v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09830v1)
- **Published**: 2019-10-22 08:35:45+00:00
- **Updated**: 2019-10-22 08:35:45+00:00
- **Authors**: Yuanxin Zhu, Zhao Yang, Li Wang, Sai Zhao, Xiao Hu, Dapeng Tao
- **Comment**: 16 pages, 10 figures
- **Journal**: None
- **Summary**: Cross-modality person re-identification is a challenging problem which retrieves a given pedestrian image in RGB modality among all the gallery images in infrared modality. The task can address the limitation of RGB-based person Re-ID in dark environments. Existing researches mainly focus on enlarging inter-class differences of feature to solve the problem. However, few studies investigate improving intra-class cross-modality similarity, which is important for this issue. In this paper, we propose a novel loss function, called Hetero-Center loss (HC loss) to reduce the intra-class cross-modality variations. Specifically, HC loss can supervise the network learning the cross-modality invariant information by constraining the intra-class center distance between two heterogenous modalities. With the joint supervision of Cross-Entropy (CE) loss and HC loss, the network is trained to achieve two vital objectives, inter-class discrepancy and intra-class cross-modality similarity as much as possible. Besides, we propose a simple and high-performance network architecture to learn local feature representations for cross-modality person re-identification, which can be a baseline for future research. Extensive experiments indicate the effectiveness of the proposed methods, which outperform state-of-the-art methods by a wide margin.



### Towards Best Practice in Explaining Neural Network Decisions with LRP
- **Arxiv ID**: http://arxiv.org/abs/1910.09840v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09840v3)
- **Published**: 2019-10-22 08:58:54+00:00
- **Updated**: 2020-07-13 20:00:09+00:00
- **Authors**: Maximilian Kohlbrenner, Alexander Bauer, Shinichi Nakajima, Alexander Binder, Wojciech Samek, Sebastian Lapuschkin
- **Comment**: 7 pages, 4 figures, 1 table. fixed table row compared to v2.
  Presented virtually at IJCNN 2020
- **Journal**: None
- **Summary**: Within the last decade, neural network based predictors have demonstrated impressive - and at times super-human - capabilities. This performance is often paid for with an intransparent prediction process and thus has sparked numerous contributions in the novel field of explainable artificial intelligence (XAI). In this paper, we focus on a popular and widely used method of XAI, the Layer-wise Relevance Propagation (LRP). Since its initial proposition LRP has evolved as a method, and a best practice for applying the method has tacitly emerged, based however on humanly observed evidence alone. In this paper we investigate - and for the first time quantify - the effect of this current best practice on feedforward neural networks in a visual object detection setting. The results verify that the layer-dependent approach to LRP applied in recent literature better represents the model's reasoning, and at the same time increases the object localization and class discriminativity of LRP.



### Fixed Pattern Noise Reduction for Infrared Images Based on Cascade Residual Attention CNN
- **Arxiv ID**: http://arxiv.org/abs/1910.09858v1
- **DOI**: 10.1016/j.neucom.2019.10.054
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09858v1)
- **Published**: 2019-10-22 09:31:46+00:00
- **Updated**: 2019-10-22 09:31:46+00:00
- **Authors**: Juntao Guan, Rui Lai, Ai Xiong, Zesheng Liu, Lin Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing fixed pattern noise reduction (FPNR) methods are easily affected by the motion state of the scene and working condition of the image sensor, which leads to over smooth effects, ghosting artifacts as well as slow convergence rate. To address these issues, we design an innovative cascade convolution neural network (CNN) model with residual skip connections to realize single frame blind FPNR operation without any parameter tuning. Moreover, a coarse-fine convolution (CF-Conv) unit is introduced to extract complementary features in various scales and fuse them to pick more spatial information. Inspired by the success of the visual attention mechanism, we further propose a particular spatial-channel noise attention unit (SCNAU) to separate the scene details from fixed pattern noise more thoroughly and recover the real scene more accurately. Experimental results on test data demonstrate that the proposed cascade CNN-FPNR method outperforms the existing FPNR methods in both of visual effect and quantitative assessment.



### A Locating Model for Pulmonary Tuberculosis Diagnosis in Radiographs
- **Arxiv ID**: http://arxiv.org/abs/1910.09900v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09900v1)
- **Published**: 2019-10-22 11:42:06+00:00
- **Updated**: 2019-10-22 11:42:06+00:00
- **Authors**: Jiwei Liu, Junyu Liu, Yang Liu, Rui Yang, Dongjun Lv, Zhengting Cai, Jingjing Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: We propose an end-to-end CNN-based locating model for pulmonary tuberculosis (TB) diagnosis in radiographs. This model makes full use of chest radiograph (X-ray) for its improved accessibility, reduced cost and high accuracy for TB disease. Methods: Several specialized improvements are proposed for detection task in medical field. A false positive (FP) restrictor head is introduced for FP reduction. Anchor-oriented network heads is proposed in the position regression section. An optimization of loss function is designed for hard example mining. Results: The experimental results show that when the threshold of intersection over union (IoU) is set to 0.3, the average precision (AP) of two test data sets provided by different hospitals reaches 0.9023 and 0.9332. Ablation experiments shows that hard example mining and change of regressor heads contribute most in this work, but FP restriction is necessary in a CAD diagnose system. Conclusion: The results prove the high precision and good generalization ability of our proposed model comparing to previous works. Significance: We first make full use of the feature extraction ability of CNNs in TB diagnostic field and make exploration in localization of TB, when the previous works focus on the weaker task of healthy-sick subject classification.



### WeatherNet: Recognising weather and visual conditions from street-level images using deep residual learning
- **Arxiv ID**: http://arxiv.org/abs/1910.09910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09910v1)
- **Published**: 2019-10-22 12:03:44+00:00
- **Updated**: 2019-10-22 12:03:44+00:00
- **Authors**: Mohamed R. Ibrahim, James Haworth, Tao Cheng
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Extracting information related to weather and visual conditions at a given time and space is indispensable for scene awareness, which strongly impacts our behaviours, from simply walking in a city to riding a bike, driving a car, or autonomous drive-assistance. Despite the significance of this subject, it is still not been fully addressed by the machine intelligence relying on deep learning and computer vision to detect the multi-labels of weather and visual conditions with a unified method that can be easily used for practice. What has been achieved to-date is rather sectorial models that address limited number of labels that do not cover the wide spectrum of weather and visual conditions. Nonetheless, weather and visual conditions are often addressed individually. In this paper, we introduce a novel framework to automatically extract this information from street-level images relying on deep learning and computer vision using a unified method without any pre-defined constraints in the processed images. A pipeline of four deep Convolutional Neural Network (CNN) models, so-called the WeatherNet, is trained, relying on residual learning using ResNet50 architecture, to extract various weather and visual conditions such as Dawn/dusk, day and night for time detection, and glare for lighting conditions, and clear, rainy, snowy, and foggy for weather conditions. The WeatherNet shows strong performance in extracting this information from user-defined images or video streams that can be used not limited to: autonomous vehicles and drive-assistance systems, tracking behaviours, safety-related research, or even for better understanding cities through images for policy-makers.



### Weakly-Supervised Completion Moment Detection using Temporal Attention
- **Arxiv ID**: http://arxiv.org/abs/1910.09920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09920v1)
- **Published**: 2019-10-22 12:31:07+00:00
- **Updated**: 2019-10-22 12:31:07+00:00
- **Authors**: Farnoosh Heidarivincheh, Majid Mirmehdi, Dima Damen
- **Comment**: None
- **Journal**: None
- **Summary**: Monitoring the progression of an action towards completion offers fine grained insight into the actor's behaviour. In this work, we target detecting the completion moment of actions, that is the moment when the action's goal has been successfully accomplished. This has potential applications from surveillance to assistive living and human-robot interactions. Previous effort required human annotations of the completion moment for training (i.e. full supervision). In this work, we present an approach for moment detection from weak video-level labels. Given both complete and incomplete sequences, of the same action, we learn temporal attention, along with accumulated completion prediction from all frames in the sequence. We also demonstrate how the approach can be used when completion moment supervision is available. We evaluate and compare our approach on actions from three datasets, namely HMDB, UCF101 and RGBD-AC, and show that temporal attention improves detection in both weakly-supervised and fully-supervised settings.



### 4-Connected Shift Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.09931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09931v1)
- **Published**: 2019-10-22 12:46:31+00:00
- **Updated**: 2019-10-22 12:46:31+00:00
- **Authors**: Andrew Brown, Pascal Mettes, Marcel Worring
- **Comment**: ICCV Neural Architects Workshop 2019
- **Journal**: None
- **Summary**: The shift operation was recently introduced as an alternative to spatial convolutions. The operation moves subsets of activations horizontally and/or vertically. Spatial convolutions are then replaced with shift operations followed by point-wise convolutions, significantly reducing computational costs. In this work, we investigate how shifts should best be applied to high accuracy CNNs. We apply shifts of two different neighbourhood groups to ResNet on ImageNet: the originally introduced 8-connected (8C) neighbourhood shift and the less well studied 4-connected (4C) neighbourhood shift. We find that when replacing ResNet's spatial convolutions with shifts, both shift neighbourhoods give equal ImageNet accuracy, showing the sufficiency of small neighbourhoods for large images. Interestingly, when incorporating shifts to all point-wise convolutions in residual networks, 4-connected shifts outperform 8-connected shifts. Such a 4-connected shift setup gives the same accuracy as full residual networks while reducing the number of parameters and FLOPs by over 40%. We then highlight that without spatial convolutions, ResNet's downsampling/upsampling bottleneck channel structure is no longer needed. We show a new, 4C shift-based residual network, much shorter than the original ResNet yet with a higher accuracy for the same computational cost. This network is the highest accuracy shift-based network yet shown, demonstrating the potential of shifting in deep neural networks.



### Exchangeable deep neural networks for set-to-set matching and learning
- **Arxiv ID**: http://arxiv.org/abs/1910.09972v2
- **DOI**: 10.1007/978-3-030-58520-4_37
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09972v2)
- **Published**: 2019-10-22 13:42:39+00:00
- **Updated**: 2021-01-28 09:34:59+00:00
- **Authors**: Yuki Saito, Takuma Nakamura, Hirotaka Hachiya, Kenji Fukumizu
- **Comment**: None
- **Journal**: None
- **Summary**: Matching two different sets of items, called heterogeneous set-to-set matching problem, has recently received attention as a promising problem. The difficulties are to extract features to match a correct pair of different sets and also preserve two types of exchangeability required for set-to-set matching: the pair of sets, as well as the items in each set, should be exchangeable. In this study, we propose a novel deep learning architecture to address the abovementioned difficulties and also an efficient training framework for set-to-set matching. We evaluate the methods through experiments based on two industrial applications: fashion set recommendation and group re-identification. In these experiments, we show that the proposed method provides significant improvements and results compared with the state-of-the-art methods, thereby validating our architecture for the heterogeneous set matching problem.



### Learning Adaptive Regularization for Image Labeling Using Geometric Assignment
- **Arxiv ID**: http://arxiv.org/abs/1910.09976v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, math.DS, 62H35, 68U10, 68T05, 90C31, 62M45, 91A22
- **Links**: [PDF](http://arxiv.org/pdf/1910.09976v2)
- **Published**: 2019-10-22 13:45:42+00:00
- **Updated**: 2020-06-25 15:55:45+00:00
- **Authors**: Ruben Hühnerbein, Fabrizio Savarino, Stefania Petra, Christoph Schnörr
- **Comment**: None
- **Journal**: None
- **Summary**: We study the inverse problem of model parameter learning for pixelwise image labeling, using the linear assignment flow and training data with ground truth. This is accomplished by a Riemannian gradient flow on the manifold of parameters that determine the regularization properties of the assignment flow. Using the symplectic partitioned Runge--Kutta method for numerical integration, it is shown that deriving the sensitivity conditions of the parameter learning problem and its discretization commute. A convenient property of our approach is that learning is based on exact inference. Carefully designed experiments demonstrate the performance of our approach, the expressiveness of the mathematical model as well as its limitations, from the viewpoint of statistical learning and optimal control.



### Image recovery from rotational and translational invariants
- **Arxiv ID**: http://arxiv.org/abs/1910.10006v1
- **DOI**: 10.1109/ICASSP40776.2020.9053932
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10006v1)
- **Published**: 2019-10-22 14:33:59+00:00
- **Updated**: 2019-10-22 14:33:59+00:00
- **Authors**: Nicholas F. Marshall, Ti-Yen Lan, Tamir Bendory, Amit Singer
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: We introduce a framework for recovering an image from its rotationally and translationally invariant features based on autocorrelation analysis. This work is an instance of the multi-target detection statistical model, which is mainly used to study the mathematical and computational properties of single-particle reconstruction using cryo-electron microscopy (cryo-EM) at low signal-to-noise ratios. We demonstrate with synthetic numerical experiments that an image can be reconstructed from rotationally and translationally invariant features and show that the reconstruction is robust to noise. These results constitute an important step towards the goal of structure determination of small biomolecules using cryo-EM.



### Unsupervised Videographic Analysis of Rodent Behaviour
- **Arxiv ID**: http://arxiv.org/abs/1910.11065v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11065v2)
- **Published**: 2019-10-22 14:44:55+00:00
- **Updated**: 2019-10-26 01:06:20+00:00
- **Authors**: Anthony Bourached, Parashkev Nachev
- **Comment**: Resubmission with fixed typos and updated data source information
- **Journal**: None
- **Summary**: Animal behaviour is complex and the amount of data in the form of video, if extracted, is copious. Manual analysis of behaviour is massively limited by two insurmountable obstacles, the complexity of the behavioural patterns and human bias. Automated visual analysis has the potential to eliminate both of these issues and also enable continuous analysis allowing a much higher bandwidth of data collection which is vital to capture complex behaviour at many different time scales. Behaviour is not confined to a finite set modules and thus we can only model it by inferring the generative distribution. In this way unpredictable, anomalous behaviour may be considered. Here we present a method of unsupervised behavioural analysis from nothing but high definition video recordings taken from a single, fixed perspective. We demonstrate that the identification of stereotyped rodent behaviour can be extracted in this way.



### Vehicle detection and counting from VHR satellite images: efforts and open issues
- **Arxiv ID**: http://arxiv.org/abs/1910.10017v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10017v2)
- **Published**: 2019-10-22 14:53:04+00:00
- **Updated**: 2019-10-25 15:21:51+00:00
- **Authors**: Alice Froidevaux, Andréa Julier, Agustin Lifschitz, Minh-Tan Pham, Romain Dambreville, Sébastien Lefèvre, Pierre Lassalle, Thanh-Long Huynh
- **Comment**: 4 pages, planned for a conference submission
- **Journal**: None
- **Summary**: Detection of new infrastructures (commercial, logistics, industrial or residential) from satellite images constitutes a proven method to investigate and follow economic and urban growth. The level of activities or exploitation of these sites may be hardly determined by building inspection, but could be inferred from vehicle presence from nearby streets and parking lots. We present in this paper two deep learning-based models for vehicle counting from optical satellite images coming from the Pleiades sensor at 50-cm spatial resolution. Both segmentation (Tiramisu) and detection (YOLO) architectures were investigated. These networks were adapted, trained and validated on a data set including 87k vehicles, annotated using an interactive semi-automatic tool developed by the authors. Experimental results show that both segmentation and detection models could achieve a precision rate higher than 85% with a recall rate also high (76.4% and 71.9% for Tiramisu and YOLO respectively).



### Towards Automatic Annotation for Semantic Segmentation in Drone Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.10026v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10026v1)
- **Published**: 2019-10-22 15:01:58+00:00
- **Updated**: 2019-10-22 15:01:58+00:00
- **Authors**: Alina Marcu, Dragos Costea, Vlad Licaret, Marius Leordeanu
- **Comment**: 7 pages, 6 figures, submitted at the International Conference on
  Robotics and Automation (ICRA) 2020
- **Journal**: None
- **Summary**: Semantic segmentation is a crucial task for robot navigation and safety. However, it requires huge amounts of pixelwise annotations to yield accurate results. While recent progress in computer vision algorithms has been heavily boosted by large ground-level datasets, the labeling time has hampered progress in low altitude UAV applications, mostly due to the difficulty imposed by large object scales and pose variations. Motivated by the lack of a large video aerial dataset, we introduce a new one, with high resolution (4K) images and manually-annotated dense labels every 50 frames. To help the video labeling process, we make an important step towards automatic annotation and propose SegProp, an iterative flow-based method with geometric constrains to propagate the semantic labels to frames that lack human annotations. This results in a dataset with more than 50k annotated frames - the largest of its kind, to the best of our knowledge. Our experiments show that SegProp surpasses current state-of-the-art label propagation methods by a significant margin. Furthermore, when training a semantic segmentation deep neural net using the automatically annotated frames, we obtain a compelling overall performance boost at test time of 16.8% mean F-measure over a baseline trained only with manually-labeled frames.   Our Ruralscapes dataset, the label propagation code and a fast segmentation tool are available at our website: https://sites.google.com/site/aerialimageunderstanding/



### Human Action Recognition in Drone Videos using a Few Aerial Training Examples
- **Arxiv ID**: http://arxiv.org/abs/1910.10027v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.10027v4)
- **Published**: 2019-10-22 15:02:36+00:00
- **Updated**: 2021-04-02 12:33:37+00:00
- **Authors**: Waqas Sultani, Mubarak Shah
- **Comment**: CVIU, 2021
- **Journal**: None
- **Summary**: Drones are enabling new forms of human actions surveillance due to their low cost and fast mobility. However, using deep neural networks for automatic aerial action recognition is difficult due to the need for a large number of training aerial human action videos. Collecting a large number of human action aerial videos is costly, time-consuming, and difficult. In this paper, we explore two alternative data sources to improve aerial action classification when only a few training aerial examples are available. As a first data source, we resort to video games. We collect plenty of aerial game action videos using two gaming engines. For the second data source, we leverage conditional Wasserstein Generative Adversarial Networks to generate aerial features from ground videos. Given that both data sources have some limitations, e.g. game videos are biased towards specific actions categories (fighting, shooting, etc.,), and it is not easy to generate good discriminative GAN-generated features for all types of actions, we need to efficiently integrate two dataset sources with few available real aerial training videos. To address this challenge of the heterogeneous nature of the data, we propose to use a disjoint multitask learning framework. We feed the network with real and game, or real and GAN-generated data in an alternating fashion to obtain an improved action classifier. We validate the proposed approach on two aerial action datasets and demonstrate that features from aerial game videos and those generated from GAN can be extremely useful for an improved action recognition in real aerial videos when only a few real aerial training examples are available.



### Scanner Invariant Multiple Sclerosis Lesion Segmentation from MRI
- **Arxiv ID**: http://arxiv.org/abs/1910.10035v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10035v1)
- **Published**: 2019-10-22 15:11:18+00:00
- **Updated**: 2019-10-22 15:11:18+00:00
- **Authors**: Shahab Aslani, Vittorio Murino, Michael Dayan, Roger Tam, Diego Sona, Ghassan Hamarneh
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a simple and effective generalization method for magnetic resonance imaging (MRI) segmentation when data is collected from multiple MRI scanning sites and as a consequence is affected by (site-)domain shifts. We propose to integrate a traditional encoder-decoder network with a regularization network. This added network includes an auxiliary loss term which is responsible for the reduction of the domain shift problem and for the resulting improved generalization. The proposed method was evaluated on multiple sclerosis lesion segmentation from MRI data. We tested the proposed model on an in-house clinical dataset including 117 patients from 56 different scanning sites. In the experiments, our method showed better generalization performance than other baseline networks.



### Unsupervised particle sorting for high-resolution single-particle cryo-EM
- **Arxiv ID**: http://arxiv.org/abs/1910.10051v1
- **DOI**: 10.1088/1361-6420/ab5ec8
- **Categories**: **cs.CV**, eess.IV, stat.AP, 62P10, I.5.1; I.5.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1910.10051v1)
- **Published**: 2019-10-22 15:45:52+00:00
- **Updated**: 2019-10-22 15:45:52+00:00
- **Authors**: Ye Zhou, Amit Moscovich, Tamir Bendory, Alberto Bartesaghi
- **Comment**: 12 pages, 7 figures
- **Journal**: Inverse Problems 36:4 (2020)
- **Summary**: Single-particle cryo-Electron Microscopy (EM) has become a popular technique for determining the structure of challenging biomolecules that are inaccessible to other technologies. Recent advances in automation, both in data collection and data processing, have significantly lowered the barrier for non-expert users to successfully execute the structure determination workflow. Many critical data processing steps, however, still require expert user intervention in order to converge to the correct high-resolution structure. In particular, strategies to identify homogeneous populations of particles rely heavily on subjective criteria that are not always consistent or reproducible among different users. Here, we explore the use of unsupervised strategies for particle sorting that are compatible with the autonomous operation of the image processing pipeline. More specifically, we show that particles can be successfully sorted based on a simple statistical model for the distribution of scores assigned during refinement. This represents an important step towards the development of automated workflows for protein structure determination using single-particle cryo-EM.



### Attacking Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1910.10053v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10053v1)
- **Published**: 2019-10-22 15:47:56+00:00
- **Updated**: 2019-10-22 15:47:56+00:00
- **Authors**: Anurag Ranjan, Joel Janai, Andreas Geiger, Michael J. Black
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.



### Predictive Coding Networks Meet Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.10056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10056v1)
- **Published**: 2019-10-22 15:53:03+00:00
- **Updated**: 2019-10-22 15:53:03+00:00
- **Authors**: Xia Huang, Hossein Mousavi, Gemma Roig
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Action recognition is a key problem in computer vision that labels videos with a set of predefined actions. Capturing both, semantic content and motion, along the video frames is key to achieve high accuracy performance on this task. Most of the state-of-the-art methods rely on RGB frames for extracting the semantics and pre-computed optical flow fields as a motion cue. Then, both are combined using deep neural networks. Yet, it has been argued that such models are not able to leverage the motion information extracted from the optical flow, but instead the optical flow allows for better recognition of people and objects in the video. This urges the need to explore different cues or models that can extract motion in a more informative fashion. To tackle this issue, we propose to explore the predictive coding network, so called PredNet, a recurrent neural network that propagates predictive coding errors across layers and time steps. We analyze whether PredNet can better capture motions in videos by estimating over time the representations extracted from pre-trained networks for action recognition. In this way, the model only relies on the video frames, and does not need pre-processed optical flows as input. We report the effectiveness of our proposed model on UCF101 and HMDB51 datasets.



### Gaze360: Physically Unconstrained Gaze Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1910.10088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10088v1)
- **Published**: 2019-10-22 16:30:55+00:00
- **Updated**: 2019-10-22 16:30:55+00:00
- **Authors**: Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Matusik, Antonio Torralba
- **Comment**: International Conference in Computer Vision, 2019
- **Journal**: None
- **Summary**: Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting. Our dataset and models are available at http://gaze360.csail.mit.edu .



### Torchreid: A Library for Deep Learning Person Re-Identification in Pytorch
- **Arxiv ID**: http://arxiv.org/abs/1910.10093v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10093v1)
- **Published**: 2019-10-22 16:33:05+00:00
- **Updated**: 2019-10-22 16:33:05+00:00
- **Authors**: Kaiyang Zhou, Tao Xiang
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Person re-identification (re-ID), which aims to re-identify people across different camera views, has been significantly advanced by deep learning in recent years, particularly with convolutional neural networks (CNNs). In this paper, we present Torchreid, a software library built on PyTorch that allows fast development and end-to-end training and evaluation of deep re-ID models. As a general-purpose framework for person re-ID research, Torchreid provides (1) unified data loaders that support 15 commonly used re-ID benchmark datasets covering both image and video domains, (2) streamlined pipelines for quick development and benchmarking of deep re-ID models, and (3) implementations of the latest re-ID CNN architectures along with their pre-trained models to facilitate reproducibility as well as future research. With a high-level modularity in its design, Torchreid offers a great flexibility to allow easy extension to new datasets, CNN models and loss functions.



### Image processing in DNA
- **Arxiv ID**: http://arxiv.org/abs/1910.10095v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.10095v2)
- **Published**: 2019-10-22 16:34:04+00:00
- **Updated**: 2021-01-25 00:50:20+00:00
- **Authors**: Chao Pan, S. M. Hossein Tabatabaei Yazdi, S Kasra Tabatabaei, Alvaro G. Hernandez, Charles Schroeder, Olgica Milenkovic
- **Comment**: 5 pages, revision of ICASSP version
- **Journal**: None
- **Summary**: The main obstacles for the practical deployment of DNA-based data storage platforms are the prohibitively high cost of synthetic DNA and the large number of errors introduced during synthesis. In particular, synthetic DNA products contain both individual oligo (fragment) symbol errors as well as missing DNA oligo errors, with rates that exceed those of modern storage systems by orders of magnitude. These errors can be corrected either through the use of a large number of redundant oligos or through cycles of writing, reading, and rewriting of information that eliminate the errors. Both approaches add to the overall storage cost and are hence undesirable. Here we propose the first method for storing quantized images in DNA that uses signal processing and machine learning techniques to deal with error and cost issues without resorting to the use of redundant oligos or rewriting. Our methods rely on decoupling the RGB channels of images, performing specialized quantization and compression on the individual color channels, and using new discoloration detection and image inpainting techniques. We demonstrate the performance of our approach experimentally on a collection of movie posters stored in DNA.



### The Practicality of Stochastic Optimization in Imaging Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1910.10100v2
- **DOI**: 10.1109/TCI.2020.3032101
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10100v2)
- **Published**: 2019-10-22 16:39:00+00:00
- **Updated**: 2019-11-08 17:24:02+00:00
- **Authors**: Junqi Tang, Karen Egiazarian, Mohammad Golbabaee, Mike Davies
- **Comment**: None
- **Journal**: Published in IEEE Transactions on Computational Imaging, Vol. 6,
  2020
- **Summary**: In this work we investigate the practicality of stochastic gradient descent and recently introduced variants with variance-reduction techniques in imaging inverse problems. Such algorithms have been shown in the machine learning literature to have optimal complexities in theory, and provide great improvement empirically over the deterministic gradient methods. Surprisingly, in some tasks such as image deblurring, many of such methods fail to converge faster than the accelerated deterministic gradient methods, even in terms of epoch counts. We investigate this phenomenon and propose a theory-inspired mechanism for the practitioners to efficiently characterize whether it is beneficial for an inverse problem to be solved by stochastic optimization techniques or not. Using standard tools in numerical linear algebra, we derive conditions on the spectral structure of the inverse problem for being a suitable application of stochastic gradient methods. Particularly, we show that, for an imaging inverse problem, if and only if its Hessain matrix has a fast-decaying eigenspectrum, then the stochastic gradient methods can be more advantageous than deterministic methods for solving such a problem. Our results also provide guidance on choosing appropriately the partition minibatch schemes, showing that a good minibatch scheme typically has relatively low correlation within each of the minibatches. Finally, we propose an accelerated primal-dual SGD algorithm in order to tackle another key bottleneck of stochastic optimization which is the heavy computation of proximal operators. The proposed method has fast convergence rate in practice, and is able to efficiently handle non-smooth regularization terms which are coupled with linear operators.



### Beyond Human Parts: Dual Part-Aligned Representations for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1910.10111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10111v1)
- **Published**: 2019-10-22 16:53:41+00:00
- **Updated**: 2019-10-22 16:53:41+00:00
- **Authors**: Jianyuan Guo, Yuhui Yuan, Lang Huang, Chao Zhang, Jinge Yao, Kai Han
- **Comment**: Accepted in ICCV 2019
- **Journal**: None
- **Summary**: Person re-identification is a challenging task due to various complex factors. Recent studies have attempted to integrate human parsing results or externally defined attributes to help capture human parts or important object regions. On the other hand, there still exist many useful contextual cues that do not fall into the scope of predefined human parts or attributes. In this paper, we address the missed contextual cues by exploiting both the accurate human parts and the coarse non-human parts. In our implementation, we apply a human parsing model to extract the binary human part masks \emph{and} a self-attention mechanism to capture the soft latent (non-human) part masks. We verify the effectiveness of our approach with new state-of-the-art performances on three challenging benchmarks: Market-1501, DukeMTMC-reID and CUHK03. Our implementation is available at https://github.com/ggjy/P2Net.pytorch.



### Establishing an Evaluation Metric to Quantify Climate Change Image Realism
- **Arxiv ID**: http://arxiv.org/abs/1910.10143v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1910.10143v1)
- **Published**: 2019-10-22 17:59:38+00:00
- **Updated**: 2019-10-22 17:59:38+00:00
- **Authors**: Sharon Zhou, Alexandra Luccioni, Gautier Cosne, Michael S. Bernstein, Yoshua Bengio
- **Comment**: Accepted to the NeurIPS 2019 Workshop, Tackling Climate Change with
  Machine Learning
- **Journal**: None
- **Summary**: With success on controlled tasks, generative models are being increasingly applied to humanitarian applications [1,2]. In this paper, we focus on the evaluation of a conditional generative model that illustrates the consequences of climate change-induced flooding to encourage public interest and awareness on the issue. Because metrics for comparing the realism of different modes in a conditional generative model do not exist, we propose several automated and human-based methods for evaluation. To do this, we adapt several existing metrics, and assess the automated metrics against gold standard human evaluation. We find that using Fr\'echet Inception Distance (FID) with embeddings from an intermediary Inception-V3 layer that precedes the auxiliary classifier produces results most correlated with human realism. While insufficient alone to establish a human-correlated automatic evaluation metric, we believe this work begins to bridge the gap between human and automated generative evaluation procedures.



### Fast and Automatic Periacetabular Osteotomy Fragment Pose Estimation Using Intraoperatively Implanted Fiducials and Single-View Fluoroscopy
- **Arxiv ID**: http://arxiv.org/abs/1910.10187v3
- **DOI**: 10.1088/1361-6560/aba089
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10187v3)
- **Published**: 2019-10-22 18:15:44+00:00
- **Updated**: 2020-06-12 19:47:18+00:00
- **Authors**: Robert Grupp, Ryan Murphy, Rachel Hegeman, Clayton Alexander, Mathias Unberath, Yoshito Otake, Benjamin McArthur, Mehran Armand, Russell Taylor
- **Comment**: Revised article to address reviewer comments. Under review for
  Physics in Medicine and Biology. Supplementary video at
  https://youtu.be/0E0U9G81q8g
- **Journal**: 2020 Phys. Med. Biol. 65 245019
- **Summary**: Accurate and consistent mental interpretation of fluoroscopy to determine the position and orientation of acetabular bone fragments in 3D space is difficult. We propose a computer assisted approach that uses a single fluoroscopic view and quickly reports the pose of an acetabular fragment without any user input or initialization. Intraoperatively, but prior to any osteotomies, two constellations of metallic ball-bearings (BBs) are injected into the wing of a patient's ilium and lateral superior pubic ramus. One constellation is located on the expected acetabular fragment, and the other is located on the remaining, larger, pelvis fragment. The 3D locations of each BB are reconstructed using three fluoroscopic views and 2D/3D registrations to a preoperative CT scan of the pelvis. The relative pose of the fragment is established by estimating the movement of the two BB constellations using a single fluoroscopic view taken after osteotomy and fragment relocation. BB detection and inter-view correspondences are automatically computed throughout the processing pipeline. The proposed method was evaluated on a multitude of fluoroscopic images collected from six cadaveric surgeries performed bilaterally on three specimens. Mean fragment rotation error was 2.4 +/- 1.0 degrees, mean translation error was 2.1 +/- 0.6 mm, and mean 3D lateral center edge angle error was 1.0 +/- 0.5 degrees. The average runtime of the single-view pose estimation was 0.7 +/- 0.2 seconds. The proposed method demonstrates accuracy similar to other state of the art systems which require optical tracking systems or multiple-view 2D/3D registrations with manual input. The errors reported on fragment poses and lateral center edge angles are within the margins required for accurate intraoperative evaluation of femoral head coverage.



### Towards an Intelligent Microscope: adaptively learned illumination for optimal sample classification
- **Arxiv ID**: http://arxiv.org/abs/1910.10209v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.10209v2)
- **Published**: 2019-10-22 19:49:06+00:00
- **Updated**: 2020-02-13 22:26:55+00:00
- **Authors**: Amey Chaware, Colin L. Cooke, Kanghyun Kim, Roarke Horstmeyer
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Recent machine learning techniques have dramatically changed how we process digital images. However, the way in which we capture images is still largely driven by human intuition and experience. This restriction is in part due to the many available degrees of freedom that alter the image acquisition process (lens focus, exposure, filtering, etc). Here we focus on one such degree of freedom - illumination within a microscope - which can drastically alter information captured by the image sensor. We present a reinforcement learning system that adaptively explores optimal patterns to illuminate specimens for immediate classification. The agent uses a recurrent latent space to encode a large set of variably-illuminated samples and illumination patterns. We train our agent using a reward that balances classification confidence with image acquisition cost. By synthesizing knowledge over multiple snapshots, the agent can classify on the basis of all previous images with higher accuracy than from naively illuminated images, thus demonstrating a smarter way to physically capture task-specific information.



### Unsupervised Robust Disentangling of Latent Characteristics for Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1910.10223v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10223v1)
- **Published**: 2019-10-22 20:48:44+00:00
- **Updated**: 2019-10-22 20:48:44+00:00
- **Authors**: Patrick Esser, Johannes Haux, Björn Ommer
- **Comment**: ICCV 2019. Project page at
  https://compvis.github.io/robust-disentangling/
- **Journal**: None
- **Summary**: Deep generative models come with the promise to learn an explainable representation for visual objects that allows image sampling, synthesis, and selective modification. The main challenge is to learn to properly model the independent latent characteristics of an object, especially its appearance and pose. We present a novel approach that learns disentangled representations of these characteristics and explains them individually. Training requires only pairs of images depicting the same object appearance, but no pose annotations. We propose an additional classifier that estimates the minimal amount of regularization required to enforce disentanglement. Thus both representations together can completely explain an image while being independent of each other. Previous methods based on adversarial approaches fail to enforce this independence, while methods based on variational approaches lead to uninformative representations. In experiments on diverse object categories, the approach successfully recombines pose and appearance to reconstruct and retarget novel synthesized images. We achieve significant improvements over state-of-the-art methods which utilize the same level of supervision, and reach performances comparable to those of pose-supervised approaches. However, we can handle the vast body of articulated object classes for which no pose models/annotations are available.



### Genetic Programming for Evolving Similarity Functions for Clustering: Representations and Analysis
- **Arxiv ID**: http://arxiv.org/abs/1910.10264v1
- **DOI**: 10.1162/evco_a_00264
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10264v1)
- **Published**: 2019-10-22 22:45:19+00:00
- **Updated**: 2019-10-22 22:45:19+00:00
- **Authors**: Andrew Lensen, Bing Xue, Mengjie Zhang
- **Comment**: 29 pages, accepted by Evolutionary Computation (Journal), MIT Press
- **Journal**: None
- **Summary**: Clustering is a difficult and widely-studied data mining task, with many varieties of clustering algorithms proposed in the literature. Nearly all algorithms use a similarity measure such as a distance metric (e.g. Euclidean distance) to decide which instances to assign to the same cluster. These similarity measures are generally pre-defined and cannot be easily tailored to the properties of a particular dataset, which leads to limitations in the quality and the interpretability of the clusters produced. In this paper, we propose a new approach to automatically evolving similarity functions for a given clustering algorithm by using genetic programming. We introduce a new genetic programming-based method which automatically selects a small subset of features (feature selection) and then combines them using a variety of functions (feature construction) to produce dynamic and flexible similarity functions that are specifically designed for a given dataset. We demonstrate how the evolved similarity functions can be used to perform clustering using a graph-based representation. The results of a variety of experiments across a range of large, high-dimensional datasets show that the proposed approach can achieve higher and more consistent performance than the benchmark methods. We further extend the proposed approach to automatically produce multiple complementary similarity functions by using a multi-tree approach, which gives further performance improvements. We also analyse the interpretability and structure of the automatically evolved similarity functions to provide insight into how and why they are superior to standard distance metrics.




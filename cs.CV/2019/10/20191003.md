# Arxiv Papers in cs.CV on 2019-10-03
### ANDA: A Novel Data Augmentation Technique Applied to Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.01256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01256v1)
- **Published**: 2019-10-03 00:00:38+00:00
- **Updated**: 2019-10-03 00:00:38+00:00
- **Authors**: Daniel V. Ruiz, Bruno A. Krinski, Eduardo Todt
- **Comment**: Accepted for presentation at the International Conference on Advanced
  Robotics (ICAR) 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel data augmentation technique (ANDA) applied to the Salient Object Detection (SOD) context. Standard data augmentation techniques proposed in the literature, such as image cropping, rotation, flipping, and resizing, only generate variations of the existing examples, providing a limited generalization. Our method has the novelty of creating new images, by combining an object with a new background while retaining part of its salience in this new context; To do so, the ANDA technique relies on the linear combination between labeled salient objects and new backgrounds, generated by removing the original salient object in a process known as image inpainting. Our proposed technique allows for more precise control of the object's position and size while preserving background information. Aiming to evaluate our proposed method, we trained multiple deep neural networks and compared the effect that our technique has in each one. We also compared our method with other data augmentation techniques. Our findings show that depending on the network improvement can be up to 14.1% in the F-measure and decay of up to 2.6% in the Mean Absolute Error.



### Kidney Recognition in CT Using YOLOv3
- **Arxiv ID**: http://arxiv.org/abs/1910.01268v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01268v1)
- **Published**: 2019-10-03 01:26:18+00:00
- **Updated**: 2019-10-03 01:26:18+00:00
- **Authors**: Andréanne Lemay
- **Comment**: None
- **Journal**: None
- **Summary**: Organ localization can be challenging considering the heterogeneity of medical images and the biological diversity from one individual to another. The contribution of this paper is to overview the performance of the object detection model, YOLOv3, on kidney localization in 2D and in 3D from CT scans. The model obtained a 0.851 Dice score in 2D and 0.742 in 3D. The SSD, a similar state-of-the-art object detection model, showed similar scores on the test set. YOLOv3 and SSD demonstrated the ability to detect kidneys on a wide variety of CT scans including patients suffering from different renal conditions.



### Learning Point Embeddings from Shape Repositories for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.01269v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.01269v1)
- **Published**: 2019-10-03 01:26:31+00:00
- **Updated**: 2019-10-03 01:26:31+00:00
- **Authors**: Gopal Sharma, Evangelos Kalogerakis, Subhransu Maji
- **Comment**: None
- **Journal**: None
- **Summary**: User generated 3D shapes in online repositories contain rich information about surfaces, primitives, and their geometric relations, often arranged in a hierarchy. We present a framework for learning representations of 3D shapes that reflect the information present in this meta data and show that it leads to improved generalization for semantic segmentation tasks. Our approach is a point embedding network that generates a vectorial representation of the 3D points such that it reflects the grouping hierarchy and tag data. The main challenge is that the data is noisy and highly variable. To this end, we present a tree-aware metric-learning approach and demonstrate that such learned embeddings offer excellent transfer to semantic segmentation tasks, especially when training data is limited. Our approach reduces the relative error by $10.2\%$ with $8$ training examples, by $11.72\%$ with $120$ training examples on the ShapeNet semantic segmentation benchmark, in comparison to the network trained from scratch. By utilizing tag data the relative error is reduced by $12.8\%$ with $8$ training examples, in comparison to the network trained from scratch. These improvements come at no additional labeling cost as the meta data is freely available.



### YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.01271v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1910.01271v1)
- **Published**: 2019-10-03 01:29:26+00:00
- **Updated**: 2019-10-03 01:29:26+00:00
- **Authors**: Alexander Wong, Mahmoud Famuori, Mohammad Javad Shafiee, Francis Li, Brendan Chwyl, Jonathan Chung
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Object detection remains an active area of research in the field of computer vision, and considerable advances and successes has been achieved in this area through the design of deep convolutional neural networks for tackling object detection. Despite these successes, one of the biggest challenges to widespread deployment of such object detection networks on edge and mobile scenarios is the high computational and memory requirements. As such, there has been growing research interest in the design of efficient deep neural network architectures catered for edge and mobile usage. In this study, we introduce YOLO Nano, a highly compact deep convolutional neural network for the task of object detection. A human-machine collaborative design strategy is leveraged to create YOLO Nano, where principled network design prototyping, based on design principles from the YOLO family of single-shot object detection network architectures, is coupled with machine-driven design exploration to create a compact network with highly customized module-level macroarchitecture and microarchitecture designs tailored for the task of embedded object detection. The proposed YOLO Nano possesses a model size of ~4.0MB (>15.1x and >8.3x smaller than Tiny YOLOv2 and Tiny YOLOv3, respectively) and requires 4.57B operations for inference (>34% and ~17% lower than Tiny YOLOv2 and Tiny YOLOv3, respectively) while still achieving an mAP of ~69.1% on the VOC 2007 dataset (~12% and ~10.7% higher than Tiny YOLOv2 and Tiny YOLOv3, respectively). Experiments on inference speed and power efficiency on a Jetson AGX Xavier embedded module at different power budgets further demonstrate the efficacy of YOLO Nano for embedded scenarios.



### A Neural Network for Detailed Human Depth Estimation from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1910.01275v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01275v2)
- **Published**: 2019-10-03 01:54:22+00:00
- **Updated**: 2019-12-24 08:35:33+00:00
- **Authors**: Sicong Tang, Feitong Tan, Kelvin Cheng, Zhaoyang Li, Siyu Zhu, Ping Tan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a neural network to estimate a detailed depth map of the foreground human in a single RGB image. The result captures geometry details such as cloth wrinkles, which are important in visualization applications. To achieve this goal, we separate the depth map into a smooth base shape and a residual detail shape and design a network with two branches to regress them respectively. We design a training strategy to ensure both base and detail shapes can be faithfully learned by the corresponding network branches. Furthermore, we introduce a novel network layer to fuse a rough depth map and surface normals to further improve the final result. Quantitative comparison with fused `ground truth' captured by real depth cameras and qualitative examples on unconstrained Internet images demonstrate the strength of the proposed method. The code is available at https://github.com/sfu-gruvi-3dv/deep_human.



### Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.01279v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01279v2)
- **Published**: 2019-10-03 02:05:15+00:00
- **Updated**: 2020-04-13 10:41:36+00:00
- **Authors**: Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, Xia Hu
- **Comment**: Accepted to CVPR 2020: Workshop on Fair, Data Efficient and Trusted
  Computer Vision
- **Journal**: None
- **Summary**: Recently, increasing attention has been drawn to the internal mechanisms of convolutional neural networks, and the reason why the network makes specific decisions. In this paper, we develop a novel post-hoc visual explanation method called Score-CAM based on class activation mapping. Unlike previous class activation mapping based approaches, Score-CAM gets rid of the dependence on gradients by obtaining the weight of each activation map through its forward passing score on target class, the final result is obtained by a linear combination of weights and activation maps. We demonstrate that Score-CAM achieves better visual performance and fairness for interpreting the decision making process. Our approach outperforms previous methods on both recognition and localization tasks, it also passes the sanity check. We also indicate its application as debugging tools. Official code has been released.



### Learning Temporal Action Proposals With Fewer Labels
- **Arxiv ID**: http://arxiv.org/abs/1910.01286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01286v1)
- **Published**: 2019-10-03 02:54:04+00:00
- **Updated**: 2019-10-03 02:54:04+00:00
- **Authors**: Jingwei Ji, Kaidi Cao, Juan Carlos Niebles
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action proposals are a common module in action detection pipelines today. Most current methods for training action proposal modules rely on fully supervised approaches that require large amounts of annotated temporal action intervals in long video sequences. The large cost and effort in annotation that this entails motivate us to study the problem of training proposal modules with less supervision. In this work, we propose a semi-supervised learning algorithm specifically designed for training temporal action proposal networks. When only a small number of labels are available, our semi-supervised method generates significantly better proposals than the fully-supervised counterpart and other strong semi-supervised baselines. We validate our method on two challenging action detection video datasets, ActivityNet v1.3 and THUMOS14. We show that our semi-supervised approach consistently matches or outperforms the fully supervised state-of-the-art approaches.



### On the Efficacy of Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1910.01348v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01348v1)
- **Published**: 2019-10-03 08:14:13+00:00
- **Updated**: 2019-10-03 08:14:13+00:00
- **Authors**: Jang Hyun Cho, Bharath Hariharan
- **Comment**: 13 pages, including Appendix
- **Journal**: ICCV 2019
- **Summary**: In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don't make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher's training early. Our results generalize across datasets and models.



### Sit-to-Stand Analysis in the Wild using Silhouettes for Longitudinal Health Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1910.01370v1
- **DOI**: 10.1007/978-3-030-27272-2_15
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01370v1)
- **Published**: 2019-10-03 09:19:12+00:00
- **Updated**: 2019-10-03 09:19:12+00:00
- **Authors**: Alessandro Masullo, Tilo Burghardt, Toby Perrett, Dima Damen, Majid Mirmehdi
- **Comment**: None
- **Journal**: None
- **Summary**: We present the first fully automated Sit-to-Stand or Stand-to-Sit (StS) analysis framework for long-term monitoring of patients in free-living environments using video silhouettes. Our method adopts a coarse-to-fine time localisation approach, where a deep learning classifier identifies possible StS sequences from silhouettes, and a smart peak detection stage provides fine localisation based on 3D bounding boxes. We tested our method on data from real homes of participants and monitored patients undergoing total hip or knee replacement. Our results show 94.4% overall accuracy in the coarse localisation and an error of 0.026 m/s in the speed of ascent measurement, highlighting important trends in the recuperation of patients who underwent surgery.



### A Cryptanalysis of Two Cancelable Biometric Schemes based on Index-of-Max Hashing
- **Arxiv ID**: http://arxiv.org/abs/1910.01389v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01389v3)
- **Published**: 2019-10-03 10:28:02+00:00
- **Updated**: 2020-01-17 23:08:19+00:00
- **Authors**: Kevin Atighehchi, Loubna Ghammam, Koray Karabina, Patrick Lacharme
- **Comment**: Some revisions and addition of acknowledgements
- **Journal**: None
- **Summary**: Cancelable biometric schemes generate secure biometric templates by combining user specific tokens and biometric data. The main objective is to create irreversible, unlinkable, and revocable templates, with high accuracy in matching. In this paper, we cryptanalyze two recent cancelable biometric schemes based on a particular locality sensitive hashing function, index-of-max (IoM): Gaussian Random Projection-IoM (GRP-IoM) and Uniformly Random Permutation-IoM (URP-IoM). As originally proposed, these schemes were claimed to be resistant against reversibility, authentication, and linkability attacks under the stolen token scenario. We propose several attacks against GRP-IoM and URP-IoM, and argue that both schemes are severely vulnerable against authentication and linkability attacks. We also propose better, but not yet practical, reversibility attacks against GRP-IoM. The correctness and practical impact of our attacks are verified over the same dataset provided by the authors of these two schemes.



### Face Manifold: Manifold Learning for Synthetic Face Generation
- **Arxiv ID**: http://arxiv.org/abs/1910.01403v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01403v2)
- **Published**: 2019-10-03 11:00:17+00:00
- **Updated**: 2019-10-04 12:27:12+00:00
- **Authors**: Kimia Dinashi, Ramin Toosi, Mohammad Ali Akhaee
- **Comment**: None
- **Journal**: None
- **Summary**: Face is one of the most important things for communication with the world around us. It also forms our identity and expressions. Estimating the face structure is a fundamental task in computer vision with applications in different areas such as face recognition and medical surgeries. Recently, deep learning techniques achieved significant results for 3D face reconstruction from flat images. The main challenge of such techniques is a vital need for large 3D face datasets. Usually, this challenge is handled by synthetic face generation. However, synthetic datasets suffer from the existence of non-possible faces. Here, we propose a face manifold learning method for synthetic diverse face dataset generation. First, the face structure is divided into the shape and expression groups. Then, a fully convolutional autoencoder network is exploited to deal with the non-possible faces, and, simultaneously, preserving the dataset diversity. Simulation results show that the proposed method is capable of denoising highly corrupted faces. The diversity of the generated dataset is evaluated qualitatively and quantitatively and compared to the existing methods. Experiments show that our manifold learning method outperforms the state of the art methods significantly.



### A General Upper Bound for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1910.01409v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01409v2)
- **Published**: 2019-10-03 11:31:14+00:00
- **Updated**: 2019-10-04 07:40:18+00:00
- **Authors**: Dexuan Zhang, Tatsuya Harada
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: In this work, we present a novel upper bound of target error to address the problem for unsupervised domain adaptation. Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks. Furthermore, a theory proposed by Ben-David et al. (2010) provides a upper bound for target error when transferring the knowledge, which can be summarized as minimizing the source error and distance between marginal distributions simultaneously. However, common methods based on the theory usually ignore the joint error such that samples from different classes might be mixed together when matching marginal distribution. And in such case, no matter how we minimize the marginal discrepancy, the target error is not bounded due to an increasing joint error. To address this problem, we propose a general upper bound taking joint error into account, such that the undesirable case can be properly penalized. In addition, we utilize constrained hypothesis space to further formalize a tighter bound as well as a novel cross margin discrepancy to measure the dissimilarity between hypotheses which alleviates instability during adversarial learning. Extensive empirical evidence shows that our proposal outperforms related approaches in image classification error rates on standard domain adaptation benchmarks.



### Exploiting multi-CNN features in CNN-RNN based Dimensional Emotion Recognition on the OMG in-the-wild Dataset
- **Arxiv ID**: http://arxiv.org/abs/1910.01417v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01417v2)
- **Published**: 2019-10-03 11:56:41+00:00
- **Updated**: 2020-04-10 10:28:25+00:00
- **Authors**: Dimitrios Kollias, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel CNN-RNN based approach, which exploits multiple CNN features for dimensional emotion recognition in-the-wild, utilizing the One-Minute Gradual-Emotion (OMG-Emotion) dataset. Our approach includes first pre-training with the relevant and large in size, Aff-Wild and Aff-Wild2 emotion databases. Low-, mid- and high-level features are extracted from the trained CNN component and are exploited by RNN subnets in a multi-task framework. Their outputs constitute an intermediate level prediction; final estimates are obtained as the mean or median values of these predictions. Fusion of the networks is also examined for boosting the obtained performance, at Decision-, or at Model-level; in the latter case a RNN was used for the fusion. Our approach, although using only the visual modality, outperformed state-of-the-art methods that utilized audio and visual modalities. Some of our developments have been submitted to the OMG-Emotion Challenge, ranking second among the technologies which used only visual information for valence estimation; ranking third overall. Through extensive experimentation, we further show that arousal estimation is greatly improved when low-level features are combined with high-level ones.



### High-dimensional Dense Residual Convolutional Neural Network for Light Field Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1910.01426v4
- **DOI**: 10.1109/TPAMI.2019.2945027
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01426v4)
- **Published**: 2019-10-03 12:15:08+00:00
- **Updated**: 2020-09-17 05:04:29+00:00
- **Authors**: Nan Meng, Hayden K. -H. So, Xing Sun, Edmund Y. Lam
- **Comment**: 14 pages. IEEE Transactions on Pattern Analysis and Machine
  Intelligence (2019)
- **Journal**: None
- **Summary**: We consider the problem of high-dimensional light field reconstruction and develop a learning-based framework for spatial and angular super-resolution. Many current approaches either require disparity clues or restore the spatial and angular details separately. Such methods have difficulties with non-Lambertian surfaces or occlusions. In contrast, we formulate light field super-resolution (LFSR) as tensor restoration and develop a learning framework based on a two-stage restoration with 4-dimensional (4D) convolution. This allows our model to learn the features capturing the geometry information encoded in multiple adjacent views. Such geometric features vary near the occlusion regions and indicate the foreground object border. To train a feasible network, we propose a novel normalization operation based on a group of views in the feature maps, design a stage-wise loss function, and develop the multi-range training strategy to further improve the performance. Evaluations are conducted on a number of light field datasets including real-world scenes, synthetic data, and microscope light fields. The proposed method achieves superior performance and less execution time comparing with other state-of-the-art schemes.



### Vulnerability of Face Recognition to Deep Morphing
- **Arxiv ID**: http://arxiv.org/abs/1910.01933v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1910.01933v1)
- **Published**: 2019-10-03 12:34:08+00:00
- **Updated**: 2019-10-03 12:34:08+00:00
- **Authors**: Pavel Korshunov, Sébastien Marcel
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1812.08685
- **Journal**: None
- **Summary**: It is increasingly easy to automatically swap faces in images and video or morph two faces into one using generative adversarial networks (GANs). The high quality of the resulted deep-morph raises the question of how vulnerable the current face recognition systems are to such fake images and videos. It also calls for automated ways to detect these GAN-generated faces. In this paper, we present the publicly available dataset of the Deepfake videos with faces morphed with a GAN-based algorithm. To generate these videos, we used open source software based on GANs, and we emphasize that training and blending parameters can significantly impact the quality of the resulted videos. We show that the state of the art face recognition systems based on VGG and Facenet neural networks are vulnerable to the deep morph videos, with 85.62 and 95.00 false acceptance rates, respectively, which means methods for detecting these videos are necessary. We consider several baseline approaches for detecting deep morphs and find that the method based on visual quality metrics (often used in presentation attack detection domain) leads to the best performance with 8.97 equal error rate. Our experiments demonstrate that GAN-generated deep morph videos are challenging for both face recognition systems and existing detection methods, and the further development of deep morphing technologies will make it even more so.



### CLEVRER: CoLlision Events for Video REpresentation and Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1910.01442v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.01442v2)
- **Published**: 2019-10-03 13:16:36+00:00
- **Updated**: 2020-03-08 00:09:07+00:00
- **Authors**: Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, Joshua B. Tenenbaum
- **Comment**: The first two authors contributed equally to this work. Accepted as
  Oral Spotlight as ICLR 2020. Project page: http://clevrer.csail.mit.edu/
- **Journal**: None
- **Summary**: The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER), a diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks. Motivated by the theory of human casual judgment, CLEVRER includes four types of questions: descriptive (e.g., "what color"), explanatory ("what is responsible for"), predictive ("what will happen next"), and counterfactual ("what if"). We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations.



### 3D Neighborhood Convolution: Learning Depth-Aware Features for RGB-D and RGB Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.01460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01460v1)
- **Published**: 2019-10-03 13:34:04+00:00
- **Updated**: 2019-10-03 13:34:04+00:00
- **Authors**: Yunlu Chen, Thomas Mensink, Efstratios Gavves
- **Comment**: None
- **Journal**: None
- **Summary**: A key challenge for RGB-D segmentation is how to effectively incorporate 3D geometric information from the depth channel into 2D appearance features. We propose to model the effective receptive field of 2D convolution based on the scale and locality from the 3D neighborhood. Standard convolutions are local in the image space ($u, v$), often with a fixed receptive field of 3x3 pixels. We propose to define convolutions local with respect to the corresponding point in the 3D real-world space ($x, y, z$), where the depth channel is used to adapt the receptive field of the convolution, which yields the resulting filters invariant to scale and focusing on the certain range of depth. We introduce 3D Neighborhood Convolution (3DN-Conv), a convolutional operator around 3D neighborhoods. Further, we can use estimated depth to use our RGB-D based semantic segmentation model from RGB input. Experimental results validate that our proposed 3DN-Conv operator improves semantic segmentation, using either ground-truth depth (RGB-D) or estimated depth (RGB).



### Face Reflectance and Geometry Modeling via Differentiable Ray Tracing
- **Arxiv ID**: http://arxiv.org/abs/1910.05200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1910.05200v1)
- **Published**: 2019-10-03 13:39:33+00:00
- **Updated**: 2019-10-03 13:39:33+00:00
- **Authors**: Abdallah Dib, Gaurav Bharaj, Junghyun Ahn, Cedric Thebault, Philippe-Henri Gosselin, Louis Chevallier
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel strategy to automatically reconstruct 3D faces from monocular images with explicitly disentangled facial geometry (pose, identity and expression), reflectance (diffuse and specular albedo), and self-shadows. The scene lights are modeled as a virtual light stage with pre-oriented area lights used in conjunction with differentiable Monte-Carlo ray tracing to optimize the scene and face parameters. With correctly disentangled self-shadows and specular reflection parameters, we can not only obtain robust facial geometry reconstruction, but also gain explicit control over these parameters, with several practical applications. We can change facial expressions with accurate resultant self-shadows or relight the scene and obtain accurate specular reflection and several other parameter combinations.



### Regularizing Neural Networks via Stochastic Branch Layers
- **Arxiv ID**: http://arxiv.org/abs/1910.01467v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01467v1)
- **Published**: 2019-10-03 13:44:55+00:00
- **Updated**: 2019-10-03 13:44:55+00:00
- **Authors**: Wonpyo Park, Paul Hongsuck Seo, Bohyung Han, Minsu Cho
- **Comment**: ACML 2019 (oral)
- **Journal**: None
- **Summary**: We introduce a novel stochastic regularization technique for deep neural networks, which decomposes a layer into multiple branches with different parameters and merges stochastically sampled combinations of the outputs from the branches during training. Since the factorized branches can collapse into a single branch through a linear operation, inference requires no additional complexity compared to the ordinary layers. The proposed regularization method, referred to as StochasticBranch, is applicable to any linear layers such as fully-connected or convolution layers. The proposed regularizer allows the model to explore diverse regions of the model parameter space via multiple combinations of branches to find better local minima. An extensive set of experiments shows that our method effectively regularizes networks and further improves the generalization performance when used together with other existing regularization techniques.



### A Stereo Algorithm for Thin Obstacles and Reflective Objects
- **Arxiv ID**: http://arxiv.org/abs/1910.04874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.04874v1)
- **Published**: 2019-10-03 14:37:56+00:00
- **Updated**: 2019-10-03 14:37:56+00:00
- **Authors**: John Keller, Sebastian Scherer
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Stereo cameras are a popular choice for obstacle avoidance for outdoor lighweight, low-cost robotics applications. However, they are unable to sense thin and reflective objects well. Currently, many algorithms are tuned to perform well on indoor scenes like the Middlebury dataset. When navigating outdoors, reflective objects, like windows and glass, and thin obstacles, like wires, are not well handled by most stereo disparity algorithms. Reflections, repeating patterns and objects parallel to the cameras' baseline causes mismatches between image pairs which leads to bad disparity estimates. Thin obstacles are difficult for many sliding window based disparity methods to detect because they do not take up large portions of the pixels in the sliding window. We use a trinocular camera setup and micropolarizer camera capable of detecting reflective objects to overcome these issues. We present a hierarchical disparity algorithm that reduces noise, separately identify wires using semantic object triangulation in three images, and use information about the polarization of light to estimate the disparity of reflective objects. We evaluate our approach on outdoor data that we collected. Our method contained an average of 9.27% of bad pixels compared to a typical stereo algorithm's 18.4% of bad pixels in scenes containing reflective objects. Our trinocular and semantic wire disparity methods detected 53% of wire pixels, whereas a typical two camera stereo algorithm detected 5%.



### Incremental learning for the detection and classification of GAN-generated images
- **Arxiv ID**: http://arxiv.org/abs/1910.01568v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01568v2)
- **Published**: 2019-10-03 16:14:57+00:00
- **Updated**: 2019-10-06 18:47:26+00:00
- **Authors**: Francesco Marra, Cristiano Saltori, Giulia Boato, Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: Current developments in computer vision and deep learning allow to automatically generate hyper-realistic images, hardly distinguishable from real ones. In particular, human face generation achieved a stunning level of realism, opening new opportunities for the creative industry but, at the same time, new scary scenarios where such content can be maliciously misused. Therefore, it is essential to develop innovative methodologies to automatically tell apart real from computer generated multimedia, possibly able to follow the evolution and continuous improvement of data in terms of quality and realism. In the last few years, several deep learning-based solutions have been proposed for this problem, mostly based on Convolutional Neural Networks (CNNs). Although results are good in controlled conditions, it is not clear how such proposals can adapt to real-world scenarios, where learning needs to continuously evolve as new types of generated data appear. In this work, we tackle this problem by proposing an approach based on incremental learning for the detection and classification of GAN-generated images. Experiments on a dataset comprising images generated by several GAN-based architectures show that the proposed method is able to correctly perform discrimination when new GANs are presented to the network



### Improving Limited Angle CT Reconstruction with a Robust GAN Prior
- **Arxiv ID**: http://arxiv.org/abs/1910.01634v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01634v4)
- **Published**: 2019-10-03 17:52:14+00:00
- **Updated**: 2020-01-29 17:40:27+00:00
- **Authors**: Rushil Anirudh, Hyojin Kim, Jayaraman J. Thiagarajan, K. Aditya Mohan, Kyle M. Champley
- **Comment**: NeurIPS 2019 Workshop on Deep Inverse Problems
- **Journal**: None
- **Summary**: Limited angle CT reconstruction is an under-determined linear inverse problem that requires appropriate regularization techniques to be solved. In this work we study how pre-trained generative adversarial networks (GANs) can be used to clean noisy, highly artifact laden reconstructions from conventional techniques, by effectively projecting onto the inferred image manifold. In particular, we use a robust version of the popularly used GAN prior for inverse problems, based on a recent technique called corruption mimicking, that significantly improves the reconstruction quality. The proposed approach operates in the image space directly, as a result of which it does not need to be trained or require access to the measurement model, is scanner agnostic, and can work over a wide range of sensing scenarios.



### Self-supervised learning for autonomous vehicles perception: A conciliation between analytical and learning methods
- **Arxiv ID**: http://arxiv.org/abs/1910.01636v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01636v2)
- **Published**: 2019-10-03 17:56:18+00:00
- **Updated**: 2020-06-07 21:03:21+00:00
- **Authors**: Florent Chiaroni, Mohamed-Cherif Rahal, Nicolas Hueber, Frederic Dufaux
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, supervised deep learning techniques yield the best state-of-the-art prediction performances for a wide variety of computer vision tasks. However, such supervised techniques generally require a large amount of manually labeled training data. In the context of autonomous vehicles perception, this requirement is critical, as the distribution of sensor data can continuously change and include several unexpected variations. It turns out that a category of learning techniques, referred to as self-supervised learning (SSL), consists of replacing the manual labeling effort by an automatic labeling process. Thanks to their ability to learn on the application time and in varying environments, state-of-the-art SSL techniques provide a valid alternative to supervised learning for a variety of different tasks, including long-range traversable area segmentation, moving obstacle instance segmentation, long-term moving obstacle tracking, or depth map prediction. In this tutorial-style article, we present an overview and a general formalization of the concept of self-supervised learning (SSL) for autonomous vehicles perception. This formalization provides helpful guidelines for developing novel frameworks based on generic SSL principles. Moreover, it enables to point out significant challenges in the design of future SSL systems.



### Information based Deep Clustering: An experimental study
- **Arxiv ID**: http://arxiv.org/abs/1910.01665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01665v2)
- **Published**: 2019-10-03 18:07:57+00:00
- **Updated**: 2019-12-11 01:14:25+00:00
- **Authors**: Jizong Peng, Christian Desrosiers, Marco Pedersoli
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, two methods have shown outstanding performance for clustering images and jointly learning the feature representation. The first, called Information Maximiz-ing Self-Augmented Training (IMSAT), maximizes the mutual information between input and clusters while using a regularization term based on virtual adversarial examples. The second, named Invariant Information Clustering (IIC), maximizes the mutual information between the clustering of a sample and its geometrically transformed version. These methods use mutual information in distinct ways and leverage different kinds of transformations. This work proposes a comprehensive analysis of transformation and losses for deep clustering, where we compare numerous combinations of these two components and evaluate how they interact with one another. Results suggest that mutual information between a sample and its transformed representation leads to state-of-the-art performance for deep clustering, especially when used jointly with geometrical and adversarial transformations.



### Exploring Generative Physics Models with Scientific Priors in Inertial Confinement Fusion
- **Arxiv ID**: http://arxiv.org/abs/1910.01666v1
- **DOI**: None
- **Categories**: **physics.comp-ph**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01666v1)
- **Published**: 2019-10-03 18:08:31+00:00
- **Updated**: 2019-10-03 18:08:31+00:00
- **Authors**: Rushil Anirudh, Jayaraman J. Thiagarajan, Shusen Liu, Peer-Timo Bremer, Brian K. Spears
- **Comment**: Machine Learning for Physical Sciences Workshop at NeurIPS 2019
- **Journal**: None
- **Summary**: There is significant interest in using modern neural networks for scientific applications due to their effectiveness in modeling highly complex, non-linear problems in a data-driven fashion. However, a common challenge is to verify the scientific plausibility or validity of outputs predicted by a neural network. This work advocates the use of known scientific constraints as a lens into evaluating, exploring, and understanding such predictions for the problem of inertial confinement fusion.



### 1-point RANSAC for Circular Motion Estimation in Computed Tomography (CT)
- **Arxiv ID**: http://arxiv.org/abs/1910.01681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01681v1)
- **Published**: 2019-10-03 18:48:32+00:00
- **Updated**: 2019-10-03 18:48:32+00:00
- **Authors**: Mikhail O. Chekanov, Oleg S. Shipitko, Egor I. Ershov
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a RANSAC-based algorithm for determining the axial rotation angle of an object from a pair of its tomographic projections. An equation is derived for calculating the rotation angle using one correct keypoints correspondence of two tomographic projections. The proposed algorithm consists of the following steps: keypoints detection and matching, rotation angle estimation for each correspondence, outliers filtering with the RANSAC algorithm, finally, calculation of the desired angle by minimizing the re-projection error from the remaining correspondences. To validate the proposed method an experimental comparison against methods based on analysis of the distribution of the angles computed from all correspondences is conducted.



### Time-Dependent Deep Image Prior for Dynamic MRI
- **Arxiv ID**: http://arxiv.org/abs/1910.01684v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.01684v2)
- **Published**: 2019-10-03 18:55:47+00:00
- **Updated**: 2021-01-25 15:47:01+00:00
- **Authors**: Jaejun Yoo, Kyong Hwan Jin, Harshit Gupta, Jerome Yerly, Matthias Stuber, Michael Unser
- **Comment**: 11 pages, 6 figures. First Author has been changed
- **Journal**: None
- **Summary**: We propose a novel unsupervised deep-learning-based algorithm for dynamic magnetic resonance imaging (MRI) reconstruction. Dynamic MRI requires rapid data acquisition for the study of moving organs such as the heart. Existing reconstruction methods suffer from restrictions either in the model design or in the absence of ground-truth data, resulting in low image quality. We introduce a generalized version of the deep-image-prior approach, which optimizes the network weights to fit a sequence of sparsely acquired dynamic MRI measurements. Our method needs neither prior training nor additional data. In particular, for cardiac images, it does not require the marking of heartbeats or the reordering of spokes. The key ingredients of our method are threefold: 1) a fixed low-dimensional manifold that encodes the temporal variations of images; 2) a network that maps the manifold into a more expressive latent space; and 3) a convolutional neural network that generates a dynamic series of MRI images from the latent variables and that favors their consistency with the measurements in k-space. Our method outperforms the state-of-the-art methods quantitatively and qualitatively in both retrospective and real fetal cardiac datasets. To the best of our knowledge, this is the first unsupervised deep-learning-based method that can reconstruct the continuous variation of dynamic MRI sequences with high spatial resolution.



### Fluid Flow Mass Transport for Generative Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.01694v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01694v2)
- **Published**: 2019-10-03 19:14:52+00:00
- **Updated**: 2019-10-07 20:04:45+00:00
- **Authors**: Jingrong Lin, Keegan Lensink, Eldad Haber
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks have been shown to be powerful in generating content. To this end, they have been studied intensively in the last few years. Nonetheless, training these networks requires solving a saddle point problem that is difficult to solve and slowly converging. Motivated from techniques in the registration of point clouds and by the fluid flow formulation of mass transport, we investigate a new formulation that is based on strict minimization, without the need for the maximization. The formulation views the problem as a matching problem rather than an adversarial one and thus allows us to quickly converge and obtain meaningful metrics in the optimization path.



### Low-cost LIDAR based Vehicle Pose Estimation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1910.01701v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01701v1)
- **Published**: 2019-10-03 19:38:04+00:00
- **Updated**: 2019-10-03 19:38:04+00:00
- **Authors**: Chen Fu, Chiyu Dong, Xiao Zhang, John M. Dolan
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting surrounding vehicles by low-cost LIDAR has been drawing enormous attention. In low-cost LIDAR, vehicles present a multi-layer L-Shape. Based on our previous optimization/criteria-based L-Shape fitting algorithm, we here propose a data-driven and model-based method for robust vehicle segmentation and tracking. The new method uses T-linkage RANSAC to take a limited amount of noisy data and performs a robust segmentation for a moving car against noise. Compared with our previous method, T-Linkage RANSAC is more tolerant of observation uncertainties, i.e., the number of sides of the target being observed, and gets rid of the L-Shape assumption. In addition, a vehicle tracking system with Multi-Model Association (MMA) is built upon the segmentation result, which provides smooth trajectories of tracked objects. A manually labeled dataset from low-cost multi-layer LIDARs for validation will also be released with the paper. Experiments on the dataset show that the new approach outperforms previous ones based on multiple criteria. The new algorithm can also run in real-time.



### 360-Indoor: Towards Learning Real-World Objects in 360° Indoor Equirectangular Images
- **Arxiv ID**: http://arxiv.org/abs/1910.01712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01712v1)
- **Published**: 2019-10-03 20:36:49+00:00
- **Updated**: 2019-10-03 20:36:49+00:00
- **Authors**: Shih-Han Chou, Cheng Sun, Wen-Yen Chang, Wan-Ting Hsu, Min Sun, Jianlong Fu
- **Comment**: None
- **Journal**: None
- **Summary**: While there are several widely used object detection datasets, current computer vision algorithms are still limited in conventional images. Such images narrow our vision in a restricted region. On the other hand, 360{\deg} images provide a thorough sight. In this paper, our goal is to provide a standard dataset to facilitate the vision and machine learning communities in 360{\deg} domain. To facilitate the research, we present a real-world 360{\deg} panoramic object detection dataset, 360-Indoor, which is a new benchmark for visual object detection and class recognition in 360{\deg} indoor images. It is achieved by gathering images of complex indoor scenes containing common objects and the intensive annotated bounding field-of-view. In addition, 360-Indoor has several distinct properties: (1) the largest category number (37 labels in total). (2) the most complete annotations on average (27 bounding boxes per image). The selected 37 objects are all common in indoor scene. With around 3k images and 90k labels in total, 360-Indoor achieves the largest dataset for detection in 360{\deg} images. In the end, extensive experiments on the state-of-the-art methods for both classification and detection are provided. We will release this dataset in the near future.



### On the Detection of Digital Face Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1910.01717v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01717v5)
- **Published**: 2019-10-03 20:51:47+00:00
- **Updated**: 2020-10-24 01:41:08+00:00
- **Authors**: Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, Anil Jain
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Detecting manipulated facial images and videos is an increasingly important topic in digital media forensics. As advanced face synthesis and manipulation methods are made available, new types of fake face representations are being created which have raised significant concerns for their use in social media. Hence, it is crucial to detect manipulated face images and localize manipulated regions. Instead of simply using multi-task learning to simultaneously detect manipulated images and predict the manipulated mask (regions), we propose to utilize an attention mechanism to process and improve the feature maps for the classification task. The learned attention maps highlight the informative regions to further improve the binary classification (genuine face v. fake face), and also visualize the manipulated regions. To enable our study of manipulated face detection and localization, we collect a large-scale database that contains numerous types of facial forgeries. With this dataset, we perform a thorough analysis of data-driven fake face detection. We show that the use of an attention mechanism improves facial forgery detection and manipulated region localization.



### Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1911.05609v1
- **DOI**: 10.1145/3363560
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05609v1)
- **Published**: 2019-10-03 21:22:47+00:00
- **Updated**: 2019-10-03 21:22:47+00:00
- **Authors**: Sicheng Zhao, Shangfei Wang, Mohammad Soleymani, Dhiraj Joshi, Qiang Ji
- **Comment**: Accepted by ACM TOMM
- **Journal**: None
- **Summary**: The wide popularity of digital photography and social networks has generated a rapidly growing volume of multimedia data (i.e., image, music, and video), resulting in a great demand for managing, retrieving, and understanding these data. Affective computing (AC) of these data can help to understand human behaviors and enable wide applications. In this article, we survey the state-of-the-art AC technologies comprehensively for large-scale heterogeneous multimedia data. We begin this survey by introducing the typical emotion representation models from psychology that are widely employed in AC. We briefly describe the available datasets for evaluating AC algorithms. We then summarize and compare the representative methods on AC of different multimedia types, i.e., images, music, videos, and multimodal data, with the focus on both handcrafted features-based methods and deep learning methods. Finally, we discuss some challenges and future directions for multimedia affective computing.



### Causal Induction from Visual Observations for Goal Directed Tasks
- **Arxiv ID**: http://arxiv.org/abs/1910.01751v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01751v1)
- **Published**: 2019-10-03 22:32:40+00:00
- **Updated**: 2019-10-03 22:32:40+00:00
- **Authors**: Suraj Nair, Yuke Zhu, Silvio Savarese, Li Fei-Fei
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Causal reasoning has been an indispensable capability for humans and other intelligent animals to interact with the physical world. In this work, we propose to endow an artificial agent with the capability of causal reasoning for completing goal-directed tasks. We develop learning-based approaches to inducing causal knowledge in the form of directed acyclic graphs, which can be used to contextualize a learned goal-conditional policy to perform tasks in novel environments with latent causal structures. We leverage attention mechanisms in our causal induction model and goal-conditional policy, enabling us to incrementally generate the causal graph from the agent's visual observations and to selectively use the induced graph for determining actions. Our experiments show that our method effectively generalizes towards completing new tasks in novel environments with previously unseen causal structures.



### Training Multiscale-CNN for Large Microscopy Image Classification in One Hour
- **Arxiv ID**: http://arxiv.org/abs/1910.04852v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04852v2)
- **Published**: 2019-10-03 22:33:48+00:00
- **Updated**: 2020-03-10 19:35:44+00:00
- **Authors**: Kushal Datta, Imtiaz Hossain, Sun Choi, Vikram Saletore, Kyle Ambert, William J. Godinez, Xian Zhang
- **Comment**: 15 pages, 10 figures
- **Journal**: Workshop on Scalable Data Analytics in Scientific Computing,
  International SuperComputing 2019, Frankfurt, Germany
- **Summary**: Existing approaches to train neural networks that use large images require to either crop or down-sample data during pre-processing, use small batch sizes, or split the model across devices mainly due to the prohibitively limited memory capacity available on GPUs and emerging accelerators. These techniques often lead to longer time to convergence or time to train (TTT), and in some cases, lower model accuracy. CPUs, on the other hand, can leverage significant amounts of memory. While much work has been done on parallelizing neural network training on multiple CPUs, little attention has been given to tune neural network training with large images on CPUs. In this work, we train a multi-scale convolutional neural network (M-CNN) to classify large biomedical images for high content screening in one hour. The ability to leverage large memory capacity on CPUs enables us to scale to larger batch sizes without having to crop or down-sample the input images. In conjunction with large batch sizes, we find a generalized methodology of linearly scaling of learning rate and train M-CNN to state-of-the-art (SOTA) accuracy of 99% within one hour. We achieve fast time to convergence using 128 two socket Intel Xeon 6148 processor nodes with 192GB DDR4 memory connected with 100Gbps Intel Omnipath architecture.




# Arxiv Papers in cs.CV on 2019-10-11
### Deep Learning for Prostate Pathology
- **Arxiv ID**: http://arxiv.org/abs/1910.04918v3
- **DOI**: None
- **Categories**: **q-bio.TO**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04918v3)
- **Published**: 2019-10-11 00:10:59+00:00
- **Updated**: 2019-10-16 00:14:28+00:00
- **Authors**: Okyaz Eminaga, Yuri Tolkach, Christian Kunder, Mahmood Abbas, Ryan Han, Rosalie Nolley, Axel Semjonow, Martin Boegemann, Sebastian Huss, Andreas Loening, Robert West, Geoffrey Sonn, Richard Fan, Olaf Bettendorf, James Brook, Daniel Rubin
- **Comment**: None
- **Journal**: None
- **Summary**: The current study detects different morphologies related to prostate pathology using deep learning models; these models were evaluated on 2,121 hematoxylin and eosin (H&E) stain histology images captured using bright field microscopy, which spanned a variety of image qualities, origins (whole slide, tissue micro array, whole mount, Internet), scanning machines, timestamps, H&E staining protocols, and institutions. For case usage, these models were applied for the annotation tasks in clinician-oriented pathology reports for prostatectomy specimens. The true positive rate (TPR) for slides with prostate cancer was 99.7% by a false positive rate of 0.785%. The F1-scores of Gleason patterns reported in pathology reports ranged from 0.795 to 1.0 at the case level. TPR was 93.6% for the cribriform morphology and 72.6% for the ductal morphology. The correlation between the ground truth and the prediction for the relative tumor volume was 0.987 n. Our models cover the major components of prostate pathology and successfully accomplish the annotation tasks.



### From Species to Cultivar: Soybean Cultivar Recognition using Multiscale Sliding Chord Matching of Leaf Images
- **Arxiv ID**: http://arxiv.org/abs/1910.04919v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04919v1)
- **Published**: 2019-10-11 00:21:43+00:00
- **Updated**: 2019-10-11 00:21:43+00:00
- **Authors**: Bin Wang, Yongsheng Gao, Xiaohan Yu, Xiaohui Yuan, Shengwu Xiong, Xianzhong Feng
- **Comment**: 33 pages, 8 figures
- **Journal**: None
- **Summary**: Leaf image recognition techniques have been actively researched for plant species identification. However it remains unclear whether leaf patterns can provide sufficient information for cultivar recognition. This paper reports the first attempt on soybean cultivar recognition from plant leaves which is not only a challenging research problem but also important for soybean cultivar evaluation, selection and production in agriculture. In this paper, we propose a novel multiscale sliding chord matching (MSCM) approach to extract leaf patterns that are distinctive for soybean cultivar identification. A chord is defined to slide along the contour for measuring the synchronised patterns of exterior shape and interior appearance of soybean leaf images. A multiscale sliding chord strategy is developed to extract features in a coarse-to-fine hierarchical order. A joint description that integrates the leaf descriptors from different parts of a soybean plant is proposed for further enhancing the discriminative power of cultivar description. We built a cultivar leaf image database, SoyCultivar, consisting of 1200 sample leaf images from 200 soybean cultivars for performance evaluation. Encouraging experimental results of the proposed method in comparison to the state-of-the-art leaf species recognition methods demonstrate the availability of cultivar information in soybean leaves and effectiveness of the proposed MSCM for soybean cultivar identification, which may advance the research in leaf recognition from species to cultivar.



### DiabDeep: Pervasive Diabetes Diagnosis based on Wearable Medical Sensors and Efficient Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.04925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04925v1)
- **Published**: 2019-10-11 01:04:38+00:00
- **Updated**: 2019-10-11 01:04:38+00:00
- **Authors**: Hongxu Yin, Bilal Mukadam, Xiaoliang Dai, Niraj K. Jha
- **Comment**: None
- **Journal**: None
- **Summary**: Diabetes impacts the quality of life of millions of people. However, diabetes diagnosis is still an arduous process, given that the disease develops and gets treated outside the clinic. The emergence of wearable medical sensors (WMSs) and machine learning points to a way forward to address this challenge. WMSs enable a continuous mechanism to collect and analyze physiological signals. However, disease diagnosis based on WMS data and its effective deployment on resource-constrained edge devices remain challenging due to inefficient feature extraction and vast computation cost. In this work, we propose a framework called DiabDeep that combines efficient neural networks (called DiabNNs) with WMSs for pervasive diabetes diagnosis. DiabDeep bypasses the feature extraction stage and acts directly on WMS data. It enables both an (i) accurate inference on the server, e.g., a desktop, and (ii) efficient inference on an edge device, e.g., a smartphone, based on varying design goals and resource budgets. On the server, we stack sparsely connected layers to deliver high accuracy. On the edge, we use a hidden-layer long short-term memory based recurrent layer to cut down on computation and storage. At the core of DiabDeep lies a grow-and-prune training flow: it leverages gradient-based growth and magnitude-based pruning algorithms to learn both weights and connections for DiabNNs. We demonstrate the effectiveness of DiabDeep through analyzing data from 52 participants. For server (edge) side inference, we achieve a 96.3% (95.3%) accuracy in classifying diabetics against healthy individuals, and a 95.7% (94.6%) accuracy in distinguishing among type-1/type-2 diabetic, and healthy individuals. Against conventional baselines, DiabNNs achieve higher accuracy, while reducing the model size (FLOPs) by up to 454.5x (8.9x). Therefore, the system can be viewed as pervasive and efficient, yet very accurate.



### FetusMap: Fetal Pose Estimation in 3D Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1910.04935v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04935v1)
- **Published**: 2019-10-11 01:45:09+00:00
- **Updated**: 2019-10-11 01:45:09+00:00
- **Authors**: Xin Yang, Wenlong Shi, Haoran Dou, Jikuan Qian, Yi Wang, Wufeng Xue, Shengli Li, Dong Ni, Pheng-Ann Heng
- **Comment**: 9 pages, 6 figures, 2 tables. Accepted by MICCAI 2019
- **Journal**: None
- **Summary**: The 3D ultrasound (US) entrance inspires a multitude of automated prenatal examinations. However, studies about the structuralized description of the whole fetus in 3D US are still rare. In this paper, we propose to estimate the 3D pose of fetus in US volumes to facilitate its quantitative analyses in global and local scales. Given the great challenges in 3D US, including the high volume dimension, poor image quality, symmetric ambiguity in anatomical structures and large variations of fetal pose, our contribution is three-fold. (i) This is the first work about 3D pose estimation of fetus in the literature. We aim to extract the skeleton of whole fetus and assign different segments/joints with correct torso/limb labels. (ii) We propose a self-supervised learning (SSL) framework to finetune the deep network to form visually plausible pose predictions. Specifically, we leverage the landmark-based registration to effectively encode case-adaptive anatomical priors and generate evolving label proxy for supervision. (iii) To enable our 3D network perceive better contextual cues with higher resolution input under limited computing resource, we further adopt the gradient check-pointing (GCP) strategy to save GPU memory and improve the prediction. Extensively validated on a large 3D US dataset, our method tackles varying fetal poses and achieves promising results. 3D pose estimation of fetus has potentials in serving as a map to provide navigation for many advanced studies.



### Coarse-To-Fine Visual Localization Using Semantic Compact Map
- **Arxiv ID**: http://arxiv.org/abs/1910.04936v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04936v3)
- **Published**: 2019-10-11 01:46:27+00:00
- **Updated**: 2020-10-13 08:13:35+00:00
- **Authors**: Ziwei Liao, Jieqi Shi, Xianyu Qi, Xiaoyu Zhang, Wei Wang, Yijia He, Ran Wei, Xiao Liu
- **Comment**: Video Demo: https://youtu.be/XbTc1YNPajc
- **Journal**: None
- **Summary**: Robust visual localization for urban vehicles remains challenging and unsolved. The limitation of computation efficiency and memory size has made it harder for large-scale applications. Since semantic information serves as a stable and compact representation of the environment, we propose a coarse-to-fine localization system based on a semantic compact map. Pole-like objects are stored in the compact map, then are extracted from semantically segmented images as observations. Localization is performed by a particle filter, followed by a pose alignment module decoupling translation and rotation to achieve better accuracy. We evaluate our system both on synthetic and realistic datasets and compare it with two baselines, a state-of-art semantic feature-based system, and a traditional SIFT feature-based system. Experiments demonstrate that even with a significantly small map, such as a 10 KB map for a 3.7 km long trajectory, our system provides a comparable accuracy with the baselines.



### An Automatic Digital Terrain Generation Technique for Terrestrial Sensing and Virtual Reality Applications
- **Arxiv ID**: http://arxiv.org/abs/1910.04944v1
- **DOI**: 10.1007/978-3-030-33720-9_48
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1910.04944v1)
- **Published**: 2019-10-11 02:26:01+00:00
- **Updated**: 2019-10-11 02:26:01+00:00
- **Authors**: Lee Easson, Alireza Tavakkoli, Jonathan Greenberg
- **Comment**: None
- **Journal**: None
- **Summary**: The identification and modeling of the terrain from point cloud data is an important component of Terrestrial Remote Sensing (TRS) applications. The main focus in terrain modeling is capturing details of complex geological features of landforms. Traditional terrain modeling approaches rely on the user to exert control over terrain features. However, relying on the user input to manually develop the digital terrain becomes intractable when considering the amount of data generated by new remote sensing systems capable of producing massive aerial and ground-based point clouds from scanned environments. This article provides a novel terrain modeling technique capable of automatically generating accurate and physically realistic Digital Terrain Models (DTM) from a variety of point cloud data. The proposed method runs efficiently on large-scale point cloud data with real-time performance over large segments of terrestrial landforms. Moreover, generated digital models are designed to effectively render within a Virtual Reality (VR) environment in real time. The paper concludes with an in-depth discussion of possible research directions and outstanding technical and scientific challenges to improve the proposed approach.



### Towards DeepSpray: Using Convolutional Neural Network to post-process Shadowgraphy Images of Liquid Atomization
- **Arxiv ID**: http://arxiv.org/abs/1910.11073v1
- **DOI**: 10.5445/IR/1000097897/v3
- **Categories**: **cs.CV**, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1910.11073v1)
- **Published**: 2019-10-11 03:00:52+00:00
- **Updated**: 2019-10-11 03:00:52+00:00
- **Authors**: Geoffroy Chaussonnet, Christian Lieber, Yan Yikang, Wenda Gu, Andreas Bartschat, Markus Reischl, Rainer Koch, Ralf Mikut, Hans-JÃ¶rg Bauer
- **Comment**: Technical report, 22 pages, 29 figures
- **Journal**: None
- **Summary**: This technical report investigates the potential of Convolutional Neural Networks to post-process images from primary atomization. Three tasks are investigated. First, the detection and segmentation of liquid droplets in degraded optical conditions. Second, the detection of overlapping ellipses and the prediction of their geometrical characteristics. This task corresponds to extrapolate the hidden contour of an ellipse with reduced visual information. Third, several features of the liquid surface during primary breakup (ligaments, bags, rims) are manually annotated on 15 experimental images. The detector is trained on this minimal database using simple data augmentation and then applied to other images from numerical simulation and from other experiment. In these three tasks, models from the literature based on Convolutional Neural Networks showed very promising results, thus demonstrating the high potential of Deep Learning to post-process liquid atomization. The next step is to embed these models into a unified framework DeepSpray.



### Scene-level Pose Estimation for Multiple Instances of Densely Packed Objects
- **Arxiv ID**: http://arxiv.org/abs/1910.04953v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04953v1)
- **Published**: 2019-10-11 03:17:55+00:00
- **Updated**: 2019-10-11 03:17:55+00:00
- **Authors**: Chaitanya Mitash, Bowen Wen, Kostas Bekris, Abdeslam Boularias
- **Comment**: To appear at the Conference on Robot Learning (CoRL) - 2019
- **Journal**: None
- **Summary**: This paper introduces key machine learning operations that allow the realization of robust, joint 6D pose estimation of multiple instances of objects either densely packed or in unstructured piles from RGB-D data. The first objective is to learn semantic and instance-boundary detectors without manual labeling. An adversarial training framework in conjunction with physics-based simulation is used to achieve detectors that behave similarly in synthetic and real data. Given the stochastic output of such detectors, candidates for object poses are sampled. The second objective is to automatically learn a single score for each pose candidate that represents its quality in terms of explaining the entire scene via a gradient boosted tree. The proposed method uses features derived from surface and boundary alignment between the observed scene and the object model placed at hypothesized poses. Scene-level, multi-instance pose estimation is then achieved by an integer linear programming process that selects hypotheses that maximize the sum of the learned individual scores, while respecting constraints, such as avoiding collisions. To evaluate this method, a dataset of densely packed objects with challenging setups for state-of-the-art approaches is collected. Experiments on this dataset and a public one show that the method significantly outperforms alternatives in terms of 6D pose accuracy while trained only with synthetic datasets.



### Adversarial Pulmonary Pathology Translation for Pairwise Chest X-ray Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.04961v2
- **DOI**: 10.1007/978-3-030-32226-7_84
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04961v2)
- **Published**: 2019-10-11 03:57:37+00:00
- **Updated**: 2020-01-22 01:34:58+00:00
- **Authors**: Yunyan Xing, Zongyuan Ge, Rui Zeng, Dwarikanath Mahapatra, Jarrel Seah, Meng Law, Tom Drummond
- **Comment**: Code: https://github.com/yunyanxing/pairwise_xray_augmentation -
  Accepted to the International Conference on Medical Image Computing and
  Computer Assisted Intervention (MICCAI) 2019
- **Journal**: None
- **Summary**: Recent works show that Generative Adversarial Networks (GANs) can be successfully applied to chest X-ray data augmentation for lung disease recognition. However, the implausible and distorted pathology features generated from the less than perfect generator may lead to wrong clinical decisions. Why not keep the original pathology region? We proposed a novel approach that allows our generative model to generate high quality plausible images that contain undistorted pathology areas. The main idea is to design a training scheme based on an image-to-image translation network to introduce variations of new lung features around the pathology ground-truth area. Moreover, our model is able to leverage both annotated disease images and unannotated healthy lung images for the purpose of generation. We demonstrate the effectiveness of our model on two tasks: (i) we invite certified radiologists to assess the quality of the generated synthetic images against real and other state-of-the-art generative models, and (ii) data augmentation to improve the performance of disease localisation.



### Interaction Relational Network for Mutual Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.04963v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04963v2)
- **Published**: 2019-10-11 04:00:40+00:00
- **Updated**: 2021-01-07 08:04:54+00:00
- **Authors**: Mauricio Perez, Jun Liu, Alex C. Kot
- **Comment**: 12 pages, 6 figures, to be published in IEEE TMM
- **Journal**: None
- **Summary**: Person-person mutual action recognition (also referred to as interaction recognition) is an important research branch of human activity analysis. Current solutions in the field -- mainly dominated by CNNs, GCNs and LSTMs -- often consist of complicated architectures and mechanisms to embed the relationships between the two persons on the architecture itself, to ensure the interaction patterns can be properly learned. Our main contribution with this work is by proposing a simpler yet very powerful architecture, named Interaction Relational Network, which utilizes minimal prior knowledge about the structure of the human body. We drive the network to identify by itself how to relate the body parts from the individuals interacting. In order to better represent the interaction, we define two different relationships, leading to specialized architectures and models for each. These multiple relationship models will then be fused into a single and special architecture, in order to leverage both streams of information for further enhancing the relational reasoning capability. Furthermore we define important structured pair-wise operations to extract meaningful extra information from each pair of joints -- distance and motion. Ultimately, with the coupling of an LSTM, our IRN is capable of paramount sequential relational reasoning. These important extensions we made to our network can also be valuable to other problems that require sophisticated relational reasoning. Our solution is able to achieve state-of-the-art performance on the traditional interaction recognition datasets SBU and UT, and also on the mutual actions from the large-scale dataset NTU RGB+D. Furthermore, it obtains competitive performance in the NTU RGB+D 120 dataset interactions subset.



### Self-enhancement of automatic tunnel accident detection (TAD) on CCTV by AI deep-learning
- **Arxiv ID**: http://arxiv.org/abs/1910.11072v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11072v1)
- **Published**: 2019-10-11 04:19:56+00:00
- **Updated**: 2019-10-11 04:19:56+00:00
- **Authors**: Kyu-Beom Lee, Hyu-Soung Shin
- **Comment**: 13 pages, 8 figures, 4 tables, The 2019 International Conference on
  Tunnels and Underground Spaces(ICTUS 19)
- **Journal**: None
- **Summary**: The deep-learning-based tunnel accident detection (TAD) system (Lee 2019) has installed a system capable of monitoring 9 CCTVs at XX site in November, 2018. The initial deep-learning training was started by studying 70,914 labeled images and label data. However, sunlight, the tail light of a vehicle, and the warning light of the working vehicle were recognized as a fire, and many pedestrians were detected in the lane of the tunnel or a black elongated black object. To solve these problems, as shown in Fig. 1, the false detection data detected in the field were trained with labeled data and reapplied in the field. As a result, false detection of pedestrians and fire could be significantly reduced.



### Multi-modal Deep Analysis for Multimedia
- **Arxiv ID**: http://arxiv.org/abs/1910.04964v2
- **DOI**: 10.1109/TCSVT.2019.2940647
- **Categories**: **cs.MM**, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04964v2)
- **Published**: 2019-10-11 04:21:36+00:00
- **Updated**: 2020-01-04 08:42:13+00:00
- **Authors**: Wenwu Zhu, Xin Wang, Hongzhi Li
- **Comment**: 25 pages, 39 figures, IEEE Transactions on Circuits and Systems for
  Video Technology
- **Journal**: None
- **Summary**: With the rapid development of Internet and multimedia services in the past decade, a huge amount of user-generated and service provider-generated multimedia data become available. These data are heterogeneous and multi-modal in nature, imposing great challenges for processing and analyzing them. Multi-modal data consist of a mixture of various types of data from different modalities such as texts, images, videos, audios etc. In this article, we present a deep and comprehensive overview for multi-modal analysis in multimedia. We introduce two scientific research problems, data-driven correlational representation and knowledge-guided fusion for multimedia analysis. To address the two scientific problems, we investigate them from the following aspects: 1) multi-modal correlational representation: multi-modal fusion of data across different modalities, and 2) multi-modal data and knowledge fusion: multi-modal fusion of data with domain knowledge. More specifically, on data-driven correlational representation, we highlight three important categories of methods, such as multi-modal deep representation, multi-modal transfer learning, and multi-modal hashing. On knowledge-guided fusion, we discuss the approaches for fusing knowledge with data and four exemplar applications that require various kinds of domain knowledge, including multi-modal visual question answering, multi-modal video summarization, multi-modal visual pattern mining and multi-modal recommendation. Finally, we bring forward our insights and future research directions.



### An application of a deep learning algorithm for automatic detection of unexpected accidents under bad CCTV monitoring conditions in tunnels
- **Arxiv ID**: http://arxiv.org/abs/1910.11094v1
- **DOI**: 10.1109/Deep-ML.2019.00010
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11094v1)
- **Published**: 2019-10-11 04:28:44+00:00
- **Updated**: 2019-10-11 04:28:44+00:00
- **Authors**: Kyu-Beom Lee, Hyu-Soung Shin
- **Comment**: 10 pages, 5 figures, 2019 International Conference on Deep Learning
  and Machine Learning in Emerging Applications (Deep-ML)
- **Journal**: 978-1-7281-2914-3/19/$31.00 \c{opyright}2019 IEEE
- **Summary**: In this paper, Object Detection and Tracking System (ODTS) in combination with a well-known deep learning network, Faster Regional Convolution Neural Network (Faster R-CNN), for Object Detection and Conventional Object Tracking algorithm will be introduced and applied for automatic detection and monitoring of unexpected events on CCTVs in tunnels, which are likely to (1) Wrong-Way Driving (WWD), (2) Stop, (3) Person out of vehicle in tunnel (4) Fire. ODTS accepts a video frame in time as an input to obtain Bounding Box (BBox) results by Object Detection and compares the BBoxs of the current and previous video frames to assign a unique ID number to each moving and detected object. This system makes it possible to track a moving object in time, which is not usual to be achieved in conventional object detection frameworks. A deep learning model in ODTS was trained with a dataset of event images in tunnels to Average Precision (AP) values of 0.8479, 0.7161 and 0.9085 for target objects: Car, Person, and Fire, respectively. Then, based on trained deep learning model, the ODTS based Tunnel CCTV Accident Detection System was tested using four accident videos which including each accident. As a result, the system can detect all accidents within 10 seconds. The more important point is that the detection capacity of ODTS could be enhanced automatically without any changes in the program codes as the training dataset becomes rich.



### Estimating Solar Irradiance Using Sky Imagers
- **Arxiv ID**: http://arxiv.org/abs/1910.04981v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04981v1)
- **Published**: 2019-10-11 05:42:55+00:00
- **Updated**: 2019-10-11 05:42:55+00:00
- **Authors**: Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler
- **Comment**: Published in Atmospheric Measurement Techniques (AMT), 2019
- **Journal**: None
- **Summary**: Ground-based whole sky cameras are extensively used for localized monitoring of clouds nowadays. They capture hemispherical images of the sky at regular intervals using a fisheye lens. In this paper, we propose a framework for estimating solar irradiance from pictures taken by those imagers. Unlike pyranometers, such sky images contain information about cloud coverage and can be used to derive cloud movement. An accurate estimation of solar irradiance using solely those images is thus a first step towards short-term forecasting of solar energy generation based on cloud movement. We derive and validate our model using pyranometers co-located with our whole sky imagers. We achieve a better performance in estimating solar irradiance and in particular its short-term variations as compared to other related methods using ground-based observations.



### VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.04985v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04985v4)
- **Published**: 2019-10-11 06:16:09+00:00
- **Updated**: 2019-11-24 09:24:06+00:00
- **Authors**: Mengjia Yan, Mengao Zhao, Zining Xu, Qian Zhang, Guoli Wang, Zhizhong Su
- **Comment**: 8 pages,2 figures. In Proceedings of the IEEE International
  Conference on Computer Vision Workshop, 2019
- **Journal**: None
- **Summary**: To improve the discriminative and generalization ability of lightweight network for face recognition, we propose an efficient variable group convolutional network called VarGFaceNet. Variable group convolution is introduced by VarGNet to solve the conflict between small computational cost and the unbalance of computational intensity inside a block. We employ variable group convolution to design our network which can support large scale face identification while reduce computational cost and parameters. Specifically, we use a head setting to reserve essential information at the start of the network and propose a particular embedding setting to reduce parameters of fully-connected layer for embedding. To enhance interpretation ability, we employ an equivalence of angular distillation loss to guide our lightweight network and we apply recursive knowledge distillation to relieve the discrepancy between the teacher model and the student model. The champion of deepglint-light track of LFR (2019) challenge demonstrates the effectiveness of our model and approach. Implementation of VarGFaceNet will be released at https://github.com/zma-c-137/VarGFaceNet soon.



### Artistic Glyph Image Synthesis via One-Stage Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.04987v2
- **DOI**: 10.1145/3355089.3356574
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1910.04987v2)
- **Published**: 2019-10-11 06:23:26+00:00
- **Updated**: 2020-05-19 09:20:50+00:00
- **Authors**: Yue Gao, Yuan Guo, Zhouhui Lian, Yingmin Tang, Jianguo Xiao
- **Comment**: Accepted by SIGGRAPH Asia 2019, code and datasets:
  https://hologerry.github.io/AGIS-Net/
- **Journal**: ACM Trans. Graph. 38, 6, Article 185 (November 2019), 12 pages
- **Summary**: Automatic generation of artistic glyph images is a challenging task that attracts many research interests. Previous methods either are specifically designed for shape synthesis or focus on texture transfer. In this paper, we propose a novel model, AGIS-Net, to transfer both shape and texture styles in one-stage with only a few stylized samples. To achieve this goal, we first disentangle the representations for content and style by using two encoders, ensuring the multi-content and multi-style generation. Then we utilize two collaboratively working decoders to generate the glyph shape image and its texture image simultaneously. In addition, we introduce a local texture refinement loss to further improve the quality of the synthesized textures. In this manner, our one-stage model is much more efficient and effective than other multi-stage stacked methods. We also propose a large-scale dataset with Chinese glyph images in various shape and texture styles, rendered from 35 professional-designed artistic fonts with 7,326 characters and 2,460 synthetic artistic fonts with 639 characters, to validate the effectiveness and extendability of our method. Extensive experiments on both English and Chinese artistic glyph image datasets demonstrate the superiority of our model in generating high-quality stylized glyph images against other state-of-the-art methods.



### Road Damage Detection Based on Unsupervised Disparity Map Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.04988v1
- **DOI**: 10.1109/TITS.2019.2947206
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04988v1)
- **Published**: 2019-10-11 06:31:28+00:00
- **Updated**: 2019-10-11 06:31:28+00:00
- **Authors**: Rui Fan, Ming Liu
- **Comment**: 6 pages, 9 figures
- **Journal**: None
- **Summary**: This paper presents a novel road damage detection algorithm based on unsupervised disparity map segmentation. Firstly, a disparity map is transformed by minimizing an energy function with respect to stereo rig roll angle and road disparity projection model. Instead of solving this energy minimization problem using non-linear optimization techniques, we directly find its numerical solution. The transformed disparity map is then segmented using Otus's thresholding method, and the damaged road areas can be extracted. The proposed algorithm requires no parameters when detecting road damage. The experimental results illustrate that our proposed algorithm performs both accurately and efficiently. The pixel-level road damage detection accuracy is approximately 97.56%.



### A sub-Riemannian model of the visual cortex with frequency and phase
- **Arxiv ID**: http://arxiv.org/abs/1910.04992v1
- **DOI**: 10.1186/s13408-020-00089-6
- **Categories**: **q-bio.NC**, cs.CV, math.AP, math.DG, math.MG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04992v1)
- **Published**: 2019-10-11 06:41:11+00:00
- **Updated**: 2019-10-11 06:41:11+00:00
- **Authors**: E. Baspinar, A. Sarti, G. Citti
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a novel model of the primary visual cortex (V1) based on orientation, frequency and phase selective behavior of the V1 simple cells. We start from the first level mechanisms of visual perception: receptive profiles. The model interprets V1 as a fiber bundle over the 2-dimensional retinal plane by introducing orientation, frequency and phase as intrinsic variables. Each receptive profile on the fiber is mathematically interpreted as a rotated, frequency modulated and phase shifted Gabor function. We start from the Gabor function and show that it induces in a natural way the model geometry and the associated horizontal connectivity modeling the neural connectivity patterns in V1. We provide an image enhancement algorithm employing the model framework. The algorithm is capable of exploiting not only orientation but also frequency and phase information existing intrinsically in a 2-dimensional input image. We provide the experimental results corresponding to the enhancement algorithm.



### End-to-End Defect Detection in Automated Fiber Placement Based on Artificially Generated Data
- **Arxiv ID**: http://arxiv.org/abs/1910.04997v1
- **DOI**: 10.1117/12.2521739
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04997v1)
- **Published**: 2019-10-11 07:10:24+00:00
- **Updated**: 2019-10-11 07:10:24+00:00
- **Authors**: Sebastian Zambal, Christoph Heindl, Christian Eitzinger, Josef Scharinger
- **Comment**: Presented at Quality Control by Artificial Vision (QCAV), 2019
- **Journal**: None
- **Summary**: Automated fiber placement (AFP) is an advanced manufacturing technology that increases the rate of production of composite materials. At the same time, the need for adaptable and fast inline control methods of such parts raises. Existing inspection systems make use of handcrafted filter chains and feature detectors, tuned for a specific measurement methods by domain experts. These methods hardly scale to new defects or different measurement devices. In this paper, we propose to formulate AFP defect detection as an image segmentation problem that can be solved in an end-to-end fashion using artificially generated training data. We employ a probabilistic graphical model to generate training images and annotations. We then train a deep neural network based on recent architectures designed for image segmentation. This leads to an appealing method that scales well with new defect types and measurement devices and requires little real world data for training.



### Shooting Labels: 3D Semantic Labeling by Virtual Reality
- **Arxiv ID**: http://arxiv.org/abs/1910.05021v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05021v2)
- **Published**: 2019-10-11 08:11:27+00:00
- **Updated**: 2020-10-24 11:49:52+00:00
- **Authors**: Pierluigi Zama Ramirez, Claudio Paternesi, Luca De Luigi, Luigi Lella, Daniele De Gregorio, Luigi Di Stefano
- **Comment**: None
- **Journal**: None
- **Summary**: Availability of a few, large-size, annotated datasets, like ImageNet, Pascal VOC and COCO, has lead deep learning to revolutionize computer vision research by achieving astonishing results in several vision tasks.We argue that new tools to facilitate generation of annotated datasets may help spreading data-driven AI throughout applications and domains. In this work we propose Shooting Labels, the first 3D labeling tool for dense 3D semantic segmentation which exploits Virtual Reality to render the labeling task as easy and fun as playing a video-game. Our tool allows for semantically labeling large scale environments very expeditiously, whatever the nature of the 3D data at hand (e.g. point clouds, mesh). Furthermore, Shooting Labels efficiently integrates multiusers annotations to improve the labeling accuracy automatically and compute a label uncertainty map. Besides, within our framework the 3D annotations can be projected into 2D images, thereby speeding up also a notoriously slow and expensive task such as pixel-wise semantic labeling. We demonstrate the accuracy and efficiency of our tool in two different scenarios: an indoor workspace provided by Matterport3D and a large-scale outdoor environment reconstructed from 1000+ KITTI images.



### CHD:Consecutive Horizontal Dropout for Human Gait Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/1910.05039v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05039v2)
- **Published**: 2019-10-11 09:16:04+00:00
- **Updated**: 2019-10-16 09:05:48+00:00
- **Authors**: Chengtao Cai, Yueyuan Zhou, Yanming Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite gait recognition and person re-identification researches have made a lot of progress, the accuracy of identification is not high enough in some specific situations, for example, people carrying bags or changing coats. In order to alleviate above situations, we propose a simple but effective Consecutive Horizontal Dropout (CHD) method apply on human feature extraction in deep learning network to avoid overfitting. Within the CHD, we intensify the robust of deep learning network for cross-view gait recognition and person re-identification. The experiments illustrate that the rank-1 accuracy on cross-view gait recognition task has been increased about 10% from 68.0% to 78.201% and 8% from 83.545% to 91.364% in person re-identification task in wearing coat or jacket condition. In addition, 100% accuracy of NM condition was first obtained with CHD. On the benchmarks of CASIA-B, above accuracies are state-of-the-arts.



### Deep Independently Recurrent Neural Network (IndRNN)
- **Arxiv ID**: http://arxiv.org/abs/1910.06251v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06251v3)
- **Published**: 2019-10-11 09:43:49+00:00
- **Updated**: 2020-12-09 09:19:00+00:00
- **Authors**: Shuai Li, Wanqing Li, Chris Cook, Yanbo Gao
- **Comment**: Extension of the CVPR2018 paper "Independently Recurrent Neural
  Network (IndRNN): Building A Longer and Deeper RNN", with significant
  improvements as described in the end of Section I. arXiv admin note: text
  overlap with arXiv:1803.04831
- **Journal**: None
- **Summary**: Recurrent neural networks (RNNs) are known to be difficult to train due to the gradient vanishing and exploding problems and thus difficult to learn long-term patterns and construct deep networks. To address these problems, this paper proposes a new type of RNNs with the recurrent connection formulated as Hadamard product, referred to as independently recurrent neural network (IndRNN), where neurons in the same layer are independent of each other and connected across layers. Due to the better behaved gradient backpropagation, IndRNN with regulated recurrent weights effectively addresses the gradient vanishing and exploding problems and thus long-term dependencies can be learned. Moreover, an IndRNN can work with non-saturated activation functions such as ReLU (rectified linear unit) and be still trained robustly. Different deeper IndRNN architectures, including the basic stacked IndRNN, residual IndRNN and densely connected IndRNN, have been investigated, all of which can be much deeper than the existing RNNs. Furthermore, IndRNN reduces the computation at each time step and can be over 10 times faster than the commonly used Long short-term memory (LSTM). Experimental results have shown that the proposed IndRNN is able to process very long sequences and construct very deep networks. Better performance has been achieved on various tasks with IndRNNs compared with the traditional RNN, LSTM and the popular Transformer.



### Noise as a Resource for Learning in Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1910.05057v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05057v2)
- **Published**: 2019-10-11 09:58:50+00:00
- **Updated**: 2020-12-15 11:52:30+00:00
- **Authors**: Elahe Arani, Fahad Sarfraz, Bahram Zonooz
- **Comment**: Accepted at IEEE Winter Conference on Applications of Computer Vision
  (WACV, 2021)
- **Journal**: None
- **Summary**: While noise is commonly considered a nuisance in computing systems, a number of studies in neuroscience have shown several benefits of noise in the nervous system from enabling the brain to carry out computations such as probabilistic inference as well as carrying additional information about the stimuli. Similarly, noise has been shown to improve the performance of deep neural networks. In this study, we further investigate the effect of adding noise in the knowledge distillation framework because of its resemblance to collaborative subnetworks in the brain regions. We empirically show that injecting constructive noise at different levels in the collaborative learning framework enables us to train the model effectively and distill desirable characteristics in the student model. In doing so, we propose three different methods that target the common challenges in deep neural networks: minimizing the performance gap between a compact model and large model (Fickle Teacher), training high performance compact adversarially robust models (Soft Randomization), and training models efficiently under label noise (Messy Collaboration). Our findings motivate further study in the role of noise as a resource for learning in a collaborative learning framework.



### Rosetta: Large scale system for text detection and recognition in images
- **Arxiv ID**: http://arxiv.org/abs/1910.05085v1
- **DOI**: 10.1145/3219819.3219861
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05085v1)
- **Published**: 2019-10-11 11:24:45+00:00
- **Updated**: 2019-10-11 11:24:45+00:00
- **Authors**: Fedor Borisyuk, Albert Gordo, Viswanath Sivakumar
- **Comment**: Proceedings of the 24th ACM SIGKDD International Conference on
  Knowledge Discovery & Data Mining (KDD) 2018, London, United Kingdom
- **Journal**: None
- **Summary**: In this paper we present a deployed, scalable optical character recognition (OCR) system, which we call Rosetta, designed to process images uploaded daily at Facebook scale. Sharing of image content has become one of the primary ways to communicate information among internet users within social networks such as Facebook and Instagram, and the understanding of such media, including its textual information, is of paramount importance to facilitate search and recommendation applications. We present modeling techniques for efficient detection and recognition of text in images and describe Rosetta's system architecture. We perform extensive evaluation of presented technologies, explain useful practical approaches to build an OCR system at scale, and provide insightful intuitions as to why and how certain components work based on the lessons learnt during the development and deployment of the system.



### Methods and open-source toolkit for analyzing and visualizing challenge results
- **Arxiv ID**: http://arxiv.org/abs/1910.05121v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1910.05121v2)
- **Published**: 2019-10-11 12:33:53+00:00
- **Updated**: 2019-12-05 13:51:36+00:00
- **Authors**: Manuel Wiesenfarth, Annika Reinke, Bennett A. Landman, Manuel Jorge Cardoso, Lena Maier-Hein, Annette Kopp-Schneider
- **Comment**: None
- **Journal**: None
- **Summary**: Biomedical challenges have become the de facto standard for benchmarking biomedical image analysis algorithms. While the number of challenges is steadily increasing, surprisingly little effort has been invested in ensuring high quality design, execution and reporting for these international competitions. Specifically, results analysis and visualization in the event of uncertainties have been given almost no attention in the literature. Given these shortcomings, the contribution of this paper is two-fold: (1) We present a set of methods to comprehensively analyze and visualize the results of single-task and multi-task challenges and apply them to a number of simulated and real-life challenges to demonstrate their specific strengths and weaknesses; (2) We release the open-source framework challengeR as part of this work to enable fast and wide adoption of the methodology proposed in this paper. Our approach offers an intuitive way to gain important insights into the relative and absolute performance of algorithms, which cannot be revealed by commonly applied visualization techniques. This is demonstrated by the experiments performed within this work. Our framework could thus become an important tool for analyzing and visualizing challenge results in the field of biomedical image analysis and beyond.



### Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1910.05134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05134v1)
- **Published**: 2019-10-11 12:39:48+00:00
- **Updated**: 2019-10-11 12:39:48+00:00
- **Authors**: Sijin Wang, Ruiping Wang, Ziwei Yao, Shiguang Shan, Xilin Chen
- **Comment**: Accepted by WACV 2020
- **Journal**: None
- **Summary**: Image-text retrieval of natural scenes has been a popular research topic. Since image and text are heterogeneous cross-modal data, one of the key challenges is how to learn comprehensive yet unified representations to express the multi-modal data. A natural scene image mainly involves two kinds of visual concepts, objects and their relationships, which are equally essential to image-text retrieval. Therefore, a good representation should account for both of them. In the light of recent success of scene graph in many CV and NLP tasks for describing complex natural scenes, we propose to represent image and text with two kinds of scene graphs: visual scene graph (VSG) and textual scene graph (TSG), each of which is exploited to jointly characterize objects and relationships in the corresponding modality. The image-text retrieval task is then naturally formulated as cross-modal scene graph matching. Specifically, we design two particular scene graph encoders in our model for VSG and TSG, which can refine the representation of each node on the graph by aggregating neighborhood information. As a result, both object-level and relationship-level cross-modal features can be obtained, which favorably enables us to evaluate the similarity of image and text in the two levels in a more plausible way. We achieve state-of-the-art results on Flickr30k and MSCOCO, which verifies the advantages of our graph matching based approach for image-text retrieval.



### Single Image BRDF Parameter Estimation with a Conditional Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1910.05148v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05148v1)
- **Published**: 2019-10-11 13:00:06+00:00
- **Updated**: 2019-10-11 13:00:06+00:00
- **Authors**: Mark Boss, Hendrik P. A. Lensch
- **Comment**: None
- **Journal**: None
- **Summary**: Creating plausible surfaces is an essential component in achieving a high degree of realism in rendering. To relieve artists, who create these surfaces in a time-consuming, manual process, automated retrieval of the spatially-varying Bidirectional Reflectance Distribution Function (SVBRDF) from a single mobile phone image is desirable. By leveraging a deep neural network, this casual capturing method can be achieved. The trained network can estimate per pixel normal, base color, metallic and roughness parameters from the Disney BRDF. The input image is taken with a mobile phone lit by the camera flash. The network is trained to compensate for environment lighting and thus learned to reduce artifacts introduced by other light sources. These losses contain a multi-scale discriminator with an additional perceptual loss, a rendering loss using a differentiable renderer, and a parameter loss. Besides the local precision, this loss formulation generates material texture maps which are globally more consistent. The network is set up as a generator network trained in an adversarial fashion to ensure that only plausible maps are produced. The estimated parameters not only reproduce the material faithfully in rendering but capture the style of hand-authored materials due to the more global loss terms compared to previous works without requiring additional post-processing. Both the resolution and the quality is improved.



### Spectral Graph Wavelet Transform as Feature Extractor for Machine Learning in Neuroimaging
- **Arxiv ID**: http://arxiv.org/abs/1910.05149v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05149v1)
- **Published**: 2019-10-11 13:00:44+00:00
- **Updated**: 2019-10-11 13:00:44+00:00
- **Authors**: Yusuf Pilavci, Nicolas Farrugia
- **Comment**: None
- **Journal**: International Conference on Acoustics, Speech, and Signal
  Processing, May 2019, Brighton, United Kingdom
- **Summary**: Graph Signal Processing has become a very useful framework for signal operations and representations defined on irregular domains. Exploiting transformations that are defined on graph models can be highly beneficial when the graph encodes relationships between signals. In this work, we present the benefits of using Spectral Graph Wavelet Transform (SGWT) as a feature extractor for machine learning on brain graphs. First, we consider a synthetic regression problem in which the smooth graph signals are generated as input with additive noise, and the target is derived from the input without noise. This enables us to optimize the spectrum coverage using different wavelet shapes. Finally, we present the benefits obtained by SGWT on a functional Magnetic Resonance Imaging (fMRI) open dataset on human subjects, with several graphs and wavelet shapes, by demonstrating significant performance improvements compared to the state of the art.



### Augmented Hard Example Mining for Generalizable Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1910.05280v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.05280v1)
- **Published**: 2019-10-11 16:19:53+00:00
- **Updated**: 2019-10-11 16:19:53+00:00
- **Authors**: Masato Tamura, Tomokazu Murakami
- **Comment**: Submit to WACV2020
- **Journal**: None
- **Summary**: Although the performance of person re-identification (Re-ID) has been much improved by using sophisticated training methods and large-scale labelled datasets, many existing methods make the impractical assumption that information of a target domain can be utilized during training. In practice, a Re-ID system often starts running as soon as it is deployed, hence training with data from a target domain is unrealistic. To make Re-ID systems more practical, methods have been proposed that achieve high performance without information of a target domain. However, they need cumbersome tuning for training and unusual operations for testing. In this paper, we propose augmented hard example mining, which can be easily integrated to a common Re-ID training process and can utilize sophisticated models without any network modification. The method discovers hard examples on the basis of classification probabilities, and to make the examples harder, various types of augmentation are applied to the examples. Among those examples, excessively augmented ones are eliminated by a classification based selection process. Extensive analysis shows that our method successfully selects effective examples and achieves state-of-the-art performance on publicly available benchmark datasets.



### Shape Constrained Network for Eye Segmentation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1910.05283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05283v1)
- **Published**: 2019-10-11 16:21:55+00:00
- **Updated**: 2019-10-11 16:21:55+00:00
- **Authors**: Bingnan Luo, Jie Shen, Shiyang Cheng, Yujiang Wang, Maja Pantic
- **Comment**: IEEE Winter Conference on Applications of Computer Vision (WACV 2020)
- **Journal**: None
- **Summary**: Semantic segmentation of eyes has long been a vital pre-processing step in many biometric applications. Majority of the works focus only on high resolution eye images, while little has been done to segment the eyes from low quality images in the wild. However, this is a particularly interesting and meaningful topic, as eyes play a crucial role in conveying the emotional state and mental well-being of a person. In this work, we take two steps toward solving this problem: (1) We collect and annotate a challenging eye segmentation dataset containing 8882 eye patches from 4461 facial images of different resolutions, illumination conditions and head poses; (2) We develop a novel eye segmentation method, Shape Constrained Network (SCN), that incorporates shape prior into the segmentation network training procedure. Specifically, we learn the shape prior from our dataset using VAE-GAN, and leverage the pre-trained encoder and discriminator to regularise the training of SegNet. To improve the accuracy and quality of predicted masks, we replace the loss of SegNet with three new losses: Intersection-over-Union (IoU) loss, shape discriminator loss and shape embedding loss. Extensive experiments shows that our method outperforms state-of-the-art segmentation and landmark detection methods in terms of mean IoU (mIoU) accuracy and the quality of segmentation masks. The eye segmentation database is available at https://www.dropbox.com/s/yvveouvxsvti08x/Eye_Segmentation_Database.zip?dl=0.



### Aff-Wild Database and AffWildNet
- **Arxiv ID**: http://arxiv.org/abs/1910.05318v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05318v2)
- **Published**: 2019-10-11 17:24:45+00:00
- **Updated**: 2019-12-13 22:42:52+00:00
- **Authors**: Mengyao Liu, Dimitrios Kollias
- **Comment**: None
- **Journal**: None
- **Summary**: In the context of HCI, building an automatic system to recognize affect of human facial expression in real-world condition is very crucial to make machine interact naturallisticaly with a man. However, existing facial emotion databases usually contain expression in the limited scenario under well-controlled condition. Aff-Wild is currently the largest database consisting of spontaneous facial expression in the wild annotated with valence and arousal. The first contribution of this project is the completion of extending Aff-Wild database which is fulfilled by collecting videos from YouTube on which the videos have spontaneous facial expressions in the wild, annotating videos with valence and arousal ranging in [-1,1], detecting faces in frames using FFLD2 detector and partitioning the whole data set into train, validate and test set, with 527056, 94223 and 135145 frames. The diversity is guaranteed regarding age, ethnicity and values of valence and arousal. The ratio of male to female is close to 1. Regarding the techniques used to build the automatic system, deep learning is outstanding since almost all winning methods in emotion challenges adopt DNN techniques. The second contribution of this project is that an end-to-end DNN is constructed to have joint CNN and RNN block and gives the estimation on valence and arousal for each frame in sequential data. VGGFace, ResNet, DenseNet with the corresponding pre-trained model for CNN block and LSTM, GRU, IndRNN, Attention mechanism for RNN block are experimented aiming to find the best combination. Fine tuning and transfer learning techniques are also tried out. By comparing the CCC evaluation value on test data, the best model is found to be pre-trained VGGFace connected with 2 layers GRU with attention mechanism. The models test performance is 0.555 CCC for valence with sequence length 80 and 0.499 CCC for arousal with sequence length 70.



### Deep Learning Based Detection and Correction of Cardiac MR Motion Artefacts During Reconstruction for High-Quality Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.05370v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.05370v4)
- **Published**: 2019-10-11 18:36:17+00:00
- **Updated**: 2020-07-03 13:59:14+00:00
- **Authors**: Ilkay Oksuz, James R. Clough, Bram Ruijsink, Esther Puyol Anton, Aurelien Bustin, Gastao Cruz, Claudia Prieto, Andrew P. King, Julia A. Schnabel
- **Comment**: Accepted for publication in IEEE TMI
- **Journal**: None
- **Summary**: Segmenting anatomical structures in medical images has been successfully addressed with deep learning methods for a range of applications. However, this success is heavily dependent on the quality of the image that is being segmented. A commonly neglected point in the medical image analysis community is the vast amount of clinical images that have severe image artefacts due to organ motion, movement of the patient and/or image acquisition related issues. In this paper, we discuss the implications of image motion artefacts on cardiac MR segmentation and compare a variety of approaches for jointly correcting for artefacts and segmenting the cardiac cavity. The method is based on our recently developed joint artefact detection and reconstruction method, which reconstructs high quality MR images from k-space using a joint loss function and essentially converts the artefact correction task to an under-sampled image reconstruction task by enforcing a data consistency term. In this paper, we propose to use a segmentation network coupled with this in an end-to-end framework. Our training optimises three different tasks: 1) image artefact detection, 2) artefact correction and 3) image segmentation. We train the reconstruction network to automatically correct for motion-related artefacts using synthetically corrupted cardiac MR k-space data and uncorrected reconstructed images. Using a test set of 500 2D+time cine MR acquisitions from the UK Biobank data set, we achieve demonstrably good image quality and high segmentation accuracy in the presence of synthetic motion artefacts. We showcase better performance compared to various image correction architectures.



### Rotation-invariant shipwreck recognition with forward-looking sonar
- **Arxiv ID**: http://arxiv.org/abs/1910.05374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05374v1)
- **Published**: 2019-10-11 18:50:18+00:00
- **Updated**: 2019-10-11 18:50:18+00:00
- **Authors**: Gustavo Neves, RÃ´mulo Cerqueira, Jan Albiez, Luciano Oliveira
- **Comment**: None
- **Journal**: None
- **Summary**: Under the sea, visible spectrum cameras have limited sensing capacity, being able to detect objects only in clear water, but in a constrained range. Considering any sea water condition, sonars are more suitable to support autonomous underwater vehicles' navigation, even in turbid condition. Despite that sonar suitability, this type of sensor does not provide high-density information, such as optical sensors, making the process of object recognition to be more complex. To deal with that problem, we propose a novel trainable method to detect and recognize (identify) specific target objects under the sea with a forward-looking sonar. Our method has a preprocessing step in charge of strongly reducing the sensor noise and seabed background. To represent the object, our proposed method uses histogram of orientation gradient (HOG) as feature extractor. HOG ultimately feed a multi-scale oriented detector combined with a support vector machine to recognize specific trained objects in a rotation-invariant way. Performance assessment demonstrated promising results, favoring the method to be applied in underwater remote sensing.



### Extreme Few-view CT Reconstruction using Deep Inference
- **Arxiv ID**: http://arxiv.org/abs/1910.05375v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.05375v1)
- **Published**: 2019-10-11 18:55:44+00:00
- **Updated**: 2019-10-11 18:55:44+00:00
- **Authors**: Hyojin Kim, Rushil Anirudh, K. Aditya Mohan, Kyle Champley
- **Comment**: Deep Inverse NeurIPS 2019 Workshop
- **Journal**: None
- **Summary**: Reconstruction of few-view x-ray Computed Tomography (CT) data is a highly ill-posed problem. It is often used in applications that require low radiation dose in clinical CT, rapid industrial scanning, or fixed-gantry CT. Existing analytic or iterative algorithms generally produce poorly reconstructed images, severely deteriorated by artifacts and noise, especially when the number of x-ray projections is considerably low. This paper presents a deep network-driven approach to address extreme few-view CT by incorporating convolutional neural network-based inference into state-of-the-art iterative reconstruction. The proposed method interprets few-view sinogram data using attention-based deep networks to infer the reconstructed image. The predicted image is then used as prior knowledge in the iterative algorithm for final reconstruction. We demonstrate effectiveness of the proposed approach by performing reconstruction experiments on a chest CT dataset.



### AffWild Net and Aff-Wild Database
- **Arxiv ID**: http://arxiv.org/abs/1910.05376v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05376v2)
- **Published**: 2019-10-11 18:57:18+00:00
- **Updated**: 2019-12-13 22:58:20+00:00
- **Authors**: Alvertos Benroumpi, Dimitrios Kollias
- **Comment**: None
- **Journal**: None
- **Summary**: Emotions recognition is the task of recognizing people's emotions. Usually it is achieved by analyzing expression of peoples faces. There are two ways for representing emotions: The categorical approach and the dimensional approach by using valence and arousal values. Valence shows how negative or positive an emotion is and arousal shows how much it is activated. Recent deep learning models, that have to do with emotions recognition, are using the second approach, valence and arousal. Moreover, a more interesting concept, which is useful in real life is the "in the wild" emotions recognition. "In the wild" means that the images analyzed for the recognition task, come from from real life sources(online videos, online photos, etc.) and not from staged experiments. So, they introduce unpredictable situations in the images, that have to be modeled. The purpose of this project is to study the previous work that was done for the "in the wild" emotions recognition concept, design a new dataset which has as a standard the "Aff-wild" database, implement new deep learning models and evaluate the results. First, already existing databases and deep learning models are presented. Then, inspired by them a new database is created which includes 507.208 frames in total from 106 videos, which were gathered from online sources. Then, the data are tested in a CNN model based on CNN-M architecture, in order to be sure about their usability. Next, the main model of this project is implemented. That is a Regression GAN which can execute unsupervised and supervised learning at the same time. More specifically, it keeps the main functionality of GANs, which is to produce fake images that look as good as the real ones, while it can also predict valence and arousal values for both real and fake images. Finally, the database created earlier is applied to this model and the results are presented and evaluated.



### TuNet: End-to-end Hierarchical Brain Tumor Segmentation using Cascaded Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.05338v3
- **DOI**: 10.1007/978-3-030-46640-4_17
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05338v3)
- **Published**: 2019-10-11 19:02:58+00:00
- **Updated**: 2019-11-25 06:43:53+00:00
- **Authors**: Minh H. Vu, Tufve Nyholm, Tommy LÃ¶fstedt
- **Comment**: Accepted at MICCAI BrainLes 2019
- **Journal**: None
- **Summary**: Glioma is one of the most common types of brain tumors; it arises in the glial cells in the human brain and in the spinal cord. In addition to having a high mortality rate, glioma treatment is also very expensive. Hence, automatic and accurate segmentation and measurement from the early stages are critical in order to prolong the survival rates of the patients and to reduce the costs of the treatment. In the present work, we propose a novel end-to-end cascaded network for semantic segmentation that utilizes the hierarchical structure of the tumor sub-regions with ResNet-like blocks and Squeeze-and-Excitation modules after each convolution and concatenation block. By utilizing cross-validation, an average ensemble technique, and a simple post-processing technique, we obtained dice scores of 88.06, 80.84, and 80.29, and Hausdorff Distances (95th percentile) of 6.10, 5.17, and 2.21 for the whole tumor, tumor core, and enhancing tumor, respectively, on the online test set.



### FuseMODNet: Real-Time Camera and LiDAR based Moving Object Detection for robust low-light Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1910.05395v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05395v3)
- **Published**: 2019-10-11 20:09:18+00:00
- **Updated**: 2019-11-21 01:18:33+00:00
- **Authors**: Hazem Rashed, Mohamed Ramzy, Victor Vaquero, Ahmad El Sallab, Ganesh Sistu, Senthil Yogamani
- **Comment**: Accepted for Oral presentation at ICCV 2019 Workshop on Autonomous
  Driving. https://sites.google.com/view/fusemodnet
- **Journal**: None
- **Summary**: Moving object detection is a critical task for autonomous vehicles. As dynamic objects represent higher collision risk than static ones, our own ego-trajectories have to be planned attending to the future states of the moving elements of the scene. Motion can be perceived using temporal information such as optical flow. Conventional optical flow computation is based on camera sensors only, which makes it prone to failure in conditions with low illumination. On the other hand, LiDAR sensors are independent of illumination, as they measure the time-of-flight of their own emitted lasers. In this work, we propose a robust and real-time CNN architecture for Moving Object Detection (MOD) under low-light conditions by capturing motion information from both camera and LiDAR sensors. We demonstrate the impact of our algorithm on KITTI dataset where we simulate a low-light environment creating a novel dataset "Dark KITTI". We obtain a 10.1% relative improvement on Dark-KITTI, and a 4.25% improvement on standard KITTI relative to our baselines. The proposed algorithm runs at 18 fps on a standard desktop GPU using $256\times1224$ resolution images.



### Capsule and convolutional neural network-based SAR ship classification in Sentinel-1 data
- **Arxiv ID**: http://arxiv.org/abs/1910.05401v1
- **DOI**: 10.1117/12.2532551
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.05401v1)
- **Published**: 2019-10-11 20:32:02+00:00
- **Updated**: 2019-10-11 20:32:02+00:00
- **Authors**: Leonardo De Laurentiis, Andrea Pomente, Fabio Del Frate, Giovanni Schiavon
- **Comment**: Please check out the original SPIE paper for a complete list of
  figures, tables, references and general content
- **Journal**: SPIE Remote Sensing 2019: Proceedings Volume 11154, Active and
  Passive Microwave Remote Sensing for Environmental Monitoring III; 1115405
  (2019)
- **Summary**: Synthetic Aperture Radar (SAR) constitutes a fundamental asset for wide-areas monitoring with high-resolution requirements. The first SAR sensors have given rise to coarse coastal and maritime monitoring applications, including oil spill, ship and ice floes detection. With the upgrade to very high-resolution sensors in the recent years, with relatively new SAR missions such as Sentinel-1, a great deal of data providing a stronger information content has been released, enabling more refined studies on general targets features and thus permitting complex classifications, as for ship classification, which has become increasingly relevant given the growing need for coastal surveillance in commercial and military segments. In the last decade, several works focused on this topic have been presented, generally based on radiometric features processing; furthermore, in the very recent years a significant amount of research works have focused on emerging deep learning techniques, in particular on Convolutional Neural Networks (CNN). Recently Capsule Neural Networks (CapsNets) have been presented, demonstrating a notable improvement in capturing the properties of given entities, improving the use of spatial informations, in particular of spatial dependence between features, a severely lacking feature in CNNs. In fact, CNNs pooling operations have been criticized for losing spatial relations, thus special capsules, along with a new iterative routing-by-agreement mechanism, have been proposed. In this work a comparison between Capsule and CNNs potential in the ship classification application domain is shown, by leveraging the OpenSARShip, a SAR Sentinel-1 ship chips dataset; in particular, a performance comparison between capsule and various convolutional architectures is built, demonstrating better performances of CapsNet in classifying ships within a small dataset.



### Snow avalanche segmentation in SAR images with Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.05411v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05411v2)
- **Published**: 2019-10-11 21:02:57+00:00
- **Updated**: 2020-11-06 13:00:56+00:00
- **Authors**: Filippo Maria Bianchi, Jakob Grahn, Markus Eckerstorfer, Eirik Malnes, Hannah Vickers
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge about frequency and location of snow avalanche activity is essential for forecasting and mapping of snow avalanche hazard. Traditional field monitoring of avalanche activity has limitations, especially when surveying large and remote areas. In recent years, avalanche detection in Sentinel-1 radar satellite imagery has been developed to improve monitoring. However, the current state-of-the-art detection algorithms, based on radar signal processing techniques, are still much less accurate than human experts. To reduce this gap, we propose a deep learning architecture for detecting avalanches in Sentinel-1 radar images. We trained a neural network on 6,345 manually labelled avalanches from 117 Sentinel-1 images, each one consisting of six channels that include backscatter and topographical information. Then, we tested our trained model on a new SAR image. Comparing to the manual labelling (the gold standard), we achieved an F1 score above 66\%, while the state-of-the-art detection algorithm sits at an F1 score of only 38\%. A visual inspection of the results generated by our deep learning model shows that only small avalanches are undetected, while some avalanches that were originally not labelled by the human expert are discovered.



### Illegible Text to Readable Text: An Image-to-Image Transformation using Conditional Sliced Wasserstein Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.05425v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05425v1)
- **Published**: 2019-10-11 22:01:24+00:00
- **Updated**: 2019-10-11 22:01:24+00:00
- **Authors**: Mostafa Karimi, Gopalkrishna Veni, Yen-Yun Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic text recognition from ancient handwritten record images is an important problem in the genealogy domain. However, critical challenges such as varying noise conditions, vanishing texts, and variations in handwriting make the recognition task difficult. We tackle this problem by developing a handwritten-to-machine-print conditional Generative Adversarial network (HW2MP-GAN) model that formulates handwritten recognition as a text-Image-to-text-Image translation problem where a given image, typically in an illegible form, is converted into another image, close to its machine-print form. The proposed model consists of three-components including a generator, and word-level and character-level discriminators. The model incorporates Sliced Wasserstein distance (SWD) and U-Net architectures in HW2MP-GAN for better quality image-to-image transformation. Our experiments reveal that HW2MP-GAN outperforms state-of-the-art baseline cGAN models by almost 30 in Frechet Handwritten Distance (FHD), 0.6 on average Levenshtein distance and 39% in word accuracy for image-to-image translation on IAM database. Further, HW2MP-GAN improves handwritten recognition word accuracy by 1.3% compared to baseline handwritten recognition models on the IAM database.



### Roweis Discriminant Analysis: A Generalized Subspace Learning Method
- **Arxiv ID**: http://arxiv.org/abs/1910.05437v1
- **DOI**: 10.1007/978-3-030-50347-5_29
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.05437v1)
- **Published**: 2019-10-11 23:14:09+00:00
- **Updated**: 2019-10-11 23:14:09+00:00
- **Authors**: Benyamin Ghojogh, Fakhri Karray, Mark Crowley
- **Comment**: This is the paper for the methods Roweis Discriminant Analysis (RDA),
  dual RDA, kernel RDA, and Roweisfaces. This is in memory of Sam Roweis (rest
  in peace) to whom subspace and manifold learning owes significantly
- **Journal**: International Conference on Image Analysis and Recognition, vol 1,
  pp. 328-342, Springer, 2020 (published under title "Generalized Subspace
  Learning by Roweis Discriminant Analysis")
- **Summary**: We present a new method which generalizes subspace learning based on eigenvalue and generalized eigenvalue problems. This method, Roweis Discriminant Analysis (RDA), is named after Sam Roweis to whom the field of subspace learning owes significantly. RDA is a family of infinite number of algorithms where Principal Component Analysis (PCA), Supervised PCA (SPCA), and Fisher Discriminant Analysis (FDA) are special cases. One of the extreme special cases, which we name Double Supervised Discriminant Analysis (DSDA), uses the labels twice; it is novel and has not appeared elsewhere. We propose a dual for RDA for some special cases. We also propose kernel RDA, generalizing kernel PCA, kernel SPCA, and kernel FDA, using both dual RDA and representation theory. Our theoretical analysis explains previously known facts such as why SPCA can use regression but FDA cannot, why PCA and SPCA have duals but FDA does not, why kernel PCA and kernel SPCA use kernel trick but kernel FDA does not, and why PCA is the best linear method for reconstruction. Roweisfaces and kernel Roweisfaces are also proposed generalizing eigenfaces, Fisherfaces, supervised eigenfaces, and their kernel variants. We also report experiments showing the effectiveness of RDA and kernel RDA on some benchmark datasets.



### Landmarks-assisted Collaborative Deep Framework for Automatic 4D Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.05445v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05445v2)
- **Published**: 2019-10-11 23:50:57+00:00
- **Updated**: 2020-02-07 09:34:25+00:00
- **Authors**: Muzammil Behzad, Nhat Vo, Xiaobai Li, Guoying Zhao
- **Comment**: Published in 15th IEEE International Conference on Automatic Face and
  Gesture Recognition
- **Journal**: None
- **Summary**: We propose a novel landmarks-assisted collaborative end-to-end deep framework for automatic 4D FER. Using 4D face scan data, we calculate its various geometrical images, and afterwards use rank pooling to generate their dynamic images encapsulating important facial muscle movements over time. As well, the given 3D landmarks are projected on a 2D plane as binary images and convolutional layers are used to extract sequences of feature vectors for every landmark video. During the training stage, the dynamic images are used to train an end-to-end deep network, while the feature vectors of landmark images are used train a long short-term memory (LSTM) network. The finally improved set of expression predictions are obtained when the dynamic and landmark images collaborate over multi-views using the proposed deep framework. Performance results obtained from extensive experimentation on the widely-adopted BU-4DFE database under globally used settings prove that our proposed collaborative framework outperforms the state-of-the-art 4D FER methods and reach a promising classification accuracy of 96.7% demonstrating its effectiveness.




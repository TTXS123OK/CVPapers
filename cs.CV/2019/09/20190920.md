# Arxiv Papers in cs.CV on 2019-09-20
### Towards Multimodal Understanding of Passenger-Vehicle Interactions in Autonomous Vehicles: Intent/Slot Recognition Utilizing Audio-Visual Data
- **Arxiv ID**: http://arxiv.org/abs/1909.13714v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.13714v1)
- **Published**: 2019-09-20 00:00:41+00:00
- **Updated**: 2019-09-20 00:00:41+00:00
- **Authors**: Eda Okur, Shachi H Kumar, Saurav Sahay, Lama Nachman
- **Comment**: Presented as a short-paper at the 23rd Workshop on the Semantics and
  Pragmatics of Dialogue (SemDial 2019 - LondonLogue), Sep 4-6, 2019, London,
  UK
- **Journal**: Proceedings of the 23rd Workshop on the Semantics and Pragmatics
  of Dialogue (SEMDIAL), pp. 213-215, London, United Kingdom, September 2019
- **Summary**: Understanding passenger intents from spoken interactions and car's vision (both inside and outside the vehicle) are important building blocks towards developing contextual dialog systems for natural interactions in autonomous vehicles (AV). In this study, we continued exploring AMIE (Automated-vehicle Multimodal In-cabin Experience), the in-cabin agent responsible for handling certain multimodal passenger-vehicle interactions. When the passengers give instructions to AMIE, the agent should parse such commands properly considering available three modalities (language/text, audio, video) and trigger the appropriate functionality of the AV system. We had collected a multimodal in-cabin dataset with multi-turn dialogues between the passengers and AMIE using a Wizard-of-Oz scheme via realistic scavenger hunt game. In our previous explorations, we experimented with various RNN-based models to detect utterance-level intents (set destination, change route, go faster, go slower, stop, park, pull over, drop off, open door, and others) along with intent keywords and relevant slots (location, position/direction, object, gesture/gaze, time-guidance, person) associated with the action to be performed in our AV scenarios. In this recent work, we propose to discuss the benefits of multimodal understanding of in-cabin utterances by incorporating verbal/language input (text and speech embeddings) together with the non-verbal/acoustic and visual input from inside and outside the vehicle (i.e., passenger gestures and gaze from in-cabin video stream, referred objects outside of the vehicle from the road view camera stream). Our experimental results outperformed text-only baselines and with multimodality, we achieved improved performances for utterance-level intent detection and slot filling.



### Fine-grained Action Segmentation using the Semi-Supervised Action GAN
- **Arxiv ID**: http://arxiv.org/abs/1909.09269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09269v1)
- **Published**: 2019-09-20 00:38:05+00:00
- **Updated**: 2019-09-20 00:38:05+00:00
- **Authors**: Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: Published in Pattern Recognition Journal
- **Journal**: None
- **Summary**: In this paper we address the problem of continuous fine-grained action segmentation, in which multiple actions are present in an unsegmented video stream. The challenge for this task lies in the need to represent the hierarchical nature of the actions and to detect the transitions between actions, allowing us to localise the actions within the video effectively. We propose a novel recurrent semi-supervised Generative Adversarial Network (GAN) model for continuous fine-grained human action segmentation. Temporal context information is captured via a novel Gated Context Extractor (GCE) module, composed of gated attention units, that directs the queued context information through the generator model, for enhanced action segmentation. The GAN is made to learn features in a semi-supervised manner, enabling the model to perform action classification jointly with the standard, unsupervised, GAN learning procedure. We perform extensive evaluations on different architectural variants to demonstrate the importance of the proposed network architecture, and show that it is capable of outperforming current state-of-the-art on three challenging datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric Activities dataset.



### Learning 3D-aware Egocentric Spatial-Temporal Interaction via Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.09272v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.09272v3)
- **Published**: 2019-09-20 00:43:09+00:00
- **Updated**: 2020-03-02 05:19:04+00:00
- **Authors**: Chengxi Li, Yue Meng, Stanley H. Chan, Yi-Ting Chen
- **Comment**: Accepted to the International Conference on Robotics and Automation
  (ICRA) 2020
- **Journal**: None
- **Summary**: To enable intelligent automated driving systems, a promising strategy is to understand how human drives and interacts with road users in complicated driving situations. In this paper, we propose a 3D-aware egocentric spatial-temporal interaction framework for automated driving applications. Graph convolution networks (GCN) is devised for interaction modeling. We introduce three novel concepts into GCN. First, we decompose egocentric interactions into ego-thing and ego-stuff interaction, modeled by two GCNs. In both GCNs, ego nodes are introduced to encode the interaction between thing objects (e.g., car and pedestrian), and interaction between stuff objects (e.g., lane marking and traffic light). Second, objects' 3D locations are explicitly incorporated into GCN to better model egocentric interactions. Third, to implement ego-stuff interaction in GCN, we propose a MaskAlign operation to extract features for irregular objects.   We validate the proposed framework on tactical driver behavior recognition. Extensive experiments are conducted using Honda Research Institute Driving Dataset, the largest dataset with diverse tactical driver behavior annotations. Our framework demonstrates substantial performance boost over baselines on the two experimental settings by 3.9% and 6.0%, respectively. Furthermore, we visualize the learned affinity matrices, which encode ego-thing and ego-stuff interactions, to showcase the proposed framework can capture interactions effectively.



### Fourier-CPPNs for Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1909.09273v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09273v1)
- **Published**: 2019-09-20 00:43:59+00:00
- **Updated**: 2019-09-20 00:43:59+00:00
- **Authors**: Mattie Tesfaldet, Xavier Snelgrove, David Vazquez
- **Comment**: Accepted at ICCV Workshops '19
- **Journal**: None
- **Summary**: Compositional Pattern Producing Networks (CPPNs) are differentiable networks that independently map (x, y) pixel coordinates to (r, g, b) colour values. Recently, CPPNs have been used for creating interesting imagery for creative purposes, e.g., neural art. However their architecture biases generated images to be overly smooth, lacking high-frequency detail. In this work, we extend CPPNs to explicitly model the frequency information for each pixel output, capturing frequencies beyond the DC component. We show that our Fourier-CPPNs (F-CPPNs) provide improved visual detail for image synthesis.



### Forecasting Future Action Sequences with Neural Memory Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.09278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09278v1)
- **Published**: 2019-09-20 01:04:38+00:00
- **Updated**: 2019-09-20 01:04:38+00:00
- **Authors**: Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: BMVC 2019 Oral
- **Journal**: None
- **Summary**: We propose a novel neural memory network based framework for future action sequence forecasting. This is a challenging task where we have to consider short-term, within sequence relationships as well as relationships in between sequences, to understand how sequences of actions evolve over time. To capture these relationships effectively, we introduce neural memory networks to our modelling scheme. We show the significance of using two input streams, the observed frames and the corresponding action labels, which provide different information cues for our prediction task. Furthermore, through the proposed method we effectively map the long-term relationships among individual input sequences through separate memory modules, which enables better fusion of the salient features. Our method outperforms the state-of-the-art approaches by a large margin on two publicly available datasets: Breakfast and 50 Salads.



### Coupled Generative Adversarial Network for Continuous Fine-grained Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.09283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09283v1)
- **Published**: 2019-09-20 01:17:00+00:00
- **Updated**: 2019-09-20 01:17:00+00:00
- **Authors**: Harshala Gammulle, Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: WACV 2019
- **Journal**: None
- **Summary**: We propose a novel conditional GAN (cGAN) model for continuous fine-grained human action segmentation, that utilises multi-modal data and learned scene context information. The proposed approach utilises two GANs: termed Action GAN and Auxiliary GAN, where the Action GAN is trained to operate over the current RGB frame while the Auxiliary GAN utilises supplementary information such as depth or optical flow. The goal of both GANs is to generate similar `action codes', a vector representation of the current action. To facilitate this process a context extractor that incorporates data and recent outputs from both modes is used to extract context information to aid recognition. The result is a recurrent GAN architecture which learns a task specific loss function from multiple feature modalities. Extensive evaluations on variants of the proposed model to show the importance of utilising different information streams such as context and auxiliary information in the proposed network; and show that our model is capable of outperforming state-of-the-art methods for three widely used datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric Activities, comprising both static and dynamic camera settings.



### Spherical Kernel for Efficient Graph Convolution on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1909.09287v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09287v2)
- **Published**: 2019-09-20 01:27:43+00:00
- **Updated**: 2020-03-23 09:48:19+00:00
- **Authors**: Huan Lei, Naveed Akhtar, Ajmal Mian
- **Comment**: Accepted to TPAMI
- **Journal**: None
- **Summary**: We propose a spherical kernel for efficient graph convolution of 3D point clouds. Our metric-based kernels systematically quantize the local 3D space to identify distinctive geometric relationships in the data. Similar to the regular grid CNN kernels, the spherical kernel maintains translation-invariance and asymmetry properties, where the former guarantees weight sharing among similar local structures in the data and the latter facilitates fine geometric learning. The proposed kernel is applied to graph neural networks without edge-dependent filter generation, making it computationally attractive for large point clouds. In our graph networks, each vertex is associated with a single point location and edges connect the neighborhood points within a defined range. The graph gets coarsened in the network with farthest point sampling. Analogous to the standard CNNs, we define pooling and unpooling operations for our network. We demonstrate the effectiveness of the proposed spherical kernel with graph neural networks for point cloud classification and semantic segmentation using ModelNet, ShapeNet, RueMonge2014, ScanNet and S3DIS datasets. The source code and the trained models can be downloaded from https://github.com/hlei-ziyan/SPH3D-GCN.



### Learning Your Way Without Map or Compass: Panoramic Target Driven Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/1909.09295v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.09295v2)
- **Published**: 2019-09-20 02:17:20+00:00
- **Updated**: 2020-09-25 08:52:13+00:00
- **Authors**: David Watkins-Valls, Jingxi Xu, Nicholas Waytowich, Peter Allen
- **Comment**: None
- **Journal**: None
- **Summary**: We present a robot navigation system that uses an imitation learning framework to successfully navigate in complex environments. Our framework takes a pre-built 3D scan of a real environment and trains an agent from pre-generated expert trajectories to navigate to any position given a panoramic view of the goal and the current visual input without relying on map, compass, odometry, or relative position of the target at runtime. Our end-to-end trained agent uses RGB and depth (RGBD) information and can handle large environments (up to $1031m^2$) across multiple rooms (up to $40$) and generalizes to unseen targets. We show that when compared to several baselines our method (1) requires fewer training examples and less training time, (2) reaches the goal location with higher accuracy, and (3) produces better solutions with shorter paths for long-range navigation tasks.



### Making the Invisible Visible: Action Recognition Through Walls and Occlusions
- **Arxiv ID**: http://arxiv.org/abs/1909.09300v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09300v1)
- **Published**: 2019-09-20 02:49:55+00:00
- **Updated**: 2019-09-20 02:49:55+00:00
- **Authors**: Tianhong Li, Lijie Fan, Mingmin Zhao, Yingcheng Liu, Dina Katabi
- **Comment**: ICCV 2019. The first two authors contributed equally to this paper
- **Journal**: None
- **Summary**: Understanding people's actions and interactions typically depends on seeing them. Automating the process of action recognition from visual data has been the topic of much research in the computer vision community. But what if it is too dark, or if the person is occluded or behind a wall? In this paper, we introduce a neural network model that can detect human actions through walls and occlusions, and in poor lighting conditions. Our model takes radio frequency (RF) signals as input, generates 3D human skeletons as an intermediate representation, and recognizes actions and interactions of multiple people over time. By translating the input to an intermediate skeleton-based representation, our model can learn from both vision-based and RF-based datasets, and allow the two tasks to help each other. We show that our model achieves comparable accuracy to vision-based action recognition systems in visible scenarios, yet continues to work accurately when people are not visible, hence addressing scenarios that are beyond the limit of today's vision-based action recognition.



### A nonlocal feature-driven exemplar-based approach for image inpainting
- **Arxiv ID**: http://arxiv.org/abs/1909.09301v2
- **DOI**: 10.1137/20M1317864
- **Categories**: **cs.CV**, 68U10, 94A08, 65D18, 65K10, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1909.09301v2)
- **Published**: 2019-09-20 03:01:24+00:00
- **Updated**: 2020-12-03 21:54:40+00:00
- **Authors**: Viktor Reshniak, Jeremy Trageser, Clayton G. Webster
- **Comment**: None
- **Journal**: SIAM J. Imaging Sci. 13(2020) 2140-2168
- **Summary**: We present a nonlocal variational image completion technique which admits simultaneous inpainting of multiple structures and textures in a unified framework. The recovery of geometric structures is achieved by using general convolution operators as a measure of behavior within an image. These are combined with a nonlocal exemplar-based approach to exploit the self-similarity of an image in the selected feature domains and to ensure the inpainting of textures. We also introduce an anisotropic patch distance metric to allow for better control of the feature selection within an image and present a nonlocal energy functional based on this metric. Finally, we derive an optimization algorithm for the proposed variational model and examine its validity experimentally with various test images.



### CNN-based RGB-D Salient Object Detection: Learn, Select and Fuse
- **Arxiv ID**: http://arxiv.org/abs/1909.09309v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09309v1)
- **Published**: 2019-09-20 03:53:53+00:00
- **Updated**: 2019-09-20 03:53:53+00:00
- **Authors**: Hao Chen, Youfu Li
- **Comment**: submitted to a journal in 12-October-2018
- **Journal**: None
- **Summary**: The goal of this work is to present a systematic solution for RGB-D salient object detection, which addresses the following three aspects with a unified framework: modal-specific representation learning, complementary cue selection and cross-modal complement fusion. To learn discriminative modal-specific features, we propose a hierarchical cross-modal distillation scheme, in which the well-learned source modality provides supervisory signals to facilitate the learning process for the new modality. To better extract the complementary cues, we formulate a residual function to incorporate complements from the paired modality adaptively. Furthermore, a top-down fusion structure is constructed for sufficient cross-modal interactions and cross-level transmissions. The experimental results demonstrate the effectiveness of the proposed cross-modal distillation scheme in zero-shot saliency detection and pre-training on a new modality, as well as the advantages in selecting and fusing cross-modal/cross-level complements.



### Infusing Learned Priors into Model-Based Multispectral Imaging
- **Arxiv ID**: http://arxiv.org/abs/1909.09313v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09313v1)
- **Published**: 2019-09-20 04:21:06+00:00
- **Updated**: 2019-09-20 04:21:06+00:00
- **Authors**: Jiaming Liu, Yu Sun, Ulugbek S. Kamilov
- **Comment**: arXiv admin note: text overlap with arXiv:1905.05113
- **Journal**: None
- **Summary**: We introduce a new algorithm for regularized reconstruction of multispectral (MS) images from noisy linear measurements. Unlike traditional approaches, the proposed algorithm regularizes the recovery problem by using a prior specified \emph{only} through a learned denoising function. More specifically, we propose a new accelerated gradient method (AGM) variant of regularization by denoising (RED) for model-based MS image reconstruction. The key ingredient of our approach is the three-dimensional (3D) deep neural net (DNN) denoiser that can fully leverage spationspectral correlations within MS images. Our results suggest the generalizability of our MS-RED algorithm, where a single trained DNN can be used to solve several different MS imaging problems.



### Learning Lightweight Pedestrian Detector with Hierarchical Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1909.09325v1
- **DOI**: 10.1109/ICIP.2019.8803079
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09325v1)
- **Published**: 2019-09-20 05:20:56+00:00
- **Updated**: 2019-09-20 05:20:56+00:00
- **Authors**: Rui Chen, Haizhou Ai, Chong Shang, Long Chen, Zijie Zhuang
- **Comment**: Accepted at ICIP 2019 as Oral
- **Journal**: 2019 IEEE International Conference on Image Processing (ICIP), pp.
  1645-1649
- **Summary**: It remains very challenging to build a pedestrian detection system for real world applications, which demand for both accuracy and speed. This work presents a novel hierarchical knowledge distillation framework to learn a lightweight pedestrian detector, which significantly reduces the computational cost and still holds the high accuracy at the same time. Following the `teacher--student' diagram that a stronger, deeper neural network can teach a lightweight network to learn better representations, we explore multiple knowledge distillation architectures and reframe this approach as a unified, hierarchical distillation framework. In particular, the proposed distillation is performed at multiple hierarchies, multiple stages in a modern detector, which empowers the student detector to learn both low-level details and high-level abstractions simultaneously. Experiment result shows that a student model trained by our framework, with 6 times compression in number of parameters, still achieves competitive performance as the teacher model on the widely used pedestrian detection benchmark.



### Deep 3D-Zoom Net: Unsupervised Learning of Photo-Realistic 3D-Zoom
- **Arxiv ID**: http://arxiv.org/abs/1909.09349v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1909.09349v2)
- **Published**: 2019-09-20 07:18:39+00:00
- **Updated**: 2019-10-02 17:23:17+00:00
- **Authors**: Juan Luis Gonzalez Bello, Munchurl Kim
- **Comment**: Check our video at https://www.youtube.com/watch?v=Gz76VYwUzZ8
- **Journal**: None
- **Summary**: The 3D-zoom operation is the positive translation of the camera in the Z-axis, perpendicular to the image plane. In contrast, the optical zoom changes the focal length and the digital zoom is used to enlarge a certain region of an image to the original image size. In this paper, we are the first to formulate an unsupervised 3D-zoom learning problem where images with an arbitrary zoom factor can be generated from a given single image. An unsupervised framework is convenient, as it is a challenging task to obtain a 3D-zoom dataset of natural scenes due to the need for special equipment to ensure camera movement is restricted to the Z-axis. In addition, the objects in the scenes should not move when being captured, which hinders the construction of a large dataset of outdoor scenes. We present a novel unsupervised framework to learn how to generate arbitrarily 3D-zoomed versions of a single image, not requiring a 3D-zoom ground truth, called the Deep 3D-Zoom Net. The Deep 3D-Zoom Net incorporates the following features: (i) transfer learning from a pre-trained disparity estimation network via a back re-projection reconstruction loss; (ii) a fully convolutional network architecture that models depth-image-based rendering (DIBR), taking into account high-frequency details without the need for estimating the intermediate disparity; and (iii) incorporating a discriminator network that acts as a no-reference penalty for unnaturally rendered areas. Even though there is no baseline to fairly compare our results, our method outperforms previous novel view synthesis research in terms of realistic appearance on large camera baselines. We performed extensive experiments to verify the effectiveness of our method on the KITTI and Cityscapes datasets.



### Interpreting Distortions in Dimensionality Reduction by Superimposing Neighbourhood Graphs
- **Arxiv ID**: http://arxiv.org/abs/1909.12902v1
- **DOI**: 10.1109/VISUAL.2019.8933568
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.12902v1)
- **Published**: 2019-09-20 07:48:26+00:00
- **Updated**: 2019-09-20 07:48:26+00:00
- **Authors**: Beno√Æt Colange, Laurent Vuillon, Sylvain Lespinats, Denys Dutykh
- **Comment**: 5 pages, 6 figures, 22 references. Paper presented at IEEE VIS 2019
  Conference. Other author's papers can be downloaded at
  http://www.denys-dutykh.com/
- **Journal**: Paper presented at IEEE Vis 2019 conference at Vancouver, Canada
- **Summary**: To perform visual data exploration, many dimensionality reduction methods have been developed. These tools allow data analysts to represent multidimensional data in a 2D or 3D space, while preserving as much relevant information as possible. Yet, they cannot preserve all structures simultaneously and they induce some unavoidable distortions. Hence, many criteria have been introduced to evaluate a map's overall quality, mostly based on the preservation of neighbourhoods. Such global indicators are currently used to compare several maps, which helps to choose the most appropriate mapping method and its hyperparameters. However, those aggregated indicators tend to hide the local repartition of distortions. Thereby, they need to be supplemented by local evaluation to ensure correct interpretation of maps. In this paper, we describe a new method, called MING, for `Map Interpretation using Neighbourhood Graphs'. It offers a graphical interpretation of pairs of map quality indicators, as well as local evaluation of the distortions. This is done by displaying on the map the nearest neighbours graphs computed in the data space and in the embedding. Shared and unshared edges exhibit reliable and unreliable neighbourhood information conveyed by the mapping. By this mean, analysts may determine whether proximity (or remoteness) of points on the map faithfully represents similarity (or dissimilarity) of original data, within the meaning of a chosen map quality criteria. We apply this approach to two pairs of widespread indicators: precision/recall and trustworthiness/continuity, chosen for their wide use in the community, which will allow an easy handling by users.



### EATEN: Entity-aware Attention for Single Shot Visual Text Extraction
- **Arxiv ID**: http://arxiv.org/abs/1909.09380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09380v1)
- **Published**: 2019-09-20 09:12:59+00:00
- **Updated**: 2019-09-20 09:12:59+00:00
- **Authors**: He guo, Xiameng Qin, Jiaming Liu, Junyu Han, Jingtuo Liu, Errui Ding
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Extracting entity from images is a crucial part of many OCR applications, such as entity recognition of cards, invoices, and receipts. Most of the existing works employ classical detection and recognition paradigm. This paper proposes an Entity-aware Attention Text Extraction Network called EATEN, which is an end-to-end trainable system to extract the entities without any post-processing. In the proposed framework, each entity is parsed by its corresponding entity-aware decoder, respectively. Moreover, we innovatively introduce a state transition mechanism which further improves the robustness of entity extraction. In consideration of the absence of public benchmarks, we construct a dataset of almost 0.6 million images in three real-world scenarios (train ticket, passport and business card), which is publicly available at https://github.com/beacandler/EATEN. To the best of our knowledge, EATEN is the first single shot method to extract entities from images. Extensive experiments on these benchmarks demonstrate the state-of-the-art performance of EATEN.



### Brain Tumor Segmentation and Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/1909.09399v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09399v1)
- **Published**: 2019-09-20 10:00:32+00:00
- **Updated**: 2019-09-20 10:00:32+00:00
- **Authors**: Rupal Agravat, Mehul S Raval
- **Comment**: 9 Pages
- **Journal**: BraTS 2019
- **Summary**: The paper demonstrates the use of the fully convolutional neural network for glioma segmentation on the BraTS 2019 dataset. Three-layers deep encoder-decoder architecture is used along with dense connection at encoder part to propagate the information from coarse layer to deep layers. This architecture is used to train three tumor sub-components separately. Subcomponent training weights are initialized with whole tumor weights to get the localization of the tumor within the brain. At the end, three segmentation results were merged to get the entire tumor segmentation. Dice Similarity of training dataset with focal loss implementation for whole tumor, tumor core and enhancing tumor is 0.92, 0.90 and 0.79 respectively. Radiomic features along with segmentation results and age are used to predict the overall survival of patients using random forest regressor to classify survival of patients in long, medium and short survival classes. 55.4% of classification accuracy is reported for training dataset with the scans whose resection status is gross-total resection.



### A Lightweight Deep Learning Model for Human Activity Recognition on Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/1909.12917v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12917v1)
- **Published**: 2019-09-20 10:16:34+00:00
- **Updated**: 2019-09-20 10:16:34+00:00
- **Authors**: Preeti Agarwal, Mansaf Alam
- **Comment**: None
- **Journal**: None
- **Summary**: Human Activity Recognition (HAR) using wearable and mobile sensors has gained momentum in last few years, in various fields, such as, healthcare, surveillance, education, entertainment. Nowadays, Edge Computing has emerged to reduce communication latency and network traffic.Edge devices are resource constrained devices and cannot support high computation. In literature, various models have been developed for HAR. In recent years, deep learning algorithms have shown high performance in HAR, but these algorithms require lot of computation making them inefficient to be deployed on edge devices. This paper, proposes a Lightweight Deep Learning Model for HAR requiring less computational power, making it suitable to be deployed on edge devices. The performance of proposed model is tested on the participants six daily activities data. Results show that the proposed model outperforms many of the existing machine learning and deep learning techniques.



### ACFNet: Attentional Class Feature Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.09408v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09408v3)
- **Published**: 2019-09-20 10:19:17+00:00
- **Updated**: 2019-10-18 02:01:35+00:00
- **Authors**: Fan Zhang, Yanqin Chen, Zhihang Li, Zhibin Hong, Jingtuo Liu, Feifei Ma, Junyu Han, Errui Ding
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Recent works have made great progress in semantic segmentation by exploiting richer context, most of which are designed from a spatial perspective. In contrast to previous works, we present the concept of class center which extracts the global context from a categorical perspective. This class-level context describes the overall representation of each class in an image. We further propose a novel module, named Attentional Class Feature (ACF) module, to calculate and adaptively combine different class centers according to each pixel. Based on the ACF module, we introduce a coarse-to-fine segmentation network, called Attentional Class Feature Network (ACFNet), which can be composed of an ACF module and any off-the-shell segmentation network (base network). In this paper, we use two types of base networks to evaluate the effectiveness of ACFNet. We achieve new state-of-the-art performance of 81.85% mIoU on Cityscapes dataset with only finely annotated data used for training.



### Weakly Supervised Semantic Segmentation Using Constrained Dominant Sets
- **Arxiv ID**: http://arxiv.org/abs/1909.09414v1
- **DOI**: 10.1007/978-3-030-30645-8_39
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09414v1)
- **Published**: 2019-09-20 10:32:48+00:00
- **Updated**: 2019-09-20 10:32:48+00:00
- **Authors**: Sinem Aslan, Marcello Pelillo
- **Comment**: None
- **Journal**: In: Image Analysis and Processing (ICIAP 2019). Lecture Notes in
  Computer Science, vol 11752. Springer, Cham (2019)
- **Summary**: The availability of large-scale data sets is an essential pre-requisite for deep learning based semantic segmentation schemes. Since obtaining pixel-level labels is extremely expensive, supervising deep semantic segmentation networks using low-cost weak annotations has been an attractive research problem in recent years. In this work, we explore the potential of Constrained Dominant Sets (CDS) for generating multi-labeled full mask predictions to train a fully convolutional network (FCN) for semantic segmentation. Our experimental results show that using CDS's yields higher-quality mask predictions compared to methods that have been adopted in the literature for the same purpose.



### Deep Aggregation of Regional Convolutional Activations for Content Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1909.09420v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.09420v2)
- **Published**: 2019-09-20 10:43:00+00:00
- **Updated**: 2019-09-24 06:53:16+00:00
- **Authors**: Konstantin Schall, Kai Uwe Barthel, Nico Hezel, Klaus Jung
- **Comment**: None
- **Journal**: None
- **Summary**: One of the key challenges of deep learning based image retrieval remains in aggregating convolutional activations into one highly representative feature vector. Ideally, this descriptor should encode semantic, spatial and low level information. Even though off-the-shelf pre-trained neural networks can already produce good representations in combination with aggregation methods, appropriate fine tuning for the task of image retrieval has shown to significantly boost retrieval performance. In this paper, we present a simple yet effective supervised aggregation method built on top of existing regional pooling approaches. In addition to the maximum activation of a given region, we calculate regional average activations of extracted feature maps. Subsequently, weights for each of the pooled feature vectors are learned to perform a weighted aggregation to a single feature vector. Furthermore, we apply our newly proposed NRA loss function for deep metric learning to fine tune the backbone neural network and to learn the aggregation weights. Our method achieves state-of-the-art results for the INRIA Holidays data set and competitive results for the Oxford Buildings and Paris data sets while reducing the training time significantly.



### Retro-Actions: Learning 'Close' by Time-Reversing 'Open' Videos
- **Arxiv ID**: http://arxiv.org/abs/1909.09422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09422v1)
- **Published**: 2019-09-20 10:52:31+00:00
- **Updated**: 2019-09-20 10:52:31+00:00
- **Authors**: Will Price, Dima Damen
- **Comment**: ICCVW 2019, 8 pages, 7 figures, 6 tables.
  https://video-reversal.willprice.dev/
- **Journal**: None
- **Summary**: We investigate video transforms that result in class-homogeneous label-transforms. These are video transforms that consistently maintain or modify the labels of all videos in each class. We propose a general approach to discover invariant classes, whose transformed examples maintain their label; pairs of equivariant classes, whose transformed examples exchange their labels; and novel-generating classes, whose transformed examples belong to a new class outside the dataset. Label transforms offer additional supervision previously unexplored in video recognition benefiting data augmentation and enabling zero-shot learning opportunities by learning a class from transformed videos of its counterpart.   Amongst such video transforms, we study horizontal-flipping, time-reversal, and their composition. We highlight errors in naively using horizontal-flipping as a form of data augmentation in video. Next, we validate the realism of time-reversed videos through a human perception study where people exhibit equal preference for forward and time-reversed videos. Finally, we test our approach on two datasets, Jester and Something-Something, evaluating the three video transforms for zero-shot learning and data augmentation. Our results show that gestures such as zooming in can be learnt from zooming out in a zero-shot setting, as well as more complex actions with state transitions such as digging something out of something from burying something in something.



### Genetic Neural Architecture Search for automatic assessment of human sperm images
- **Arxiv ID**: http://arxiv.org/abs/1909.09432v2
- **DOI**: 10.1016/j.eswa.2021.115937
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML, 68T01 (Primary) 68T45, 68T20 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1909.09432v2)
- **Published**: 2019-09-20 11:25:05+00:00
- **Updated**: 2020-09-17 11:12:35+00:00
- **Authors**: Erfan Miahi, Seyed Abolghasem Mirroshandel, Alexis Nasr
- **Comment**: None
- **Journal**: Expert Systems with Applications, 2021
- **Summary**: Male infertility is a disease which affects approximately 7% of men. Sperm morphology analysis (SMA) is one of the main diagnosis methods for this problem. Manual SMA is an inexact, subjective, non-reproducible, and hard to teach process. As a result, in this paper, we introduce a novel automatic SMA based on a neural architecture search algorithm termed Genetic Neural Architecture Search (GeNAS). For this purpose, we used a collection of images called MHSMA dataset contains 1,540 sperm images which have been collected from 235 patients with infertility problems. GeNAS is a genetic algorithm that acts as a meta-controller which explores the constrained search space of plain convolutional neural network architectures. Every individual of the genetic algorithm is a convolutional neural network trained to predict morphological deformities in different segments of human sperm (head, vacuole, and acrosome), and its fitness is calculated by a novel proposed method named GeNAS-WF especially designed for noisy, low resolution, and imbalanced datasets. Also, a hashing method is used to save each trained neural architecture fitness, so we could reuse them during fitness evaluation and speed up the algorithm. Besides, in terms of running time and computation power, our proposed architecture search method is far more efficient than most of the other existing neural architecture search algorithms. Additionally, other proposed methods have been evaluated on balanced datasets, whereas GeNAS is built specifically for noisy, low quality, and imbalanced datasets which are common in the field of medical imaging. In our experiments, the best neural architecture found by GeNAS has reached an accuracy of 91.66%, 77.33%, and 77.66% in the vacuole, head, and acrosome abnormality detection, respectively. In comparison to other proposed algorithms for MHSMA dataset, GeNAS achieved state-of-the-art results.



### Underwater Image Super-Resolution using Deep Residual Multipliers
- **Arxiv ID**: http://arxiv.org/abs/1909.09437v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09437v3)
- **Published**: 2019-09-20 11:53:07+00:00
- **Updated**: 2020-02-24 20:42:34+00:00
- **Authors**: Md Jahidul Islam, Sadman Sakib Enan, Peigen Luo, Junaed Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep residual network-based generative model for single image super-resolution (SISR) of underwater imagery for use by autonomous underwater robots. We also provide an adversarial training pipeline for learning SISR from paired data. In order to supervise the training, we formulate an objective function that evaluates the \textit{perceptual quality} of an image based on its global content, color, and local style information. Additionally, we present USR-248, a large-scale dataset of three sets of underwater images of 'high' (640x480) and 'low' (80x60, 160x120, and 320x240) spatial resolution. USR-248 contains paired instances for supervised training of 2x, 4x, or 8x SISR models. Furthermore, we validate the effectiveness of our proposed model through qualitative and quantitative experiments and compare the results with several state-of-the-art models' performances. We also analyze its practical feasibility for applications such as scene understanding and attention modeling in noisy visual conditions.



### Document Rectification and Illumination Correction using a Patch-based CNN
- **Arxiv ID**: http://arxiv.org/abs/1909.09470v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.3; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1909.09470v1)
- **Published**: 2019-09-20 12:47:40+00:00
- **Updated**: 2019-09-20 12:47:40+00:00
- **Authors**: Xiaoyu Li, Bo Zhang, Jing Liao, Pedro V. Sander
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: We propose a novel learning method to rectify document images with various distortion types from a single input image. As opposed to previous learning-based methods, our approach seeks to first learn the distortion flow on input image patches rather than the entire image. We then present a robust technique to stitch the patch results into the rectified document by processing in the gradient domain. Furthermore, we propose a second network to correct the uneven illumination, further improving the readability and OCR accuracy. Due to the less complex distortion present on the smaller image patches, our patch-based approach followed by stitching and illumination correction can significantly improve the overall accuracy in both the synthetic and real datasets.



### Adversarial Learning with Margin-based Triplet Embedding Regularization
- **Arxiv ID**: http://arxiv.org/abs/1909.09481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09481v1)
- **Published**: 2019-09-20 13:08:12+00:00
- **Updated**: 2019-09-20 13:08:12+00:00
- **Authors**: Yaoyao Zhong, Weihong Deng
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: The Deep neural networks (DNNs) have achieved great success on a variety of computer vision tasks, however, they are highly vulnerable to adversarial attacks. To address this problem, we propose to improve the local smoothness of the representation space, by integrating a margin-based triplet embedding regularization term into the classification objective, so that the obtained model learns to resist adversarial examples. The regularization term consists of two steps optimizations which find potential perturbations and punish them by a large margin in an iterative way. Experimental results on MNIST, CASIA-WebFace, VGGFace2 and MS-Celeb-1M reveal that our approach increases the robustness of the network against both feature and label adversarial attacks in simple object classification and deep face recognition.



### Multi-user Augmented Reality Application for Video Communication in Virtual Space
- **Arxiv ID**: http://arxiv.org/abs/1909.09529v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.09529v1)
- **Published**: 2019-09-20 14:32:54+00:00
- **Updated**: 2019-09-20 14:32:54+00:00
- **Authors**: Kumar Mridul, M. Ramanathan, Kunal Ahirwar, Mansi Sharma
- **Comment**: European Light Field Imaging Workshop (ELFI 2019), Borovets, Bulgaria
- **Journal**: None
- **Summary**: Communication is the most useful tool to impart knowledge, understand ideas, clarify thoughts and expressions, organize plan and manage every single day-to-day activity. Although there are different modes of communication, physical barrier always affects the clarity of the message due to the absence of body language and facial expressions. These barriers are overcome by video calling, which is technically the most advance mode of communication at present. The proposed work concentrates around the concept of video calling in a more natural and seamless way using Augmented Reality (AR). AR can be helpful in giving the users an experience of physical presence in each other's environment. Our work provides an entirely new platform for video calling, wherein the users can enjoy the privilege of their own virtual space to interact with the individual's environment. Moreover, there is no limitation of sharing the same screen space. Any number of participants can be accommodated over a single conference without having to compromise the screen size.



### A Transfer Learning Approach for Automated Segmentation of Prostate Whole Gland and Transition Zone in Diffusion Weighted MRI
- **Arxiv ID**: http://arxiv.org/abs/1909.09541v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1909.09541v2)
- **Published**: 2019-09-20 14:44:50+00:00
- **Updated**: 2020-10-28 15:45:32+00:00
- **Authors**: Saman Motamed, Isha Gujrathi, Dominik Deniffel, Anton Oentoro, Masoom A. Haider, Farzad Khalvati
- **Comment**: None
- **Journal**: None
- **Summary**: The segmentation of prostate whole gland and transition zone in Diffusion Weighted MRI (DWI) are the first step in designing computer-aided detection algorithms for prostate cancer. However, variations in MRI acquisition parameters and scanner manufacturing result in different appearances of prostate tissue in the images. Convolutional neural networks (CNNs) which have shown to be successful in various medical image analysis tasks including segmentation are typically sensitive to the variations in imaging parameters. This sensitivity leads to poor segmentation performance of CNNs trained on a source cohort and tested on a target cohort from a different scanner and hence, it limits the applicability of CNNs for cross-cohort training and testing. Contouring prostate whole gland and transition zone in DWI images are time-consuming and expensive. Thus, it is important to enable CNNs pretrained on images of source domain, to segment images of target domain with minimum requirement for manual segmentation of images from the target domain. In this work, we propose a transfer learning method based on a modified U-net architecture and loss function, for segmentation of prostate whole gland and transition zone in DWIs using a CNN pretrained on a source dataset and tested on the target dataset. We explore the effect of the size of subset of target dataset used for fine-tuning the pre-trained CNN on the overall segmentation accuracy. Our results show that with a fine-tuning data as few as 30 patients from the target domain, the proposed transfer learning-based algorithm can reach dice score coefficient of 0.80 for both prostate whole gland and transition zone segmentation. Using a fine-tuning data of 115 patients from the target domain, dice score coefficient of 0.85 and 0.84 are achieved for segmentation of whole gland and transition zone, respectively, in the target domain.



### An Efficient Sampling-based Method for Online Informative Path Planning in Unknown Environments
- **Arxiv ID**: http://arxiv.org/abs/1909.09548v2
- **DOI**: 10.1109/LRA.2020.2969191
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09548v2)
- **Published**: 2019-09-20 15:07:14+00:00
- **Updated**: 2020-01-14 15:09:43+00:00
- **Authors**: Lukas Schmid, Michael Pantic, Raghav Khanna, Lionel Ott, Roland Siegwart, Juan Nieto
- **Comment**: 8 pages, 6 figures, video: https://youtu.be/lEadqJ1_8Do, framework:
  https://github.com/ethz-asl/mav_active_3d_planning
- **Journal**: IEEE Robotics and Automation Letters, Vol. 5, Iss. 2, April 2020
- **Summary**: The ability to plan informative paths online is essential to robot autonomy. In particular, sampling-based approaches are often used as they are capable of using arbitrary information gain formulations. However, they are prone to local minima, resulting in sub-optimal trajectories, and sometimes do not reach global coverage. In this paper, we present a new RRT*-inspired online informative path planning algorithm. Our method continuously expands a single tree of candidate trajectories and rewires segments to maintain the tree and refine intermediate trajectories. This allows the algorithm to achieve global coverage and maximize the utility of a path in a global context, using a single objective function. We demonstrate the algorithm's capabilities in the applications of autonomous indoor exploration as well as accurate Truncated Signed Distance Field (TSDF)-based 3D reconstruction on-board a Micro Aerial vehicle (MAV). We study the impact of commonly used information gain and cost formulations in these scenarios and propose a novel TSDF-based 3D reconstruction gain and cost-utility formulation. Detailed evaluation in realistic simulation environments show that our approach outperforms state of the art methods in these tasks. Experiments on a real MAV demonstrate the ability of our method to robustly plan in real-time, exploring an indoor environment solely with on-board sensing and computation. We make our framework available for future research.



### Defending Against Physically Realizable Attacks on Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.09552v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09552v2)
- **Published**: 2019-09-20 15:11:09+00:00
- **Updated**: 2020-02-14 20:07:55+00:00
- **Authors**: Tong Wu, Liang Tong, Yevgeniy Vorobeychik
- **Comment**: camera-ready
- **Journal**: None
- **Summary**: We study the problem of defending deep neural network approaches for image classification from physically realizable attacks. First, we demonstrate that the two most scalable and effective methods for learning robust models, adversarial training with PGD attacks and randomized smoothing, exhibit very limited effectiveness against three of the highest profile physical attacks. Next, we propose a new abstract adversarial model, rectangular occlusion attacks, in which an adversary places a small adversarially crafted rectangle in an image, and develop two approaches for efficiently computing the resulting adversarial examples. Finally, we demonstrate that adversarial training using our new attack yields image classification models that exhibit high robustness against the physically realizable attacks we study, offering the first effective generic defense against such attacks.



### Target-Specific Action Classification for Automated Assessment of Human Motor Behavior from Video
- **Arxiv ID**: http://arxiv.org/abs/1909.09566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09566v1)
- **Published**: 2019-09-20 15:40:05+00:00
- **Updated**: 2019-09-20 15:40:05+00:00
- **Authors**: Behnaz Rezaei, Yiorgos Christakis, Bryan Ho, Kevin Thomas, Kelley Erb, Sarah Ostadabbas, Shyamal Patel
- **Comment**: This manuscript is under submission to the Sensors journal
- **Journal**: None
- **Summary**: Objective monitoring and assessment of human motor behavior can improve the diagnosis and management of several medical conditions. Over the past decade, significant advances have been made in the use of wearable technology for continuously monitoring human motor behavior in free-living conditions. However, wearable technology remains ill-suited for applications which require monitoring and interpretation of complex motor behaviors (e.g. involving interactions with the environment). Recent advances in computer vision and deep learning have opened up new possibilities for extracting information from video recordings. In this paper, we present a hierarchical vision-based behavior phenotyping method for classification of basic human actions in video recordings performed using a single RGB camera. Our method addresses challenges associated with tracking multiple human actors and classification of actions in videos recorded in changing environments with different fields of view. We implement a cascaded pose tracker that uses temporal relationships between detections for short-term tracking and appearance-based tracklet fusion for long-term tracking. Furthermore, for action classification, we use pose evolution maps derived from the cascaded pose tracker as low-dimensional and interpretable representations of the movement sequences for training a convolutional neural network. The cascaded pose tracker achieves an average accuracy of 88\% in tracking the target human actor in our video recordings, and overall system achieves average test accuracy of 84\% for target-specific action classification in untrimmed video recordings.



### Understanding Architectures Learnt by Cell-based Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1909.09569v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09569v3)
- **Published**: 2019-09-20 15:49:45+00:00
- **Updated**: 2020-01-01 13:57:23+00:00
- **Authors**: Yao Shu, Wei Wang, Shaofeng Cai
- **Comment**: Published as a conference paper at ICLR 2020
- **Journal**: None
- **Summary**: Neural architecture search (NAS) searches architectures automatically for given tasks, e.g., image classification and language modeling. Improving the search efficiency and effectiveness have attracted increasing attention in recent years. However, few efforts have been devoted to understanding the generated architectures. In this paper, we first reveal that existing NAS algorithms (e.g., DARTS, ENAS) tend to favor architectures with wide and shallow cell structures. These favorable architectures consistently achieve fast convergence and are consequently selected by NAS algorithms. Our empirical and theoretical study further confirms that their fast convergence derives from their smooth loss landscape and accurate gradient information. Nonetheless, these architectures may not necessarily lead to better generalization performance compared with other candidate architectures in the same search space, and therefore further improvement is possible by revising existing NAS algorithms.



### PST900: RGB-Thermal Calibration, Dataset and Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/1909.10980v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.10980v1)
- **Published**: 2019-09-20 15:59:20+00:00
- **Updated**: 2019-09-20 15:59:20+00:00
- **Authors**: Shreyas S. Shivakumar, Neil Rodrigues, Alex Zhou, Ian D. Miller, Vijay Kumar, Camillo J. Taylor
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: In this work we propose long wave infrared (LWIR) imagery as a viable supporting modality for semantic segmentation using learning-based techniques. We first address the problem of RGB-thermal camera calibration by proposing a passive calibration target and procedure that is both portable and easy to use. Second, we present PST900, a dataset of 894 synchronized and calibrated RGB and Thermal image pairs with per pixel human annotations across four distinct classes from the DARPA Subterranean Challenge. Lastly, we propose a CNN architecture for fast semantic segmentation that combines both RGB and Thermal imagery in a way that leverages RGB imagery independently. We compare our method against the state-of-the-art and show that our method outperforms them in our dataset.



### Unsupervised Learning for Real-World Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1909.09629v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.09629v1)
- **Published**: 2019-09-20 17:37:55+00:00
- **Updated**: 2019-09-20 17:37:55+00:00
- **Authors**: Andreas Lugmayr, Martin Danelljan, Radu Timofte
- **Comment**: To appear in the AIM 2019 workshop at ICCV. Includes supplementary
  material
- **Journal**: None
- **Summary**: Most current super-resolution methods rely on low and high resolution image pairs to train a network in a fully supervised manner. However, such image pairs are not available in real-world applications. Instead of directly addressing this problem, most works employ the popular bicubic downsampling strategy to artificially generate a corresponding low resolution image. Unfortunately, this strategy introduces significant artifacts, removing natural sensor noise and other real-world characteristics. Super-resolution networks trained on such bicubic images therefore struggle to generalize to natural images. In this work, we propose an unsupervised approach for image super-resolution. Given only unpaired data, we learn to invert the effects of bicubic downsampling in order to restore the natural image characteristics present in the data. This allows us to generate realistic image pairs, faithfully reflecting the distribution of real-world images. Our super-resolution network can therefore be trained with direct pixel-wise supervision in the high resolution domain, while robustly generalizing to real input. We demonstrate the effectiveness of our approach in quantitative and qualitative experiments.



### Understanding and Robustifying Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1909.09656v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.09656v2)
- **Published**: 2019-09-20 18:03:06+00:00
- **Updated**: 2020-01-28 14:14:05+00:00
- **Authors**: Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, Frank Hutter
- **Comment**: In: International Conference on Learning Representations (ICLR 2020);
  28 pages, 30 figures
- **Journal**: None
- **Summary**: Differentiable Architecture Search (DARTS) has attracted a lot of attention due to its simplicity and small search costs achieved by a continuous relaxation and an approximation of the resulting bi-level optimization problem. However, DARTS does not work robustly for new problems: we identify a wide range of search spaces for which DARTS yields degenerate architectures with very poor test performance. We study this failure mode and show that, while DARTS successfully minimizes validation loss, the found solutions generalize poorly when they coincide with high validation loss curvature in the architecture space. We show that by adding one of various types of regularization we can robustify DARTS to find solutions with less curvature and better generalization properties. Based on these observations, we propose several simple variations of DARTS that perform substantially more robustly in practice. Our observations are robust across five search spaces on three image classification tasks and also hold for the very different domains of disparity estimation (a dense regression task) and language modelling.



### Map as The Hidden Sensor: Fast Odometry-Based Global Localization
- **Arxiv ID**: http://arxiv.org/abs/1910.00572v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00572v1)
- **Published**: 2019-09-20 18:28:03+00:00
- **Updated**: 2019-09-20 18:28:03+00:00
- **Authors**: Cheng Peng, David Weikersdorfer
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and robust global localization is essential to robotics applications. We propose a novel global localization method that employs the map traversability as a hidden observation. The resulting map-corrected odometry localization is able to provide an accurate belief tensor of the robot state. Our method can be used for blind robots in dark or highly reflective areas. In contrast to odometry drift in long-term, our method using only odometry and the map converges in longterm. Our method can also be integrated with other sensors to boost the localization performance. The algorithm does not have any initial state assumption and tracks all possible robot states at all times. Therefore, our method is global and is robust in the event of ambiguous observations. We parallel each step of our algorithm such that it can be performed in real-time (up to ~ 300 Hz) using GPU. We validate our algorithm in different publicly available floor-plans and show that it is able to converge to the ground truth fast while being robust to ambiguities.



### Cross-Dataset Person Re-Identification via Unsupervised Pose Disentanglement and Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1909.09675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09675v1)
- **Published**: 2019-09-20 18:54:05+00:00
- **Updated**: 2019-09-20 18:54:05+00:00
- **Authors**: Yu-Jhe Li, Ci-Siang Lin, Yan-Bo Lin, Yu-Chiang Frank Wang
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Person re-identification (re-ID) aims at recognizing the same person from images taken across different cameras. To address this challenging task, existing re-ID models typically rely on a large amount of labeled training data, which is not practical for real-world applications. To alleviate this limitation, researchers now targets at cross-dataset re-ID which focuses on generalizing the discriminative ability to the unlabeled target domain when given a labeled source domain dataset. To achieve this goal, our proposed Pose Disentanglement and Adaptation Network (PDA-Net) aims at learning deep image representation with pose and domain information properly disentangled. With the learned cross-domain pose invariant feature space, our proposed PDA-Net is able to perform pose disentanglement across domains without supervision in identities, and the resulting features can be applied to cross-dataset re-ID. Both of our qualitative and quantitative results on two benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art cross-dataset Re-ID approaches.



### Gradual Network for Single Image De-raining
- **Arxiv ID**: http://arxiv.org/abs/1909.09677v1
- **DOI**: 10.1145/3343031.3350883
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.09677v1)
- **Published**: 2019-09-20 18:56:08+00:00
- **Updated**: 2019-09-20 18:56:08+00:00
- **Authors**: Zhe Huang, Weijiang Yu, Wayne Zhang, Litong Feng, Nong Xiao
- **Comment**: In Proceedings of the 27th ACM International Conference on Multimedia
  (MM 2019)
- **Journal**: None
- **Summary**: Most advances in single image de-raining meet a key challenge, which is removing rain streaks with different scales and shapes while preserving image details. Existing single image de-raining approaches treat rain-streak removal as a process of pixel-wise regression directly. However, they are lacking in mining the balance between over-de-raining (e.g. removing texture details in rain-free regions) and under-de-raining (e.g. leaving rain streaks). In this paper, we firstly propose a coarse-to-fine network called Gradual Network (GraNet) consisting of coarse stage and fine stage for delving into single image de-raining with different granularities. Specifically, to reveal coarse-grained rain-streak characteristics (e.g. long and thick rain streaks/raindrops), we propose a coarse stage by utilizing local-global spatial dependencies via a local-global subnetwork composed of region-aware blocks. Taking the residual result (the coarse de-rained result) between the rainy image sample (i.e. the input data) and the output of coarse stage (i.e. the learnt rain mask) as input, the fine stage continues to de-rain by removing the fine-grained rain streaks (e.g. light rain streaks and water mist) to get a rain-free and well-reconstructed output image via a unified contextual merging sub-network with dense blocks and a merging block. Solid and comprehensive experiments on synthetic and real data demonstrate that our GraNet can significantly outperform the state-of-the-art methods by removing rain streaks with various densities, scales and shapes while keeping the image details of rain-free regions well-preserved.



### SkyNet: a Hardware-Efficient Method for Object Detection and Tracking on Embedded Systems
- **Arxiv ID**: http://arxiv.org/abs/1909.09709v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09709v2)
- **Published**: 2019-09-20 20:26:43+00:00
- **Updated**: 2020-02-29 22:48:22+00:00
- **Authors**: Xiaofan Zhang, Haoming Lu, Cong Hao, Jiachen Li, Bowen Cheng, Yuhong Li, Kyle Rupnow, Jinjun Xiong, Thomas Huang, Honghui Shi, Wen-mei Hwu, Deming Chen
- **Comment**: Published as a conference paper at Conference on Machine Learning and
  Systems (MLSys) 2020
- **Journal**: None
- **Summary**: Object detection and tracking are challenging tasks for resource-constrained embedded systems. While these tasks are among the most compute-intensive tasks from the artificial intelligence domain, they are only allowed to use limited computation and memory resources on embedded devices. In the meanwhile, such resource-constrained implementations are often required to satisfy additional demanding requirements such as real-time response, high-throughput performance, and reliable inference accuracy. To overcome these challenges, we propose SkyNet, a hardware-efficient neural network to deliver the state-of-the-art detection accuracy and speed for embedded systems. Instead of following the common top-down flow for compact DNN (Deep Neural Network) design, SkyNet provides a bottom-up DNN design approach with comprehensive understanding of the hardware constraints at the very beginning to deliver hardware-efficient DNNs. The effectiveness of SkyNet is demonstrated by winning the competitive System Design Contest for low power object detection in the 56th IEEE/ACM Design Automation Conference (DAC-SDC), where our SkyNet significantly outperforms all other 100+ competitors: it delivers 0.731 Intersection over Union (IoU) and 67.33 frames per second (FPS) on a TX2 embedded GPU; and 0.716 IoU and 25.05 FPS on an Ultra96 embedded FPGA. The evaluation of SkyNet is also extended to GOT-10K, a recent large-scale high-diversity benchmark for generic object tracking in the wild. For state-of-the-art object trackers SiamRPN++ and SiamMask, where ResNet-50 is employed as the backbone, implementations using our SkyNet as the backbone DNN are 1.60X and 1.73X faster with better or similar accuracy when running on a 1080Ti GPU, and 37.20X smaller in terms of parameter size for significantly better memory and storage footprint.



### Neural Style Transfer Improves 3D Cardiovascular MR Image Segmentation on Inconsistent Data
- **Arxiv ID**: http://arxiv.org/abs/1909.09716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09716v1)
- **Published**: 2019-09-20 20:58:45+00:00
- **Updated**: 2019-09-20 20:58:45+00:00
- **Authors**: Chunwei Ma, Zhanghexuan Ji, Mingchen Gao
- **Comment**: 22nd International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI 2019) early accept
- **Journal**: None
- **Summary**: Three-dimensional medical image segmentation is one of the most important problems in medical image analysis and plays a key role in downstream diagnosis and treatment. Recent years, deep neural networks have made groundbreaking success in medical image segmentation problem. However, due to the high variance in instrumental parameters, experimental protocols, and subject appearances, the generalization of deep learning models is often hindered by the inconsistency in medical images generated by different machines and hospitals. In this work, we present StyleSegor, an efficient and easy-to-use strategy to alleviate this inconsistency issue. Specifically, neural style transfer algorithm is applied to unlabeled data in order to minimize the differences in image properties including brightness, contrast, texture, etc. between the labeled and unlabeled data. We also apply probabilistic adjustment on the network output and integrate multiple predictions through ensemble learning. On a publicly available whole heart segmentation benchmarking dataset from MICCAI HVSMR 2016 challenge, we have demonstrated an elevated dice accuracy surpassing current state-of-the-art method and notably, an improvement of the total score by 29.91\%. StyleSegor is thus corroborated to be an accurate tool for 3D whole heart segmentation especially on highly inconsistent data, and is available at https://github.com/horsepurve/StyleSegor.



### Persian Signature Verification using Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.09720v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09720v1)
- **Published**: 2019-09-20 21:24:03+00:00
- **Updated**: 2019-09-20 21:24:03+00:00
- **Authors**: Mohammad Rezaei, Nader Naderi
- **Comment**: None
- **Journal**: 2nd National Conference on New Researches in Electrical and
  Computer Engineering, 2017
- **Summary**: Fully convolutional networks (FCNs) have been recently used for feature extraction and classification in image and speech recognition, where their inputs have been raw signal or other complicated features. Persian signature verification is done using conventional convolutional neural networks (CNNs). In this paper, we propose to use FCN for learning a robust feature extraction from the raw signature images. FCN can be considered as a variant of CNN where its fully connected layers are replaced with a global pooling layer. In the proposed manner, FCN inputs are raw signature images and convolution filter size is fixed. Recognition accuracy on UTSig database, shows that FCN with a global average pooling outperforms CNN.



### Content-based image retrieval using Mix histogram
- **Arxiv ID**: http://arxiv.org/abs/1909.09722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09722v1)
- **Published**: 2019-09-20 21:24:16+00:00
- **Updated**: 2019-09-20 21:24:16+00:00
- **Authors**: Mohammad Rezaei, Ali Ahmadi, Navid Naderi
- **Comment**: None
- **Journal**: 2d National Conference on New Research in Electrical and Computer
  Engineering, 2017
- **Summary**: This paper presents a new method to extract image low-level features, namely mix histogram (MH), for content-based image retrieval. Since color and edge orientation features are important visual information which help the human visual system percept and discriminate different images, this method extracts and integrates color and edge orientation information in order to measure similarity between different images. Traditional color histograms merely focus on the global distribution of color in the image and therefore fail to extract other visual features. The MH is attempting to overcome this problem by extracting edge orientations as well as color feature. The unique characteristic of the MH is that it takes into consideration both color and edge orientation information in an effective manner. Experimental results show that it outperforms many existing methods which were originally developed for image retrieval purposes.



### Context-Aware Image Matting for Simultaneous Foreground and Alpha Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.09725v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09725v2)
- **Published**: 2019-09-20 21:36:30+00:00
- **Updated**: 2019-10-02 23:24:17+00:00
- **Authors**: Qiqi Hou, Feng Liu
- **Comment**: This is the camera ready version of ICCV2019 paper
- **Journal**: None
- **Summary**: Natural image matting is an important problem in computer vision and graphics. It is an ill-posed problem when only an input image is available without any external information. While the recent deep learning approaches have shown promising results, they only estimate the alpha matte. This paper presents a context-aware natural image matting method for simultaneous foreground and alpha matte estimation. Our method employs two encoder networks to extract essential information for matting. Particularly, we use a matting encoder to learn local features and a context encoder to obtain more global context information. We concatenate the outputs from these two encoders and feed them into decoder networks to simultaneously estimate the foreground and alpha matte. To train this whole deep neural network, we employ both the standard Laplacian loss and the feature loss: the former helps to achieve high numerical performance while the latter leads to more perceptually plausible results. We also report several data augmentation strategies that greatly improve the network's generalization performance. Our qualitative and quantitative experiments show that our method enables high-quality matting for a single natural image. Our inference codes and models have been made publicly available at https://github.com/hqqxyy/Context-Aware-Matting.



### Deep Generative Models for Library Augmentation in Multiple Endmember Spectral Mixture Analysis
- **Arxiv ID**: http://arxiv.org/abs/1909.09741v2
- **DOI**: 10.1109/LGRS.2020.3007161
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.09741v2)
- **Published**: 2019-09-20 23:51:24+00:00
- **Updated**: 2020-07-02 01:12:29+00:00
- **Authors**: Ricardo Augusto Borsoi, Tales Imbiriba, Jos√© Carlos Moreira Bermudez, C√©dric Richard
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple Endmember Spectral Mixture Analysis (MESMA) is one of the leading approaches to perform spectral unmixing (SU) considering variability of the endmembers (EMs). It represents each EM in the image using libraries of spectral signatures acquired a priori. However, existing spectral libraries are often small and unable to properly capture the variability of each EM in practical scenes, which compromises the performance of MESMA. In this paper, we propose a library augmentation strategy to increase the diversity of existing spectral libraries, thus improving their ability to represent the materials in real images. First, we leverage the power of deep generative models to learn the statistical distribution of the EMs based on the spectral signatures available in the existing libraries. Afterwards, new samples can be drawn from the learned EM distributions and used to augment the spectral libraries, improving the overall quality of the SU process. Experimental results using synthetic and real data attest the superior performance of the proposed method even under library mismatch conditions.




# Arxiv Papers in cs.CV on 2019-09-10
### Confidence Measure Guided Single Image De-raining
- **Arxiv ID**: http://arxiv.org/abs/1909.04207v1
- **DOI**: 10.1109/TIP.2020.2973802
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04207v1)
- **Published**: 2019-09-10 00:42:51+00:00
- **Updated**: 2019-09-10 00:42:51+00:00
- **Authors**: Rajeev Yasarla, Vishal M. Patel
- **Comment**: TIP2019 submission. arXiv admin note: substantial text overlap with
  arXiv:1906.11129
- **Journal**: None
- **Summary**: Single image de-raining is an extremely challenging problem since the rainy images contain rain streaks which often vary in size, direction and density. This varying characteristic of rain streaks affect different parts of the image differently. Previous approaches have attempted to address this problem by leveraging some prior information to remove rain streaks from a single image. One of the major limitations of these approaches is that they do not consider the location information of rain drops in the image. The proposed Image Quality-based single image Deraining using Confidence measure (QuDeC), network addresses this issue by learning the quality or distortion level of each patch in the rainy image, and further processes this information to learn the rain content at different scales. In addition, we introduce a technique which guides the network to learn the network weights based on the confidence measure about the estimate of both quality at each location and residual rain streak information (residual map). Extensive experiments on synthetic and real datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods.



### Swapped Face Detection using Deep Learning and Subjective Assessment
- **Arxiv ID**: http://arxiv.org/abs/1909.04217v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.04217v1)
- **Published**: 2019-09-10 01:06:43+00:00
- **Updated**: 2019-09-10 01:06:43+00:00
- **Authors**: Xinyi Ding, Zohreh Raziei, Eric C. Larson, Eli V. Olinick, Paul Krueger, Michael Hahsler
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: The tremendous success of deep learning for imaging applications has resulted in numerous beneficial advances. Unfortunately, this success has also been a catalyst for malicious uses such as photo-realistic face swapping of parties without consent. Transferring one person's face from a source image to a target image of another person, while keeping the image photo-realistic overall has become increasingly easy and automatic, even for individuals without much knowledge of image processing. In this study, we use deep transfer learning for face swapping detection, showing true positive rates >96% with very few false alarms. Distinguished from existing methods that only provide detection accuracy, we also provide uncertainty for each prediction, which is critical for trust in the deployment of such detection systems. Moreover, we provide a comparison to human subjects. To capture human recognition performance, we build a website to collect pairwise comparisons of images from human subjects. Based on these comparisons, images are ranked from most real to most fake. We compare this ranking to the outputs from our automatic model, showing good, but imperfect, correspondence with linear correlations >0.75. Overall, the results show the effectiveness of our method. As part of this study, we create a novel, publicly available dataset that is, to the best of our knowledge, the largest public swapped face dataset created using still images. Our goal of this study is to inspire more research in the field of image forensics through the creation of a public dataset and initial analysis.



### Multimodal Attention Branch Network for Perspective-Free Sentence Generation
- **Arxiv ID**: http://arxiv.org/abs/1909.05664v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.05664v1)
- **Published**: 2019-09-10 01:10:24+00:00
- **Updated**: 2019-09-10 01:10:24+00:00
- **Authors**: Aly Magassouba, Komei Sugiura, Hisashi Kawai
- **Comment**: 10 pages, 4 figures. Accepted for CoRL 2019
- **Journal**: None
- **Summary**: In this paper, we address the automatic sentence generation of fetching instructions for domestic service robots. Typical fetching commands such as "bring me the yellow toy from the upper part of the white shelf" includes referring expressions, i.e., "from the white upper part of the white shelf". To solve this task, we propose a multimodal attention branch network (Multi-ABN) which generates natural sentences in an end-to-end manner. Multi-ABN uses multiple images of the same fixed scene to generate sentences that are not tied to a particular viewpoint. This approach combines a linguistic attention branch mechanism with several attention branch mechanisms. We evaluated our approach, which outperforms the state-of-the-art method on a standard metrics. Our method also allows us to visualize the alignment between the linguistic input and the visual features.



### MVP-Net: Multi-view FPN with Position-aware Attention for Deep Universal Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.04247v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04247v3)
- **Published**: 2019-09-10 02:48:20+00:00
- **Updated**: 2019-12-16 12:14:12+00:00
- **Authors**: Zihao Li, Shu Zhang, Junge Zhang, Kaiqi Huang, Yizhou Wang, Yizhou Yu
- **Comment**: Accepted by MICCAI 2019 (Medical Image Computing and Computer
  Assisted Intervention)
- **Journal**: None
- **Summary**: Universal lesion detection (ULD) on computed tomography (CT) images is an important but underdeveloped problem. Recently, deep learning-based approaches have been proposed for ULD, aiming to learn representative features from annotated CT data. However, the hunger for data of deep learning models and the scarcity of medical annotation hinders these approaches to advance further. In this paper, we propose to incorporate domain knowledge in clinical practice into the model design of universal lesion detectors. Specifically, as radiologists tend to inspect multiple windows for an accurate diagnosis, we explicitly model this process and propose a multi-view feature pyramid network (FPN), where multi-view features are extracted from images rendered with varied window widths and window levels; to effectively combine this multi-view information, we further propose a position-aware attention module. With the proposed model design, the data-hunger problem is relieved as the learning task is made easier with the correctly induced clinical practice prior. We show promising results with the proposed model, achieving an absolute gain of $\mathbf{5.65\%}$ (in the sensitivity of FPs@4.0) over the previous state-of-the-art on the NIH DeepLesion dataset.



### Real-time Scalable Dense Surfel Mapping
- **Arxiv ID**: http://arxiv.org/abs/1909.04250v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04250v1)
- **Published**: 2019-09-10 02:56:40+00:00
- **Updated**: 2019-09-10 02:56:40+00:00
- **Authors**: Kaixuan Wang, Fei Gao, Shaojie Shen
- **Comment**: This is a ICRA 2019 paper. Source code available at
  https://github.com/HKUST-Aerial-Robotics/DenseSurfelMapping
- **Journal**: ICRA 2019
- **Summary**: In this paper, we propose a novel dense surfel mapping system that scales well in different environments with only CPU computation. Using a sparse SLAM system to estimate camera poses, the proposed mapping system can fuse intensity images and depth images into a globally consistent model. The system is carefully designed so that it can build from room-scale environments to urban-scale environments using depth images from RGB-D cameras, stereo cameras or even a monocular camera. First, superpixels extracted from both intensity and depth images are used to model surfels in the system. superpixel-based surfels make our method both run-time efficient and memory efficient. Second, surfels are further organized according to the pose graph of the SLAM system to achieve $O(1)$ fusion time regardless of the scale of reconstructed models. Third, a fast map deformation using the optimized pose graph enables the map to achieve global consistency in real-time. The proposed surfel mapping system is compared with other state-of-the-art methods on synthetic datasets. The performances of urban-scale and room-scale reconstruction are demonstrated using the KITTI dataset and autonomous aggressive flights, respectively. The code is available for the benefit of the community.



### GlassLoc: Plenoptic Grasp Pose Detection in Transparent Clutter
- **Arxiv ID**: http://arxiv.org/abs/1909.04269v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04269v2)
- **Published**: 2019-09-10 03:53:15+00:00
- **Updated**: 2019-09-17 18:25:19+00:00
- **Authors**: Zheming Zhou, Tianyang Pan, Shiyu Wu, Haonan Chang, Odest Chadwicke Jenkins
- **Comment**: Accepted to the 2019 IEEE/RSJ International Conference on Intelligent
  Robots and Systems. Contact: Zheming Zhou, zhezhou@umich.edu
- **Journal**: None
- **Summary**: Transparent objects are prevalent across many environments of interest for dexterous robotic manipulation. Such transparent material leads to considerable uncertainty for robot perception and manipulation, and remains an open challenge for robotics. This problem is exacerbated when multiple transparent objects cluster into piles of clutter. In household environments, for example, it is common to encounter piles of glassware in kitchens, dining rooms, and reception areas, which are essentially invisible to modern robots. We present the GlassLoc algorithm for grasp pose detection of transparent objects in transparent clutter using plenoptic sensing. GlassLoc classifies graspable locations in space informed by a Depth Likelihood Volume (DLV) descriptor. We extend the DLV to infer the occupancy of transparent objects over a given space from multiple plenoptic viewpoints. We demonstrate and evaluate the GlassLoc algorithm on a Michigan Progress Fetch mounted with a first-generation Lytro. The effectiveness of our algorithm is evaluated through experiments for grasp detection and execution with a variety of transparent glassware in minor clutter.



### Knowledge Transfer Graph for Deep Collaborative Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.04286v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.04286v2)
- **Published**: 2019-09-10 04:56:29+00:00
- **Updated**: 2019-12-17 03:46:16+00:00
- **Authors**: Soma Minami, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Knowledge transfer among multiple networks using their outputs or intermediate activations have evolved through extensive manual design from a simple teacher-student approach (knowledge distillation) to a bidirectional cohort one (deep mutual learning). The key factors of such knowledge transfer involve the network size, the number of networks, the transfer direction, and the design of the loss function. However, because these factors are enormous when combined and become intricately entangled, the methods of conventional knowledge transfer have explored only limited combinations. In this paper, we propose a new graph-based approach for more flexible and diverse combinations of knowledge transfer. To achieve the knowledge transfer, we propose a novel graph representation called knowledge transfer graph that provides a unified view of the knowledge transfer and has the potential to represent diverse knowledge transfer patterns. We also propose four gate functions that are introduced into loss functions. The four gates, which control the gradient, can deliver diverse combinations of knowledge transfer. Searching the graph structure enables us to discover more effective knowledge transfer methods than a manually designed one. Experimental results on the CIFAR-10, -100, and Tiny-ImageNet datasets show that the proposed method achieved significant performance improvements and was able to find remarkable graph structures.



### Bayesian Relational Memory for Semantic Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/1909.04306v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.04306v1)
- **Published**: 2019-09-10 06:02:15+00:00
- **Updated**: 2019-09-10 06:02:15+00:00
- **Authors**: Yi Wu, Yuxin Wu, Aviv Tamar, Stuart Russell, Georgia Gkioxari, Yuandong Tian
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: We introduce a new memory architecture, Bayesian Relational Memory (BRM), to improve the generalization ability for semantic visual navigation agents in unseen environments, where an agent is given a semantic target to navigate towards. BRM takes the form of a probabilistic relation graph over semantic entities (e.g., room types), which allows (1) capturing the layout prior from training environments, i.e., prior knowledge, (2) estimating posterior layout at test time, i.e., memory update, and (3) efficient planning for navigation, altogether. We develop a BRM agent consisting of a BRM module for producing sub-goals and a goal-conditioned locomotion module for control. When testing in unseen environments, the BRM agent outperforms baselines that do not explicitly utilize the probabilistic relational memory structure



### Learning Actions from Human Demonstration Video for Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1909.04312v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.04312v1)
- **Published**: 2019-09-10 06:20:46+00:00
- **Updated**: 2019-09-10 06:20:46+00:00
- **Authors**: Shuo Yang, Wei Zhang, Weizhi Lu, Hesheng Wang, Yibin Li
- **Comment**: Accepted by IROS 2019
- **Journal**: None
- **Summary**: Learning actions from human demonstration is an emerging trend for designing intelligent robotic systems, which can be referred as video to command. The performance of such approach highly relies on the quality of video captioning. However, the general video captioning methods focus more on the understanding of the full frame, lacking of consideration on the specific object of interests in robotic manipulations. We propose a novel deep model to learn actions from human demonstration video for robotic manipulation. It consists of two deep networks, grasp detection network (GNet) and video captioning network (CNet). GNet performs two functions: providing grasp solutions and extracting the local features for the object of interests in robotic manipulation. CNet outputs the captioning results by fusing the features of both full frames and local objects. Experimental results on UR5 robotic arm show that our method could produce more accurate command from video demonstration than state-of-the-art work, thereby leading to more robust grasping performance.



### Inducing Hierarchical Compositional Model by Sparsifying Generator Network
- **Arxiv ID**: http://arxiv.org/abs/1909.04324v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.04324v2)
- **Published**: 2019-09-10 07:06:33+00:00
- **Updated**: 2020-06-20 05:02:00+00:00
- **Authors**: Xianglei Xing, Tianfu Wu, Song-Chun Zhu, Ying Nian Wu
- **Comment**: This is the CVPR version
- **Journal**: None
- **Summary**: This paper proposes to learn hierarchical compositional AND-OR model for interpretable image synthesis by sparsifying the generator network. The proposed method adopts the scene-objects-parts-subparts-primitives hierarchy in image representation. A scene has different types (i.e., OR) each of which consists of a number of objects (i.e., AND). This can be recursively formulated across the scene-objects-parts-subparts hierarchy and is terminated at the primitive level (e.g., wavelets-like basis). To realize this AND-OR hierarchy in image synthesis, we learn a generator network that consists of the following two components: (i) Each layer of the hierarchy is represented by an over-complete set of convolutional basis functions. Off-the-shelf convolutional neural architectures are exploited to implement the hierarchy. (ii) Sparsity-inducing constraints are introduced in end-to-end training, which induces a sparsely activated and sparsely connected AND-OR model from the initially densely connected generator network. A straightforward sparsity-inducing constraint is utilized, that is to only allow the top-$k$ basis functions to be activated at each layer (where $k$ is a hyper-parameter). The learned basis functions are also capable of image reconstruction to explain the input images. In experiments, the proposed method is tested on four benchmark datasets. The results show that meaningful and interpretable hierarchical representations are learned with better qualities of image synthesis and reconstruction obtained than baselines.



### Universal Physical Camouflage Attacks on Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1909.04326v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04326v2)
- **Published**: 2019-09-10 07:16:32+00:00
- **Updated**: 2020-04-21 22:27:51+00:00
- **Authors**: Lifeng Huang, Chengying Gao, Yuyin Zhou, Cihang Xie, Alan Yuille, Changqing Zou, Ning Liu
- **Comment**: CVPR 2020; codes, models, and demos are available at
  https://mesunhlf.github.io/index_physical.html
- **Journal**: None
- **Summary**: In this paper, we study physical adversarial attacks on object detectors in the wild. Previous works mostly craft instance-dependent perturbations only for rigid or planar objects. To this end, we propose to learn an adversarial pattern to effectively attack all instances belonging to the same object category, referred to as Universal Physical Camouflage Attack (UPC). Concretely, UPC crafts camouflage by jointly fooling the region proposal network, as well as misleading the classifier and the regressor to output errors. In order to make UPC effective for non-rigid or non-planar objects, we introduce a set of transformations for mimicking deformable properties. We additionally impose optimization constraint to make generated patterns look natural to human observers. To fairly evaluate the effectiveness of different physical-world attacks, we present the first standardized virtual database, AttackScenes, which simulates the real 3D world in a controllable and reproducible environment. Extensive experiments suggest the superiority of our proposed UPC compared with existing physical adversarial attackers not only in virtual environments (AttackScenes), but also in real-world physical environments. Code and dataset are available at https://mesunhlf.github.io/index_physical.html.



### PARN: Position-Aware Relation Networks for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.04332v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04332v1)
- **Published**: 2019-09-10 07:39:32+00:00
- **Updated**: 2019-09-10 07:39:32+00:00
- **Authors**: Ziyang Wu, Yuwei Li, Lihua Guo, Kui Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning presents a challenge that a classifier must quickly adapt to new classes that do not appear in the training set, given only a few labeled examples of each new class. This paper proposes a position-aware relation network (PARN) to learn a more flexible and robust metric ability for few-shot learning. Relation networks (RNs), a kind of architectures for relational reasoning, can acquire a deep metric ability for images by just being designed as a simple convolutional neural network (CNN) [23]. However, due to the inherent local connectivity of CNN, the CNN-based relation network (RN) can be sensitive to the spatial position relationship of semantic objects in two compared images. To address this problem, we introduce a deformable feature extractor (DFE) to extract more efficient features, and design a dual correlation attention mechanism (DCA) to deal with its inherent local connectivity. Successfully, our proposed approach extents the potential of RN to be position-aware of semantic objects by introducing only a small number of parameters. We evaluate our approach on two major benchmark datasets, i.e., Omniglot and Mini-Imagenet, and on both of the datasets our approach achieves state-of-the-art performance with the setting of using a shallow feature extraction network. It's worth noting that our 5-way 1-shot result on Omniglot even outperforms the previous 5-way 5-shot results.



### A Meta-Learning Framework for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.04344v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.04344v1)
- **Published**: 2019-09-10 08:11:46+00:00
- **Updated**: 2019-09-10 08:11:46+00:00
- **Authors**: Vinay Kumar Verma, Dhanajit Brahma, Piyush Rai
- **Comment**: Under Submission
- **Journal**: None
- **Summary**: Learning to classify unseen class samples at test time is popularly referred to as zero-shot learning (ZSL). If test samples can be from training (seen) as well as unseen classes, it is a more challenging problem due to the existence of strong bias towards seen classes. This problem is generally known as \emph{generalized} zero-shot learning (GZSL). Thanks to the recent advances in generative models such as VAEs and GANs, sample synthesis based approaches have gained considerable attention for solving this problem. These approaches are able to handle the problem of class bias by synthesizing unseen class samples. However, these ZSL/GZSL models suffer due to the following key limitations: $(i)$ Their training stage learns a class-conditioned generator using only \emph{seen} class data and the training stage does not \emph{explicitly} learn to generate the unseen class samples; $(ii)$ They do not learn a generic optimal parameter which can easily generalize for both seen and unseen class generation; and $(iii)$ If we only have access to a very few samples per seen class, these models tend to perform poorly. In this paper, we propose a meta-learning based generative model that naturally handles these limitations. The proposed model is based on integrating model-agnostic meta learning with a Wasserstein GAN (WGAN) to handle $(i)$ and $(iii)$, and uses a novel task distribution to handle $(ii)$. Our proposed model yields significant improvements on standard ZSL as well as more challenging GZSL setting. In ZSL setting, our model yields 4.5\%, 6.0\%, 9.8\%, and 27.9\% relative improvements over the current state-of-the-art on CUB, AWA1, AWA2, and aPY datasets, respectively.



### FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from Single RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1909.04349v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.04349v3)
- **Published**: 2019-09-10 08:29:58+00:00
- **Updated**: 2019-09-13 09:04:40+00:00
- **Authors**: Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max Argus, Thomas Brox
- **Comment**: Accepted to ICCV 2019, Project page:
  https://lmb.informatik.uni-freiburg.de/projects/freihand/
- **Journal**: None
- **Summary**: Estimating 3D hand pose from single RGB images is a highly ambiguous problem that relies on an unbiased training dataset. In this paper, we analyze cross-dataset generalization when training on existing datasets. We find that approaches perform well on the datasets they are trained on, but do not generalize to other datasets or in-the-wild scenarios. As a consequence, we introduce the first large-scale, multi-view hand dataset that is accompanied by both 3D hand pose and shape annotations. For annotating this real-world dataset, we propose an iterative, semi-automated `human-in-the-loop' approach, which includes hand fitting optimization to infer both the 3D pose and shape for each sample. We show that methods trained on our dataset consistently perform well when tested on other datasets. Moreover, the dataset allows us to train a network that predicts the full articulated hand shape from a single RGB image. The evaluation set can serve as a benchmark for articulated hand shape estimation.



### Cross-Spectral Face Hallucination via Disentangling Independent Factors
- **Arxiv ID**: http://arxiv.org/abs/1909.04365v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04365v2)
- **Published**: 2019-09-10 09:26:42+00:00
- **Updated**: 2020-03-31 08:52:08+00:00
- **Authors**: Boyan Duan, Chaoyou Fu, Yi Li, Xingguang Song, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: The cross-sensor gap is one of the challenges that have aroused much research interests in Heterogeneous Face Recognition (HFR). Although recent methods have attempted to fill the gap with deep generative networks, most of them suffer from the inevitable misalignment between different face modalities. Instead of imaging sensors, the misalignment primarily results from facial geometric variations that are independent of the spectrum. Rather than building a monolithic but complex structure, this paper proposes a Pose Aligned Cross-spectral Hallucination (PACH) approach to disentangle the independent factors and deal with them in individual stages. In the first stage, an Unsupervised Face Alignment (UFA) module is designed to align the facial shapes of the near-infrared (NIR) images with those of the visible (VIS) images in a generative way, where UV maps are effectively utilized as the shape guidance. Thus the task of the second stage becomes spectrum translation with aligned paired data. We develop a Texture Prior Synthesis (TPS) module to achieve complexion control and consequently generate more realistic VIS images than existing methods. Experiments on three challenging NIR-VIS datasets verify the effectiveness of our approach in producing visually appealing images and achieving state-of-the-art performance in HFR.



### Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.04366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04366v1)
- **Published**: 2019-09-10 09:29:50+00:00
- **Updated**: 2019-09-10 09:29:50+00:00
- **Authors**: Yingyue Xu, Dan Xu, Xiaopeng Hong, Wanli Ouyang, Rongrong Ji, Min Xu, Guoying Zhao
- **Comment**: Accepted to ICCV
- **Journal**: None
- **Summary**: Recent saliency models extensively explore to incorporate multi-scale contextual information from Convolutional Neural Networks (CNNs). Besides direct fusion strategies, many approaches introduce message-passing to enhance CNN features or predictions. However, the messages are mainly transmitted in two ways, by feature-to-feature passing, and by prediction-to-prediction passing. In this paper, we add message-passing between features and predictions and propose a deep unified CRF saliency model . We design a novel cascade CRFs architecture with CNN to jointly refine deep features and predictions at each scale and progressively compute a final refined saliency map. We formulate the CRF graphical model that involves message-passing of feature-feature, feature-prediction, and prediction-prediction, from the coarse scale to the finer scale, to update the features and the corresponding predictions. Also, we formulate the mean-field updates for joint end-to-end model training with CNN through back propagation. The proposed deep unified CRF saliency model is evaluated over six datasets and shows highly competitive performance among the state of the arts.



### GBDT-MO: Gradient Boosted Decision Trees for Multiple Outputs
- **Arxiv ID**: http://arxiv.org/abs/1909.04373v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.04373v2)
- **Published**: 2019-09-10 09:48:04+00:00
- **Updated**: 2019-12-28 08:29:43+00:00
- **Authors**: Zhendong Zhang, Cheolkon Jung
- **Comment**: 10 pages, 5 figtures
- **Journal**: None
- **Summary**: Gradient boosted decision trees (GBDTs) are widely used in machine learning, and the output of current GBDT implementations is a single variable. When there are multiple outputs, GBDT constructs multiple trees corresponding to the output variables. The correlations between variables are ignored by such a strategy causing redundancy of the learned tree structures. In this paper, we propose a general method to learn GBDT for multiple outputs, called GBDT-MO. Each leaf of GBDT-MO constructs predictions of all variables or a subset of automatically selected variables. This is achieved by considering the summation of objective gains over all output variables. Moreover, we extend histogram approximation into multiple output case to speed up the training process. Various experiments on synthetic and real-world datasets verify that GBDT-MO achieves outstanding performance in terms of both accuracy and training speed. Our codes are available on-line.



### RefineFace: Refinement Neural Network for High Performance Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.04376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04376v1)
- **Published**: 2019-09-10 09:58:50+00:00
- **Updated**: 2019-09-10 09:58:50+00:00
- **Authors**: Shifeng Zhang, Cheng Chi, Zhen Lei, Stan Z. Li
- **Comment**: Journal extension of our previous conference paper: arXiv:1809.02693.
  arXiv admin note: text overlap with arXiv:1901.02350 by other authors
- **Journal**: None
- **Summary**: Face detection has achieved significant progress in recent years. However, high performance face detection still remains a very challenging problem, especially when there exists many tiny faces. In this paper, we present a single-shot refinement face detector namely RefineFace to achieve high performance. Specifically, it consists of five modules: Selective Two-step Regression (STR), Selective Two-step Classification (STC), Scale-aware Margin Loss (SML), Feature Supervision Module (FSM) and Receptive Field Enhancement (RFE). To enhance the regression ability for high location accuracy, STR coarsely adjusts locations and sizes of anchors from high level detection layers to provide better initialization for subsequent regressor. To improve the classification ability for high recall efficiency, STC first filters out most simple negatives from low level detection layers to reduce search space for subsequent classifier, then SML is applied to better distinguish faces from background at various scales and FSM is introduced to let the backbone learn more discriminative features for classification. Besides, RFE is presented to provide more diverse receptive field to better capture faces in some extreme poses. Extensive experiments conducted on WIDER FACE, AFW, PASCAL Face, FDDB, MAFA demonstrate that our method achieves state-of-the-art results and runs at $37.3$ FPS with ResNet-18 for VGA-resolution images.



### FDA: Feature Disruptive Attack
- **Arxiv ID**: http://arxiv.org/abs/1909.04385v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.04385v1)
- **Published**: 2019-09-10 10:09:38+00:00
- **Updated**: 2019-09-10 10:09:38+00:00
- **Authors**: Aditya Ganeshan, B. S. Vivek, R. Venkatesh Babu
- **Comment**: Accepted in ICCV;19. Code Available at
  https://github.com/BardOfCodes/fda
- **Journal**: None
- **Summary**: Though Deep Neural Networks (DNN) show excellent performance across various computer vision tasks, several works show their vulnerability to adversarial samples, i.e., image samples with imperceptible noise engineered to manipulate the network's prediction. Adversarial sample generation methods range from simple to complex optimization techniques. Majority of these methods generate adversaries through optimization objectives that are tied to the pre-softmax or softmax output of the network. In this work we, (i) show the drawbacks of such attacks, (ii) propose two new evaluation metrics: Old Label New Rank (OLNR) and New Label Old Rank (NLOR) in order to quantify the extent of damage made by an attack, and (iii) propose a new adversarial attack FDA: Feature Disruptive Attack, to address the drawbacks of existing attacks. FDA works by generating image perturbation that disrupt features at each layer of the network and causes deep-features to be highly corrupt. This allows FDA adversaries to severely reduce the performance of deep networks. We experimentally validate that FDA generates stronger adversaries than other state-of-the-art methods for image classification, even in the presence of various defense measures. More importantly, we show that FDA disrupts feature-representation based tasks even without access to the task-specific network or methodology. Code available at: https://github.com/BardOfCodes/fda



### JSI-GAN: GAN-Based Joint Super-Resolution and Inverse Tone-Mapping with Pixel-Wise Task-Specific Filters for UHD HDR Video
- **Arxiv ID**: http://arxiv.org/abs/1909.04391v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04391v2)
- **Published**: 2019-09-10 10:30:35+00:00
- **Updated**: 2019-12-16 06:20:32+00:00
- **Authors**: Soo Ye Kim, Jihyong Oh, Munchurl Kim
- **Comment**: The first two authors contributed equally to this work. Accepted at
  AAAI 2020. (Camera-ready version)
- **Journal**: None
- **Summary**: Joint learning of super-resolution (SR) and inverse tone-mapping (ITM) has been explored recently, to convert legacy low resolution (LR) standard dynamic range (SDR) videos to high resolution (HR) high dynamic range (HDR) videos for the growing need of UHD HDR TV/broadcasting applications. However, previous CNN-based methods directly reconstruct the HR HDR frames from LR SDR frames, and are only trained with a simple L2 loss. In this paper, we take a divide-and-conquer approach in designing a novel GAN-based joint SR-ITM network, called JSI-GAN, which is composed of three task-specific subnets: an image reconstruction subnet, a detail restoration (DR) subnet and a local contrast enhancement (LCE) subnet. We delicately design these subnets so that they are appropriately trained for the intended purpose, learning a pair of pixel-wise 1D separable filters via the DR subnet for detail restoration and a pixel-wise 2D local filter by the LCE subnet for contrast enhancement. Moreover, to train the JSI-GAN effectively, we propose a novel detail GAN loss alongside the conventional GAN loss, which helps enhancing both local details and contrasts to reconstruct high quality HR HDR results. When all subnets are jointly trained well, the predicted HR HDR results of higher quality are obtained with at least 0.41 dB gain in PSNR over those generated by the previous methods.



### Compositional Generalization in Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1909.04402v2
- **DOI**: 10.18653/v1/K19-1009
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.04402v2)
- **Published**: 2019-09-10 10:55:56+00:00
- **Updated**: 2019-09-16 15:53:45+00:00
- **Authors**: Mitja Nikolaus, Mostafa Abdou, Matthew Lamm, Rahul Aralikatte, Desmond Elliott
- **Comment**: To appear at CoNLL 2019, EMNLP
- **Journal**: Proceedings of the 23rd Conference on Computational Natural
  Language Learning (CoNLL), pp. 87--98, ACL, 2019
- **Summary**: Image captioning models are usually evaluated on their ability to describe a held-out set of images, not on their ability to generalize to unseen concepts. We study the problem of compositional generalization, which measures how well a model composes unseen combinations of concepts when describing images. State-of-the-art image captioning models show poor generalization performance on this task. We propose a multi-task model to address the poor performance, that combines caption generation and image--sentence ranking, and uses a decoding mechanism that re-ranks the captions according their similarity to the image. This model is substantially better at generalizing to unseen combinations of concepts compared to state-of-the-art captioning models.



### U-net super-neural segmentation and similarity calculation to realize vegetation change assessment in satellite imagery
- **Arxiv ID**: http://arxiv.org/abs/1909.04410v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04410v1)
- **Published**: 2019-09-10 11:13:55+00:00
- **Updated**: 2019-09-10 11:13:55+00:00
- **Authors**: Chunxue Wu, Bobo Ju, Naixue Xiong, Guisong Yang, Yan Wu, Hongming Yang, Jiaying Huang, Zhiyong Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Vegetation is the natural linkage connecting soil, atmosphere and water. It can represent the change of land cover to a certain extent and serve as an indicator for global change research. Methods for measuring coverage can be divided into two types: surface measurement and remote sensing. Because vegetation cover has significant spatial and temporal differentiation characteristics, remote sensing has become an important technical means to estimate vegetation coverage. This paper firstly uses U-net to perform remote sensing image semantic segmentation training, then uses the result of semantic segmentation, and then uses the integral progressive method to calculate the forestland change rate, and finally realizes automated valuation of woodland change rate.



### Cross-X Learning for Fine-Grained Visual Categorization
- **Arxiv ID**: http://arxiv.org/abs/1909.04412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04412v1)
- **Published**: 2019-09-10 11:20:07+00:00
- **Updated**: 2019-09-10 11:20:07+00:00
- **Authors**: Wei Luo, Xitong Yang, Xianjie Mo, Yuheng Lu, Larry S. Davis, Jun Li, Jian Yang, Ser-Nam Lim
- **Comment**: accepted by ICCV 2019
- **Journal**: None
- **Summary**: Recognizing objects from subcategories with very subtle differences remains a challenging task due to the large intra-class and small inter-class variation. Recent work tackles this problem in a weakly-supervised manner: object parts are first detected and the corresponding part-specific features are extracted for fine-grained classification. However, these methods typically treat the part-specific features of each image in isolation while neglecting their relationships between different images. In this paper, we propose Cross-X learning, a simple yet effective approach that exploits the relationships between different images and between different network layers for robust multi-scale feature learning. Our approach involves two novel components: (i) a cross-category cross-semantic regularizer that guides the extracted features to represent semantic parts and, (ii) a cross-layer regularizer that improves the robustness of multi-scale features by matching the prediction distribution across multiple layers. Our approach can be easily trained end-to-end and is scalable to large datasets like NABirds. We empirically analyze the contributions of different components of our approach and demonstrate its robustness, effectiveness and state-of-the-art performance on five benchmark datasets. Code is available at \url{https://github.com/cswluo/CrossX}.



### The Mapillary Traffic Sign Dataset for Detection and Classification on a Global Scale
- **Arxiv ID**: http://arxiv.org/abs/1909.04422v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04422v2)
- **Published**: 2019-09-10 11:41:01+00:00
- **Updated**: 2020-05-07 11:10:54+00:00
- **Authors**: Christian Ertler, Jerneja Mislej, Tobias Ollmann, Lorenzo Porzi, Gerhard Neuhold, Yubin Kuang
- **Comment**: 17 pages, 15 figures
- **Journal**: None
- **Summary**: Traffic signs are essential map features globally in the era of autonomous driving and smart cities. To develop accurate and robust algorithms for traffic sign detection and classification, a large-scale and diverse benchmark dataset is required. In this paper, we introduce a traffic sign benchmark dataset of 100K street-level images around the world that encapsulates diverse scenes, wide coverage of geographical locations, and varying weather and lighting conditions and covers more than 300 manually annotated traffic sign classes. The dataset includes 52K images that are fully annotated and 48K images that are partially annotated. This is the largest and the most diverse traffic sign dataset consisting of images from all over world with fine-grained annotations of traffic sign classes. We have run extensive experiments to establish strong baselines for both the detection and the classification tasks. In addition, we have verified that the diversity of this dataset enables effective transfer learning for existing large-scale benchmark datasets on traffic sign detection and classification. The dataset is freely available for academic research: https://www.mapillary.com/dataset/trafficsign.



### Novel tracking approach based on fully-unsupervised disentanglement of the geometrical factors of variation
- **Arxiv ID**: http://arxiv.org/abs/1909.04427v2
- **DOI**: 10.1088/1748-0221/15/03/P03009
- **Categories**: **cs.CV**, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/1909.04427v2)
- **Published**: 2019-09-10 12:05:59+00:00
- **Updated**: 2020-02-13 15:29:58+00:00
- **Authors**: Mykhailo Vladymyrov, Akitaka Ariga
- **Comment**: Accepted for publication in JINST
- **Journal**: None
- **Summary**: Efficient tracking algorithms are a crucial part of particle tracking detectors. While a lot of work has been done in designing a plethora of algorithms, these usually require tedious tuning for each use case. (Weakly) supervised Machine Learning-based approaches can leverage the actual raw data for maximal performance. Yet in realistic scenarios, sufficient high-quality labeled data is not available. While training might be performed on simulated data, the reproduction of realistic signal and noise in the detector requires substantial effort, compromising this approach.   Here we propose a novel, fully unsupervised, approach to track reconstruction. The introduced model for learning to disentangle the factors of variation in a geometrically meaningful way employs geometrical space invariances. We train it through constraints on the equivariance between the image space and the latent representation in a Deep Convolutional Autoencoder. Using experimental results on synthetic data we show that a combination of different space transformations is required for meaningful disentanglement of factors of variation. We also demonstrate the performance of our model on real data from tracking detectors.



### Raiders of the Lost Art
- **Arxiv ID**: http://arxiv.org/abs/1909.05677v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.05677v1)
- **Published**: 2019-09-10 12:14:04+00:00
- **Updated**: 2019-09-10 12:14:04+00:00
- **Authors**: Anthony Bourached, George Cann
- **Comment**: Submitted to NeurIPS workshop on Machine Learning for Creativity and
  Design
- **Journal**: None
- **Summary**: Neural style transfer, first proposed by Gatys et al. (2015), can be used to create novel artistic work through rendering a content image in the form of a style image. We present a novel method of reconstructing lost artwork, by applying neural style transfer to x-radiographs of artwork with secondary interior artwork beneath a primary exterior, so as to reconstruct lost artwork. Finally we reflect on AI art exhibitions and discuss the social, cultural, ethical, and philosophical impact of these technical innovations.



### Chargrid-OCR: End-to-end Trainable Optical Character Recognition for Printed Documents using Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.04469v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.04469v4)
- **Published**: 2019-09-10 13:30:55+00:00
- **Updated**: 2020-02-27 12:44:54+00:00
- **Authors**: Christian Reisswig, Anoop R Katti, Marco Spinaci, Johannes HÃ¶hne
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We present an end-to-end trainable approach for Optical Character Recognition (OCR) on printed documents. Specifically, we propose a model that predicts a) a two-dimensional character grid (\emph{chargrid}) representation of a document image as a semantic segmentation task and b) character boxes for delineating character instances as an object detection task. For training the model, we build two large-scale datasets without resorting to any manual annotation - synthetic documents with clean labels and real documents with noisy labels. We demonstrate experimentally that our method, trained on the combination of these datasets, (i) outperforms previous state-of-the-art approaches in accuracy (ii) is easily parallelizable on GPU and is, therefore, significantly faster and (iii) is easy to train and adapt to a new domain.



### VACL: Variance-Aware Cross-Layer Regularization for Pruning Deep Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.04485v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1909.04485v1)
- **Published**: 2019-09-10 13:59:04+00:00
- **Updated**: 2019-09-10 13:59:04+00:00
- **Authors**: Shuang Gao, Xin Liu, Lung-Sheng Chien, William Zhang, Jose M. Alvarez
- **Comment**: ICCV Workshop
- **Journal**: None
- **Summary**: Improving weight sparsity is a common strategy for producing light-weight deep neural networks. However, pruning models with residual learning is more challenging. In this paper, we introduce Variance-Aware Cross-Layer (VACL), a novel approach to address this problem. VACL consists of two parts, a Cross-Layer grouping and a Variance Aware regularization. In Cross-Layer grouping the $i^{th}$ filters of layers connected by skip-connections are grouped into one regularization group. Then, the Variance-Aware regularization term takes into account both the first and second-order statistics of the connected layers to constrain the variance within a group. Our approach can effectively improve the structural sparsity of residual models. For CIFAR10, the proposed method reduces a ResNet model by up to 79.5% with no accuracy drop and reduces a ResNeXt model by up to 82% with less than 1% accuracy drop. For ImageNet, it yields a pruned ratio of up to 63.3% with less than 1% top-5 accuracy drop. Our experimental results show that the proposed approach significantly outperforms other state-of-the-art methods in terms of overall model size and accuracy.



### Virtual organelle self-coding for fluorescence imaging via adversarial learning
- **Arxiv ID**: http://arxiv.org/abs/1909.04518v1
- **DOI**: 10.1117/1.JBO.25.9.096009
- **Categories**: **eess.IV**, cs.CV, q-bio.QM, 92B20
- **Links**: [PDF](http://arxiv.org/pdf/1909.04518v1)
- **Published**: 2019-09-10 14:26:38+00:00
- **Updated**: 2019-09-10 14:26:38+00:00
- **Authors**: Thanh Nguyen, Vy Bui, Anh Thai, Van Lam, Christopher B. Raub, Lin-Ching Chang, George Nehmetallah
- **Comment**: 20 pages, 9 figures
- **Journal**: None
- **Summary**: Fluorescence microscopy plays a vital role in understanding the subcellular structures of living cells. However, it requires considerable effort in sample preparation related to chemical fixation, staining, cost, and time. To reduce those factors, we present a virtual fluorescence staining method based on deep neural networks (VirFluoNet) to transform fluorescence images of molecular labels into other molecular fluorescence labels in the same field-of-view. To achieve this goal, we develop and train a conditional generative adversarial network (cGAN) to perform digital fluorescence imaging demonstrated on human osteosarcoma U2OS cell fluorescence images captured under Cell Painting staining protocol. A detailed comparative analysis is also conducted on the performance of the cGAN network between predicting fluorescence channels based on phase contrast or based on another fluorescence channel using human breast cancer MDA-MB-231 cell line as a test case. In addition, we implement a deep learning model to perform autofocusing on another human U2OS fluorescence dataset as a preprocessing step to defocus an out-focus channel in U2OS dataset. A quantitative index of image prediction error is introduced based on signal pixel-wise spatial and intensity differences with ground truth to evaluate the performance of prediction to high-complex and throughput fluorescence. This index provides a rational way to perform image segmentation on error signals and to understand the likelihood of mis-interpreting biology from the predicted image. In total, these findings contribute to the utility of deep learning image regression for fluorescence microscopy datasets of biological cells, balanced against savings of cost, time, and experimental effort. Furthermore, the approach introduced here holds promise for modeling the internal relationships between organelles and biomolecules within living cells.



### Skin cancer detection based on deep learning and entropy to detect outlier samples
- **Arxiv ID**: http://arxiv.org/abs/1909.04525v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.04525v2)
- **Published**: 2019-09-10 14:36:16+00:00
- **Updated**: 2020-01-05 22:45:19+00:00
- **Authors**: Andre G. C. Pacheco, Abder-Rahman Ali, Thomas Trappenberg
- **Comment**: 3rd and 4th places in tasks 1 and 2 respectively, at ISIC challenge
  2019 @ MICCAI workshop 2019
- **Journal**: None
- **Summary**: We describe our methods that achieved the 3rd and 4th places in tasks 1 and 2, respectively, at ISIC challenge 2019. The goal of this challenge is to provide the diagnostic for skin cancer using images and meta-data. There are nine classes in the dataset, nonetheless, one of them is an outlier and is not present on it. To tackle the challenge, we apply an ensemble of classifiers, which has 13 convolutional neural networks (CNN), we develop two approaches to handle the outlier class and we propose a straightforward method to use the meta-data along with the images. Throughout this report, we detail each methodology and parameters to make it easy to replicate our work. The results obtained are in accordance with the previous challenges and the approaches to detect the outlier class and to address the meta-data seem to be work properly.



### DeepPrivacy: A Generative Adversarial Network for Face Anonymization
- **Arxiv ID**: http://arxiv.org/abs/1909.04538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.04538v1)
- **Published**: 2019-09-10 14:52:24+00:00
- **Updated**: 2019-09-10 14:52:24+00:00
- **Authors**: HÃ¥kon HukkelÃ¥s, Rudolf Mester, Frank Lindseth
- **Comment**: Accepted to ISVC 2019
- **Journal**: None
- **Summary**: We propose a novel architecture which is able to automatically anonymize faces in images while retaining the original data distribution. We ensure total anonymization of all faces in an image by generating images exclusively on privacy-safe information. Our model is based on a conditional generative adversarial network, generating images considering the original pose and image background. The conditional information enables us to generate highly realistic faces with a seamless transition between the generated face and the existing background. Furthermore, we introduce a diverse dataset of human faces, including unconventional poses, occluded faces, and a vast variability in backgrounds. Finally, we present experimental results reflecting the capability of our model to anonymize images while preserving the data distribution, making the data suitable for further training of deep learning models. As far as we know, no other solution has been proposed that guarantees the anonymization of faces while generating realistic images.



### Integrating cross-modality hallucinated MRI with CT to aid mediastinal lung tumor segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.04542v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04542v1)
- **Published**: 2019-09-10 14:56:32+00:00
- **Updated**: 2019-09-10 14:56:32+00:00
- **Authors**: Jue Jiang, Jason Hu, Neelam Tyagi, Andreas Rimner, Sean L. Berry, Joseph O. Deasy, Harini Veeraraghavan
- **Comment**: This paper has been accepted by MICCAI 2019
- **Journal**: None
- **Summary**: Lung tumors, especially those located close to or surrounded by soft tissues like the mediastinum, are difficult to segment due to the low soft tissue contrast on computed tomography images. Magnetic resonance images contain superior soft-tissue contrast information that can be leveraged if both modalities were available for training. Therefore, we developed a cross-modality educed learning approach where MR information that is educed from CT is used to hallucinate MRI and improve CT segmentation. Our approach, called cross-modality educed deep learning segmentation (CMEDL) combines CT and pseudo MR produced from CT by aligning their features to obtain segmentation on CT. Features computed in the last two layers of parallelly trained CT and MR segmentation networks are aligned. We implemented this approach on U-net and dense fully convolutional networks (dense-FCN). Our networks were trained on unrelated cohorts from open-source the Cancer Imaging Archive CT images (N=377), an internal archive T2-weighted MR (N=81), and evaluated using separate validation (N=304) and testing (N=333) CT-delineated tumors. Our approach using both networks were significantly more accurate (U-net $P <0.001$; denseFCN $P <0.001$) than CT-only networks and achieved an accuracy (Dice similarity coefficient) of 0.71$\pm$0.15 (U-net), 0.74$\pm$0.12 (denseFCN) on validation and 0.72$\pm$0.14 (U-net), 0.73$\pm$0.12 (denseFCN) on the testing sets. Our novel approach demonstrated that educing cross-modality information through learned priors enhances CT segmentation performance



### Automatic Hip Fracture Identification and Functional Subclassification with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.06326v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1909.06326v1)
- **Published**: 2019-09-10 15:03:43+00:00
- **Updated**: 2019-09-10 15:03:43+00:00
- **Authors**: Justin D Krogue, Kaiyang V Cheng, Kevin M Hwang, Paul Toogood, Eric G Meinberg, Erik J Geiger, Musa Zaid, Kevin C McGill, Rina Patel, Jae Ho Sohn, Alexandra Wright, Bryan F Darger, Kevin A Padrez, Eugene Ozhinsky, Sharmila Majumdar, Valentina Pedoia
- **Comment**: Presented at Orthopaedic Research Society, Austin, TX, Feb 2, 2019,
  currently in submission for publication
- **Journal**: None
- **Summary**: Purpose: Hip fractures are a common cause of morbidity and mortality. Automatic identification and classification of hip fractures using deep learning may improve outcomes by reducing diagnostic errors and decreasing time to operation. Methods: Hip and pelvic radiographs from 1118 studies were reviewed and 3034 hips were labeled via bounding boxes and classified as normal, displaced femoral neck fracture, nondisplaced femoral neck fracture, intertrochanteric fracture, previous ORIF, or previous arthroplasty. A deep learning-based object detection model was trained to automate the placement of the bounding boxes. A Densely Connected Convolutional Neural Network (DenseNet) was trained on a subset of the bounding box images, and its performance evaluated on a held out test set and by comparison on a 100-image subset to two groups of human observers: fellowship-trained radiologists and orthopaedists, and senior residents in emergency medicine, radiology, and orthopaedics. Results: The binary accuracy for fracture of our model was 93.8% (95% CI, 91.3-95.8%), with sensitivity of 92.7% (95% CI, 88.7-95.6%), and specificity 95.0% (95% CI, 91.5-97.3%). Multiclass classification accuracy was 90.4% (95% CI, 87.4-92.9%). When compared to human observers, our model achieved at least expert-level classification under all conditions. Additionally, when the model was used as an aid, human performance improved, with aided resident performance approximating unaided fellowship-trained expert performance. Conclusions: Our deep learning model identified and classified hip fractures with at least expert-level accuracy, and when used as an aid improved human performance, with aided resident performance approximating that of unaided fellowship-trained attendings.



### Semantic Foreground Inpainting from Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/1909.04564v3
- **DOI**: 10.1109/LRA.2020.2967712
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.04564v3)
- **Published**: 2019-09-10 15:17:24+00:00
- **Updated**: 2020-02-16 14:34:26+00:00
- **Authors**: Chenyang Lu, Gijs Dubbelman
- **Comment**: RA-L and ICRA'20
- **Journal**: None
- **Summary**: Semantic scene understanding is an essential task for self-driving vehicles and mobile robots. In our work, we aim to estimate a semantic segmentation map, in which the foreground objects are removed and semantically inpainted with background classes, from a single RGB image. This semantic foreground inpainting task is performed by a single-stage convolutional neural network (CNN) that contains our novel max-pooling as inpainting (MPI) module, which is trained with weak supervision, i.e., it does not require manual background annotations for the foreground regions to be inpainted. Our approach is inherently more efficient than the previous two-stage state-of-the-art method, and outperforms it by a margin of 3% IoU for the inpainted foreground regions on Cityscapes. The performance margin increases to 6% IoU, when tested on the unseen KITTI dataset. The code and the manually annotated datasets for testing are shared with the research community at https://github.com/Chenyang-Lu/semantic-foreground-inpainting.



### Structure-Attentioned Memory Network for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.04594v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04594v1)
- **Published**: 2019-09-10 16:05:38+00:00
- **Updated**: 2019-09-10 16:05:38+00:00
- **Authors**: Jing Zhu, Yunxiao Shi, Mengwei Ren, Yi Fang, Kuo-Chin Lien, Junli Gu
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Monocular depth estimation is a challenging task that aims to predict a corresponding depth map from a given single RGB image. Recent deep learning models have been proposed to predict the depth from the image by learning the alignment of deep features between the RGB image and the depth domains. In this paper, we present a novel approach, named Structure-Attentioned Memory Network, to more effectively transfer domain features for monocular depth estimation by taking into account the common structure regularities (e.g., repetitive structure patterns, planar surfaces, symmetries) in domain adaptation. To this end, we introduce a new Structure-Oriented Memory (SOM) module to learn and memorize the structure-specific information between RGB image domain and the depth domain. More specifically, in the SOM module, we develop a Memorable Bank of Filters (MBF) unit to learn a set of filters that memorize the structure-aware image-depth residual pattern, and also an Attention Guided Controller (AGC) unit to control the filter selection in the MBF given image features queries. Given the query image feature, the trained SOM module is able to adaptively select the best customized filters for cross-domain feature transferring with an optimal structural disparity between image and depth. In summary, we focus on addressing this structure-specific domain adaption challenge by proposing a novel end-to-end multi-scale memorable network for monocular depth estimation. The experiments show that our proposed model demonstrates the superior performance compared to the existing supervised monocular depth estimation approaches on the challenging KITTI and NYU Depth V2 benchmarks.



### Prediction of Overall Survival of Brain Tumor Patients
- **Arxiv ID**: http://arxiv.org/abs/1909.04596v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.04596v1)
- **Published**: 2019-09-10 16:09:12+00:00
- **Updated**: 2019-09-10 16:09:12+00:00
- **Authors**: Rupal Agravat, Mehul S Raval
- **Comment**: 5 pages, IEEE TENCON 2019
- **Journal**: None
- **Summary**: Automated brain tumor segmentation plays an important role in the diagnosis and prognosis of the patient. In addition, features from the tumorous brain help in predicting patients overall survival. The main focus of this paper is to segment tumor from BRATS 2018 benchmark dataset and use age, shape and volumetric features to predict overall survival of patients. The random forest classifier achieves overall survival accuracy of 59% on the test dataset and 67% on the dataset with resection status as gross total resection. The proposed approach uses fewer features but achieves better accuracy than state of the art methods.



### Deep Hashing Learning for Visual and Semantic Retrieval of Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1909.04614v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04614v1)
- **Published**: 2019-09-10 16:45:02+00:00
- **Updated**: 2019-09-10 16:45:02+00:00
- **Authors**: Weiwei Song, Shutao Li, Jon Atli Benediktsson
- **Comment**: None
- **Journal**: None
- **Summary**: Driven by the urgent demand for managing remote sensing big data, large-scale remote sensing image retrieval (RSIR) attracts increasing attention in the remote sensing field. In general, existing retrieval methods can be regarded as visual-based retrieval approaches which search and return a set of similar images from a database to a given query image. Although retrieval methods have achieved great success, there is still a question that needs to be responded to: Can we obtain the accurate semantic labels of the returned similar images to further help analyzing and processing imagery? Inspired by the above question, in this paper, we redefine the image retrieval problem as visual and semantic retrieval of images. Specifically, we propose a novel deep hashing convolutional neural network (DHCNN) to simultaneously retrieve the similar images and classify their semantic labels in a unified framework. In more detail, a convolutional neural network (CNN) is used to extract high-dimensional deep features. Then, a hash layer is perfectly inserted into the network to transfer the deep features into compact hash codes. In addition, a fully connected layer with a softmax function is performed on hash layer to generate class distribution. Finally, a loss function is elaborately designed to simultaneously consider the label loss of each image and similarity loss of pairs of images. Experimental results on two remote sensing datasets demonstrate that the proposed method achieves the state-of-art retrieval and classification performance.



### Video Representation Learning by Dense Predictive Coding
- **Arxiv ID**: http://arxiv.org/abs/1909.04656v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04656v3)
- **Published**: 2019-09-10 17:58:32+00:00
- **Updated**: 2019-09-27 00:35:02+00:00
- **Authors**: Tengda Han, Weidi Xie, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: The objective of this paper is self-supervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We make three contributions: First, we introduce the Dense Predictive Coding (DPC) framework for self-supervised representation learning on videos. This learns a dense encoding of spatio-temporal blocks by recurrently predicting future representations; Second, we propose a curriculum training scheme to predict further into the future with progressively less temporal context. This encourages the model to only encode slowly varying spatial-temporal signals, therefore leading to semantic representations; Third, we evaluate the approach by first training the DPC model on the Kinetics-400 dataset with self-supervised learning, and then finetuning the representation on a downstream task, i.e. action recognition. With single stream (RGB only), DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet.



### Disentangled Image Matting
- **Arxiv ID**: http://arxiv.org/abs/1909.04686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04686v1)
- **Published**: 2019-09-10 18:00:58+00:00
- **Updated**: 2019-09-10 18:00:58+00:00
- **Authors**: Shaofan Cai, Xiaoshuai Zhang, Haoqiang Fan, Haibin Huang, Jiangyu Liu, Jiaming Liu, Jiaying Liu, Jue Wang, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Most previous image matting methods require a roughly-specificed trimap as input, and estimate fractional alpha values for all pixels that are in the unknown region of the trimap. In this paper, we argue that directly estimating the alpha matte from a coarse trimap is a major limitation of previous methods, as this practice tries to address two difficult and inherently different problems at the same time: identifying true blending pixels inside the trimap region, and estimate accurate alpha values for them. We propose AdaMatting, a new end-to-end matting framework that disentangles this problem into two sub-tasks: trimap adaptation and alpha estimation. Trimap adaptation is a pixel-wise classification problem that infers the global structure of the input image by identifying definite foreground, background, and semi-transparent image regions. Alpha estimation is a regression problem that calculates the opacity value of each blended pixel. Our method separately handles these two sub-tasks within a single deep convolutional neural network (CNN). Extensive experiments show that AdaMatting has additional structure awareness and trimap fault-tolerance. Our method achieves the state-of-the-art performance on Adobe Composition-1k dataset both qualitatively and quantitatively. It is also the current best-performing method on the alphamatting.com online evaluation for all commonly-used metrics.



### Sampling Strategies for GAN Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1909.04689v1
- **DOI**: 10.1109/ICASSP40776.2020.9054677
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04689v1)
- **Published**: 2019-09-10 18:07:32+00:00
- **Updated**: 2019-09-10 18:07:32+00:00
- **Authors**: Binod Bhattarai, Seungryul Baek, Rumeysa Bodur, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been used widely to generate large volumes of synthetic data. This data is being utilized for augmenting with real examples in order to train deep Convolutional Neural Networks (CNNs). Studies have shown that the generated examples lack sufficient realism to train deep CNNs and are poor in diversity. Unlike previous studies of randomly augmenting the synthetic data with real data, we present our simple, effective and easy to implement synthetic data sampling methods to train deep CNNs more efficiently and accurately. To this end, we propose to maximally utilize the parameters learned during training of the GAN itself. These include discriminator's realism confidence score and the confidence on the target label of the synthetic data. In addition to this, we explore reinforcement learning (RL) to automatically search a subset of meaningful synthetic examples from a large pool of GAN synthetic data. We evaluate our method on two challenging face attribute classification data sets viz. AffectNet and CelebA. Our extensive experiments clearly demonstrate the need of sampling synthetic data before augmentation, which also improves the performance of one of the state-of-the-art deep CNNs in vitro.



### Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation
- **Arxiv ID**: http://arxiv.org/abs/1909.04696v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1909.04696v1)
- **Published**: 2019-09-10 18:18:45+00:00
- **Updated**: 2019-09-10 18:18:45+00:00
- **Authors**: Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee, Giedrius Burachas
- **Comment**: 2019 Conference on Empirical Methods in Natural Language Processing
  (EMNLP 2019)
- **Journal**: None
- **Summary**: While models for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these models lack consistency. For instance, if a model answers "red" to "What color is the balloon?", it might answer "no" if asked, "Is the balloon red?". These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language. In this work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation of consistency in VQA. For a given observable fact in an image (e.g. the balloon's color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQA's answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the ConVQA datasets and is a strong baseline for further research.



### Reasoning About Human-Object Interactions Through Dual Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.04743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04743v1)
- **Published**: 2019-09-10 20:45:08+00:00
- **Updated**: 2019-09-10 20:45:08+00:00
- **Authors**: Tete Xiao, Quanfu Fan, Dan Gutfreund, Mathew Monfort, Aude Oliva, Bolei Zhou
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Objects are entities we act upon, where the functionality of an object is determined by how we interact with it. In this work we propose a Dual Attention Network model which reasons about human-object interactions. The dual-attentional framework weights the important features for objects and actions respectively. As a result, the recognition of objects and actions mutually benefit each other. The proposed model shows competitive classification performance on the human-object interaction dataset Something-Something. Besides, it can perform weak spatiotemporal localization and affordance segmentation, despite being trained only with video-level labels. The model not only finds when an action is happening and which object is being manipulated, but also identifies which part of the object is being interacted with. Project page: \url{https://dual-attention-network.github.io/}.



### Accelerating Training using Tensor Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1909.05675v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1909.05675v1)
- **Published**: 2019-09-10 21:15:46+00:00
- **Updated**: 2019-09-10 21:15:46+00:00
- **Authors**: Mostafa Elhoushi, Ye Henry Tian, Zihao Chen, Farhan Shafiq, Joey Yiwei Li
- **Comment**: None
- **Journal**: AAAI 2020 Artificial Intelligence of Things Workshop
- **Summary**: Tensor decomposition is one of the well-known approaches to reduce the latency time and number of parameters of a pre-trained model. However, in this paper, we propose an approach to use tensor decomposition to reduce training time of training a model from scratch. In our approach, we train the model from scratch (i.e., randomly initialized weights) with its original architecture for a small number of epochs, then the model is decomposed, and then continue training the decomposed model till the end. There is an optional step in our approach to convert the decomposed architecture back to the original architecture. We present results of using this approach on both CIFAR10 and Imagenet datasets, and show that there can be upto 2x speed up in training time with accuracy drop of upto 1.5% only, and in other cases no accuracy drop. This training acceleration approach is independent of hardware and is expected to have similar speed ups on both CPU and GPU platforms.



### Localized Adversarial Training for Increased Accuracy and Robustness in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1909.04779v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.04779v1)
- **Published**: 2019-09-10 22:26:48+00:00
- **Updated**: 2019-09-10 22:26:48+00:00
- **Authors**: Eitan Rothberg, Tingting Chen, Luo Jie, Hao Ji
- **Comment**: 4 pages (excluding references). Presented at AdvML: 1st Workshop on
  Adversarial Learning Methods for Machine Learning and Data Mining at KDD '19
- **Journal**: None
- **Summary**: Today's state-of-the-art image classifiers fail to correctly classify carefully manipulated adversarial images. In this work, we develop a new, localized adversarial attack that generates adversarial examples by imperceptibly altering the backgrounds of normal images. We first use this attack to highlight the unnecessary sensitivity of neural networks to changes in the background of an image, then use it as part of a new training technique: localized adversarial training. By including locally adversarial images in the training set, we are able to create a classifier that suffers less loss than a non-adversarially trained counterpart model on both natural and adversarial inputs. The evaluation of our localized adversarial training algorithm on MNIST and CIFAR-10 datasets shows decreased accuracy loss on natural images, and increased robustness against adversarial inputs.



### SDM-Net: A Simple and Effective Model for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.04790v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.04790v2)
- **Published**: 2019-09-10 23:27:24+00:00
- **Updated**: 2020-12-31 10:27:37+00:00
- **Authors**: Shabnam Daghaghi, Tharun Medini, Anshumali Shrivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) is a classification task where we do not have even a single training labeled example from a set of unseen classes. Instead, we only have prior information (or description) about seen and unseen classes, often in the form of physically realizable or descriptive attributes. Lack of any single training example from a set of classes prohibits use of standard classification techniques and losses, including the popular crossentropy loss. Currently, state-of-the-art approaches encode the prior class information into dense vectors and optimize some distance between the learned projections of the input vector and the corresponding class vector (collectively known as embedding models). In this paper, we propose a novel architecture of casting zero-shot learning as a standard neural-network with crossentropy loss. During training our approach performs soft-labeling by combining the observed training data for the seen classes with the similarity information from the attributes for which we have no training data or unseen classes. To the best of our knowledge, such similarity based soft-labeling is not explored in the field of deep learning. We evaluate the proposed model on the four benchmark datasets for zero-shot learning, AwA, aPY, SUN and CUB datasets, and show that our model achieves significant improvement over the state-of-the-art methods in Generalized-ZSL and ZSL settings on all of these datasets consistently.




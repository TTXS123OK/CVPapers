# Arxiv Papers in cs.CV on 2019-09-05
### Atypical Facial Landmark Localisation with Stacked Hourglass Networks: A Study on 3D Facial Modelling for Medical Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1909.02157v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02157v1)
- **Published**: 2019-09-05 00:08:28+00:00
- **Updated**: 2019-09-05 00:08:28+00:00
- **Authors**: Gary Storey, Ahmed Bouridane, Richard Jiang, Chang-tsun Li
- **Comment**: In press, 2019
- **Journal**: Deep Biometrics, Springer Book, 2019
- **Summary**: While facial biometrics has been widely used for identification purpose, it has recently been researched as medical biometrics for a range of diseases. In this chapter, we investigate the facial landmark detection for atypical 3D facial modelling in facial palsy cases, while potentially such modelling can assist the medical diagnosis using atypical facial features. In our work, a study of landmarks localisation methods such as stacked hourglass networks is conducted and evaluated to ascertain their accuracy when presented with unseen atypical faces. The evaluation highlights that the state-of-the-art stacked hourglass architecture outperforms other traditional methods.



### Poly-GAN: Multi-Conditioned GAN for Fashion Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1909.02165v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02165v1)
- **Published**: 2019-09-05 00:29:39+00:00
- **Updated**: 2019-09-05 00:29:39+00:00
- **Authors**: Nilesh Pandey, Andreas Savakis
- **Comment**: None
- **Journal**: None
- **Summary**: We present Poly-GAN, a novel conditional GAN architecture that is motivated by Fashion Synthesis, an application where garments are automatically placed on images of human models at an arbitrary pose. Poly-GAN allows conditioning on multiple inputs and is suitable for many tasks, including image alignment, image stitching, and inpainting. Existing methods have a similar pipeline where three different networks are used to first align garments with the human pose, then perform stitching of the aligned garment and finally refine the results. Poly-GAN is the first instance where a common architecture is used to perform all three tasks. Our novel architecture enforces the conditions at all layers of the encoder and utilizes skip connections from the coarse layers of the encoder to the respective layers of the decoder. Poly-GAN is able to perform a spatial transformation of the garment based on the RGB skeleton of the model at an arbitrary pose. Additionally, Poly-GAN can perform image stitching, regardless of the garment orientation, and inpainting on the garment mask when it contains irregular holes. Our system achieves state-of-the-art quantitative results on Structural Similarity Index metric and Inception Score metric using the DeepFashion dataset.



### Future Frame Prediction Using Convolutional VRNN for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.02168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02168v2)
- **Published**: 2019-09-05 00:34:33+00:00
- **Updated**: 2019-10-18 21:48:03+00:00
- **Authors**: Yiwei Lu, Mahesh Kumar Krishna Reddy, Seyed shahabeddin Nabavi, Yang Wang
- **Comment**: Accepted to AVSS 2019
- **Journal**: None
- **Summary**: Anomaly detection in videos aims at reporting anything that does not conform the normal behaviour or distribution. However, due to the sparsity of abnormal video clips in real life, collecting annotated data for supervised learning is exceptionally cumbersome. Inspired by the practicability of generative models for semi-supervised learning, we propose a novel sequential generative model based on variational autoencoder (VAE) for future frame prediction with convolutional LSTM (ConvLSTM). To the best of our knowledge, this is the first work that considers temporal information in future frame prediction based anomaly detection framework from the model perspective. Our experiments demonstrate that our approach is superior to the state-of-the-art methods on three benchmark datasets.



### Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1909.02201v2
- **DOI**: 10.18653/v1/D19-1208
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1909.02201v2)
- **Published**: 2019-09-05 04:16:48+00:00
- **Updated**: 2019-11-21 07:01:02+00:00
- **Authors**: Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, In So Kweon
- **Comment**: EMNLP 2019. Project page :
  https://sites.google.com/view/emnlp19scarcecaption
- **Journal**: None
- **Summary**: Constructing an organized dataset comprised of a large number of images and several captions for each image is a laborious task, which requires vast human effort. On the other hand, collecting a large number of images and sentences separately may be immensely easier. In this paper, we develop a novel data-efficient semi-supervised framework for training an image captioning model. We leverage massive unpaired image and caption data by learning to associate them. To this end, our proposed semi-supervised learning method assigns pseudo-labels to unpaired samples via Generative Adversarial Networks to learn the joint distribution of image and caption. To evaluate, we construct scarcely-paired COCO dataset, a modified version of MS COCO caption dataset. The empirical results show the effectiveness of our method compared to several strong baselines, especially when the amount of the paired samples are scarce.



### DeepIST: Deep Image-based Spatio-Temporal Network for Travel Time Estimation
- **Arxiv ID**: http://arxiv.org/abs/1909.05637v1
- **DOI**: 10.1145/3357384.3357870
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.05637v1)
- **Published**: 2019-09-05 04:38:42+00:00
- **Updated**: 2019-09-05 04:38:42+00:00
- **Authors**: Tao-yang Fu, Wang-Chien Lee
- **Comment**: 10 pages, accepted by The 28th ACM International Conference on
  Information and Knowledge Management (CIKM) 2019
- **Journal**: The 28th ACM International Conference on Information and Knowledge
  Management (CIKM) 2019
- **Summary**: Estimating the travel time for a given path is a fundamental problem in many urban transportation systems. However, prior works fail to well capture moving behaviors embedded in paths and thus do not estimate the travel time accurately. To fill in this gap, in this work, we propose a novel neural network framework, namely {\em Deep Image-based Spatio-Temporal network (DeepIST)}, for travel time estimation of a given path. The novelty of DeepIST lies in the following aspects: 1) we propose to plot a path as a sequence of "generalized images" which include sub-paths along with additional information, such as traffic conditions, road network and traffic signals, in order to harness the power of convolutional neural network model (CNN) on image processing; 2) we design a novel two-dimensional CNN, namely {\em PathCNN}, to extract spatial patterns for lines in images by regularization and adopting multiple pooling methods; and 3) we apply a one-dimensional CNN to capture temporal patterns among the spatial patterns along the paths for the estimation. Empirical results show that DeepIST soundly outperforms the state-of-the-art travel time estimation models by 24.37\% to 25.64\% of mean absolute error (MAE) in multiple large-scale real-world datasets.



### Gravity as a Reference for Estimating a Person's Height from Video
- **Arxiv ID**: http://arxiv.org/abs/1909.02211v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02211v2)
- **Published**: 2019-09-05 05:03:46+00:00
- **Updated**: 2019-10-16 06:07:08+00:00
- **Authors**: Didier Bieler, Semih Günel, Pascal Fua, Helge Rhodin
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Estimating the metric height of a person from monocular imagery without additional assumptions is ill-posed. Existing solutions either require manual calibration of ground plane and camera geometry, special cameras, or reference objects of known size. We focus on motion cues and exploit gravity on earth as an omnipresent reference 'object' to translate acceleration, and subsequently height, measured in image-pixels to values in meters. We require videos of motion as input, where gravity is the only external force. This limitation is different to those of existing solutions that recover a person's height and, therefore, our method opens up new application fields. We show theoretically and empirically that a simple motion trajectory analysis suffices to translate from pixel measurements to the person's metric height, reaching a MAE of up to 3.9 cm on jumping motions, and that this works without camera and ground plane calibration.



### Auxiliary Learning for Deep Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.02214v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.02214v2)
- **Published**: 2019-09-05 05:29:15+00:00
- **Updated**: 2019-11-28 01:46:55+00:00
- **Authors**: Yifan Liu, Bohan Zhuang, Chunhua Shen, Hao Chen, Wei Yin
- **Comment**: First two authors contributed equally to this work
- **Journal**: None
- **Summary**: Multi-task learning (MTL) is an efficient solution to solve multiple tasks simultaneously in order to get better speed and performance than handling each single-task in turn. The most current methods can be categorized as either: (i) hard parameter sharing where a subset of the parameters is shared among tasks while other parameters are task-specific; or (ii) soft parameter sharing where all parameters are task-specific but they are jointly regularized. Both methods suffer from limitations: the shared hidden layers of the former are difficult to optimize due to the competing objectives while the complexity of the latter grows linearly with the increasing number of tasks. To mitigate those drawbacks, this paper proposes an alternative, where we explicitly construct an auxiliary module to mimic the soft parameter sharing for assisting the optimization of the hard parameter sharing layers in the training phase. In particular, the auxiliary module takes the outputs of the shared hidden layers as inputs and is supervised by the auxiliary task loss. During training, the auxiliary module is jointly optimized with the MTL network, serving as a regularization by introducing an inductive bias to the shared layers. In the testing phase, only the original MTL network is kept. Thus our method avoids the limitation of both categories. We evaluate the proposed auxiliary module on pixel-wise prediction tasks, including semantic segmentation, depth estimation, and surface normal prediction with different network structures. The extensive experiments over various settings verify the effectiveness of our methods.



### Synthesizing Coupled 3D Face Modalities by Trunk-Branch Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.02215v3
- **DOI**: 10.1007/978-3-030-58526-6_25
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1909.02215v3)
- **Published**: 2019-09-05 05:33:50+00:00
- **Updated**: 2020-12-02 01:12:57+00:00
- **Authors**: Baris Gecer, Alexander Lattas, Stylianos Ploumpis, Jiankang Deng, Athanasios Papaioannou, Stylianos Moschoglou, Stefanos Zafeiriou
- **Comment**: Check project page: https://github.com/barisgecer/TBGAN for the full
  resolution results and the accompanying video
- **Journal**: In: European conference on computer vision. Springer, Cham, 2020.
  p. 415-433
- **Summary**: Generating realistic 3D faces is of high importance for computer graphics and computer vision applications. Generally, research on 3D face generation revolves around linear statistical models of the facial surface. Nevertheless, these models cannot represent faithfully either the facial texture or the normals of the face, which are very crucial for photo-realistic face synthesis. Recently, it was demonstrated that Generative Adversarial Networks (GANs) can be used for generating high-quality textures of faces. Nevertheless, the generation process either omits the geometry and normals, or independent processes are used to produce 3D shape information. In this paper, we present the first methodology that generates high-quality texture, shape, and normals jointly, which can be used for photo-realistic synthesis. To do so, we propose a novel GAN that can generate data from different modalities while exploiting their correlations. Furthermore, we demonstrate how we can condition the generation on the expression and create faces with various facial expressions. The qualitative results shown in this paper are compressed due to size limitations, full-resolution results and the accompanying video can be found in the supplementary documents. The code and models are available at the project page: https://github.com/barisgecer/TBGAN.



### REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1909.02217v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02217v1)
- **Published**: 2019-09-05 05:44:46+00:00
- **Updated**: 2019-09-05 05:44:46+00:00
- **Authors**: Ming Jiang, Junjie Hu, Qiuyuan Huang, Lei Zhang, Jana Diesner, Jianfeng Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Popular metrics used for evaluating image captioning systems, such as BLEU and CIDEr, provide a single score to gauge the system's overall effectiveness. This score is often not informative enough to indicate what specific errors are made by a given system. In this study, we present a fine-grained evaluation method REO for automatically measuring the performance of image captioning systems. REO assesses the quality of captions from three perspectives: 1) Relevance to the ground truth, 2) Extraness of the content that is irrelevant to the ground truth, and 3) Omission of the elements in the images and human references. Experiments on three benchmark datasets demonstrate that our method achieves a higher consistency with human judgments and provides more intuitive evaluation results than alternative metrics.



### A Better Way to Attend: Attention with Trees for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1909.02218v1
- **DOI**: 10.1109/TIP.2018.2859820
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.02218v1)
- **Published**: 2019-09-05 05:48:51+00:00
- **Updated**: 2019-09-05 05:48:51+00:00
- **Authors**: Hongyang Xue, Wenqing Chu, Zhou Zhao, Deng Cai
- **Comment**: 12 pages
- **Journal**: IEEE Transactions on Image Processing ( Volume: 27 , Issue: 11 ,
  Nov. 2018 )
- **Summary**: We propose a new attention model for video question answering. The main idea of the attention models is to locate on the most informative parts of the visual data. The attention mechanisms are quite popular these days. However, most existing visual attention mechanisms regard the question as a whole. They ignore the word-level semantics where each word can have different attentions and some words need no attention. Neither do they consider the semantic structure of the sentences. Although the Extended Soft Attention (E-SA) model for video question answering leverages the word-level attention, it performs poorly on long question sentences. In this paper, we propose the heterogeneous tree-structured memory network (HTreeMN) for video question answering. Our proposed approach is based upon the syntax parse trees of the question sentences. The HTreeMN treats the words differently where the \textit{visual} words are processed with an attention module and the \textit{verbal} ones not. It also utilizes the semantic structure of the sentences by combining the neighbors based on the recursive structure of the parse trees. The understandings of the words and the videos are propagated and merged from leaves to the root. Furthermore, we build a hierarchical attention mechanism to distill the attended features. We evaluate our approach on two datasets. The experimental results show the superiority of our HTreeMN model over the other attention models especially on complex questions. Our code is available on github.   Our code is available at https://github.com/ZJULearning/TreeAttention



### Super-resolved Chromatic Mapping of Snapshot Mosaic Image Sensors via a Texture Sensitive Residual Network
- **Arxiv ID**: http://arxiv.org/abs/1909.02221v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02221v1)
- **Published**: 2019-09-05 06:05:31+00:00
- **Updated**: 2019-09-05 06:05:31+00:00
- **Authors**: Mehrdad Shoeiby, Lars Petersson, Mohammad Ali Armin, Sadegh Aliakbarian, Antonio Robles-Kelly
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel method to simultaneously super-resolve and colour-predict images acquired by snapshot mosaic sensors. These sensors allow for spectral images to be acquired using low-power, small form factor, solid-state CMOS sensors that can operate at video frame rates without the need for complex optical setups. Despite their desirable traits, their main drawback stems from the fact that the spatial resolution of the imagery acquired by these sensors is low. Moreover, chromatic mapping in snapshot mosaic sensors is not straightforward since the bands delivered by the sensor tend to be narrow and unevenly distributed across the range in which they operate. We tackle this drawback as applied to chromatic mapping by using a residual channel attention network equipped with a texture sensitive block. Our method significantly outperforms the traditional approach of interpolating the image and, afterwards, applying a colour matching function. This work establishes state-of-the-art in this domain while also making available to the research community a dataset containing 296 registered stereo multi-spectral/RGB images pairs.



### POD: Practical Object Detection with Scale-Sensitive Network
- **Arxiv ID**: http://arxiv.org/abs/1909.02225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02225v1)
- **Published**: 2019-09-05 06:24:50+00:00
- **Updated**: 2019-09-05 06:24:50+00:00
- **Authors**: Junran Peng, Ming Sun, Zhaoxiang Zhang, Tieniu Tan, Junjie Yan
- **Comment**: arXiv admin note: text overlap with arXiv:1901.06563 by other authors
- **Journal**: None
- **Summary**: Scale-sensitive object detection remains a challenging task, where most of the existing methods could not learn it explicitly and are not robust to scale variance. In addition, the most existing methods are less efficient during training or slow during inference, which are not friendly to real-time applications. In this paper, we propose a practical object detection method with scale-sensitive network.Our method first predicts a global continuous scale ,which is shared by all position, for each convolution filter of each network stage. To effectively learn the scale, we average the spatial features and distill the scale from channels. For fast-deployment, we propose a scale decomposition method that transfers the robust fractional scale into combination of fixed integral scales for each convolution filter, which exploits the dilated convolution. We demonstrate it on one-stage and two-stage algorithms under different configurations. For practical applications, training of our method is of efficiency and simplicity which gets rid of complex data sampling or optimize strategy. During test-ing, the proposed method requires no extra operation and is very supportive of hardware acceleration like TensorRT and TVM. On the COCO test-dev, our model could achieve a 41.5 mAP on one-stage detector and 42.1 mAP on two-stage detectors based on ResNet-101, outperforming base-lines by 2.4 and 2.1 respectively without extra FLOPS.



### Effective Domain Knowledge Transfer with Soft Fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/1909.02236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02236v1)
- **Published**: 2019-09-05 07:10:55+00:00
- **Updated**: 2019-09-05 07:10:55+00:00
- **Authors**: Zhichen Zhao, Bowen Zhang, Yuning Jiang, Li Xu, Lei Li, Wei-Ying Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks require numerous data for training. Considering the difficulties in data collection and labeling in some specific tasks, existing approaches generally use models pre-trained on a large source domain (e.g. ImageNet), and then fine-tune them on these tasks. However, the datasets from source domain are simply discarded in the fine-tuning process. We argue that the source datasets could be better utilized and benefit fine-tuning. This paper firstly introduces the concept of general discrimination to describe ability of a network to distinguish untrained patterns, and then experimentally demonstrates that general discrimination could potentially enhance the total discrimination ability on target domain. Furthermore, we propose a novel and light-weighted method, namely soft fine-tuning. Unlike traditional fine-tuning which directly replaces optimization objective by a loss function on the target domain, soft fine-tuning effectively keeps general discrimination by holding the previous loss and removes it softly. By doing so, soft fine-tuning improves the robustness of the network to data bias, and meanwhile accelerates the convergence. We evaluate our approach on several visual recognition tasks. Extensive experimental results support that soft fine-tuning provides consistent improvement on all evaluated tasks, and outperforms the state-of-the-art significantly. Codes will be made available to the public.



### Adaptive Graph Representation Learning for Video Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1909.02240v2
- **DOI**: 10.1109/TIP.2020.3001693
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02240v2)
- **Published**: 2019-09-05 07:18:06+00:00
- **Updated**: 2020-06-12 02:12:40+00:00
- **Authors**: Yiming Wu, Omar El Farouk Bourahla, Xi Li, Fei Wu, Qi Tian, Xue Zhou
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Recent years have witnessed the remarkable progress of applying deep learning models in video person re-identification (Re-ID). A key factor for video person Re-ID is to effectively construct discriminative and robust video feature representations for many complicated situations. Part-based approaches employ spatial and temporal attention to extract representative local features. While correlations between parts are ignored in the previous methods, to leverage the relations of different parts, we propose an innovative adaptive graph representation learning scheme for video person Re-ID, which enables the contextual interactions between relevant regional features. Specifically, we exploit the pose alignment connection and the feature affinity connection to construct an adaptive structure-aware adjacency graph, which models the intrinsic relations between graph nodes. We perform feature propagation on the adjacency graph to refine regional features iteratively, and the neighbor nodes' information is taken into account for part feature representation. To learn compact and discriminative representations, we further propose a novel temporal resolution-aware regularization, which enforces the consistency among different temporal resolutions for the same identities. We conduct extensive evaluations on four benchmarks, i.e. iLIDS-VID, PRID2011, MARS, and DukeMTMC-VideoReID, experimental results achieve the competitive performance which demonstrates the effectiveness of our proposed method. The code is available at https://github.com/weleen/AGRL.pytorch.



### Robust Navigation with Language Pretraining and Stochastic Sampling
- **Arxiv ID**: http://arxiv.org/abs/1909.02244v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.02244v1)
- **Published**: 2019-09-05 07:31:58+00:00
- **Updated**: 2019-09-05 07:31:58+00:00
- **Authors**: Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah Smith, Yejin Choi
- **Comment**: 8 pages, 4 figures, EMNLP 2019
- **Journal**: None
- **Summary**: Core to the vision-and-language navigation (VLN) challenge is building robust instruction representations and action decoding schemes, which can generalize well to previously unseen instructions and environments. In this paper, we report two simple but highly effective methods to address these challenges and lead to a new state-of-the-art performance. First, we adapt large-scale pretrained language models to learn text representations that generalize better to previously unseen instructions. Second, we propose a stochastic sampling scheme to reduce the considerable gap between the expert actions in training and sampled actions in test, so that the agent can learn to correct its own mistakes during long sequential action decoding. Combining the two techniques, we achieve a new state of the art on the Room-to-Room benchmark with 6% absolute gain over the previous best result (47% -> 53%) on the Success Rate weighted by Path Length metric.



### PRSNet: Part Relation and Selection Network for Bone Age Assessment
- **Arxiv ID**: http://arxiv.org/abs/1909.05651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05651v1)
- **Published**: 2019-09-05 08:35:33+00:00
- **Updated**: 2019-09-05 08:35:33+00:00
- **Authors**: Yuanfeng Ji, Hao Chen, Dan Lin, Xiaohua Wu, Di Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Bone age is one of the most important indicators for assessing bone's maturity, which can help to interpret human's growth development level and potential progress. In the clinical practice, bone age assessment (BAA) of X-ray images requires the joint consideration of the appearance and location information of hand bones. These kinds of information can be effectively captured by the relation of different anatomical parts of hand bone. Recently developed methods differ mostly in how they model the part relation and choose useful parts for BAA. However, these methods neglect the mining of relationship among different parts, which can help to improve the assessment accuracy. In this paper, we propose a novel part relation module, which accurately discovers the underlying concurrency of parts by using multi-scale context information of deep learning feature representation. Furthermore, based on the part relation, we explore a new part selection module, which comprehensively measures the importance of parts and select the top ranking parts for assisting BAA. We jointly train our part relation and selection modules in an end-to-end way, achieving state-of-the-art performance on the public RSNA 2017 Pediatric Bone Age benchmark dataset and outperforming other competitive methods by a significant margin.



### Efficient Neural Architecture Transformation Searchin Channel-Level for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.02293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02293v1)
- **Published**: 2019-09-05 10:05:57+00:00
- **Updated**: 2019-09-05 10:05:57+00:00
- **Authors**: Junran Peng, Ming Sun, Zhaoxiang Zhang, Tieniu Tan, Junjie Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Neural Architecture Search has achieved great success in large-scale image classification. In contrast, there have been limited works focusing on architecture search for object detection, mainly because the costly ImageNet pre-training is always required for detectors. Training from scratch, as a substitute, demands more epochs to converge and brings no computation saving. To overcome this obstacle, we introduce a practical neural architecture transformation search(NATS)algorithm for object detection in this paper. Instead of searching and constructing an entire network, NATS explores the architecture space on the base of existing network and reusing its weights. We propose a novel neural architecture search strategy in channel-level instead of path-level and devise a search space specially targeting at object detection. With the combination of these two designs, an architecture transformation scheme could be discovered to adapt a network designed for image classification to task of object detection. Since our method is gradient-based and only searches for a transformation scheme, the weights of models pretrained inImageNet could be utilized in both searching and retraining stage, which makes the whole process very efficient. The transformed network requires no extra parameters and FLOPs, and is friendly to hardware optimization, which is practical to use in real-time application. In experiments, we demonstrate the effectiveness of NATSon networks like ResNet and ResNeXt. Our transformed networks, combined with various detection frameworks, achieve significant improvements on the COCO dataset while keeping fast.



### Depth Map Estimation for Free-Viewpoint Television
- **Arxiv ID**: http://arxiv.org/abs/1909.02294v1
- **DOI**: 10.1109/ACCESS.2019.2963487
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.02294v1)
- **Published**: 2019-09-05 10:06:28+00:00
- **Updated**: 2019-09-05 10:06:28+00:00
- **Authors**: Dawid Mieloch, Olgierd Stankiewicz, Marek Domański
- **Comment**: None
- **Journal**: None
- **Summary**: The paper presents a new method of depth estimation dedicated for free-viewpoint television (FTV). The estimation is performed for segments and thus their size can be used to control a trade-off between the quality of depth maps and the processing time of their estimation. The proposed algorithm can take as its input multiple arbitrarily positioned views which are simultaneously used to produce multiple inter view consistent output depth maps. The presented depth estimation method uses novel parallelization and temporal consistency enhancement methods that significantly reduce the processing time of depth estimation. An experimental assessment of the proposals has been performed, based on the analysis of virtual view quality in FTV. The results show that the proposed method provides an improvement of the depth map quality over the state of-the-art method, simultaneously reducing the complexity of depth estimation. The consistency of depth maps, which is crucial for the quality of the synthesized video and thus the quality of experience of navigating through a 3D scene, is also vastly improved.



### Detector With Focus: Normalizing Gradient In Image Pyramid
- **Arxiv ID**: http://arxiv.org/abs/1909.02301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02301v1)
- **Published**: 2019-09-05 10:18:55+00:00
- **Updated**: 2019-09-05 10:18:55+00:00
- **Authors**: Yonghyun Kim, Bong-Nam Kang, Daijin Kim
- **Comment**: None
- **Journal**: None
- **Summary**: An image pyramid can extend many object detection algorithms to solve detection on multiple scales. However, interpolation during the resampling process of an image pyramid causes gradient variation, which is the difference of the gradients between the original image and the scaled images. Our key insight is that the increased variance of gradients makes the classifiers have difficulty in correctly assigning categories. We prove the existence of the gradient variation by formulating the ratio of gradient expectations between an original image and scaled images, then propose a simple and novel gradient normalization method to eliminate the effect of this variation. The proposed normalization method reduce the variance in an image pyramid and allow the classifier to focus on a smaller coverage. We show the improvement in three different visual recognition problems: pedestrian detection, pose estimation, and object detection. The method is generally applicable to many vision algorithms based on an image pyramid with gradients.



### The application of Convolutional Neural Networks to Detect Slow, Sustained Deformation in InSAR Timeseries
- **Arxiv ID**: http://arxiv.org/abs/1909.02321v1
- **DOI**: 10.1029/2019GL084993
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02321v1)
- **Published**: 2019-09-05 11:09:28+00:00
- **Updated**: 2019-09-05 11:09:28+00:00
- **Authors**: N. Anantrasirichai, J. Biggs, F. Albino, D. Bull
- **Comment**: None
- **Journal**: None
- **Summary**: Automated systems for detecting deformation in satellite InSAR imagery could be used to develop a global monitoring system for volcanic and urban environments. Here we explore the limits of a CNN for detecting slow, sustained deformations in wrapped interferograms. Using synthetic data, we estimate a detection threshold of 3.9cm for deformation signals alone, and 6.3cm when atmospheric artefacts are considered. Over-wrapping reduces this to 1.8cm and 5.0cm respectively as more fringes are generated without altering SNR. We test the approach on timeseries of cumulative deformation from Campi Flegrei and Dallol, where over-wrapping improves classication performance by up to 15%. We propose a mean-filtering method for combining results of different wrap parameters to flag deformation. At Campi Flegrei, deformation of 8.5cm/yr was detected after 60days and at Dallol, deformation of 3.5cm/yr was detected after 310 days. This corresponds to cumulative displacements of 3 cm and 4 cm consistent with estimates based on synthetic data.



### An Active Learning Approach for Reducing Annotation Cost in Skin Lesion Analysis
- **Arxiv ID**: http://arxiv.org/abs/1909.02344v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02344v1)
- **Published**: 2019-09-05 12:00:01+00:00
- **Updated**: 2019-09-05 12:00:01+00:00
- **Authors**: Xueying Shi, Qi Dou, Cheng Xue, Jing Qin, Hao Chen, Pheng-Ann Heng
- **Comment**: Accepted by MIML2019
- **Journal**: None
- **Summary**: Automated skin lesion analysis is very crucial in clinical practice, as skin cancer is among the most common human malignancy. Existing approaches with deep learning have achieved remarkable performance on this challenging task, however, heavily relying on large-scale labelled datasets. In this paper, we present a novel active learning framework for cost-effective skin lesion analysis. The goal is to effectively select and utilize much fewer labelled samples, while the network can still achieve state-of-the-art performance. Our sample selection criteria complementarily consider both informativeness and representativeness, derived from decoupled aspects of measuring model certainty and covering sample diversity. To make wise use of the selected samples, we further design a simple yet effective strategy to aggregate intra-class images in pixel space, as a new form of data augmentation. We validate our proposed method on data of ISIC 2017 Skin Lesion Classification Challenge for two tasks. Using only up to 50% of samples, our approach can achieve state-of-the-art performances on both tasks, which are comparable or exceeding the accuracies with full-data training, and outperform other well-known active learning methods by a large margin.



### Tensor Oriented No-Reference Light Field Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1909.02358v2
- **DOI**: 10.1109/TIP.2020.2969777
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.02358v2)
- **Published**: 2019-09-05 12:27:44+00:00
- **Updated**: 2022-02-18 01:01:49+00:00
- **Authors**: Wei Zhou, Likun Shi, Zhibo Chen, Jinglin Zhang
- **Comment**: Published on IEEE TIP
- **Journal**: None
- **Summary**: Light field image (LFI) quality assessment is becoming more and more important, which helps to better guide the acquisition, processing and application of immersive media. However, due to the inherent high dimensional characteristics of LFI, the LFI quality assessment turns into a multi-dimensional problem that requires consideration of the quality degradation in both spatial and angular dimensions. Therefore, we propose a novel Tensor oriented No-reference Light Field image Quality evaluator (Tensor-NLFQ) based on tensor theory. Specifically, since the LFI is regarded as a low-rank 4D tensor, the principal components of four oriented sub-aperture view stacks are obtained via Tucker decomposition. Then, the Principal Component Spatial Characteristic (PCSC) is designed to measure the spatial-dimensional quality of LFI considering its global naturalness and local frequency properties. Finally, the Tensor Angular Variation Index (TAVI) is proposed to measure angular consistency quality by analyzing the structural similarity distribution between the first principal component and each view in the view stack. Extensive experimental results on four publicly available LFI quality databases demonstrate that the proposed Tensor-NLFQ model outperforms state-of-the-art 2D, 3D, multi-view, and LFI quality assessment algorithms.



### TFCheck : A TensorFlow Library for Detecting Training Issues in Neural Network Programs
- **Arxiv ID**: http://arxiv.org/abs/1909.02562v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02562v1)
- **Published**: 2019-09-05 13:21:22+00:00
- **Updated**: 2019-09-05 13:21:22+00:00
- **Authors**: Houssem Ben Braiek, Foutse Khomh
- **Comment**: None
- **Journal**: None
- **Summary**: The increasing inclusion of Machine Learning (ML) models in safety critical systems like autonomous cars have led to the development of multiple model-based ML testing techniques. One common denominator of these testing techniques is their assumption that training programs are adequate and bug-free. These techniques only focus on assessing the performance of the constructed model using manually labeled data or automatically generated data. However, their assumptions about the training program are not always true as training programs can contain inconsistencies and bugs. In this paper, we examine training issues in ML programs and propose a catalog of verification routines that can be used to detect the identified issues, automatically. We implemented the routines in a Tensorflow-based library named TFCheck. Using TFCheck, practitioners can detect the aforementioned issues automatically. To assess the effectiveness of TFCheck, we conducted a case study with real-world, mutants, and synthetic training programs. Results show that TFCheck can successfully detect training issues in ML code implementations.



### Knowledge-guided Pairwise Reconstruction Network for Weakly Supervised Referring Expression Grounding
- **Arxiv ID**: http://arxiv.org/abs/1909.02860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02860v1)
- **Published**: 2019-09-05 13:22:08+00:00
- **Updated**: 2019-09-05 13:22:08+00:00
- **Authors**: Xuejing Liu, Liang Li, Shuhui Wang, Zheng-Jun Zha, Li Su, Qingming Huang
- **Comment**: Accepted by ACMMM 2019. arXiv admin note: text overlap with
  arXiv:1908.10568
- **Journal**: None
- **Summary**: Weakly supervised referring expression grounding (REG) aims at localizing the referential entity in an image according to linguistic query, where the mapping between the image region (proposal) and the query is unknown in the training stage. In referring expressions, people usually describe a target entity in terms of its relationship with other contextual entities as well as visual attributes. However, previous weakly supervised REG methods rarely pay attention to the relationship between the entities. In this paper, we propose a knowledge-guided pairwise reconstruction network (KPRN), which models the relationship between the target entity (subject) and contextual entity (object) as well as grounds these two entities. Specifically, we first design a knowledge extraction module to guide the proposal selection of subject and object. The prior knowledge is obtained in a specific form of semantic similarities between each proposal and the subject/object. Second, guided by such knowledge, we design the subject and object attention module to construct the subject-object proposal pairs. The subject attention excludes the unrelated proposals from the candidate proposals. The object attention selects the most suitable proposal as the contextual proposal. Third, we introduce a pairwise attention and an adaptive weighting scheme to learn the correspondence between these proposal pairs and the query. Finally, a pairwise reconstruction module is used to measure the grounding for weakly supervised learning. Extensive experiments on four large-scale datasets show our method outperforms existing state-of-the-art methods by a large margin.



### Utilizing Temporal Information in Deep Convolutional Network for Efficient Soccer Ball Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1909.02406v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02406v2)
- **Published**: 2019-09-05 13:40:27+00:00
- **Updated**: 2019-09-06 10:41:39+00:00
- **Authors**: Anna Kukleva, Mohammad Asif Khan, Hafez Farazi, Sven Behnke
- **Comment**: 23rd RoboCup International Symposium, Sydney, Australia, 2019
- **Journal**: None
- **Summary**: Soccer ball detection is identified as one of the critical challenges in the RoboCup competition. It requires an efficient vision system capable of handling the task of detection with high precision and recall and providing robust and low inference time. In this work, we present a novel convolutional neural network (CNN) approach to detect the soccer ball in an image sequence. In contrast to the existing methods where only the current frame or an image is used for the detection, we make use of the history of frames. Using history allows to efficiently track the ball in situations where the ball disappears or gets partially occluded in some of the frames. Our approach exploits spatio-temporal correlation and detects the ball based on the trajectory of its movements. We present our results with three convolutional methods, namely temporal convolutional networks (TCN), ConvLSTM, and ConvGRU. We first solve the detection task for an image using fully convolutional encoder-decoder architecture, and later, we use it as an input to our temporal models and jointly learn the detection task in sequences of images. We evaluate all our experiments on a novel dataset prepared as a part of this work. Furthermore, we present empirical results to support the effectiveness of using the history of the ball in challenging scenarios.



### DeepEvolution: A Search-Based Testing Approach for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.02563v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02563v1)
- **Published**: 2019-09-05 13:42:08+00:00
- **Updated**: 2019-09-05 13:42:08+00:00
- **Authors**: Houssem Ben Braiek, Foutse khomh
- **Comment**: None
- **Journal**: None
- **Summary**: The increasing inclusion of Deep Learning (DL) models in safety-critical systems such as autonomous vehicles have led to the development of multiple model-based DL testing techniques. One common denominator of these testing techniques is the automated generation of test cases, e.g., new inputs transformed from the original training data with the aim to optimize some test adequacy criteria. So far, the effectiveness of these approaches has been hindered by their reliance on random fuzzing or transformations that do not always produce test cases with a good diversity. To overcome these limitations, we propose, DeepEvolution, a novel search-based approach for testing DL models that relies on metaheuristics to ensure a maximum diversity in generated test cases. We assess the effectiveness of DeepEvolution in testing computer-vision DL models and found that it significantly increases the neuronal coverage of generated test cases. Moreover, using DeepEvolution, we could successfully find several corner-case behaviors. Finally, DeepEvolution outperformed Tensorfuzz (a coverage-guided fuzzing tool developed at Google Brain) in detecting latent defects introduced during the quantization of the models. These results suggest that search-based approaches can help build effective testing tools for DL systems.



### Semantic-Aware Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.02410v3
- **DOI**: 10.1016/j.patcog.2020.107256
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02410v3)
- **Published**: 2019-09-05 13:44:03+00:00
- **Updated**: 2020-01-22 16:03:52+00:00
- **Authors**: Alejandro López-Cifuentes, Marcos Escudero-Viñolo, Jesús Bescós, Álvaro García-Martín
- **Comment**: Paper submitted for publication to Elsevier Pattern Recognition
  journal
- **Journal**: Pattern Recognition Volume 102, June 2020, 107256
- **Summary**: Scene recognition is currently one of the top-challenging research fields in computer vision. This may be due to the ambiguity between classes: images of several scene classes may share similar objects, which causes confusion among them. The problem is aggravated when images of a particular scene class are notably different. Convolutional Neural Networks (CNNs) have significantly boosted performance in scene recognition, albeit it is still far below from other recognition tasks (e.g., object or image recognition). In this paper, we describe a novel approach for scene recognition based on an end-to-end multi-modal CNN that combines image and context information by means of an attention module. Context information, in the shape of semantic segmentation, is used to gate features extracted from the RGB image by leveraging on information encoded in the semantic representation: the set of scene objects and stuff, and their relative locations. This gating process reinforces the learning of indicative scene content and enhances scene disambiguation by refocusing the receptive fields of the CNN towards them. Experimental results on four publicly available datasets show that the proposed approach outperforms every other state-of-the-art method while significantly reducing the number of network parameters. All the code and data used along this paper is available at https://github.com/vpulab/Semantic-Aware-Scene-Recognition



### FreeAnchor: Learning to Match Anchors for Visual Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.02466v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.02466v2)
- **Published**: 2019-09-05 14:57:53+00:00
- **Updated**: 2019-11-12 05:10:29+00:00
- **Authors**: Xiaosong Zhang, Fang Wan, Chang Liu, Rongrong Ji, Qixiang Ye
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Modern CNN-based object detectors assign anchors for ground-truth objects under the restriction of object-anchor Intersection-over-Unit (IoU). In this study, we propose a learning-to-match approach to break IoU restriction, allowing objects to match anchors in a flexible manner. Our approach, referred to as FreeAnchor, updates hand-crafted anchor assignment to "free" anchor matching by formulating detector training as a maximum likelihood estimation (MLE) procedure. FreeAnchor targets at learning features which best explain a class of objects in terms of both classification and localization. FreeAnchor is implemented by optimizing detection customized likelihood and can be fused with CNN-based detectors in a plug-and-play manner. Experiments on COCO demonstrate that FreeAnchor consistently outperforms their counterparts with significant margins.



### Intrinsic Dynamic Shape Prior for Fast, Sequential and Dense Non-Rigid Structure from Motion with Detection of Temporally-Disjoint Rigidity
- **Arxiv ID**: http://arxiv.org/abs/1909.02468v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02468v2)
- **Published**: 2019-09-05 14:58:55+00:00
- **Updated**: 2020-06-29 18:37:45+00:00
- **Authors**: Vladislav Golyanik, André Jonas, Didier Stricker, Christian Theobalt
- **Comment**: 16 pages, 13 figures, 5 tables
- **Journal**: None
- **Summary**: While dense non-rigid structure from motion (NRSfM) has been extensively studied from the perspective of the reconstructability problem over the recent years, almost no attempts have been undertaken to bring it into the practical realm. The reasons for the slow dissemination are the severe ill-posedness, high sensitivity to motion and deformation cues and the difficulty to obtain reliable point tracks in the vast majority of practical scenarios. To fill this gap, we propose a hybrid approach that extracts prior shape knowledge from an input sequence with NRSfM and uses it as a dynamic shape prior for sequential surface recovery in scenarios with recurrence. Our Dynamic Shape Prior Reconstruction (DSPR) method can be combined with existing dense NRSfM techniques while its energy functional is optimised with stochastic gradient descent at real-time rates for new incoming point tracks. The proposed versatile framework with a new core NRSfM approach outperforms several other methods in the ability to handle inaccurate and noisy point tracks, provided we have access to a representative (in terms of the deformation variety) image sequence. Comprehensive experiments highlight convergence properties and the accuracy of DSPR under different disturbing effects. We also perform a joint study of tracking and reconstruction and show applications to shape compression and heart reconstruction under occlusions. We achieve state-of-the-art metrics (accuracy and compression ratios) in different scenarios.



### AFP-Net: Realtime Anchor-Free Polyp Detection in Colonoscopy
- **Arxiv ID**: http://arxiv.org/abs/1909.02477v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.02477v3)
- **Published**: 2019-09-05 15:23:34+00:00
- **Updated**: 2019-09-26 17:47:55+00:00
- **Authors**: Dechun Wang, Ning Zhang, Xinzi Sun, Pengfei Zhang, Chenxi Zhang, Yu Cao, Benyuan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Colorectal cancer (CRC) is a common and lethal disease. Globally, CRC is the third most commonly diagnosed cancer in males and the second in females. For colorectal cancer, the best screening test available is the colonoscopy. During a colonoscopic procedure, a tiny camera at the tip of the endoscope generates a video of the internal mucosa of the colon. The video data are displayed on a monitor for the physician to examine the lining of the entire colon and check for colorectal polyps. Detection and removal of colorectal polyps are associated with a reduction in mortality from colorectal cancer. However, the miss rate of polyp detection during colonoscopy procedure is often high even for very experienced physicians. The reason lies in the high variation of polyp in terms of shape, size, textural, color and illumination. Though challenging, with the great advances in object detection techniques, automated polyp detection still demonstrates a great potential in reducing the false negative rate while maintaining a high precision. In this paper, we propose a novel anchor free polyp detector that can localize polyps without using predefined anchor boxes. To further strengthen the model, we leverage a Context Enhancement Module and Cosine Ground truth Projection. Our approach can respond in real time while achieving state-of-the-art performance with 99.36% precision and 96.44% recall.



### Stack-VS: Stacked Visual-Semantic Attention for Image Caption Generation
- **Arxiv ID**: http://arxiv.org/abs/1909.02489v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1909.02489v1)
- **Published**: 2019-09-05 15:41:53+00:00
- **Updated**: 2019-09-05 15:41:53+00:00
- **Authors**: Wei Wei, Ling Cheng, Xianling Mao, Guangyou Zhou, Feida Zhu
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Recently, automatic image caption generation has been an important focus of the work on multimodal translation task. Existing approaches can be roughly categorized into two classes, i.e., top-down and bottom-up, the former transfers the image information (called as visual-level feature) directly into a caption, and the later uses the extracted words (called as semanticlevel attribute) to generate a description. However, previous methods either are typically based one-stage decoder or partially utilize part of visual-level or semantic-level information for image caption generation. In this paper, we address the problem and propose an innovative multi-stage architecture (called as Stack-VS) for rich fine-gained image caption generation, via combining bottom-up and top-down attention models to effectively handle both visual-level and semantic-level information of an input image. Specifically, we also propose a novel well-designed stack decoder model, which is constituted by a sequence of decoder cells, each of which contains two LSTM-layers work interactively to re-optimize attention weights on both visual-level feature vectors and semantic-level attribute embeddings for generating a fine-gained image caption. Extensive experiments on the popular benchmark dataset MSCOCO show the significant improvements on different evaluation metrics, i.e., the improvements on BLEU-4/CIDEr/SPICE scores are 0.372, 1.226 and 0.216, respectively, as compared to the state-of-the-arts.



### A Novel Design of Adaptive and Hierarchical Convolutional Neural Networks using Partial Reconfiguration on FPGA
- **Arxiv ID**: http://arxiv.org/abs/1909.05653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05653v1)
- **Published**: 2019-09-05 16:00:19+00:00
- **Updated**: 2019-09-05 16:00:19+00:00
- **Authors**: Mohammad Farhadi, Mehdi Ghasemi, Yezhou Yang
- **Comment**: 2019 IEEE High Performance Extreme Computing Conference
- **Journal**: None
- **Summary**: Nowadays most research in visual recognition using Convolutional Neural Networks (CNNs) follows the "deeper model with deeper confidence" belief to gain a higher recognition accuracy. At the same time, deeper model brings heavier computation. On the other hand, for a large chunk of recognition challenges, a system can classify images correctly using simple models or so-called shallow networks. Moreover, the implementation of CNNs faces with the size, weight, and energy constraints on the embedded devices. In this paper, we implement the adaptive switching between shallow and deep networks to reach the highest throughput on a resource-constrained MPSoC with CPU and FPGA. To this end, we develop and present a novel architecture for the CNNs where a gate makes the decision whether using the deeper model is beneficial or not. Due to resource limitation on FPGA, the idea of partial reconfiguration has been used to accommodate deep CNNs on the FPGA resources. We report experimental results on CIFAR-10, CIFAR-100, and SVHN datasets to validate our approach. Using confidence metric as the decision making factor, only 69.8%, 71.8%, and 43.8% of the computation in the deepest network is done for CIFAR-10, CIFAR-100, and SVHN while it can maintain the desired accuracy with the throughput of around 400 images per second for SVHN dataset.



### Discriminative Video Representation Learning Using Support Vector Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1909.02856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02856v1)
- **Published**: 2019-09-05 16:12:27+00:00
- **Updated**: 2019-09-05 16:12:27+00:00
- **Authors**: Jue Wang, Anoop Cherian
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1803.10628
- **Journal**: None
- **Summary**: Most popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action---many are common across multiple actions---pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To identify these useful features, we resort to a negative bag consisting of features that are known to be irrelevant, for example, they are sampled either from datasets that are unrelated to our actions of interest or are CNN features produced via random noise as input. With the features from the video as a positive bag and the irrelevant features as the negative bag, we cast an objective to learn a (nonlinear) hyperplane that separates the unknown useful features from the rest in a multiple instance learning formulation within a support vector machine setup. We use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they can be treated as a weighted average pooling of the features from the bags, with zero weights given to non-support vectors. Our pooling scheme is end-to-end trainable within a deep learning framework. We report results from experiments on eight computer vision benchmark datasets spanning a variety of video-related tasks and demonstrate state-of-the-art performance across these tasks.



### CT Data Curation for Liver Patients: Phase Recognition in Dynamic Contrast-Enhanced CT
- **Arxiv ID**: http://arxiv.org/abs/1909.02511v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02511v2)
- **Published**: 2019-09-05 16:31:40+00:00
- **Updated**: 2019-09-27 21:48:31+00:00
- **Authors**: Bo Zhou, Adam P. Harrison, Jiawen Yao, Chi-Tung Cheng, Jing Xiao, Chien-Hung Liao, Le Lu
- **Comment**: 11 pages, accepted by 2019 MICCAI - Medical Image Learning with Less
  Labels and Imperfect Data Workshop
- **Journal**: None
- **Summary**: As the demand for more descriptive machine learning models grows within medical imaging, bottlenecks due to data paucity will exacerbate. Thus, collecting enough large-scale data will require automated tools to harvest data/label pairs from messy and real-world datasets, such as hospital PACS. This is the focus of our work, where we present a principled data curation tool to extract multi-phase CT liver studies and identify each scan's phase from a real-world and heterogenous hospital PACS dataset. Emulating a typical deployment scenario, we first obtain a set of noisy labels from our institutional partners that are text mined using simple rules from DICOM tags. We train a deep learning system, using a customized and streamlined 3D SE architecture, to identify non-contrast, arterial, venous, and delay phase dynamic CT liver scans, filtering out anything else, including other types of liver contrast studies. To exploit as much training data as possible, we also introduce an aggregated cross entropy loss that can learn from scans only identified as "contrast". Extensive experiments on a dataset of 43K scans of 7680 patient imaging studies demonstrate that our 3DSE architecture, armed with our aggregated loss, can achieve a mean F1 of 0.977 and can correctly harvest up to 92.7% of studies, which significantly outperforms the text-mined and standard-loss approach, and also outperforms other, and more complex, model architectures.



### Neural Style-Preserving Visual Dubbing
- **Arxiv ID**: http://arxiv.org/abs/1909.02518v2
- **DOI**: 10.1145/3355089.3356500
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.02518v2)
- **Published**: 2019-09-05 16:41:20+00:00
- **Updated**: 2019-09-06 10:53:40+00:00
- **Authors**: Hyeongwoo Kim, Mohamed Elgharib, Michael Zollhöfer, Hans-Peter Seidel, Thabo Beeler, Christian Richardt, Christian Theobalt
- **Comment**: SIGGRAPH Asia 2019
- **Journal**: None
- **Summary**: Dubbing is a technique for translating video content from one language to another. However, state-of-the-art visual dubbing techniques directly copy facial expressions from source to target actors without considering identity-specific idiosyncrasies such as a unique type of smile. We present a style-preserving visual dubbing approach from single video inputs, which maintains the signature style of target actors when modifying facial expressions, including mouth motions, to match foreign languages. At the heart of our approach is the concept of motion style, in particular for facial expressions, i.e., the person-specific expression change that is yet another essential factor beyond visual accuracy in face editing applications. Our method is based on a recurrent generative adversarial network that captures the spatiotemporal co-activation of facial expressions, and enables generating and modifying the facial expressions of the target actor while preserving their style. We train our model with unsynchronized source and target videos in an unsupervised manner using cycle-consistency and mouth expression losses, and synthesize photorealistic video frames using a layered neural face renderer. Our approach generates temporally coherent results, and handles dynamic backgrounds. Our results show that our dubbing approach maintains the idiosyncratic style of the target actor better than previous approaches, even for widely differing source and target actors.



### C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion
- **Arxiv ID**: http://arxiv.org/abs/1909.02533v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1909.02533v2)
- **Published**: 2019-09-05 17:16:15+00:00
- **Updated**: 2019-10-15 16:39:18+00:00
- **Authors**: David Novotny, Nikhila Ravi, Benjamin Graham, Natalia Neverova, Andrea Vedaldi
- **Comment**: Added a link to the source code into the abstract
- **Journal**: IEEE/CVF International Conference on Computer Vision 2019
- **Summary**: We propose C3DPO, a method for extracting 3D models of deformable objects from 2D keypoint annotations in unconstrained images. We do so by learning a deep network that reconstructs a 3D object from a single view at a time, accounting for partial occlusions, and explicitly factoring the effects of viewpoint changes and object deformations. In order to achieve this factorization, we introduce a novel regularization technique. We first show that the factorization is successful if, and only if, there exists a certain canonicalization function of the reconstructed shapes. Then, we learn the canonicalization function together with the reconstruction one, which constrains the result to be consistent. We demonstrate state-of-the-art reconstruction results for methods that do not use ground-truth 3D supervision for a number of benchmarks, including Up3D and PASCAL3D+. Source code has been made available at https://github.com/facebookresearch/c3dpo_nrsfm.



### Harnessing the Power of Deep Learning Methods in Healthcare: Neonatal Pain Assessment from Crying Sound
- **Arxiv ID**: http://arxiv.org/abs/1909.02543v1
- **DOI**: 10.1109/HI-POCT45284.2019.8962827
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02543v1)
- **Published**: 2019-09-05 17:33:45+00:00
- **Updated**: 2019-09-05 17:33:45+00:00
- **Authors**: Md Sirajus Salekin, Ghada Zamzmi, Rahul Paul, Dmitry Goldgof, Rangachar Kasturi, Thao Ho, Yu Sun
- **Comment**: Accepted to IEEE HI-POCT 2019
- **Journal**: None
- **Summary**: Neonatal pain assessment in clinical environments is challenging as it is discontinuous and biased. Facial/body occlusion can occur in such settings due to clinical condition, developmental delays, prone position, or other external factors. In such cases, crying sound can be used to effectively assess neonatal pain. In this paper, we investigate the use of a novel CNN architecture (N-CNN) along with other CNN architectures (VGG16 and ResNet50) for assessing pain from crying sounds of neonates. The experimental results demonstrate that using our novel N-CNN for assessing pain from the sounds of neonates has a strong clinical potential and provides a viable alternative to the current assessment practice.



### Deep Visual Template-Free Form Parsing
- **Arxiv ID**: http://arxiv.org/abs/1909.02576v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02576v2)
- **Published**: 2019-09-05 18:00:09+00:00
- **Updated**: 2019-09-18 21:58:56+00:00
- **Authors**: Brian Davis, Bryan Morse, Scott Cohen, Brian Price, Chris Tensmeyer
- **Comment**: Accepted at ICDAR 2019. Updated results with average of repeated
  experiments
- **Journal**: None
- **Summary**: Automatic, template-free extraction of information from form images is challenging due to the variety of form layouts. This is even more challenging for historical forms due to noise and degradation. A crucial part of the extraction process is associating input text with pre-printed labels. We present a learned, template-free solution to detecting pre-printed text and input text/handwriting and predicting pair-wise relationships between them. While previous approaches to this problem have been focused on clean images and clear layouts, we show our approach is effective in the domain of noisy, degraded, and varied form images. We introduce a new dataset of historical form images (late 1800s, early 1900s) for training and validating our approach. Our method uses a convolutional network to detect pre-printed text and input text lines. We pool features from the detection network to classify possible relationships in a language-agnostic way. We show that our proposed pairing method outperforms heuristic rules and that visual features are critical to obtaining high accuracy.



### What can computational models learn from human selective attention? A review from an audiovisual crossmodal perspective
- **Arxiv ID**: http://arxiv.org/abs/1909.05654v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.05654v1)
- **Published**: 2019-09-05 18:12:05+00:00
- **Updated**: 2019-09-05 18:12:05+00:00
- **Authors**: Di Fu, Cornelius Weber, Guochun Yang, Matthias Kerzel, Weizhi Nan, Pablo Barros, Haiyan Wu, Xun Liu, Stefan Wermter
- **Comment**: 29pages, 5 figures, 1 table, journal article
- **Journal**: None
- **Summary**: Selective attention plays an essential role in information acquisition and utilization from the environment. In the past 50 years, research on selective attention has been a central topic in cognitive science. Compared with unimodal studies, crossmodal studies are more complex but necessary to solve real-world challenges in both human experiments and computational modeling. Although an increasing number of findings on crossmodal selective attention have shed light on humans' behavioral patterns and neural underpinnings, a much better understanding is still necessary to yield the same benefit for computational intelligent agents. This article reviews studies of selective attention in unimodal visual and auditory and crossmodal audiovisual setups from the multidisciplinary perspectives of psychology and cognitive neuroscience, and evaluates different ways to simulate analogous mechanisms in computational models and robotics. We discuss the gaps between these fields in this interdisciplinary review and provide insights about how to use psychological findings and theories in artificial intelligence from different perspectives.



### Multi-layer Domain Adaptation for Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.02620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02620v1)
- **Published**: 2019-09-05 20:24:49+00:00
- **Updated**: 2019-09-05 20:24:49+00:00
- **Authors**: Ozan Ciga, Jianan Chen, Anne Martel
- **Comment**: To be presented on Domain Adaptation and Representation Transfer 2019
- **Journal**: None
- **Summary**: Despite their success in many computer vision tasks, convolutional networks tend to require large amounts of labeled data to achieve generalization. Furthermore, the performance is not guaranteed on a sample from an unseen domain at test time, if the network was not exposed to similar samples from that domain at training time. This hinders the adoption of these techniques in clinical setting where the imaging data is scarce, and where the intra- and inter-domain variance of the data can be substantial. We propose a domain adaptation technique that is especially suitable for deep networks to alleviate this requirement of labeled data. Our method utilizes gradient reversal layers and Squeezeand-Excite modules to stabilize the training in deep networks. The proposed method was applied to publicly available histopathology and chest X-ray databases and achieved superior performance to existing state-of-the-art networks with and without domain adaptation. Depending on the application, our method can improve multi-class classification accuracy by 5-20% compared to DANN introduced in (Ganin, 2014).



### On Learning Disentangled Representations for Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.03051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.03051v1)
- **Published**: 2019-09-05 20:50:10+00:00
- **Updated**: 2019-09-05 20:50:10+00:00
- **Authors**: Ziyuan Zhang, Luan Tran, Feng Liu, Xiaoming Liu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.04925
- **Journal**: None
- **Summary**: Gait, the walking pattern of individuals, is one of the important biometrics modalities. Most of the existing gait recognition methods take silhouettes or articulated body models as gait features. These methods suffer from degraded recognition performance when handling confounding variables, such as clothing, carrying and viewing angle. To remedy this issue, we propose a novel AutoEncoder framework, GaitNet, to explicitly disentangle appearance, canonical and pose features from RGB imagery. The LSTM integrates pose features over time as a dynamic gait feature while canonical features are averaged as a static gait feature. Both of them are utilized as classification features. In addition, we collect a Frontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view walking, which is a challenging problem since it contains minimal gait cues compared to other views. FVG also includes other important variations, e.g., walking speed, carrying, and clothing. With extensive experiments on CASIA-B, USF, and FVG datasets, our method demonstrates superior performance to the SOTA quantitatively, the ability of feature disentanglement qualitatively, and promising computational efficiency. We further compare our GaitNet with state-of-the-art face recognition to demonstrate the advantages of gait biometrics identification under certain scenarios, e.g., long distance/lower resolutions, cross viewing angles.



### Deep Iterative Frame Interpolation for Full-frame Video Stabilization
- **Arxiv ID**: http://arxiv.org/abs/1909.02641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1909.02641v1)
- **Published**: 2019-09-05 21:38:45+00:00
- **Updated**: 2019-09-05 21:38:45+00:00
- **Authors**: Jinsoo Choi, In So Kweon
- **Comment**: Accepted to ACM Transactions on Graphics, To be presented at SIGGRAPH
  Asia 2019
- **Journal**: None
- **Summary**: Video stabilization is a fundamental and important technique for higher quality videos. Prior works have extensively explored video stabilization, but most of them involve cropping of the frame boundaries and introduce moderate levels of distortion. We present a novel deep approach to video stabilization which can generate video frames without cropping and low distortion. The proposed framework utilizes frame interpolation techniques to generate in between frames, leading to reduced inter-frame jitter. Once applied in an iterative fashion, the stabilization effect becomes stronger. A major advantage is that our framework is end-to-end trainable in an unsupervised manner. In addition, our method is able to run in near real-time (15 fps). To the best of our knowledge, this is the first work to propose an unsupervised deep approach to full-frame video stabilization. We show the advantages of our method through quantitative and qualitative evaluations comparing to the state-of-the-art methods.



### Intensity augmentation for domain transfer of whole breast segmentation in MRI
- **Arxiv ID**: http://arxiv.org/abs/1909.02642v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.02642v1)
- **Published**: 2019-09-05 21:40:02+00:00
- **Updated**: 2019-09-05 21:40:02+00:00
- **Authors**: Linde S. Hesse, Grey Kuling, Mitko Veta, Anne L. Martel
- **Comment**: Preprint
- **Journal**: None
- **Summary**: The segmentation of the breast from the chest wall is an important first step in the analysis of breast magnetic resonance images. 3D U-nets have been shown to obtain high segmentation accuracy and appear to generalize well when trained on one scanner type and tested on another scanner, provided that a very similar T1-weighted MR protocol is used. There has, however, been little work addressing the problem of domain adaptation when image intensities or patient orientation differ markedly between the training set and an unseen test set. To overcome the domain shift we propose to apply extensive intensity augmentation in addition to geometric augmentation during training. We explored both style transfer and a novel intensity remapping approach as intensity augmentation strategies. For our experiments, we trained a 3D U-net on T1-weighted scans and tested on T2-weighted scans. By applying intensity augmentation we increased segmentation performance from a DSC of 0.71 to 0.90. This performance is very close to the baseline performance of training and testing on T2-weighted scans (0.92). Furthermore, we applied our network to an independent test set made up of publicly available scans acquired using a T1-weighted TWIST sequence and a different coil configuration. On this dataset we obtained a performance of 0.89, close to the inter-observer variability of the ground truth segmentations (0.92). Our results show that using intensity augmentation in addition to geometric augmentation is a suitable method to overcome the intensity domain shift and we expect it to be useful for a wide range of segmentation tasks.



### Semantic Correlation Promoted Shape-Variant Context for Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.02651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.02651v1)
- **Published**: 2019-09-05 22:09:41+00:00
- **Updated**: 2019-09-05 22:09:41+00:00
- **Authors**: Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, Gang Wang
- **Comment**: CVPR 2019, Oral
- **Journal**: None
- **Summary**: Context is essential for semantic segmentation. Due to the diverse shapes of objects and their complex layout in various scene images, the spatial scales and shapes of contexts for different objects have very large variation. It is thus ineffective or inefficient to aggregate various context information from a predefined fixed region. In this work, we propose to generate a scale- and shape-variant semantic mask for each pixel to confine its contextual region. To this end, we first propose a novel paired convolution to infer the semantic correlation of the pair and based on that to generate a shape mask. Using the inferred spatial scope of the contextual region, we propose a shape-variant convolution, of which the receptive field is controlled by the shape mask that varies with the appearance of input. In this way, the proposed network aggregates the context information of a pixel from its semantic-correlated region instead of a predefined fixed region. Furthermore, this work also proposes a labeling denoising model to reduce wrong predictions caused by the noisy low-level features. Without bells and whistles, the proposed segmentation network achieves new state-of-the-arts consistently on the six public segmentation datasets.




# Arxiv Papers in cs.CV on 2019-09-16
### Learning to Map Nearly Anything
- **Arxiv ID**: http://arxiv.org/abs/1909.06928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06928v1)
- **Published**: 2019-09-16 01:40:44+00:00
- **Updated**: 2019-09-16 01:40:44+00:00
- **Authors**: Tawfiq Salem, Connor Greenwell, Hunter Blanton, Nathan Jacobs
- **Comment**: None
- **Journal**: None
- **Summary**: Looking at the world from above, it is possible to estimate many properties of a given location, including the type of land cover and the expected land use. Historically, such tasks have relied on relatively coarse-grained categories due to the difficulty of obtaining fine-grained annotations. In this work, we propose an easily extensible approach that makes it possible to estimate fine-grained properties from overhead imagery. In particular, we propose a cross-modal distillation strategy to learn to predict the distribution of fine-grained properties from overhead imagery, without requiring any manual annotation of overhead imagery. We show that our learned models can be used directly for applications in mapping and image localization.



### Self-Supervised Correspondence in Visuomotor Policy Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.06933v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.06933v1)
- **Published**: 2019-09-16 01:52:39+00:00
- **Updated**: 2019-09-16 01:52:39+00:00
- **Authors**: Peter Florence, Lucas Manuelli, Russ Tedrake
- **Comment**: Video at: https://sites.google.com/view/visuomotor-correspondence
- **Journal**: None
- **Summary**: In this paper we explore using self-supervised correspondence for improving the generalization performance and sample efficiency of visuomotor policy learning. Prior work has primarily used approaches such as autoencoding, pose-based losses, and end-to-end policy optimization in order to train the visual portion of visuomotor policies. We instead propose an approach using self-supervised dense visual correspondence training, and show this enables visuomotor policy learning with surprisingly high generalization performance with modest amounts of data: using imitation learning, we demonstrate extensive hardware validation on challenging manipulation tasks with as few as 50 demonstrations. Our learned policies can generalize across classes of objects, react to deformable object configurations, and manipulate textureless symmetrical objects in a variety of backgrounds, all with closed-loop, real-time vision-based policies. Simulated imitation learning experiments suggest that correspondence training offers sample complexity and generalization benefits compared to autoencoding and end-to-end training.



### Multi-graph Fusion for Multi-view Spectral Clustering
- **Arxiv ID**: http://arxiv.org/abs/1909.06940v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.06940v1)
- **Published**: 2019-09-16 02:22:02+00:00
- **Updated**: 2019-09-16 02:22:02+00:00
- **Authors**: Zhao Kang, Guoxin Shi, Shudong Huang, Wenyu Chen, Xiaorong Pu, Joey Tianyi Zhou, Zenglin Xu
- **Comment**: submitted to Knowledge-based Systems
- **Journal**: None
- **Summary**: A panoply of multi-view clustering algorithms has been developed to deal with prevalent multi-view data. Among them, spectral clustering-based methods have drawn much attention and demonstrated promising results recently. Despite progress, there are still two fundamental questions that stay unanswered to date. First, how to fuse different views into one graph. More often than not, the similarities between samples may be manifested differently by different views. Many existing algorithms either simply take the average of multiple views or just learn a common graph. These simple approaches fail to consider the flexible local manifold structures of all views. Hence, the rich heterogeneous information is not fully exploited. Second, how to learn the explicit cluster structure. Most existing methods don't pay attention to the quality of the graphs and perform graph learning and spectral clustering separately. Those unreliable graphs might lead to suboptimal clustering results. To fill these gaps, in this paper, we propose a novel multi-view spectral clustering model which performs graph fusion and spectral clustering simultaneously. The fusion graph approximates the original graph of each individual view but maintains an explicit cluster structure. Experiments on four widely used data sets confirm the superiority of the proposed method.



### PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer
- **Arxiv ID**: http://arxiv.org/abs/1909.06956v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06956v2)
- **Published**: 2019-09-16 02:52:00+00:00
- **Updated**: 2019-11-26 08:25:05+00:00
- **Authors**: Wentao Jiang, Si Liu, Chen Gao, Jie Cao, Ran He, Jiashi Feng, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the makeup transfer task, which aims to transfer the makeup from a reference image to a source image. Existing methods have achieved promising progress in constrained scenarios, but transferring between images with large pose and expression differences is still challenging. Besides, they cannot realize customizable transfer that allows a controllable shade of makeup or specifies the part to transfer, which limits their applications. To address these issues, we propose Pose and expression robust Spatial-aware GAN (PSGAN). It first utilizes Makeup Distill Network to disentangle the makeup of the reference image as two spatial-aware makeup matrices. Then, Attentive Makeup Morphing module is introduced to specify how the makeup of a pixel in the source image is morphed from the reference image. With the makeup matrices and the source image, Makeup Apply Network is used to perform makeup transfer. Our PSGAN not only achieves state-of-the-art results even when large pose and expression differences exist but also is able to perform partial and shade-controllable makeup transfer. We also collected a dataset containing facial images with various poses and expressions for evaluations.



### Multimodal Deep Models for Predicting Affective Responses Evoked by Movies
- **Arxiv ID**: http://arxiv.org/abs/1909.06957v2
- **DOI**: None
- **Categories**: **cs.CV**, 97R40, 68T45, 68Txx, 92B20
- **Links**: [PDF](http://arxiv.org/pdf/1909.06957v2)
- **Published**: 2019-09-16 02:55:11+00:00
- **Updated**: 2019-09-17 05:24:05+00:00
- **Authors**: Ha Thi Phuong Thao, Dorien Herremans, Gemma Roig
- **Comment**: 10 pages, 7 figures, Preprint accepted for publication in the
  Proceedings of the 2nd International Workshop on Computer Vision for
  Physiological Measurement as part of ICCV. Seoul, South Korea. 2019
- **Journal**: Proceedings of the 2nd International Workshop on Computer Vision
  for Physiological Measurement as part of ICCV. Seoul, South Korea. 2019
- **Summary**: The goal of this study is to develop and analyze multimodal models for predicting experienced affective responses of viewers watching movie clips. We develop hybrid multimodal prediction models based on both the video and audio of the clips. For the video content, we hypothesize that both image content and motion are crucial features for evoked emotion prediction. To capture such information, we extract features from RGB frames and optical flow using pre-trained neural networks. For the audio model, we compute an enhanced set of low-level descriptors including intensity, loudness, cepstrum, linear predictor coefficients, pitch and voice quality. Both visual and audio features are then concatenated to create audio-visual features, which are used to predict the evoked emotion. To classify the movie clips into the corresponding affective response categories, we propose two approaches based on deep neural network models. The first one is based on fully connected layers without memory on the time component, the second incorporates the sequential dependency with a long short-term memory recurrent neural network (LSTM). We perform a thorough analysis of the importance of each feature set. Our experiments reveal that in our set-up, predicting emotions at each time step independently gives slightly better accuracy performance than with the LSTM. Interestingly, we also observe that the optical flow is more informative than the RGB in videos, and overall, models using audio features are more accurate than those based on video features when making the final prediction of evoked emotions.



### Perspective-Guided Convolution Networks for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1909.06966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06966v1)
- **Published**: 2019-09-16 03:16:17+00:00
- **Updated**: 2019-09-16 03:16:17+00:00
- **Authors**: Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei Wen, Errui Ding
- **Comment**: Accepted by ICCV 2019
- **Journal**: ICCV 2019
- **Summary**: In this paper, we propose a novel perspective-guided convolution (PGC) for convolutional neural network (CNN) based crowd counting (i.e. PGCNet), which aims to overcome the dramatic intra-scene scale variations of people due to the perspective effect. While most state-of-the-arts adopt multi-scale or multi-column architectures to address such issue, they generally fail in modeling continuous scale variations since only discrete representative scales are considered. PGCNet, on the other hand, utilizes perspective information to guide the spatially variant smoothing of feature maps before feeding them to the successive convolutions. An effective perspective estimation branch is also introduced to PGCNet, which can be trained in either supervised setting or weakly-supervised setting when the branch has been pre-trained. Our PGCNet is single-column with moderate increase in computation, and extensive experimental results on four benchmark datasets show the improvements of our method against the state-of-the-arts. Additionally, we also introduce Crowd Surveillance, a large scale dataset for crowd counting that contains 13,000+ high-resolution images with challenging scenarios.



### A few filters are enough: Convolutional Neural Network for P300 Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.06970v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.06970v3)
- **Published**: 2019-09-16 03:48:28+00:00
- **Updated**: 2020-09-13 19:38:37+00:00
- **Authors**: Alicia Montserrat Alvarado-Gonzalez, Gibran Fuentes-Pineda, Jorge Cervantes-Ojeda
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past decade, convolutional neural networks (CNNs) have become the driving force of an ever-increasing set of applications, achieving state-of-the-art performance. Most of the modern CNN architectures are composed of many convolutional and fully connected layers and typically require thousands or millions of parameters to learn. CNNs have also been effective in the detection of Event-Related Potentials from electroencephalogram (EEG) signals, notably the P300 component which is frequently employed in Brain-Computer Interfaces (BCIs). However, for this task, the increase in detection rates compared to approaches based on human-engineered features has not been as impressive as in other areas and might not justify such a large number of parameters. In this paper, we study the performances of existing CNN architectures with diverse complexities for single-trial within-subject and cross-subject P300 detection on four different datasets. We also proposed SepConv1D, a very simple CNN architecture consisting of a single depthwise separable 1D convolutional layer followed by a fully connected Sigmoid classification neuron. We found that with as few as four filters in its convolutional layer and a small overall number of parameters, SepConv1D obtained competitive performances in the four datasets. We believe this may represent an important step towards building simpler, cheaper, faster, and more portable BCIs.



### Interpreting and Improving Adversarial Robustness of Deep Neural Networks with Neuron Sensitivity
- **Arxiv ID**: http://arxiv.org/abs/1909.06978v3
- **DOI**: 10.1109/TIP.2020.3042083
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06978v3)
- **Published**: 2019-09-16 04:10:13+00:00
- **Updated**: 2020-11-30 15:13:09+00:00
- **Authors**: Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, Tianlin Li
- **Comment**: Accepted by IEEE Transaction on Image Processing
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are vulnerable to adversarial examples where inputs with imperceptible perturbations mislead DNNs to incorrect results. Despite the potential risk they bring, adversarial examples are also valuable for providing insights into the weakness and blind-spots of DNNs. Thus, the interpretability of a DNN in the adversarial setting aims to explain the rationale behind its decision-making process and makes deeper understanding which results in better practical applications. To address this issue, we try to explain adversarial robustness for deep models from a new perspective of neuron sensitivity which is measured by neuron behavior variation intensity against benign and adversarial examples. In this paper, we first draw the close connection between adversarial robustness and neuron sensitivities, as sensitive neurons make the most non-trivial contributions to model predictions in the adversarial setting. Based on that, we further propose to improve adversarial robustness by constraining the similarities of sensitive neurons between benign and adversarial examples which stabilizes the behaviors of sensitive neurons towards adversarial noises. Moreover, we demonstrate that state-of-the-art adversarial training methods improve model robustness by reducing neuron sensitivities which in turn confirms the strong connections between adversarial robustness and neuron sensitivity as well as the effectiveness of using sensitive neurons to build robust models. Extensive experiments on various datasets demonstrate that our algorithm effectively achieves excellent results.



### Visuomotor Understanding for Representation Learning of Driving Scenes
- **Arxiv ID**: http://arxiv.org/abs/1909.06979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06979v1)
- **Published**: 2019-09-16 04:12:37+00:00
- **Updated**: 2019-09-16 04:12:37+00:00
- **Authors**: Seokju Lee, Junsik Kim, Tae-Hyun Oh, Yongseop Jeong, Donggeun Yoo, Stephen Lin, In So Kweon
- **Comment**: BMVC 2019. Supplementary material:
  https://bmvc2019.org/wp-content/uploads/papers/0002-supplementary.zip
  Dataset: http://github.com/SeokjuLee/driving-dataset-doc
- **Journal**: None
- **Summary**: Dashboard cameras capture a tremendous amount of driving scene video each day. These videos are purposefully coupled with vehicle sensing data, such as from the speedometer and inertial sensors, providing an additional sensing modality for free. In this work, we leverage the large-scale unlabeled yet naturally paired data for visual representation learning in the driving scenario. A representation is learned in an end-to-end self-supervised framework for predicting dense optical flow from a single frame with paired sensing data. We postulate that success on this task requires the network to learn semantic and geometric knowledge in the ego-centric view. For example, forecasting a future view to be seen from a moving vehicle requires an understanding of scene depth, scale, and movement of objects. We demonstrate that our learned representation can benefit other tasks that require detailed scene understanding and outperforms competing unsupervised representations on semantic segmentation.



### kPAM-SC: Generalizable Manipulation Planning using KeyPoint Affordance and Shape Completion
- **Arxiv ID**: http://arxiv.org/abs/1909.06980v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06980v1)
- **Published**: 2019-09-16 04:15:12+00:00
- **Updated**: 2019-09-16 04:15:12+00:00
- **Authors**: Wei Gao, Russ Tedrake
- **Comment**: In submission. The video demo and source code are available on
  https://sites.google.com/view/generalizable-manipulation/
- **Journal**: None
- **Summary**: Manipulation planning is the task of computing robot trajectories that move a set of objects to their target configuration while satisfying physically feasibility. In contrast to existing works that assume known object templates, we are interested in manipulation planning for a category of objects with potentially unknown instances and large intra-category shape variation. To achieve it, we need an object representation with which the manipulation planner can reason about both the physical feasibility and desired object configuration, while being generalizable to novel instances. The widely-used pose representation is not suitable, as representing an object with a parameterized transformation from a fixed template cannot capture large intra-category shape variation. Hence, we propose a new hybrid object representation consisting of semantic keypoint and dense geometry (a point cloud or mesh) as the interface between the perception module and motion planner. Leveraging advances in learning-based keypoint detection and shape completion, both dense geometry and keypoints can be perceived from raw sensor input. Using the proposed hybrid object representation, we formulate the manipulation task as a motion planning problem which encodes both the object target configuration and physical feasibility for a category of objects. In this way, many existing manipulation planners can be generalized to categories of objects, and the resulting perception-to-action manipulation pipeline is robust to large intra-category shape variation. Extensive hardware experiments demonstrate our pipeline can produce robot trajectories that accomplish tasks with never-before-seen objects.



### Framelet Representation of Tensor Nuclear Norm for Third-Order Tensor Completion
- **Arxiv ID**: http://arxiv.org/abs/1909.06982v2
- **DOI**: 10.1109/TIP.2020.3000349
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06982v2)
- **Published**: 2019-09-16 04:31:49+00:00
- **Updated**: 2020-01-25 10:10:17+00:00
- **Authors**: Tai-Xiang Jiang, Michael K. Ng, Xi-Le Zhao, Ting-Zhu Huang
- **Comment**: None
- **Journal**: None
- **Summary**: The main aim of this paper is to develop a framelet representation of the tensor nuclear norm for third-order tensor completion. In the literature, the tensor nuclear norm can be computed by using tensor singular value decomposition based on the discrete Fourier transform matrix, and tensor completion can be performed by the minimization of the tensor nuclear norm which is the relaxation of the sum of matrix ranks from all Fourier transformed matrix frontal slices. These Fourier transformed matrix frontal slices are obtained by applying the discrete Fourier transform on the tubes of the original tensor. In this paper, we propose to employ the framelet representation of each tube so that a framelet transformed tensor can be constructed. Because of framelet basis redundancy, the representation of each tube is sparsely represented. When the matrix slices of the original tensor are highly correlated, we expect the corresponding sum of matrix ranks from all framelet transformed matrix frontal slices would be small, and the resulting tensor completion can be performed much better. The proposed minimization model is convex and global minimizers can be obtained. Numerical results on several types of multi-dimensional data (videos, multispectral images, and magnetic resonance imaging data) have tested and shown that the proposed method outperformed the other testing methods.



### Unsupervised Eyeglasses Removal in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1909.06989v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06989v4)
- **Published**: 2019-09-16 05:13:47+00:00
- **Updated**: 2020-05-15 15:08:18+00:00
- **Authors**: Bingwen Hu, Zhedong Zheng, Ping Liu, Wankou Yang, Mingwu Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Eyeglasses removal is challenging in removing different kinds of eyeglasses, e.g., rimless glasses, full-rim glasses and sunglasses, and recovering appropriate eyes. Due to the large visual variants, the conventional methods lack scalability. Most existing works focus on the frontal face images in the controlled environment, such as the laboratory, and need to design specific systems for different eyeglass types. To address the limitation, we propose a unified eyeglass removal model called Eyeglasses Removal Generative Adversarial Network (ERGAN), which could handle different types of glasses in the wild. The proposed method does not depend on the dense annotation of eyeglasses location but benefits from the large-scale face images with weak annotations. Specifically, we study the two relevant tasks simultaneously, i.e., removing and wearing eyeglasses. Given two facial images with and without eyeglasses, the proposed model learns to swap the eye area in two faces. The generation mechanism focuses on the eye area and invades the difficulty of generating a new face. In the experiment, we show the proposed method achieves a competitive removal quality in terms of realism and diversity. Furthermore, we evaluate ERGAN on several subsequent tasks, such as face verification and facial expression recognition. The experiment shows that our method could serve as a pre-processing method for these tasks.



### Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal Representations
- **Arxiv ID**: http://arxiv.org/abs/1909.06993v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.06993v2)
- **Published**: 2019-09-16 05:23:14+00:00
- **Updated**: 2020-03-08 13:22:41+00:00
- **Authors**: Rogerio Bonatti, Ratnesh Madaan, Vibhav Vineet, Sebastian Scherer, Ashish Kapoor
- **Comment**: None
- **Journal**: None
- **Summary**: Machines are a long way from robustly solving open-world perception-control tasks, such as first-person view (FPV) aerial navigation. While recent advances in end-to-end Machine Learning, especially Imitation and Reinforcement Learning appear promising, they are constrained by the need of large amounts of difficult-to-collect labeled real-world data. Simulated data, on the other hand, is easy to generate, but generally does not render safe behaviors in diverse real-life scenarios. In this work we propose a novel method for learning robust visuomotor policies for real-world deployment which can be trained purely with simulated data. We develop rich state representations that combine supervised and unsupervised environment data. Our approach takes a cross-modal perspective, where separate modalities correspond to the raw camera data and the system states relevant to the task, such as the relative pose of gates to the drone in the case of drone racing. We feed both data modalities into a novel factored architecture, which learns a joint low-dimensional embedding via Variational Auto Encoders. This compact representation is then fed into a control policy, which we trained using imitation learning with expert trajectories in a simulator. We analyze the rich latent spaces learned with our proposed representations, and show that the use of our cross-modal architecture significantly improves control policy performance as compared to end-to-end learning or purely unsupervised feature extractors. We also present real-world results for drone navigation through gates in different track configurations and environmental conditions. Our proposed method, which runs fully onboard, can successfully generalize the learned representations and policies across simulation and reality, significantly outperforming baseline approaches.   Supplementary video: https://youtu.be/VKc3A5HlUU8



### Real-time 3-D Mapping with Estimating Acoustic Materials
- **Arxiv ID**: http://arxiv.org/abs/1909.06998v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.06998v1)
- **Published**: 2019-09-16 05:40:38+00:00
- **Updated**: 2019-09-16 05:40:38+00:00
- **Authors**: Taeyoung Kim, Youngsun Kwon, Sung-eui Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a real-time system integrating an acoustic material estimation from visual appearance and an on-the-fly mapping in the 3-dimension. The proposed method estimates the acoustic materials of surroundings in indoor scenes and incorporates them to a 3-D occupancy map, as a robot moves around the environment. To estimate the acoustic material from the visual cue, we apply the state-of-the-art semantic segmentation CNN network based on the assumption that the visual appearance and the acoustic materials have a strong association. Furthermore, we introduce an update policy to handle the material estimations during the online mapping process. As a result, our environment map with acoustic material can be used for sound-related robotics applications, such as sound source localization taking into account various acoustic propagation (e.g., reflection).



### Learning Residual Flow as Dynamic Motion from Stereo Videos
- **Arxiv ID**: http://arxiv.org/abs/1909.06999v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.06999v1)
- **Published**: 2019-09-16 05:48:42+00:00
- **Updated**: 2019-09-16 05:48:42+00:00
- **Authors**: Seokju Lee, Sunghoon Im, Stephen Lin, In So Kweon
- **Comment**: IROS 2019. https://sites.google.com/site/seokjucv/
- **Journal**: None
- **Summary**: We present a method for decomposing the 3D scene flow observed from a moving stereo rig into stationary scene elements and dynamic object motion. Our unsupervised learning framework jointly reasons about the camera motion, optical flow, and 3D motion of moving objects. Three cooperating networks predict stereo matching, camera motion, and residual flow, which represents the flow component due to object motion and not from camera motion. Based on rigid projective geometry, the estimated stereo depth is used to guide the camera motion estimation, and the depth and camera motion are used to guide the residual flow estimation. We also explicitly estimate the 3D scene flow of dynamic objects based on the residual flow and scene depth. Experiments on the KITTI dataset demonstrate the effectiveness of our approach and show that our method outperforms other state-of-the-art algorithms on the optical flow and visual odometry tasks.



### Multi-person Pose Tracking using Sequential Monte Carlo with Probabilistic Neural Pose Predictor
- **Arxiv ID**: http://arxiv.org/abs/1909.07031v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.07031v2)
- **Published**: 2019-09-16 07:25:28+00:00
- **Updated**: 2020-02-27 07:54:14+00:00
- **Authors**: Masashi Okada, Shinji Takenaka, Tadahiro Taniguchi
- **Comment**: Accepted to ICRA2020; Camera-ready ver
- **Journal**: None
- **Summary**: It is an effective strategy for the multi-person pose tracking task in videos to employ prediction and pose matching in a frame-by-frame manner. For this type of approach, uncertainty-aware modeling is essential because precise prediction is impossible. However, previous studies have relied on only a single prediction without incorporating uncertainty, which can cause critical tracking errors if the prediction is unreliable. This paper proposes an extension to this approach with Sequential Monte Carlo (SMC). This naturally reformulates the tracking scheme to handle multiple predictions (or hypotheses) of poses, thereby mitigating the negative effect of prediction errors. An important component of SMC, i.e., a proposal distribution, is designed as a probabilistic neural pose predictor, which can propose diverse and plausible hypotheses by incorporating epistemic uncertainty and heteroscedastic aleatoric uncertainty. In addition, a recurrent architecture is introduced to our neural modeling to utilize time-sequence information of poses to manage difficult situations, such as the frequent disappearance and reappearances of poses. Compared to existing baselines, the proposed method achieves a state-of-the-art MOTA score on the PoseTrack2018 validation dataset by reducing approximately 50% of tracking errors from a state-of-the art baseline method.



### Boosting Real-Time Driving Scene Parsing with Shared Semantics
- **Arxiv ID**: http://arxiv.org/abs/1909.07038v3
- **DOI**: 10.1109/LRA.2020.2965075
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.07038v3)
- **Published**: 2019-09-16 07:38:26+00:00
- **Updated**: 2019-11-28 05:20:23+00:00
- **Authors**: Zhenzhen Xiang, Anbo Bao, Jie Li, Jianbo Su
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time scene parsing is a fundamental feature for autonomous driving vehicles with multiple cameras. In this letter we demonstrate that sharing semantics between cameras with different perspectives and overlapped views can boost the parsing performance when compared with traditional methods, which individually process the frames from each camera. Our framework is based on a deep neural network for semantic segmentation but with two kinds of additional modules for sharing and fusing semantics. On the one hand, a semantics sharing module is designed to establish the pixel-wise mapping between the input images. Features as well as semantics are shared by the map to reduce duplicated workload which leads to more efficient computation. On the other hand, feature fusion modules are designed to combine different modal of semantic features, which leverage the information from both inputs for better accuracy. To evaluate the effectiveness of the proposed framework, we have applied our network to a dual-camera vision system for driving scene parsing. Experimental results show that our network outperforms the baseline method on the parsing accuracy with comparable computations.



### $360^o$ Surface Regression with a Hyper-Sphere Loss
- **Arxiv ID**: http://arxiv.org/abs/1909.07043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07043v1)
- **Published**: 2019-09-16 07:55:49+00:00
- **Updated**: 2019-09-16 07:55:49+00:00
- **Authors**: Antonis Karakottas, Nikolaos Zioulis, Stamatis Samaras, Dimitrios Ataloglou, Vasileios Gkitsas, Dimitrios Zarpalas, Petros Daras
- **Comment**: Pre-print of the main paper and supplementary material combined that
  is going to be presented at 3DV2019
- **Journal**: None
- **Summary**: Omnidirectional vision is becoming increasingly relevant as more efficient $360^o$ image acquisition is now possible. However, the lack of annotated $360^o$ datasets has hindered the application of deep learning techniques on spherical content. This is further exaggerated on tasks where ground truth acquisition is difficult, such as monocular surface estimation. While recent research approaches on the 2D domain overcome this challenge by relying on generating normals from depth cues using RGB-D sensors, this is very difficult to apply on the spherical domain. In this work, we address the unavailability of sufficient $360^o$ ground truth normal data, by leveraging existing 3D datasets and remodelling them via rendering. We present a dataset of $360^o$ images of indoor spaces with their corresponding ground truth surface normal, and train a deep convolutional neural network (CNN) on the task of monocular 360 surface estimation. We achieve this by minimizing a novel angular loss function defined on the hyper-sphere using simple quaternion algebra. We put an effort to appropriately compare with other state of the art methods trained on planar datasets and finally, present the practical applicability of our trained model on a spherical image re-lighting task using completely unseen data by qualitatively showing the promising generalization ability of our dataset and model. The dataset is available at: vcl3d.github.io/HyperSphereSurfaceRegression.



### Identifying Pediatric Vascular Anomalies With Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.07046v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1909.07046v1)
- **Published**: 2019-09-16 08:11:04+00:00
- **Updated**: 2019-09-16 08:11:04+00:00
- **Authors**: Justin Chan, Sharat Raju, Randall Bly, Jonathan A. Perkins, Shyamnath Gollakota
- **Comment**: None
- **Journal**: None
- **Summary**: Vascular anomalies, more colloquially known as birthmarks, affect up to 1 in 10 infants. Though many of these lesions self-resolve, some types can result in medical complications or disfigurement without proper diagnosis or management. Accurately diagnosing vascular anomalies is challenging for pediatricians and primary care physicians due to subtle visual differences and similarity to other pediatric dermatologic conditions. This can result in delayed or incorrect referrals for treatment. To address this problem, we developed a convolutional neural network (CNN) to automatically classify images of vascular anomalies and other pediatric skin conditions to aid physicians with diagnosis. We constructed a dataset of 21,681 clinical images, including data collected between 2002-2018 at Seattle Children's hospital as well as five dermatologist-curated online repositories, and built a taxonomy over vascular anomalies and other common pediatric skin lesions. The CNN achieved an average AUC of 0.9731 when ten-fold cross-validation was performed across a taxonomy of 12 classes. The classifier's average AUC and weighted F1 score was 0.9889 and 0.9732 respectively when evaluated on a previously unseen test set of six of these classes. Further, when used as an aid by pediatricians (n = 7), the classifier increased their average visual diagnostic accuracy from 73.10% to 91.67%. The classifier runs in real-time on a smartphone and has the potential to improve diagnosis of these conditions, particularly in resource-limited areas.



### A Single Multi-Task Deep Neural Network with Post-Processing for Object Detection with Reasoning and Robotic Grasp Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.07050v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.07050v1)
- **Published**: 2019-09-16 08:24:23+00:00
- **Updated**: 2019-09-16 08:24:23+00:00
- **Authors**: Dongwon Park, Yonghyeok Seo, Dongju Shin, Jaesik Choi, Se Young Chun
- **Comment**: Dongwon Park and Yonghyeok Seo are equally contributed to this work
- **Journal**: None
- **Summary**: Recently, robotic grasp detection (GD) and object detection (OD) with reasoning have been investigated using deep neural networks (DNNs). There have been works to combine these multi-tasks using separate networks so that robots can deal with situations of grasping specific target objects in the cluttered, stacked, complex piles of novel objects from a single RGB-D camera. We propose a single multi-task DNN that yields the information on GD, OD and relationship reasoning among objects with a simple post-processing. Our proposed methods yielded state-of-the-art performance with the accuracy of 98.6% and 74.2% and the computation speed of 33 and 62 frame per second on VMRD and Cornell datasets, respectively. Our methods also yielded 95.3% grasp success rate for single novel object grasping with a 4-axis robot arm and 86.7% grasp success rate in cluttered novel objects with a Baxter robot.



### Learning Spatial Awareness to Improve Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1909.07057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07057v1)
- **Published**: 2019-09-16 08:39:01+00:00
- **Updated**: 2019-09-16 08:39:01+00:00
- **Authors**: Zhi-Qi Cheng, Jun-Xiu Li, Qi Dai, Xiao Wu, Alexander Hauptmann
- **Comment**: ICCV 2019 Oral
- **Journal**: None
- **Summary**: The aim of crowd counting is to estimate the number of people in images by leveraging the annotation of center positions for pedestrians' heads. Promising progresses have been made with the prevalence of deep Convolutional Neural Networks. Existing methods widely employ the Euclidean distance (i.e., $L_2$ loss) to optimize the model, which, however, has two main drawbacks: (1) the loss has difficulty in learning the spatial awareness (i.e., the position of head) since it struggles to retain the high-frequency variation in the density map, and (2) the loss is highly sensitive to various noises in crowd counting, such as the zero-mean noise, head size changes, and occlusions. Although the Maximum Excess over SubArrays (MESA) loss has been previously proposed to address the above issues by finding the rectangular subregion whose predicted density map has the maximum difference from the ground truth, it cannot be solved by gradient descent, thus can hardly be integrated into the deep learning framework. In this paper, we present a novel architecture called SPatial Awareness Network (SPANet) to incorporate spatial context for crowd counting. The Maximum Excess over Pixels (MEP) loss is proposed to achieve this by finding the pixel-level subregion with high discrepancy to the ground truth. To this end, we devise a weakly supervised learning scheme to generate such region with a multi-branch architecture. The proposed framework can be integrated into existing deep crowd counting methods and is end-to-end trainable. Extensive experiments on four challenging benchmarks show that our method can significantly improve the performance of baselines. More remarkably, our approach outperforms the state-of-the-art methods on all benchmark datasets.



### Motion Guided Attention for Video Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.07061v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07061v2)
- **Published**: 2019-09-16 08:44:02+00:00
- **Updated**: 2019-10-03 17:03:23+00:00
- **Authors**: Haofeng Li, Guanqi Chen, Guanbin Li, Yizhou Yu
- **Comment**: 10 pages, 4 figures, ICCV 2019, code:
  https://github.com/lhaof/Motion-Guided-Attention
- **Journal**: None
- **Summary**: Video salient object detection aims at discovering the most visually distinctive objects in a video. How to effectively take object motion into consideration during video salient object detection is a critical issue. Existing state-of-the-art methods either do not explicitly model and harvest motion cues or ignore spatial contexts within optical flow images. In this paper, we develop a multi-task motion guided video salient object detection network, which learns to accomplish two sub-tasks using two sub-networks, one sub-network for salient object detection in still images and the other for motion saliency detection in optical flow images. We further introduce a series of novel motion guided attention modules, which utilize the motion saliency sub-network to attend and enhance the sub-network for still images. These two sub-networks learn to adapt to each other by end-to-end training. Experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art algorithms on a wide range of benchmarks. We hope our simple and effective approach will serve as a solid baseline and help ease future research in video salient object detection. Code and models will be made available.



### Fault-Diagnosing SLAM for Varying Scale Change Detection
- **Arxiv ID**: http://arxiv.org/abs/1909.09592v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.09592v1)
- **Published**: 2019-09-16 08:52:45+00:00
- **Updated**: 2019-09-16 08:52:45+00:00
- **Authors**: Sugimoto Takuma, Yamaguchi Kousuke, Tanaka Kanji
- **Comment**: 7 pages, 4 figures, technical report
- **Journal**: None
- **Summary**: In this paper, we present a new fault diagnosis (FD) -based approach for detection of imagery changes that can detect significant changes as inconsistencies between different sub-modules (e.g., self-localizaiton) of visual SLAM. Unlike classical change detection approaches such as pairwise image comparison (PC) and anomaly detection (AD), neither the memorization of each map image nor the maintenance of up-to-date place-specific anomaly detectors are required in this FD approach. A significant challenge that is encountered when incorporating different SLAM sub-modules into FD involves dealing with the varying scales of objects that have changed (e.g., the appearance of small dangerous obstacles on the floor). To address this issue, we reconsider the bag-of-words (BoW) image representation, by exploiting its recent advances in terms of self-localization and change detection. As a key advantage, BoW image representation can be reorganized into any different scaling by simply cropping the original BoW image. Furthermore, we propose to combine different self-localization modules with strong and weak BoW features with different discriminativity, and to treat inconsistency between strong and weak self-localization as an indicator of change. The efficacy of the proposed approach for FD with/without AD and/or PC was experimentally validated.



### Pose Neural Fabrics Search
- **Arxiv ID**: http://arxiv.org/abs/1909.07068v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.07068v4)
- **Published**: 2019-09-16 08:56:14+00:00
- **Updated**: 2020-12-05 05:40:24+00:00
- **Authors**: Sen Yang, Wankou Yang, Zhen Cui
- **Comment**: 11 pages, 8 figures, 7 tables. Code is available at
  https://github.com/yangsenius/PoseNFS
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) technologies have emerged in many domains to jointly learn the architectures and weights of the neural network. However, most existing NAS works claim they are task-specific and focus only on optimizing a single architecture to replace a human-designed neural network, in fact, their search processes are almost independent of domain knowledge of the tasks. In this paper, we propose Pose Neural Fabrics Search (PoseNFS). We explore a new solution for NAS and human pose estimation task: part-specific neural architecture search, which can be seen as a variant of multi-task learning. Firstly, we design a new neural architecture search space, Cell-based Neural Fabric (CNF), to learn micro as well as macro neural architecture using a differentiable search strategy. Then, we view locating human keypoints as multiple disentangled prediction sub-tasks, and then use prior knowledge of body structure as guidance to search for multiple part-specific neural architectures for different human parts. After search, all these part-specific CNFs have distinct micro and macro architecture parameters. The results show that such knowledge-guided NAS-based architectures have obvious performance improvements to a hand-designed part-based baseline model. The experiments on MPII and MS-COCO datasets demonstrate that PoseNFS\footnote{Code is available at \url{https://github.com/yangsenius/PoseNFS}} can achieve comparable performance to some efficient and state-of-the-art methods.



### A Real-Time Cross-modality Correlation Filtering Method for Referring Expression Comprehension
- **Arxiv ID**: http://arxiv.org/abs/1909.07072v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07072v4)
- **Published**: 2019-09-16 09:01:45+00:00
- **Updated**: 2020-04-27 03:50:23+00:00
- **Authors**: Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen Qian, Bo Li
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Referring expression comprehension aims to localize the object instance described by a natural language expression. Current referring expression methods have achieved good performance. However, none of them is able to achieve real-time inference without accuracy drop. The reason for the relatively slow inference speed is that these methods artificially split the referring expression comprehension into two sequential stages including proposal generation and proposal ranking. It does not exactly conform to the habit of human cognition. To this end, we propose a novel Realtime Cross-modality Correlation Filtering method (RCCF). RCCF reformulates the referring expression comprehension as a correlation filtering process. The expression is first mapped from the language domain to the visual domain and then treated as a template (kernel) to perform correlation filtering on the image feature map. The peak value in the correlation heatmap indicates the center points of the target box. In addition, RCCF also regresses a 2-D object size and 2-D offset. The center point coordinates, object size and center point offset together to form the target bounding box. Our method runs at 40 FPS while achieving leading performance in RefClef, RefCOCO, RefCOCO+ and RefCOCOg benchmarks. In the challenging RefClef dataset, our methods almost double the state-of-the-art performance (34.70% increased to 63.79%). We hope this work can arouse more attention and studies to the new cross-modality correlation filtering framework as well as the one-stage framework for referring expression comprehension.



### Temporally Consistent Depth Prediction with Flow-Guided Memory Units
- **Arxiv ID**: http://arxiv.org/abs/1909.07074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07074v1)
- **Published**: 2019-09-16 09:10:10+00:00
- **Updated**: 2019-09-16 09:10:10+00:00
- **Authors**: Chanho Eom, Hyunjong Park, Bumsub Ham
- **Comment**: IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Predicting depth from a monocular video sequence is an important task for autonomous driving. Although it has advanced considerably in the past few years, recent methods based on convolutional neural networks (CNNs) discard temporal coherence in the video sequence and estimate depth independently for each frame, which often leads to undesired inconsistent results over time. To address this problem, we propose to memorize temporal consistency in the video sequence, and leverage it for the task of depth prediction. To this end, we introduce a two-stream CNN with a flow-guided memory module, where each stream encodes visual and temporal features, respectively. The memory module, implemented using convolutional gated recurrent units (ConvGRUs), inputs visual and temporal features sequentially together with optical flow tailored to our task. It memorizes trajectories of individual features selectively and propagates spatial information over time, enforcing a long-term temporal consistency to prediction results. We evaluate our method on the KITTI benchmark dataset in terms of depth prediction accuracy, temporal consistency and runtime, and achieve a new state of the art. We also provide an extensive experimental analysis, clearly demonstrating the effectiveness of our approach to memorizing temporal consistency for depth prediction.



### Classification-Specific Parts for Improving Fine-Grained Visual Categorization
- **Arxiv ID**: http://arxiv.org/abs/1909.07075v1
- **DOI**: 10.1007/978-3-030-33676-9_5
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.07075v1)
- **Published**: 2019-09-16 09:13:47+00:00
- **Updated**: 2019-09-16 09:13:47+00:00
- **Authors**: Dimitri Korsch, Paul Bodesheim, Joachim Denzler
- **Comment**: Presented at the GCPR2019
- **Journal**: None
- **Summary**: Fine-grained visual categorization is a classification task for distinguishing categories with high intra-class and small inter-class variance. While global approaches aim at using the whole image for performing the classification, part-based solutions gather additional local information in terms of attentions or parts. We propose a novel classification-specific part estimation that uses an initial prediction as well as back-propagation of feature importance via gradient computations in order to estimate relevant image regions. The subsequently detected parts are then not only selected by a-posteriori classification knowledge, but also have an intrinsic spatial extent that is determined automatically. This is in contrast to most part-based approaches and even to available ground-truth part annotations, which only provide point coordinates and no additional scale information. We show in our experiments on various widely-used fine-grained datasets the effectiveness of the mentioned part selection method in conjunction with the extracted part features.



### Controllable Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1909.07083v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.07083v2)
- **Published**: 2019-09-16 09:29:52+00:00
- **Updated**: 2019-12-19 18:30:18+00:00
- **Authors**: Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, Philip H. S. Torr
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel controllable text-to-image generative adversarial network (ControlGAN), which can effectively synthesise high-quality images and also control parts of the image generation according to natural language descriptions. To achieve this, we introduce a word-level spatial and channel-wise attention-driven generator that can disentangle different visual attributes, and allow the model to focus on generating and manipulating subregions corresponding to the most relevant words. Also, a word-level discriminator is proposed to provide fine-grained supervisory feedback by correlating words with image regions, facilitating training an effective generator which is able to manipulate specific visual attributes without affecting the generation of other content. Furthermore, perceptual loss is adopted to reduce the randomness involved in the image generation, and to encourage the generator to manipulate specific attributes required in the modified text. Extensive experiments on benchmark datasets demonstrate that our method outperforms existing state of the art, and is able to effectively manipulate synthetic images using natural language descriptions. Code is available at https://github.com/mrlibw/ControlGAN.



### Encoding CT Anatomy Knowledge for Unpaired Chest X-ray Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1909.12922v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.12922v1)
- **Published**: 2019-09-16 09:47:47+00:00
- **Updated**: 2019-09-16 09:47:47+00:00
- **Authors**: Zeju Li, Han Li, Hu Han, Gonglei Shi, Jiannan Wang, S. Kevin Zhou
- **Comment**: 9 pages with 4 figures
- **Journal**: None
- **Summary**: Although chest X-ray (CXR) offers a 2D projection with overlapped anatomies, it is widely used for clinical diagnosis. There is clinical evidence supporting that decomposing an X-ray image into different components (e.g., bone, lung and soft tissue) improves diagnostic value. We hereby propose a decomposition generative adversarial network (DecGAN) to anatomically decompose a CXR image but with unpaired data. We leverage the anatomy knowledge embedded in CT, which features a 3D volume with clearly visible anatomies. Our key idea is to embed CT priori decomposition knowledge into the latent space of unpaired CXR autoencoder. Specifically, we train DecGAN with a decomposition loss, adversarial losses, cycle-consistency losses and a mask loss to guarantee that the decomposed results of the latent space preserve realistic body structures. Extensive experiments demonstrate that DecGAN provides superior unsupervised CXR bone suppression results and the feasibility of modulating CXR components by latent space disentanglement. Furthermore, we illustrate the diagnostic value of DecGAN and demonstrate that it outperforms the state-of-the-art approaches in terms of predicting 11 out of 14 common lung diseases.



### CELNet: Evidence Localization for Pathology Images using Weakly Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.07097v1
- **DOI**: 10.1007/978-3-030-32239-7_68
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07097v1)
- **Published**: 2019-09-16 10:02:51+00:00
- **Updated**: 2019-09-16 10:02:51+00:00
- **Authors**: Yongxiang Huang, Albert C. S. Chung
- **Comment**: Accepted for MICCAI 2019
- **Journal**: None
- **Summary**: Despite deep convolutional neural networks boost the performance of image classification and segmentation in digital pathology analysis, they are usually weak in interpretability for clinical applications or require heavy annotations to achieve object localization. To overcome this problem, we propose a weakly supervised learning-based approach that can effectively learn to localize the discriminative evidence for a diagnostic label from weakly labeled training data. Experimental results show that our proposed method can reliably pinpoint the location of cancerous evidence supporting the decision of interest, while still achieving a competitive performance on glimpse-level and slide-level histopathologic cancer detection tasks.



### TextSR: Content-Aware Text Super-Resolution Guided by Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.07113v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07113v4)
- **Published**: 2019-09-16 10:46:06+00:00
- **Updated**: 2019-10-20 03:30:58+00:00
- **Authors**: Wenjia Wang, Enze Xie, Peize Sun, Wenhai Wang, Lixun Tian, Chunhua Shen, Ping Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Scene text recognition has witnessed rapid development with the advance of convolutional neural networks. Nonetheless, most of the previous methods may not work well in recognizing text with low resolution which is often seen in natural scene images. An intuitive solution is to introduce super-resolution techniques as pre-processing. However, conventional super-resolution methods in the literature mainly focus on reconstructing the detailed texture of natural images, which typically do not work well for text due to the unique characteristics of text. To tackle these problems, in this work, we propose a content-aware text super-resolution network to generate the information desired for text recognition. In particular, we design an end-to-end network that can perform super-resolution and text recognition simultaneously. Different from previous super-resolution methods, we use the loss of text recognition as the Text Perceptual Loss to guide the training of the super-resolution network, and thus it pays more attention to the text content, rather than the irrelevant background area. Extensive experiments on several challenging benchmarks demonstrate the effectiveness of our proposed method in restoring a sharp high-resolution image from a small blurred one, and show that the recognition performance clearly boosts up the performance of text recognizer. To our knowledge, this is the first work focusing on text super-resolution. Code will be released in https://github.com/xieenze/TextSR.



### PLIN: A Network for Pseudo-LiDAR Point Cloud Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1909.07137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07137v1)
- **Published**: 2019-09-16 11:50:04+00:00
- **Updated**: 2019-09-16 11:50:04+00:00
- **Authors**: Haojie Liu, Kang Liao, Chunyu Lin, Yao Zhao, Yulan Guo
- **Comment**: 7 pages, 5 figures, Submitted to ICRA2020
- **Journal**: None
- **Summary**: LiDAR sensors can provide dependable 3D spatial information at a low frequency (around 10Hz) and have been widely applied in the field of autonomous driving and UAV. However, the camera with a higher frequency (around 20Hz) has to be decreased so as to match with LiDAR in a multi-sensor system. In this paper, we propose a novel Pseudo-LiDAR interpolation network (PLIN) to increase the frequency of LiDAR sensors. PLIN can generate temporally and spatially high-quality point cloud sequences to match the high frequency of cameras. To achieve this goal, we design a coarse interpolation stage guided by consecutive sparse depth maps and motion relationship. We also propose a refined interpolation stage guided by the realistic scene. Using this coarse-to-fine cascade structure, our method can progressively perceive multi-modal information and generate accurate intermediate point clouds. To the best of our knowledge, this is the first deep framework for Pseudo-LiDAR point cloud interpolation, which shows appealing applications in navigation systems equipped with LiDAR and cameras. Experimental results demonstrate that PLIN achieves promising performance on the KITTI dataset, significantly outperforming the traditional interpolation method and the state-of-the-art video interpolation technique.



### Meta Reinforcement Learning for Sim-to-real Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1909.12906v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.12906v1)
- **Published**: 2019-09-16 11:59:40+00:00
- **Updated**: 2019-09-16 11:59:40+00:00
- **Authors**: Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, Ville Kyrki
- **Comment**: Submitted to ICRA 2020
- **Journal**: None
- **Summary**: Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.



### ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT)
- **Arxiv ID**: http://arxiv.org/abs/1909.07145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07145v1)
- **Published**: 2019-09-16 12:19:00+00:00
- **Updated**: 2019-09-16 12:19:00+00:00
- **Authors**: Chee-Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, Chee Seng Chan, Lianwen Jin
- **Comment**: Technical report of ICDAR2019 Robust Reading Challenge on
  Arbitrary-Shaped Text (RRC-ArT) Competition
- **Journal**: None
- **Summary**: This paper reports the ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT) that consists of three major challenges: i) scene text detection, ii) scene text recognition, and iii) scene text spotting. A total of 78 submissions from 46 unique teams/individuals were received for this competition. The top performing score of each challenge is as follows: i) T1 - 82.65%, ii) T2.1 - 74.3%, iii) T2.2 - 85.32%, iv) T3.1 - 53.86%, and v) T3.2 - 54.91%. Apart from the results, this paper also details the ArT dataset, tasks description, evaluation metrics and participants methods. The dataset, the evaluation kit as well as the results are publicly available at https://rrc.cvc.uab.es/?ch=14



### [Extended version] Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks
- **Arxiv ID**: http://arxiv.org/abs/1909.07830v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.07830v3)
- **Published**: 2019-09-16 13:48:08+00:00
- **Updated**: 2019-11-02 16:03:32+00:00
- **Authors**: Lixin Fan, Kam Woh Ng, Chee Seng Chan
- **Comment**: This paper is accepted by NeurIPS 2019; Our code is available at
  https://github.com/kamwoh/DeepIPR. This is the extended version
- **Journal**: None
- **Summary**: With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN), there emerges an urgent need to protect these inventions from being illegally copied, redistributed, or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line, we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks, which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole, this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that, the DNN inference performance of an original task will be significantly deteriorated due to forged passports. In other words, genuine passports are not only verified by looking for the predefined signatures, but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes. Code and models are available at https://github.com/kamwoh/DeepIPR



### MuPNet: Multi-modal Predictive Coding Network for Place Recognition by Unsupervised Learning of Joint Visuo-Tactile Latent Representations
- **Arxiv ID**: http://arxiv.org/abs/1909.07201v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07201v1)
- **Published**: 2019-09-16 13:54:11+00:00
- **Updated**: 2019-09-16 13:54:11+00:00
- **Authors**: Oliver Struckmeier, Kshitij Tiwari, Shirin Dora, Martin J. Pearson, Sander M. Bohte, Cyriel MA Pennartz, Ville Kyrki
- **Comment**: Submitted to ICRA 2020. 6+1 Pages with 5 figures
- **Journal**: None
- **Summary**: Extracting and binding salient information from different sensory modalities to determine common features in the environment is a significant challenge in robotics. Here we present MuPNet (Multi-modal Predictive Coding Network), a biologically plausible network architecture for extracting joint latent features from visuo-tactile sensory data gathered from a biomimetic mobile robot. In this study we evaluate MuPNet applied to place recognition as a simulated biomimetic robot platform explores visually aliased environments. The F1 scores demonstrate that its performance over prior hand-crafted sensory feature extraction techniques is equivalent under controlled conditions, with significant improvement when operating in novel environments.



### Global Aggregation then Local Distribution in Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.07229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07229v1)
- **Published**: 2019-09-16 14:18:22+00:00
- **Updated**: 2019-09-16 14:18:22+00:00
- **Authors**: Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, Yunhai Tong
- **Comment**: accepted at BMVC 2019
- **Journal**: None
- **Summary**: It has been widely proven that modelling long-range dependencies in fully convolutional networks (FCNs) via global aggregation modules is critical for complex scene understanding tasks such as semantic segmentation and object detection. However, global aggregation is often dominated by features of large patterns and tends to oversmooth regions that contain small patterns (e.g., boundaries and small objects). To resolve this problem, we propose to first use \emph{Global Aggregation} and then \emph{Local Distribution}, which is called GALD, where long-range dependencies are more confidently used inside large pattern regions and vice versa. The size of each pattern at each position is estimated in the network as a per-channel mask map. GALD is end-to-end trainable and can be easily plugged into existing FCNs with various global aggregation modules for a wide range of vision tasks, and consistently improves the performance of state-of-the-art object detection and instance segmentation approaches. In particular, GALD used in semantic segmentation achieves new state-of-the-art performance on Cityscapes test set with mIoU 83.3\%. Code is available at: \url{https://github.com/lxtGH/GALD-Net}



### DeepTIO: A Deep Thermal-Inertial Odometry with Visual Hallucination
- **Arxiv ID**: http://arxiv.org/abs/1909.07231v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.07231v2)
- **Published**: 2019-09-16 14:19:42+00:00
- **Updated**: 2020-01-19 22:42:04+00:00
- **Authors**: Muhamad Risqi U. Saputra, Pedro P. B. de Gusmao, Chris Xiaoxuan Lu, Yasin Almalioglu, Stefano Rosa, Changhao Chen, Johan Wahlstrm, Wei Wang, Andrew Markham, Niki Trigoni
- **Comment**: Accepted to IEEE Robotics and Automation Letters (RAL)
- **Journal**: None
- **Summary**: Visual odometry shows excellent performance in a wide range of environments. However, in visually-denied scenarios (e.g. heavy smoke or darkness), pose estimates degrade or even fail. Thermal cameras are commonly used for perception and inspection when the environment has low visibility. However, their use in odometry estimation is hampered by the lack of robust visual features. In part, this is as a result of the sensor measuring the ambient temperature profile rather than scene appearance and geometry. To overcome this issue, we propose a Deep Neural Network model for thermal-inertial odometry (DeepTIO) by incorporating a visual hallucination network to provide the thermal network with complementary information. The hallucination network is taught to predict fake visual features from thermal images by using Huber loss. We also employ selective fusion to attentively fuse the features from three different modalities, i.e thermal, hallucination, and inertial features. Extensive experiments are performed in hand-held and mobile robot data in benign and smoke-filled environments, showing the efficacy of the proposed model.



### The impact of patient clinical information on automated skin cancer detection
- **Arxiv ID**: http://arxiv.org/abs/1909.12912v1
- **DOI**: 10.1016/j.compbiomed.2019.103545
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.12912v1)
- **Published**: 2019-09-16 14:27:12+00:00
- **Updated**: 2019-09-16 14:27:12+00:00
- **Authors**: Andre G. C. Pacheco, Renato A. Krohling
- **Comment**: None
- **Journal**: None
- **Summary**: Skin cancer is one of the most common types of cancer around the world. For this reason, over the past years, different approaches have been proposed to assist detect it. Nonetheless, most of them are based only on dermoscopy images and do not take into account the patient clinical information. In this work, first, we present a new dataset that contains clinical images, acquired from smartphones, and patient clinical information of the skin lesions. Next, we introduce a straightforward approach to combine the clinical data and the images using different well-known deep learning models. These models are applied to the presented dataset using only the images and combining them with the patient clinical information. We present a comprehensive study to show the impact of the clinical data on the final predictions. The results obtained by combining both sets of information show a general improvement of around 7% in the balanced accuracy for all models. In addition, the statistical test indicates significant differences between the models with and without considering both data. The improvement achieved shows the potential of using patient clinical information in skin cancer detection and indicates that this piece of information is important to leverage skin cancer detection systems.



### BMVC 2019: Workshop on Interpretable and Explainable Machine Vision
- **Arxiv ID**: http://arxiv.org/abs/1909.07245v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.07245v1)
- **Published**: 2019-09-16 14:44:19+00:00
- **Updated**: 2019-09-16 14:44:19+00:00
- **Authors**: Alun Preece
- **Comment**: None
- **Journal**: None
- **Summary**: Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019.



### A Fast and Robust Place Recognition Approach for Stereo Visual Odometry Using LiDAR Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1909.07267v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07267v4)
- **Published**: 2019-09-16 15:14:55+00:00
- **Updated**: 2020-07-26 05:38:47+00:00
- **Authors**: Jiawei Mo, Junaed Sattar
- **Comment**: Accepted by IROS2020
- **Journal**: None
- **Summary**: Place recognition is a core component of Simultaneous Localization and Mapping (SLAM) algorithms. Particularly in visual SLAM systems, previously-visited places are recognized by measuring the appearance similarity between images representing these locations. However, such approaches are sensitive to visual appearance change and also can be computationally expensive. In this paper, we propose an alternative approach adapting LiDAR descriptors for 3D points obtained from stereo-visual odometry for place recognition. 3D points are potentially more reliable than 2D visual cues (e.g., 2D features) against environmental changes (e.g., variable illumination) and this may benefit visual SLAM systems in long-term deployment scenarios. Stereo-visual odometry generates 3D points with an absolute scale, which enables us to use LiDAR descriptors for place recognition with high computational efficiency. Through extensive evaluations on standard benchmark datasets, we demonstrate the accuracy, efficiency, and robustness of using 3D points for place recognition over 2D methods.



### More About Covariance Descriptors for Image Set Coding: Log-Euclidean Framework based Kernel Matrix Representation
- **Arxiv ID**: http://arxiv.org/abs/1909.07273v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07273v2)
- **Published**: 2019-09-16 15:22:40+00:00
- **Updated**: 2019-09-26 14:17:18+00:00
- **Authors**: Kai-Xuan Chen, Xiao-Jun Wu, Jie-Yi Ren, Rui Wang, Josef Kittler
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We consider a family of structural descriptors for visual data, namely covariance descriptors (CovDs) that lie on a non-linear symmetric positive definite (SPD) manifold, a special type of Riemannian manifolds. We propose an improved version of CovDs for image set coding by extending the traditional CovDs from Euclidean space to the SPD manifold. Specifically, the manifold of SPD matrices is a complete inner product space with the operations of logarithmic multiplication and scalar logarithmic multiplication defined in the Log-Euclidean framework. In this framework, we characterise covariance structure in terms of the arc-cosine kernel which satisfies Mercer's condition and propose the operation of mean centralization on SPD matrices. Furthermore, we combine arc-cosine kernels of different orders using mixing parameters learnt by kernel alignment in a supervised manner. Our proposed framework provides a lower-dimensional and more discriminative data representation for the task of image set classification. The experimental results demonstrate its superior performance, measured in terms of recognition accuracy, as compared with the state-of-the-art methods.



### Self-Paced Video Data Augmentation with Dynamic Images Generated by Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.12929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.12929v1)
- **Published**: 2019-09-16 16:03:08+00:00
- **Updated**: 2019-09-16 16:03:08+00:00
- **Authors**: Yumeng Zhang, Gaoguo Jia, Li Chen, Mingrui Zhang, Junhai Yong
- **Comment**: None
- **Journal**: None
- **Summary**: There is an urgent need for an effective video classification method by means of a small number of samples. The deficiency of samples could be effectively alleviated by generating samples through Generative Adversarial Networks (GAN), but the generation of videos on a typical category remains to be underexplored since the complex actions and the changeable viewpoints are difficult to simulate. In this paper, we propose a generative data augmentation method for temporal stream of the Temporal Segment Networks with the dynamic image. The dynamic image compresses the motion information of video into a still image, removing the interference factors such as the background. Thus it is easier to generate images with categorical motion information using GAN. We use the generated dynamic images to enhance the features, with regularization achieved as well, thereby to achieve the effect of video augmentation. In order to deal with the uneven quality of generated images, we propose a Self-Paced Selection (SPS) method, which automatically selects the high-quality generated samples to be added to the network training. Our method is verified on two benchmark datasets, HMDB51 and UCF101. The experimental results show that the method can improve the accuracy of video classification under the circumstance of sample insufficiency and sample imbalance.



### Recognition of Russian traffic signs in winter conditions. Solutions of the "Ice Vision" competition winners
- **Arxiv ID**: http://arxiv.org/abs/1909.07311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07311v1)
- **Published**: 2019-09-16 16:15:51+00:00
- **Updated**: 2019-09-16 16:15:51+00:00
- **Authors**: Artem L. Pavlov, Azat Davletshin, Alexey Kharlamov, Maksim S. Koriukin, Artem Vasenin, Pavel Solovev, Pavel Ostyakov, Pavel A. Karpyshev, George V. Ovchinnikov, Ivan V. Oseledets, Dzmitry Tsetserukou
- **Comment**: Submitted to IEEE ICRA 2020
- **Journal**: None
- **Summary**: With the advancements of various autonomous car projects aiming to achieve SAE Level 5, real-time detection of traffic signs in real-life scenarios has become a highly relevant problem for the industry. Even though a great progress has been achieved in this field, there is still no clear consensus on what the state-of-the-art in this field is.   Moreover, it is important to develop and test systems in various regions and conditions. This is why the "Ice Vision" competition has focused on the detection of Russian traffic signs in winter conditions. The IceVisionSet dataset used for this competition features real-world collection of lossless frame sequences with traffic sign annotations. The sequences were collected in varying conditions, including: different weather, camera exposure, illumination and moving speeds.   In this work we describe the competition and present the solutions of the 3 top teams.



### Regular Partitions and Their Use in Structural Pattern Recognition
- **Arxiv ID**: http://arxiv.org/abs/1909.07420v2
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07420v2)
- **Published**: 2019-09-16 18:14:05+00:00
- **Updated**: 2020-03-24 21:15:53+00:00
- **Authors**: Marco Fiorucci
- **Comment**: PhD Thesis (Mar 2019), Ca Foscari University, Venice. arXiv admin
  note: text overlap with arXiv:1704.07114 by other authors
- **Journal**: None
- **Summary**: Recent years are characterized by an unprecedented quantity of available network data which are produced at an astonishing rate by an heterogeneous variety of interconnected sensors and devices. This high-throughput generation calls for the development of new effective methods to store, retrieve, understand and process massive network data. In this thesis, we tackle this challenge by introducing a framework to summarize large graphs based on Szemer\'edi's Regularity Remma (RL), which roughly states that any sufficiently large graph can almost entirely be partitioned into a bounded number of random-like bipartite graphs. The partition resulting from the RL gives rise to a summary, which inherits many of the essential structural properties of the original graph. We first extend an heuristic version of the RL to improve its efficiency and its robustness. We use the proposed algorithm to address graph-based clustering and image segmentation tasks. In the second part of the thesis, we introduce a new heuristic algorithm which is characterized by an improvement of the summary quality both in terms of reconstruction error and of noise filtering. We use the proposed heuristic to address the graph search problem defined under a similarity measure. Finally, we study the linkage among the regularity lemma, the stochastic block model and the minimum description length. This study provide us a principled way to develop a graph decomposition algorithm based on stochastic block model which is fitted using likelihood maximization.



### Reproducibility of an airway tapering measurement in CT with application to bronchiectasis
- **Arxiv ID**: http://arxiv.org/abs/1909.07454v1
- **DOI**: 10.1117/1.JMI.6.3.034003
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1909.07454v1)
- **Published**: 2019-09-16 19:49:01+00:00
- **Updated**: 2019-09-16 19:49:01+00:00
- **Authors**: Kin Quan, Ryutaro Tanno, Rebecca J. Shipley, Jeremy S. Brown, Joseph Jacob, John R. Hurst, David J. Hawkes
- **Comment**: 55 pages, 18 figures, The manuscript was originally published in
  Journal of Medical Imaging
- **Journal**: J. Med. Imag. 6(3), 034003 (2019)
- **Summary**: Purpose: This paper proposes a pipeline to acquire a scalar tapering measurement from the carina to the most distal point of an individual airway visible on CT. We show the applicability of using tapering measurements on clinically acquired data by quantifying the reproducibility of the tapering measure. Methods: We generate a spline from the centreline of an airway to measure the area and arclength at contiguous intervals. The tapering measurement is the gradient of the linear regression between area in log space and arclength. The reproducibility of the measure was assessed by analysing different radiation doses, voxel sizes and reconstruction kernel on single timepoint and longitudinal CT scans and by evaluating the effct of airway bifurcations. Results: Using 74 airways from 10 CT scans, we show a statistical difference, p = 3.4 $\times$ 10$^{-4}$ in tapering between healthy airways (n = 35) and those affected by bronchiectasis (n = 39). The difference between the mean of the two populations was 0.011mm$^{-1}$ and the difference between the medians of the two populations was 0.006mm$^{-1}$. The tapering measurement retained a 95\% confidence interval of $\pm$0.005mm$^{-1}$ in a simulated 25 mAs scan and retained a 95% confidence of $\pm$0.005mm$^{-1}$ on simulated CTs up to 1.5 times the original voxel size. Conclusion: We have established an estimate of the precision of the tapering measurement and estimated the effect on precision of simulated voxel size and CT scan dose. We recommend that the scanner calibration be undertaken with the phantoms as described, on the specific CT scanner, radiation dose and reconstruction algorithm that is to be used in any quantitative studies. Our code is available at https://github.com/quan14/AirwayTaperingInCT



### Bridging Visual Perception with Contextual Semantics for Understanding Robot Manipulation Tasks
- **Arxiv ID**: http://arxiv.org/abs/1909.07459v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07459v2)
- **Published**: 2019-09-16 20:06:54+00:00
- **Updated**: 2020-07-26 11:15:04+00:00
- **Authors**: Chen Jiang, Martin Jagersand
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding manipulation scenarios allows intelligent robots to plan for appropriate actions to complete a manipulation task successfully. It is essential for intelligent robots to semantically interpret manipulation knowledge by describing entities, relations and attributes in a structural manner. In this paper, we propose an implementing framework to generate high-level conceptual dynamic knowledge graphs from video clips. A combination of a Vision-Language model and an ontology system, in correspondence with visual perception and contextual semantics, is used to represent robot manipulation knowledge with Entity-Relation-Entity (E-R-E) and Entity-Attribute-Value (E-A-V) tuples. The proposed method is flexible and well-versed. Using the framework, we present a case study where robot performs manipulation actions in a kitchen environment, bridging visual perception with contextual semantics using the generated dynamic knowledge graphs.



### Efficient 3D Fully Convolutional Networks for Pulmonary Lobe Segmentation in CT Images
- **Arxiv ID**: http://arxiv.org/abs/1909.07474v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.07474v1)
- **Published**: 2019-09-16 20:47:48+00:00
- **Updated**: 2019-09-16 20:47:48+00:00
- **Authors**: Hoileong Lee, Tahreema Matin, Fergus Gleeson, Vicente Grau
- **Comment**: None
- **Journal**: None
- **Summary**: The human lung is a complex respiratory organ, consisting of five distinct anatomic compartments called lobes. Accurate and automatic segmentation of these pulmonary lobes from computed tomography (CT) images is of clinical importance for lung disease assessment and treatment planning. However, this task is challenging due to ambiguous lobar boundaries, anatomical variations and pathological deformations. In this paper, we propose a high-resolution and efficient 3D fully convolutional network to automatically segment the lobes. We refer to the network as Pulmonary Lobe Segmentation Network (PLS-Net), which is designed to efficiently exploit 3D spatial and contextual information from high-resolution volumetric CT images for effective volume-to-volume learning and inference. The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components: (i) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations; (ii) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation; and (iii) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to convolutional and downsampling operations. We evaluated the proposed PLS-Net on a multi-institutional dataset that consists of 210 CT images acquired from patients with a wide range of lung abnormalities. Experimental results show that our PLS-Net achieves state-of-the-art performance with better computational efficiency. Further experiments confirm the effectiveness of each novel component of the PLS-Net.



### Z-Net: an Anisotropic 3D DCNN for Medical CT Volume Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.07480v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.07480v2)
- **Published**: 2019-09-16 20:56:13+00:00
- **Updated**: 2020-03-09 16:12:39+00:00
- **Authors**: Peichao Li, Xiao-Yun Zhou, Zhao-Yang Wang, Guang-Zhong Yang
- **Comment**: 8 pages, 9 figures, two tables
- **Journal**: None
- **Summary**: Accurate volume segmentation from the Computed Tomography (CT) scan is a common prerequisite for pre-operative planning, intra-operative guidance and quantitative assessment of therapeutic outcomes in robot-assisted Minimally Invasive Surgery (MIS). 3D Deep Convolutional Neural Network (DCNN) is a viable solution for this task, but is memory intensive. Small isotropic patches are cropped from the original and large CT volume to mitigate this issue in practice, but it may cause discontinuities between the adjacent patches and severe class-imbalances within individual sub-volumes. This paper presents a new 3D DCNN framework, namely Z-Net, to tackle the discontinuity and class-imbalance issue by preserving a full field-of-view of the objects in the XY planes using anisotropic spatial separable convolutions. The proposed Z-Net can be seamlessly integrated into existing 3D DCNNs with isotropic convolutions such as 3D U-Net and V-Net, with improved volume segmentation Intersection over Union (IoU) - up to $12.6\%$. Detailed validation of Z-Net is provided for CT aortic, liver and lung segmentation, demonstrating the effectiveness and practical value of Z-Net for intra-operative 3D navigation in robot-assisted MIS.



### Instantiation-Net: 3D Mesh Reconstruction from Single 2D Image for Right Ventricle
- **Arxiv ID**: http://arxiv.org/abs/1909.08986v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.08986v1)
- **Published**: 2019-09-16 21:22:53+00:00
- **Updated**: 2019-09-16 21:22:53+00:00
- **Authors**: Zhao-Yang Wang, Xiao-Yun Zhou, Peichao Li, Celia Riga, Guang-Zhong Yang
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: 3D shape instantiation which reconstructs the 3D shape of a target from limited 2D images or projections is an emerging technique for surgical intervention. It improves the currently less-informative and insufficient 2D navigation schemes for robot-assisted Minimally Invasive Surgery (MIS) to 3D navigation. Previously, a general and registration-free framework was proposed for 3D shape instantiation based on Kernel Partial Least Square Regression (KPLSR), requiring manually segmented anatomical structures as the pre-requisite. Two hyper-parameters including the Gaussian width and component number also need to be carefully adjusted. Deep Convolutional Neural Network (DCNN) based framework has also been proposed to reconstruct a 3D point cloud from a single 2D image, with end-to-end and fully automatic learning. In this paper, an Instantiation-Net is proposed to reconstruct the 3D mesh of a target from its a single 2D image, by using DCNN to extract features from the 2D image and Graph Convolutional Network (GCN) to reconstruct the 3D mesh, and using Fully Connected (FC) layers to connect the DCNN to GCN. Detailed validation was performed to demonstrate the practical strength of the method and its potential clinical use.



### They Might NOT Be Giants: Crafting Black-Box Adversarial Examples with Fewer Queries Using Particle Swarm Optimization
- **Arxiv ID**: http://arxiv.org/abs/1909.07490v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.07490v1)
- **Published**: 2019-09-16 21:24:19+00:00
- **Updated**: 2019-09-16 21:24:19+00:00
- **Authors**: Rayan Mosli, Matthew Wright, Bo Yuan, Yin Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning models have been found to be susceptible to adversarial examples that are often indistinguishable from the original inputs. These adversarial examples are created by applying adversarial perturbations to input samples, which would cause them to be misclassified by the target models. Attacks that search and apply the perturbations to create adversarial examples are performed in both white-box and black-box settings, depending on the information available to the attacker about the target. For black-box attacks, the only capability available to the attacker is the ability to query the target with specially crafted inputs and observing the labels returned by the model. Current black-box attacks either have low success rates, requires a high number of queries, or produce adversarial examples that are easily distinguishable from their sources. In this paper, we present AdversarialPSO, a black-box attack that uses fewer queries to create adversarial examples with high success rates. AdversarialPSO is based on the evolutionary search algorithm Particle Swarm Optimization, a populationbased gradient-free optimization algorithm. It is flexible in balancing the number of queries submitted to the target vs the quality of imperceptible adversarial examples. The attack has been evaluated using the image classification benchmark datasets CIFAR-10, MNIST, and Imagenet, achieving success rates of 99.6%, 96.3%, and 82.0%, respectively, while submitting substantially fewer queries than the state-of-the-art. We also present a black-box method for isolating salient features used by models when making classifications. This method, called Swarms with Individual Search Spaces or SWISS, creates adversarial examples by finding and modifying the most important features in the input.



### Learning Geo-Temporal Image Features
- **Arxiv ID**: http://arxiv.org/abs/1909.07499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.07499v1)
- **Published**: 2019-09-16 22:00:59+00:00
- **Updated**: 2019-09-16 22:00:59+00:00
- **Authors**: Menghua Zhai, Tawfiq Salem, Connor Greenwell, Scott Workman, Robert Pless, Nathan Jacobs
- **Comment**: British Machine Vision Conference (BMVC) 2018
- **Journal**: None
- **Summary**: We propose to implicitly learn to extract geo-temporal image features, which are mid-level features related to when and where an image was captured, by explicitly optimizing for a set of location and time estimation tasks. To train our method, we take advantage of a large image dataset, captured by outdoor webcams and cell phones. The only form of supervision we provide are the known capture time and location of each image. We find that our approach learns features that are related to natural appearance changes in outdoor scenes. Additionally, we demonstrate the application of these geo-temporal features to time and location estimation.



### Scene Compliant Trajectory Forecast with Agent-Centric Spatio-Temporal Grids
- **Arxiv ID**: http://arxiv.org/abs/1909.07507v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1909.07507v1)
- **Published**: 2019-09-16 22:33:27+00:00
- **Updated**: 2019-09-16 22:33:27+00:00
- **Authors**: Daniela Ridel, Nachiket Deo, Denis Wolf, Mohan Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: Forecasting long-term human motion is a challenging task due to the non-linearity, multi-modality and inherent uncertainty in future trajectories. The underlying scene and past motion of agents can provide useful cues to predict their future motion. However, the heterogeneity of the two inputs poses a challenge for learning a joint representation of the scene and past trajectories. To address this challenge, we propose a model based on grid representations to forecast agent trajectories. We represent the past trajectories of agents using binary 2-D grids, and the underlying scene as a RGB birds-eye view (BEV) image, with an agent-centric frame of reference. We encode the scene and past trajectories using convolutional layers and generate trajectory forecasts using a Convolutional LSTM (ConvLSTM) decoder. Results on the publicly available Stanford Drone Dataset (SDD) show that our model outperforms prior approaches and outputs realistic future trajectories that comply with scene structure and past motion.




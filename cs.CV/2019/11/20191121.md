# Arxiv Papers in cs.CV on 2019-11-21
### FLNet: Landmark Driven Fetching and Learning Network for Faithful Talking Facial Animation Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1911.09224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09224v1)
- **Published**: 2019-11-21 00:07:43+00:00
- **Updated**: 2019-11-21 00:07:43+00:00
- **Authors**: Kuangxiao Gu, Yuqian Zhou, Thomas Huang
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Talking face synthesis has been widely studied in either appearance-based or warping-based methods. Previous works mostly utilize single face image as a source, and generate novel facial animations by merging other person's facial features. However, some facial regions like eyes or teeth, which may be hidden in the source image, can not be synthesized faithfully and stably. In this paper, We present a landmark driven two-stream network to generate faithful talking facial animation, in which more facial details are created, preserved and transferred from multiple source images instead of a single one. Specifically, we propose a network consisting of a learning and fetching stream. The fetching sub-net directly learns to attentively warp and merge facial regions from five source images of distinctive landmarks, while the learning pipeline renders facial organs from the training face space to compensate. Compared to baseline algorithms, extensive experiments demonstrate that the proposed method achieves a higher performance both quantitatively and qualitatively. Codes are at https://github.com/kgu3/FLNet_AAAI2020.



### Unsupervised Object Segmentation with Explicit Localization Module
- **Arxiv ID**: http://arxiv.org/abs/1911.09228v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09228v1)
- **Published**: 2019-11-21 00:50:48+00:00
- **Updated**: 2019-11-21 00:50:48+00:00
- **Authors**: Weitang Liu, Lifeng Wei, James Sharpnack, John D. Owens
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel architecture that iteratively discovers and segments out the objects of a scene based on the image reconstruction quality. Different from other approaches, our model uses an explicit localization module that localizes objects of the scene based on the pixel-level reconstruction qualities at each iteration, where simpler objects tend to be reconstructed better at earlier iterations and thus are segmented out first. We show that our localization module improves the quality of the segmentation, especially on a challenging background.



### Multi-Label Classification with Label Graph Superimposing
- **Arxiv ID**: http://arxiv.org/abs/1911.09243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09243v1)
- **Published**: 2019-11-21 02:08:20+00:00
- **Updated**: 2019-11-21 02:08:20+00:00
- **Authors**: Ya Wang, Dongliang He, Fu Li, Xiang Long, Zhichao Zhou, Jinwen Ma, Shilei Wen
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: Images or videos always contain multiple objects or actions. Multi-label recognition has been witnessed to achieve pretty performance attribute to the rapid development of deep learning technologies. Recently, graph convolution network (GCN) is leveraged to boost the performance of multi-label recognition. However, what is the best way for label correlation modeling and how feature learning can be improved with label system awareness are still unclear. In this paper, we propose a label graph superimposing framework to improve the conventional GCN+CNN framework developed for multi-label recognition in the following two aspects. Firstly, we model the label correlations by superimposing label graph built from statistical co-occurrence information into the graph constructed from knowledge priors of labels, and then multi-layer graph convolutions are applied on the final superimposed graph for label embedding abstraction. Secondly, we propose to leverage embedding of the whole label system for better representation learning. In detail, lateral connections between GCN and CNN are added at shallow, middle and deep layers to inject information of label system into backbone CNN for label-awareness in the feature learning process. Extensive experiments are carried out on MS-COCO and Charades datasets, showing that our proposed solution can greatly improve the recognition performance and achieves new state-of-the-art recognition performance.



### Consensus-based Optimization for 3D Human Pose Estimation in Camera Coordinates
- **Arxiv ID**: http://arxiv.org/abs/1911.09245v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09245v3)
- **Published**: 2019-11-21 02:19:08+00:00
- **Updated**: 2021-08-20 13:53:55+00:00
- **Authors**: Diogo C Luvizon, Hedi Tabia, David Picard
- **Comment**: Source code is available at
  https://github.com/dluvizon/3d-pose-consensus
- **Journal**: None
- **Summary**: 3D human pose estimation is frequently seen as the task of estimating 3D poses relative to the root body joint. Alternatively, we propose a 3D human pose estimation method in camera coordinates, which allows effective combination of 2D annotated data and 3D poses and a straightforward multi-view generalization. To that end, we cast the problem as a view frustum space pose estimation, where absolute depth prediction and joint relative depth estimations are disentangled. Final 3D predictions are obtained in camera coordinates by the inverse camera projection. Based on this, we also present a consensus-based optimization algorithm for multi-view predictions from uncalibrated images, which requires a single monocular training procedure. Although our method is indirectly tied to the training camera intrinsics, it still converges for cameras with different intrinsic parameters, resulting in coherent estimations up to a scale factor. Our method improves the state of the art on well known 3D human pose datasets, reducing the prediction error by 32% in the most common benchmark. We also reported our results in absolute pose position error, achieving 80~mm for monocular estimations and 51~mm for multi-view, on average.



### Semantic Segmentation of Thigh Muscle using 2.5D Deep Learning Network Trained with Limited Datasets
- **Arxiv ID**: http://arxiv.org/abs/1911.09249v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.09249v1)
- **Published**: 2019-11-21 02:30:31+00:00
- **Updated**: 2019-11-21 02:30:31+00:00
- **Authors**: Hasnine Haque, Masahiro Hashimoto, Nozomu Uetake, Masahiro Jinzaki
- **Comment**: 7 pages, 5 figures, This manuscript was a detailed version of our
  accepted oral paper in RSNA 2018. Ref: Haque,H, Hashimoto,M, Uetake,N,
  Jinzaki,M, End to End Solution for Complete Thigh Muscle Semantic
  Segmentation from Musculoskeletal CT using Deep Learning.
  http://archive.rsna.org/2018/18006583.html
- **Journal**: None
- **Summary**: Purpose: We propose a 2.5D deep learning neural network (DLNN) to automatically classify thigh muscle into 11 classes and evaluate its classification accuracy over 2D and 3D DLNN when trained with limited datasets. Enables operator invariant quantitative assessment of the thigh muscle volume change with respect to the disease progression. Materials and methods: Retrospective datasets consist of 48 thigh volume (TV) cropped from CT DICOM images. Cropped volumes were aligned with femur axis and resample in 2 mm voxel-spacing. Proposed 2.5D DLNN consists of three 2D U-Net trained with axial, coronal and sagittal muscle slices respectively. A voting algorithm was used to combine the output of U-Nets to create final segmentation. 2.5D U-Net was trained on PC with 38 TV and the remaining 10 TV were used to evaluate segmentation accuracy of 10 classes within Thigh. The result segmentation of both left and right thigh were de-cropped to original CT volume space. Finally, segmentation accuracies were compared between proposed DLNN and 2D/3D U-Net. Results: Average segmentation DSC score accuracy of all classes with 2.5D U-Net as 91.18% and Average Surface distance (ASD) accuracy as 0.84 mm. We found, mean DSC score for 2D U-Net was 3.3% lower than the that of 2.5D U-Net and mean DSC score of 3D U-Net was 5.7% lower than that of 2.5D U-Net when trained with same datasets. Conclusion: We achieved a faster computationally efficient and automatic segmentation of thigh muscle into 11 classes with reasonable accuracy. Enables quantitative evaluation of muscle atrophy with disease progression.



### DeepLABNet: End-to-end Learning of Deep Radial Basis Networks with Fully Learnable Basis Functions
- **Arxiv ID**: http://arxiv.org/abs/1911.09257v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.09257v1)
- **Published**: 2019-11-21 03:06:15+00:00
- **Updated**: 2019-11-21 03:06:15+00:00
- **Authors**: Andrew Hryniowski, Alexander Wong
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: From fully connected neural networks to convolutional neural networks, the learned parameters within a neural network have been primarily relegated to the linear parameters (e.g., convolutional filters). The non-linear functions (e.g., activation functions) have largely remained, with few exceptions in recent years, parameter-less, static throughout training, and seen limited variation in design. Largely ignored by the deep learning community, radial basis function (RBF) networks provide an interesting mechanism for learning more complex non-linear activation functions in addition to the linear parameters in a network. However, the interest in RBF networks has waned over time due to the difficulty of integrating RBFs into more complex deep neural network architectures in a tractable and stable manner. In this work, we present a novel approach that enables end-to-end learning of deep RBF networks with fully learnable activation basis functions in an automatic and tractable manner. We demonstrate that our approach for enabling the use of learnable activation basis functions in deep neural networks, which we will refer to as DeepLABNet, is an effective tool for automated activation function learning within complex network architectures.



### EnAET: A Self-Trained framework for Semi-Supervised and Supervised Learning with Ensemble Transformations
- **Arxiv ID**: http://arxiv.org/abs/1911.09265v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09265v2)
- **Published**: 2019-11-21 03:20:48+00:00
- **Updated**: 2021-02-01 16:15:05+00:00
- **Authors**: Xiao Wang, Daisuke Kihara, Jiebo Luo, Guo-Jun Qi
- **Comment**: 10 pages, 3 figures, conference
- **Journal**: None
- **Summary**: Deep neural networks have been successfully applied to many real-world applications. However, such successes rely heavily on large amounts of labeled data that is expensive to obtain. Recently, many methods for semi-supervised learning have been proposed and achieved excellent performance. In this study, we propose a new EnAET framework to further improve existing semi-supervised methods with self-supervised information. To our best knowledge, all current semi-supervised methods improve performance with prediction consistency and confidence ideas. We are the first to explore the role of {\bf self-supervised} representations in {\bf semi-supervised} learning under a rich family of transformations. Consequently, our framework can integrate the self-supervised information as a regularization term to further improve {\it all} current semi-supervised methods. In the experiments, we use MixMatch, which is the current state-of-the-art method on semi-supervised learning, as a baseline to test the proposed EnAET framework. Across different datasets, we adopt the same hyper-parameters, which greatly improves the generalization ability of the EnAET framework. Experiment results on different datasets demonstrate that the proposed EnAET framework greatly improves the performance of current semi-supervised algorithms. Moreover, this framework can also improve {\bf supervised learning} by a large margin, including the extremely challenging scenarios with only 10 images per class. The code and experiment records are available in \url{https://github.com/maple-research-lab/EnAET}.



### Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1911.09267v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.09267v3)
- **Published**: 2019-11-21 03:26:15+00:00
- **Updated**: 2020-02-11 05:49:15+00:00
- **Authors**: Ceyuan Yang, Yujun Shen, Bolei Zhou
- **Comment**: 15 pages, 20 figures
- **Journal**: None
- **Summary**: Despite the success of Generative Adversarial Networks (GANs) in image synthesis, there lacks enough understanding on what generative models have learned inside the deep generative representations and how photo-realistic images are able to be composed of the layer-wise stochasticity introduced in recent GANs. In this work, we show that highly-structured semantic hierarchy emerges as variation factors from synthesizing scenes from the generative representations in state-of-the-art GAN models, like StyleGAN and BigGAN. By probing the layer-wise representations with a broad set of semantics at different abstraction levels, we are able to quantify the causality between the activations and semantics occurring in the output image. Such a quantification identifies the human-understandable variation factors learned by GANs to compose scenes. The qualitative and quantitative results further suggest that the generative representations learned by the GANs with layer-wise latent codes are specialized to synthesize different hierarchical semantics: the early layers tend to determine the spatial layout and configuration, the middle layers control the categorical objects, and the later layers finally render the scene attributes as well as color scheme. Identifying such a set of manipulatable latent variation factors facilitates semantic scene manipulation.



### Video Person Re-ID: Fantastic Techniques and Where to Find Them
- **Arxiv ID**: http://arxiv.org/abs/1912.05295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05295v1)
- **Published**: 2019-11-21 03:52:41+00:00
- **Updated**: 2019-11-21 03:52:41+00:00
- **Authors**: Priyank Pathak, Amir Erfan Eshratifar, Michael Gormish
- **Comment**: 2 Page (Student Abstract) accepted in AAAI-20
- **Journal**: None
- **Summary**: The ability to identify the same person from multiple camera views without the explicit use of facial recognition is receiving commercial and academic interest. The current status-quo solutions are based on attention neural models. In this paper, we propose Attention and CL loss, which is a hybrid of center and Online Soft Mining (OSM) loss added to the attention loss on top of a temporal attention-based neural network. The proposed loss function applied with bag-of-tricks for training surpasses the state of the art on the common person Re-ID datasets, MARS and PRID 2011. Our source code is publicly available on github.



### NaMemo: Enhancing Lecturers' Interpersonal Competence of Remembering Students' Names
- **Arxiv ID**: http://arxiv.org/abs/1911.09279v4
- **DOI**: 10.1145/3393914.3395860
- **Categories**: **cs.HC**, cs.CV, H.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1911.09279v4)
- **Published**: 2019-11-21 04:13:55+00:00
- **Updated**: 2020-04-19 06:45:07+00:00
- **Authors**: Guang Jiang, Mengzhen Shi, Ying Su, Pengcheng An, Brian Y. Lim, Yunlong Wang
- **Comment**: DIS '20 Companion
- **Journal**: None
- **Summary**: Addressing students by their names helps a teacher to start building rapport with students and thus facilitates their classroom participation. However, this basic yet effective skill has become rather challenging for university lecturers, who have to handle large-sized (sometimes exceeding 100) groups in their daily teaching. To enhance lecturers' competence in delivering interpersonal interaction, we developed NaMemo, a real-time name-indicating system based on a dedicated face-recognition pipeline. This paper presents the system design, the pilot feasibility test, and our plan for the following study, which aims to evaluate NaMemo's impacts on learning and teaching, as well as to probe design implications including privacy considerations.



### Band-limited Training and Inference for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09287v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09287v1)
- **Published**: 2019-11-21 04:43:02+00:00
- **Updated**: 2019-11-21 04:43:02+00:00
- **Authors**: Adam Dziedzic, John Paparrizos, Sanjay Krishnan, Aaron Elmore, Michael Franklin
- **Comment**: Published at International Conference on Machine Learning (ICML)
- **Journal**: None
- **Summary**: The convolutional layers are core building blocks of neural network architectures. In general, a convolutional filter applies to the entire frequency spectrum of the input data. We explore artificially constraining the frequency spectra of these filters and data, called band-limiting, during training. The frequency domain constraints apply to both the feed-forward and back-propagation steps. Experimentally, we observe that Convolutional Neural Networks (CNNs) are resilient to this compression scheme and results suggest that CNNs learn to leverage lower-frequency components. In particular, we found: (1) band-limited training can effectively control the resource usage (GPU and memory); (2) models trained with band-limited layers retain high prediction accuracy; and (3) requires no modification to existing training algorithms or neural network architectures to use unlike other compression schemes.



### Controversial stimuli: pitting neural networks against each other as models of human recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.09288v2
- **DOI**: 10.1073/pnas.1912334117
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1911.09288v2)
- **Published**: 2019-11-21 04:55:41+00:00
- **Updated**: 2020-07-15 04:47:43+00:00
- **Authors**: Tal Golan, Prashant C. Raju, Nikolaus Kriegeskorte
- **Comment**: None
- **Journal**: Proceedings of the National Academy of Sciences. Nov 2020,
  201912334
- **Summary**: Distinct scientific theories can make similar predictions. To adjudicate between theories, we must design experiments for which the theories make distinct predictions. Here we consider the problem of comparing deep neural networks as models of human visual recognition. To efficiently compare models' ability to predict human responses, we synthesize controversial stimuli: images for which different models produce distinct responses. We applied this approach to two visual recognition tasks, handwritten digits (MNIST) and objects in small natural images (CIFAR-10). For each task, we synthesized controversial stimuli to maximize the disagreement among models which employed different architectures and recognition algorithms. Human subjects viewed hundreds of these stimuli, as well as natural examples, and judged the probability of presence of each digit/object category in each image. We quantified how accurately each model predicted the human judgments. The best performing models were a generative Analysis-by-Synthesis model (based on variational autoencoders) for MNIST and a hybrid discriminative-generative Joint Energy Model for CIFAR-10. These DNNs, which model the distribution of images, performed better than purely discriminative DNNs, which learn only to map images to labels. None of the candidate models fully explained the human responses. Controversial stimuli generalize the concept of adversarial examples, obviating the need to assume a ground-truth model. Unlike natural images, controversial stimuli are not constrained to the stimulus distribution models are trained on, thus providing severe out-of-distribution tests that reveal the models' inductive biases. Controversial stimuli therefore provide powerful probes of discrepancies between models and human perception.



### Large-scale Multi-view Subspace Clustering in Linear Time
- **Arxiv ID**: http://arxiv.org/abs/1911.09290v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09290v1)
- **Published**: 2019-11-21 05:10:29+00:00
- **Updated**: 2019-11-21 05:10:29+00:00
- **Authors**: Zhao Kang, Wangtao Zhou, Zhitong Zhao, Junming Shao, Meng Han, Zenglin Xu
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: A plethora of multi-view subspace clustering (MVSC) methods have been proposed over the past few years. Researchers manage to boost clustering accuracy from different points of view. However, many state-of-the-art MVSC algorithms, typically have a quadratic or even cubic complexity, are inefficient and inherently difficult to apply at large scales. In the era of big data, the computational issue becomes critical. To fill this gap, we propose a large-scale MVSC (LMVSC) algorithm with linear order complexity. Inspired by the idea of anchor graph, we first learn a smaller graph for each view. Then, a novel approach is designed to integrate those graphs so that we can implement spectral clustering on a smaller graph. Interestingly, it turns out that our model also applies to single-view scenario. Extensive experiments on various large-scale benchmark data sets validate the effectiveness and efficiency of our approach with respect to state-of-the-art clustering methods.



### xBD: A Dataset for Assessing Building Damage from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1911.09296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09296v1)
- **Published**: 2019-11-21 05:30:13+00:00
- **Updated**: 2019-11-21 05:30:13+00:00
- **Authors**: Ritwik Gupta, Richard Hosfelt, Sandra Sajeev, Nirav Patel, Bryce Goodman, Jigar Doshi, Eric Heim, Howie Choset, Matthew Gaston
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: We present xBD, a new, large-scale dataset for the advancement of change detection and building damage assessment for humanitarian assistance and disaster recovery research. Natural disaster response requires an accurate understanding of damaged buildings in an affected region. Current response strategies require in-person damage assessments within 24-48 hours of a disaster. Massive potential exists for using aerial imagery combined with computer vision algorithms to assess damage and reduce the potential danger to human life. In collaboration with multiple disaster response agencies, xBD provides pre- and post-event satellite imagery across a variety of disaster events with building polygons, ordinal labels of damage level, and corresponding satellite metadata. Furthermore, the dataset contains bounding boxes and labels for environmental factors such as fire, water, and smoke. xBD is the largest building damage assessment dataset to date, containing 850,736 building annotations across 45,362 km\textsuperscript{2} of imagery.



### Robust Conditional GAN from Uncertainty-Aware Pairwise Comparisons
- **Arxiv ID**: http://arxiv.org/abs/1911.09298v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09298v2)
- **Published**: 2019-11-21 05:37:46+00:00
- **Updated**: 2020-03-05 00:08:23+00:00
- **Authors**: Ligong Han, Ruijiang Gao, Mun Kim, Xin Tao, Bo Liu, Dimitris Metaxas
- **Comment**: Accepted for spotlight at AAAI-20
- **Journal**: None
- **Summary**: Conditional generative adversarial networks have shown exceptional generation performance over the past few years. However, they require large numbers of annotations. To address this problem, we propose a novel generative adversarial network utilizing weak supervision in the form of pairwise comparisons (PC-GAN) for image attribute editing. In the light of Bayesian uncertainty estimation and noise-tolerant adversarial training, PC-GAN can estimate attribute rating efficiently and demonstrate robust performance in noise resistance. Through extensive experiments, we show both qualitatively and quantitatively that PC-GAN performs comparably with fully-supervised methods and outperforms unsupervised baselines.



### Furnishing Your Room by What You See: An End-to-End Furniture Set Retrieval Framework with Rich Annotated Benchmark Dataset
- **Arxiv ID**: http://arxiv.org/abs/1911.09299v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09299v2)
- **Published**: 2019-11-21 05:42:19+00:00
- **Updated**: 2020-01-31 10:50:06+00:00
- **Authors**: Bingyuan Liu, Jiantao Zhang, Xiaoting Zhang, Wei Zhang, Chuanhui Yu, Yuan Zhou
- **Comment**: project website :
  https://www.kujiale.com/festatic/furnitureSetRetrieval
- **Journal**: None
- **Summary**: Understanding interior scenes has attracted enormous interest in computer vision community. However, few works focus on the understanding of furniture within the scenes and a large-scale dataset is also lacked to advance the field. In this paper, we first fill the gap by presenting DeepFurniture, a richly annotated large indoor scene dataset, including 24k indoor images, 170k furniture instances and 20k unique furniture identities. On the dataset, we introduce a new benchmark, named furniture set retrieval. Given an indoor photo as input, the task requires to detect all the furniture instances and search a matched set of furniture identities. To address this challenging task, we propose a feature and context embedding based framework. It contains 3 major contributions: (1) An improved Mask-RCNN model with an additional mask-based classifier is introduced for better utilizing the mask information to relieve the occlusion problems in furniture detection context. (2) A multi-task style Siamese network is proposed to train the feature embedding model for retrieval, which is composed of a classification subnet supervised by self-clustered pseudo attributes and a verification subnet to estimate whether the input pair is matched. (3) In order to model the relationship of the furniture entities in an interior design, a context embedding model is employed to re-rank the retrieval results. Extensive experiments demonstrate the effectiveness of each module and the overall system.



### Image Aesthetics Assessment using Multi Channel Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09301v1)
- **Published**: 2019-11-21 05:57:44+00:00
- **Updated**: 2019-11-21 05:57:44+00:00
- **Authors**: Nishi Doshi, Gitam Shikhenawis, Suman K Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: Image Aesthetics Assessment is one of the emerging domains in research. The domain deals with classification of images into categories depending on the basis of how pleasant they are for the users to watch. In this article, the focus is on categorizing the images in high quality and low quality image. Deep convolutional neural networks are used to classify the images. Instead of using just the raw image as input, different crops and saliency maps of the images are also used, as input to the proposed multi channel CNN architecture. The experiments reported on widely used AVA database show improvement in the aesthetic assessment performance over existing approaches.



### Efficient Querying from Weighted Binary Codes
- **Arxiv ID**: http://arxiv.org/abs/1912.05006v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1912.05006v2)
- **Published**: 2019-11-21 06:48:29+00:00
- **Updated**: 2020-06-11 02:41:13+00:00
- **Authors**: Zhenyu Weng, Yuesheng Zhu
- **Comment**: 13 pages, accepted by AAAI2020
- **Journal**: None
- **Summary**: Binary codes are widely used to represent the data due to their small storage and efficient computation. However, there exists an ambiguity problem that lots of binary codes share the same Hamming distance to a query. To alleviate the ambiguity problem, weighted binary codes assign different weights to each bit of binary codes and compare the binary codes by the weighted Hamming distance. Till now, performing the querying from the weighted binary codes efficiently is still an open issue. In this paper, we propose a new method to rank the weighted binary codes and return the nearest weighted binary codes of the query efficiently. In our method, based on the multi-index hash tables, two algorithms, the table bucket finding algorithm and the table merging algorithm, are proposed to select the nearest weighted binary codes of the query in a non-exhaustive and accurate way. The proposed algorithms are justified by proving their theoretic properties. The experiments on three large-scale datasets validate both the search efficiency and the search accuracy of our method. Especially for the number of weighted binary codes up to one billion, our method shows a great improvement of more than 1000 times faster than the linear scan.



### Relation Network for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1911.09318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09318v2)
- **Published**: 2019-11-21 07:21:11+00:00
- **Updated**: 2019-11-25 06:37:10+00:00
- **Authors**: Hyunjong Park, Bumsub Ham
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Person re-identification (reID) aims at retrieving an image of the person of interest from a set of images typically captured by multiple cameras. Recent reID methods have shown that exploiting local features describing body parts, together with a global feature of a person image itself, gives robust feature representations, even in the case of missing body parts. However, using the individual part-level features directly, without considering relations between body parts, confuses differentiating identities of different persons having similar attributes in corresponding parts. To address this issue, we propose a new relation network for person reID that considers relations between individual body parts and the rest of them. Our model makes a single part-level feature incorporate partial information of other body parts as well, supporting it to be more discriminative. We also introduce a global contrastive pooling (GCP) method to obtain a global feature of a person image. We propose to use contrastive features for GCP to complement conventional max and averaging pooling techniques. We show that our model outperforms the state of the art on the Market1501, DukeMTMC-reID and CUHK03 datasets, demonstrating the effectiveness of our approach on discriminative person representations.



### Data Proxy Generation for Fast and Efficient Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1911.09322v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09322v1)
- **Published**: 2019-11-21 07:39:57+00:00
- **Updated**: 2019-11-21 07:39:57+00:00
- **Authors**: Minje Park
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the recent advances on Neural Architecture Search (NAS), it gains popularity in designing best networks for specific tasks. Although it shows promising results on many benchmarks and competitions, NAS still suffers from its demanding computation cost for searching high dimensional architectural design space, and this problem becomes even worse when we want to use a large-scale dataset. If we can make a reliable data proxy for NAS, the efficiency of NAS approaches increase accordingly. Our basic observation for making a data proxy is that each example in a specific dataset has a different impact on NAS process and most of examples are redundant from a relative accuracy ranking perspective, which we should preserve when making a data proxy. We propose a systematic approach to measure the importance of each example from this relative accuracy ranking point of view, and make a reliable data proxy based on the statistics of training and testing examples. Our experiment shows that we can preserve the almost same relative accuracy ranking between all possible network configurations even with 10-20$\times$ smaller data proxy.



### Simultaneous Implementation Features Extraction and Recognition Using C3D Network for WiFi-based Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.09325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09325v1)
- **Published**: 2019-11-21 07:45:46+00:00
- **Updated**: 2019-11-21 07:45:46+00:00
- **Authors**: Liu Yafeng, Chen Tian, Liu Zhongyu, Zhang Lei, Hu Yanjun, Ding Enjie
- **Comment**: 11 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: Human actions recognition has attracted more and more people's attention. Many technology have been developed to express human action's features, such as image, skeleton-based, and channel state information(CSI). Among them, on account of CSI's easy to be equipped and undemanding for light, and it has gained more and more attention in some special scene. However, the relationship between CSI signal and human actions is very complex, and some preliminary work must be done to make CSI features easy to understand for computer. Nowadays, many work departed CSI-based features' action dealing into two parts. One part is for features extraction and dimension reduce, and the other part is for time series problems. Some of them even omitted one of the two part work. Therefore, the accuracies of current recognition systems are far from satisfactory. In this paper, we propose a new deep learning based approach, i.e. C3D network and C3D network with attention mechanism, for human actions recognition using CSI signals. This kind of network can make feature extraction from spatial convolution and temporal convolution simultaneously, and through this network the two part of CSI-based human actions recognition mentioned above can be realized at the same time. The entire algorithm structure is simplified. The experimental results show that our proposed C3D network is able to achieve the best recognition performance for all activities when compared with some benchmark approaches.



### LCD: Learned Cross-Domain Descriptors for 2D-3D Matching
- **Arxiv ID**: http://arxiv.org/abs/1911.09326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09326v1)
- **Published**: 2019-11-21 07:56:13+00:00
- **Updated**: 2019-11-21 07:56:13+00:00
- **Authors**: Quang-Hieu Pham, Mikaela Angelina Uy, Binh-Son Hua, Duc Thanh Nguyen, Gemma Roig, Sai-Kit Yeung
- **Comment**: Accepted to AAAI 2020 (Oral)
- **Journal**: None
- **Summary**: In this work, we present a novel method to learn a local cross-domain descriptor for 2D image and 3D point cloud matching. Our proposed method is a dual auto-encoder neural network that maps 2D and 3D input into a shared latent space representation. We show that such local cross-domain descriptors in the shared embedding are more discriminative than those obtained from individual training in 2D and 3D domains. To facilitate the training process, we built a new dataset by collecting $\approx 1.4$ millions of 2D-3D correspondences with various lighting conditions and settings from publicly available RGB-D scenes. Our descriptor is evaluated in three main experiments: 2D-3D matching, cross-domain retrieval, and sparse-to-dense depth estimation. Experimental results confirm the robustness of our approach as well as its competitive performance not only in solving cross-domain tasks but also in being able to generalize to solve sole 2D and 3D tasks. Our dataset and code are released publicly at \url{https://hkust-vgd.github.io/lcd}.



### Heart Segmentation From MRI Scans Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1911.09332v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09332v1)
- **Published**: 2019-11-21 08:20:48+00:00
- **Updated**: 2019-11-21 08:20:48+00:00
- **Authors**: Shakeel Muhammad Ibrahim, Muhammad Sohail Ibrahim, Muhammad Usman, Imran Naseem, Muhammad Moinuddin
- **Comment**: Accepted for oral presentation at 13th International Conference -
  Mathematics, Actuarial, Computer Science & Statistics (MACS 13) at IoBM,
  Karachi, Pakistan
- **Journal**: None
- **Summary**: Heart is one of the vital organs of human body. A minor dysfunction of heart even for a short time interval can be fatal, therefore, efficient monitoring of its physiological state is essential for the patients with cardiovascular diseases. In the recent past, various computer assisted medical imaging systems have been proposed for the segmentation of the organ of interest. However, for the segmentation of heart using MRI, only few methods have been proposed each with its own merits and demerits. For further advancement in this area of research, we analyze automated heart segmentation methods for magnetic resonance images. The analysis are based on deep learning methods that processes a full MR scan in a slice by slice fashion to predict desired mask for heart region. We design two encoder decoder type fully convolutional neural network models



### Voice-Face Cross-modal Matching and Retrieval: A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1911.09338v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09338v2)
- **Published**: 2019-11-21 08:34:34+00:00
- **Updated**: 2019-12-30 12:36:50+00:00
- **Authors**: Chuyuan Xiong, Deyuan Zhang, Tao Liu, Xiaoyong Du
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modal associations between voice and face from a person can be learnt algorithmically, which can benefit a lot of applications. The problem can be defined as voice-face matching and retrieval tasks. Much research attention has been paid on these tasks recently. However, this research is still in the early stage. Test schemes based on random tuple mining tend to have low test confidence. Generalization ability of models can not be evaluated by small scale datasets. Performance metrics on various tasks are scarce. A benchmark for this problem needs to be established. In this paper, first, a framework based on comprehensive studies is proposed for voice-face matching and retrieval. It achieves state-of-the-art performance with various performance metrics on different tasks and with high test confidence on large scale datasets, which can be taken as a baseline for the follow-up research. In this framework, a voice anchored L2-Norm constrained metric space is proposed, and cross-modal embeddings are learned with CNN-based networks and triplet loss in the metric space. The embedding learning process can be more effective and efficient with this strategy. Different network structures of the framework and the cross language transfer abilities of the model are also analyzed. Second, a voice-face dataset (with 1.15M face data and 0.29M audio data) from Chinese speakers is constructed, and a convenient and quality controllable dataset collection tool is developed. The dataset and source code of the paper will be published together with this paper.



### Empirical Autopsy of Deep Video Captioning Frameworks
- **Arxiv ID**: http://arxiv.org/abs/1911.09345v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.09345v1)
- **Published**: 2019-11-21 08:47:36+00:00
- **Updated**: 2019-11-21 08:47:36+00:00
- **Authors**: Nayyer Aafaq, Naveed Akhtar, Wei Liu, Ajmal Mian
- **Comment**: 09 pages, 05 figures
- **Journal**: None
- **Summary**: Contemporary deep learning based video captioning follows encoder-decoder framework. In encoder, visual features are extracted with 2D/3D Convolutional Neural Networks (CNNs) and a transformed version of those features is passed to the decoder. The decoder uses word embeddings and a language model to map visual features to natural language captions. Due to its composite nature, the encoder-decoder pipeline provides the freedom of multiple choices for each of its components, e.g the choices of CNNs models, feature transformations, word embeddings, and language models etc. Component selection can have drastic effects on the overall video captioning performance. However, current literature is void of any systematic investigation in this regard. This article fills this gap by providing the first thorough empirical analysis of the role that each major component plays in a contemporary video captioning pipeline. We perform extensive experiments by varying the constituent components of the video captioning framework, and quantify the performance gains that are possible by mere component selection. We use the popular MSVD dataset as the test-bed, and demonstrate that substantial performance gains are possible by careful selection of the constituent components without major changes to the pipeline itself. These results are expected to provide guiding principles for future research in the fast growing direction of video captioning.



### An End-to-End Audio Classification System based on Raw Waveforms and Mix-Training Strategy
- **Arxiv ID**: http://arxiv.org/abs/1911.09349v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.09349v1)
- **Published**: 2019-11-21 08:54:48+00:00
- **Updated**: 2019-11-21 08:54:48+00:00
- **Authors**: Jiaxu Chen, Jing Hao, Kai Chen, Di Xie, Shicai Yang, Shiliang Pu
- **Comment**: InterSpeech 2019
- **Journal**: None
- **Summary**: Audio classification can distinguish different kinds of sounds, which is helpful for intelligent applications in daily life. However, it remains a challenging task since the sound events in an audio clip is probably multiple, even overlapping. This paper introduces an end-to-end audio classification system based on raw waveforms and mix-training strategy. Compared to human-designed features which have been widely used in existing research, raw waveforms contain more complete information and are more appropriate for multi-label classification. Taking raw waveforms as input, our network consists of two variants of ResNet structure which can learn a discriminative representation. To explore the information in intermediate layers, a multi-level prediction with attention structure is applied in our model. Furthermore, we design a mix-training strategy to break the performance limitation caused by the amount of training data. Experiments show that the mean average precision of the proposed audio classification system on Audio Set dataset is 37.2%. Without using extra training data, our system exceeds the state-of-the-art multi-level attention model.



### Gliding vertex on the horizontal bounding box for multi-oriented object detection
- **Arxiv ID**: http://arxiv.org/abs/1911.09358v2
- **DOI**: 10.1109/TPAMI.2020.2974745
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09358v2)
- **Published**: 2019-11-21 09:28:14+00:00
- **Updated**: 2020-04-08 07:09:37+00:00
- **Authors**: Yongchao Xu, Mingtao Fu, Qimeng Wang, Yukang Wang, Kai Chen, Gui-Song Xia, Xiang Bai
- **Comment**: Accepted by TPAMI 2020. The experiments of pedestrian detection are
  updated as the benchmark has been changed
- **Journal**: None
- **Summary**: Object detection has recently experienced substantial progress. Yet, the widely adopted horizontal bounding box representation is not appropriate for ubiquitous oriented objects such as objects in aerial images and scene texts. In this paper, we propose a simple yet effective framework to detect multi-oriented objects. Instead of directly regressing the four vertices, we glide the vertex of the horizontal bounding box on each corresponding side to accurately describe a multi-oriented object. Specifically, We regress four length ratios characterizing the relative gliding offset on each corresponding side. This may facilitate the offset learning and avoid the confusion issue of sequential label points for oriented objects. To further remedy the confusion issue for nearly horizontal objects, we also introduce an obliquity factor based on area ratio between the object and its horizontal bounding box, guiding the selection of horizontal or oriented detection for each object. We add these five extra target variables to the regression head of faster R-CNN, which requires ignorable extra computation time. Extensive experimental results demonstrate that without bells and whistles, the proposed method achieves superior performances on multiple multi-oriented object detection benchmarks including object detection in aerial images, scene text detection, pedestrian detection in fisheye images.



### ChartNet: Visual Reasoning over Statistical Charts using MAC-Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09375v1
- **DOI**: 10.1109/IJCNN.2019.8852427
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.09375v1)
- **Published**: 2019-11-21 10:03:25+00:00
- **Updated**: 2019-11-21 10:03:25+00:00
- **Authors**: Monika Sharma, Shikha Gupta, Arindam Chowdhury, Lovekesh Vig
- **Comment**: None
- **Journal**: International Joint Conference on Neural Networks (IJCNN) 2019
- **Summary**: Despite the improvements in perception accuracies brought about via deep learning, developing systems combining accurate visual perception with the ability to reason over the visual percepts remains extremely challenging. A particular application area of interest from an accessibility perspective is that of reasoning over statistical charts such as bar and pie charts. To this end, we formulate the problem of reasoning over statistical charts as a classification task using MAC-Networks to give answers from a predefined vocabulary of generic answers. Additionally, we enhance the capabilities of MAC-Networks to give chart-specific answers to open-ended questions by replacing the classification layer by a regression layer to localize the textual answers present over the images. We call our network ChartNet, and demonstrate its efficacy on predicting both in vocabulary and out of vocabulary answers. To test our methods, we generated our own dataset of statistical chart images and corresponding question answer pairs. Results show that ChartNet consistently outperform other state-of-the-art methods on reasoning over these questions and may be a viable candidate for applications containing images of statistical charts.



### Classification-driven Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1911.09389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09389v1)
- **Published**: 2019-11-21 10:25:54+00:00
- **Updated**: 2019-11-21 10:25:54+00:00
- **Authors**: Yanting Pei, Yaping Huang, Xingyuan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing dehazing algorithms often use hand-crafted features or Convolutional Neural Networks (CNN)-based methods to generate clear images using pixel-level Mean Square Error (MSE) loss. The generated images generally have better visual appeal, but not always have better performance for high-level vision tasks, e.g. image classification. In this paper, we investigate a new point of view in addressing this problem. Instead of focusing only on achieving good quantitative performance on pixel-based metrics such as Peak Signal to Noise Ratio (PSNR), we also ensure that the dehazed image itself does not degrade the performance of the high-level vision tasks such as image classification. To this end, we present an unified CNN architecture that includes three parts: a dehazing sub-network (DNet), a classification-driven Conditional Generative Adversarial Networks sub-network (CCGAN) and a classification sub-network (CNet) related to image classification, which has better performance both on visual appeal and image classification. We conduct comprehensive experiments on two challenging benchmark datasets for fine-grained and object classification: CUB-200-2011 and Caltech-256. Experimental results demonstrate that the proposed method outperforms many recent state-of-the-art single image dehazing methods in terms of image dehazing metrics and classification accuracy.



### Segmenting Medical MRI via Recurrent Decoding Cell
- **Arxiv ID**: http://arxiv.org/abs/1911.09401v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09401v1)
- **Published**: 2019-11-21 10:46:42+00:00
- **Updated**: 2019-11-21 10:46:42+00:00
- **Authors**: Ying Wen, Kai Xie, Lianghua He
- **Comment**: 8 pages, 7 figures, AAAI-20
- **Journal**: None
- **Summary**: The encoder-decoder networks are commonly used in medical image segmentation due to their remarkable performance in hierarchical feature fusion. However, the expanding path for feature decoding and spatial recovery does not consider the long-term dependency when fusing feature maps from different layers, and the universal encoder-decoder network does not make full use of the multi-modality information to improve the network robustness especially for segmenting medical MRI. In this paper, we propose a novel feature fusion unit called Recurrent Decoding Cell (RDC) which leverages convolutional RNNs to memorize the long-term context information from the previous layers in the decoding phase. An encoder-decoder network, named Convolutional Recurrent Decoding Network (CRDN), is also proposed based on RDC for segmenting multi-modality medical MRI. CRDN adopts CNN backbone to encode image features and decode them hierarchically through a chain of RDCs to obtain the final high-resolution score map. The evaluation experiments on BrainWeb, MRBrainS and HVSMR datasets demonstrate that the introduction of RDC effectively improves the segmentation accuracy as well as reduces the model size, and the proposed CRDN owns its robustness to image noise and intensity non-uniformity in medical MRI.



### MSD: Multi-Self-Distillation Learning via Multi-classifiers within Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09418v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09418v3)
- **Published**: 2019-11-21 11:35:50+00:00
- **Updated**: 2019-12-02 12:04:32+00:00
- **Authors**: Yunteng Luan, Hanyu Zhao, Zhi Yang, Yafei Dai
- **Comment**: None
- **Journal**: None
- **Summary**: As the development of neural networks, more and more deep neural networks are adopted in various tasks, such as image classification. However, as the huge computational overhead, these networks could not be applied on mobile devices or other low latency scenes. To address this dilemma, multi-classifier convolutional network is proposed to allow faster inference via early classifiers with the corresponding classifiers. These networks utilize sophisticated designing to increase the early classifier accuracy. However, naively training the multi-classifier network could hurt the performance (accuracy) of deep neural networks as early classifiers throughout interfere with the feature generation process.   In this paper, we propose a general training framework named multi-self-distillation learning (MSD), which mining knowledge of different classifiers within the same network and increase every classifier accuracy. Our approach can be applied not only to multi-classifier networks, but also modern CNNs (e.g., ResNet Series) augmented with additional side branch classifiers. We use sampling-based branch augmentation technique to transform a single-classifier network into a multi-classifier network. This reduces the gap of capacity between different classifiers, and improves the effectiveness of applying MSD. Our experiments show that MSD improves the accuracy of various networks: enhancing the accuracy of every classifier significantly for existing multi-classifier network (MSDNet), improving vanilla single-classifier networks with internal classifiers with high accuracy, while also improving the final accuracy.



### Single Image Super Resolution based on a Modified U-net with Mixed Gradient Loss
- **Arxiv ID**: http://arxiv.org/abs/1911.09428v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.09428v1)
- **Published**: 2019-11-21 12:02:38+00:00
- **Updated**: 2019-11-21 12:02:38+00:00
- **Authors**: Zhengyang Lu, Ying Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution (SISR) is the task of inferring a high-resolution image from a single low-resolution image. Recent research on super-resolution has achieved great progress due to the development of deep convolutional neural networks in the field of computer vision. Existing super-resolution reconstruction methods have high performances in the criterion of Mean Square Error (MSE) but most methods fail to reconstruct an image with shape edges. To solve this problem, the mixed gradient error, which is composed by MSE and a weighted mean gradient error, is proposed in this work and applied to a modified U-net network as the loss function. The modified U-net removes all batch normalization layers and one of the convolution layers in each block. The operation reduces the number of parameters, and therefore accelerates the reconstruction. Compared with the existing image super-resolution algorithms, the proposed reconstruction method has better performance and time consumption. The experiments demonstrate that modified U-net network architecture with mixed gradient loss yields high-level results on three image datasets: SET14, BSD300, ICDAR2003. Code is available online.



### TEINet: Towards an Efficient Architecture for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.09435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09435v1)
- **Published**: 2019-11-21 12:16:32+00:00
- **Updated**: 2019-11-21 12:16:32+00:00
- **Authors**: Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Tong Lu
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Efficiency is an important issue in designing video architectures for action recognition. 3D CNNs have witnessed remarkable progress in action recognition from videos. However, compared with their 2D counterparts, 3D convolutions often introduce a large amount of parameters and cause high computational cost. To relieve this problem, we propose an efficient temporal module, termed as Temporal Enhancement-and-Interaction (TEI Module), which could be plugged into the existing 2D CNNs (denoted by TEINet). The TEI module presents a different paradigm to learn temporal features by decoupling the modeling of channel correlation and temporal interaction. First, it contains a Motion Enhanced Module (MEM) which is to enhance the motion-related features while suppress irrelevant information (e.g., background). Then, it introduces a Temporal Interaction Module (TIM) which supplements the temporal contextual information in a channel-wise manner. This two-stage modeling scheme is not only able to capture temporal structure flexibly and effectively, but also efficient for model inference. We conduct extensive experiments to verify the effectiveness of TEINet on several benchmarks (e.g., Something-Something V1&V2, Kinetics, UCF101 and HMDB51). Our proposed TEINet can achieve a good recognition accuracy on these datasets but still preserve a high efficiency.



### Heuristic Black-box Adversarial Attacks on Video Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/1911.09449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09449v1)
- **Published**: 2019-11-21 13:04:33+00:00
- **Updated**: 2019-11-21 13:04:33+00:00
- **Authors**: Zhipeng Wei, Jingjing Chen, Xingxing Wei, Linxi Jiang, Tat-Seng Chua, Fengfeng Zhou, Yu-Gang Jiang
- **Comment**: AAAI-2020 Oral
- **Journal**: None
- **Summary**: We study the problem of attacking video recognition models in the black-box setting, where the model information is unknown and the adversary can only make queries to detect the predicted top-1 class and its probability. Compared with the black-box attack on images, attacking videos is more challenging as the computation cost for searching the adversarial perturbations on a video is much higher due to its high dimensionality. To overcome this challenge, we propose a heuristic black-box attack model that generates adversarial perturbations only on the selected frames and regions. More specifically, a heuristic-based algorithm is proposed to measure the importance of each frame in the video towards generating the adversarial examples. Based on the frames' importance, the proposed algorithm heuristically searches a subset of frames where the generated adversarial example has strong adversarial attack ability while keeps the perturbations lower than the given bound. Besides, to further boost the attack efficiency, we propose to generate the perturbations only on the salient regions of the selected frames. In this way, the generated perturbations are sparse in both temporal and spatial domains. Experimental results of attacking two mainstream video recognition methods on the UCF-101 dataset and the HMDB-51 dataset demonstrate that the proposed heuristic black-box adversarial attack method can significantly reduce the computation cost and lead to more than 28\% reduction in query numbers for the untargeted attack on both datasets.



### Few Shot Network Compression via Cross Distillation
- **Arxiv ID**: http://arxiv.org/abs/1911.09450v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09450v2)
- **Published**: 2019-11-21 13:07:52+00:00
- **Updated**: 2020-05-02 10:20:20+00:00
- **Authors**: Haoli Bai, Jiaxiang Wu, Irwin King, Michael Lyu
- **Comment**: AAAI 2020
- **Journal**: The Thirty-Fourth AAAI Conference on Artificial Intelligence
  (AAAI), 2020
- **Summary**: Model compression has been widely adopted to obtain light-weighted deep neural networks. Most prevalent methods, however, require fine-tuning with sufficient training data to ensure accuracy, which could be challenged by privacy and security issues. As a compromise between privacy and performance, in this paper we investigate few shot network compression: given few samples per class, how can we effectively compress the network with negligible performance drop? The core challenge of few shot network compression lies in high estimation errors from the original network during inference, since the compressed network can easily over-fits on the few training instances. The estimation errors could propagate and accumulate layer-wisely and finally deteriorate the network output. To address the problem, we propose cross distillation, a novel layer-wise knowledge distillation approach. By interweaving hidden layers of teacher and student network, layer-wisely accumulated estimation errors can be effectively reduced.The proposed method offers a general framework compatible with prevalent network compression techniques such as pruning. Extensive experiments on benchmark datasets demonstrate that cross distillation can significantly improve the student network's accuracy when only a few training instances are available.



### Quantization Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09464v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09464v2)
- **Published**: 2019-11-21 13:44:03+00:00
- **Updated**: 2019-11-28 02:37:31+00:00
- **Authors**: Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, Xiansheng Hua
- **Comment**: 10 pages, CVPR2019
- **Journal**: None
- **Summary**: Although deep neural networks are highly effective, their high computational and memory costs severely challenge their applications on portable devices. As a consequence, low-bit quantization, which converts a full-precision neural network into a low-bitwidth integer version, has been an active and promising research topic. Existing methods formulate the low-bit quantization of networks as an approximation or optimization problem. Approximation-based methods confront the gradient mismatch problem, while optimization-based methods are only suitable for quantizing weights and could introduce high computational cost in the training stage. In this paper, we propose a novel perspective of interpreting and implementing neural network quantization by formulating low-bit quantization as a differentiable non-linear function (termed quantization function). The proposed quantization function can be learned in a lossless and end-to-end manner and works for any weights and activations of neural networks in a simple and uniform way. Extensive experiments on image classification and object detection tasks show that our quantization networks outperform the state-of-the-art methods. We believe that the proposed method will shed new insights on the interpretation of neural network quantization. Our code is available at https://github.com/aliyun/alibabacloud-quantization-networks.



### Deep Representations for Cross-spectral Ocular Biometrics
- **Arxiv ID**: http://arxiv.org/abs/1911.09509v1
- **DOI**: 10.1049/iet-bmt.2019.0116
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09509v1)
- **Published**: 2019-11-21 14:54:46+00:00
- **Updated**: 2019-11-21 14:54:46+00:00
- **Authors**: Luiz A. Zanlorensi, Diego R. Lucio, Alceu S. Britto Jr., Hugo Proena, David Menotti
- **Comment**: This paper is a postprint of a paper submitted to and accepted for
  publication inIET Biometrics and is subject to Institution of Engineering and
  Technology Copyright. The copy of the record is available at the IET Digital
  Library
- **Journal**: None
- **Summary**: One of the major challenges in ocular biometrics is the cross-spectral scenario, i.e., how to match images acquired in different wavelengths (typically visible (VIS) against near-infrared (NIR)). This article designs and extensively evaluates cross-spectral ocular verification methods, for both the closed and open-world settings, using well known deep learning representations based on the iris and periocular regions. Using as inputs the bounding boxes of non-normalized iris/periocular regions, we fine-tune Convolutional Neural Network(CNN) models (based either on VGG16 or ResNet-50 architectures), originally trained for face recognition. Based on the experiments carried out in two publicly available cross-spectral ocular databases, we report results for intra-spectral and cross-spectral scenarios, with the best performance being observed when fusing ResNet-50 deep representations from both the periocular and iris regions. When compared to the state-of-the-art, we observed that the proposed solution consistently reduces the Equal Error Rate(EER) values by 90% / 93% / 96% and 61% / 77% / 83% on the cross-spectral scenario and in the PolyU Bi-spectral and Cross-eye-cross-spectral datasets. Lastly, we evaluate the effect that the "deepness" factor of feature representations has in recognition effectiveness, and - based on a subjective analysis of the most problematic pairwise comparisons - we point out further directions for this field of research.



### Learning Spatial Fusion for Single-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.09516v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09516v2)
- **Published**: 2019-11-21 15:05:28+00:00
- **Updated**: 2019-11-25 01:59:05+00:00
- **Authors**: Songtao Liu, Di Huang, Yunhong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Pyramidal feature representation is the common practice to address the challenge of scale variation in object detection. However, the inconsistency across different feature scales is a primary limitation for the single-shot detectors based on feature pyramid. In this work, we propose a novel and data driven strategy for pyramidal feature fusion, referred to as adaptively spatial feature fusion (ASFF). It learns the way to spatially filter conflictive information to suppress the inconsistency, thus improving the scale-invariance of features, and introduces nearly free inference overhead. With the ASFF strategy and a solid baseline of YOLOv3, we achieve the best speed-accuracy trade-off on the MS COCO dataset, reporting 38.1% AP at 60 FPS, 42.4% AP at 45 FPS and 43.9% AP at 29 FPS. The code is available at https://github.com/ruinmessi/ASFF



### All You Need Is Boundary: Toward Arbitrary-Shaped Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/1911.09550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09550v1)
- **Published**: 2019-11-21 15:45:56+00:00
- **Updated**: 2019-11-21 15:45:56+00:00
- **Authors**: Hao Wang, Pu Lu, Hui Zhang, Mingkun Yang, Xiang Bai, Yongchao Xu, Mengchao He, Yongpan Wang, Wenyu Liu
- **Comment**: Accepted to AAAI2020
- **Journal**: None
- **Summary**: Recently, end-to-end text spotting that aims to detect and recognize text from cluttered images simultaneously has received particularly growing interest in computer vision. Different from the existing approaches that formulate text detection as bounding box extraction or instance segmentation, we localize a set of points on the boundary of each text instance. With the representation of such boundary points, we establish a simple yet effective scheme for end-to-end text spotting, which can read the text of arbitrary shapes. Experiments on three challenging datasets, including ICDAR2015, TotalText and COCO-Text demonstrate that the proposed method consistently surpasses the state-of-the-art in both scene text detection and end-to-end text recognition tasks.



### Knowledge Graph Transfer Network for Few-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.09579v2
- **DOI**: 10.1609/aaai.v34i07.6630
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09579v2)
- **Published**: 2019-11-21 16:16:22+00:00
- **Updated**: 2020-10-15 13:06:09+00:00
- **Authors**: Riquan Chen, Tianshui Chen, Xiaolu Hui, Hefeng Wu, Guanbin Li, Liang Lin
- **Comment**: accepted by AAAI 2020 as oral paper
- **Journal**: None
- **Summary**: Few-shot learning aims to learn novel categories from very few samples given some base categories with sufficient training samples. The main challenge of this task is the novel categories are prone to dominated by color, texture, shape of the object or background context (namely specificity), which are distinct for the given few training samples but not common for the corresponding categories (see Figure 1). Fortunately, we find that transferring information of the correlated based categories can help learn the novel concepts and thus avoid the novel concept being dominated by the specificity. Besides, incorporating semantic correlations among different categories can effectively regularize this information transfer. In this work, we represent the semantic correlations in the form of structured knowledge graph and integrate this graph into deep neural networks to promote few-shot learning by a novel Knowledge Graph Transfer Network (KGTN). Specifically, by initializing each node with the classifier weight of the corresponding category, a propagation mechanism is learned to adaptively propagate node message through the graph to explore node interaction and transfer classifier information of the base categories to those of the novel ones. Extensive experiments on the ImageNet dataset show significant performance improvement compared with current leading competitors. Furthermore, we construct an ImageNet-6K dataset that covers larger scale categories, i.e, 6,000 categories, and experiments on this dataset further demonstrate the effectiveness of our proposed model. Our codes and models are available at https://github.com/MyChocer/KGTN .



### Synthesizing Visual Illusions Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09599v1)
- **Published**: 2019-11-21 16:49:43+00:00
- **Updated**: 2019-11-21 16:49:43+00:00
- **Authors**: Alexander Gomez-Villa, Adrian Martn, Javier Vazquez-Corral, Jess Malo, Marcelo Bertalmo
- **Comment**: None
- **Journal**: None
- **Summary**: Visual illusions are a very useful tool for vision scientists, because they allow them to better probe the limits, thresholds and errors of the visual system. In this work we introduce the first ever framework to generate novel visual illusions with an artificial neural network (ANN). It takes the form of a generative adversarial network, with a generator of visual illusion candidates and two discriminator modules, one for the inducer background and another that decides whether or not the candidate is indeed an illusion. The generality of the model is exemplified by synthesizing illusions of different types, and validated with psychophysical experiments that corroborate that the outputs of our ANN are indeed visual illusions to human observers. Apart from synthesizing new visual illusions, which may help vision researchers, the proposed model has the potential to open new ways to study the similarities and differences between ANN and human visual perception.



### Ocular Recognition Databases and Competitions: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1911.09646v3
- **DOI**: 10.1007/s10462-021-10028-w
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09646v3)
- **Published**: 2019-11-21 18:09:37+00:00
- **Updated**: 2022-02-04 15:45:25+00:00
- **Authors**: Luiz A. Zanlorensi, Rayson Laroca, Eduardo Luz, Alceu S. Britto Jr., Luiz S. Oliveira, David Menotti
- **Comment**: Artificial Intelligence Review, vol. 55, pp. 129-180, 2022
- **Journal**: None
- **Summary**: The use of the iris and periocular region as biometric traits has been extensively investigated, mainly due to the singularity of the iris features and the use of the periocular region when the image resolution is not sufficient to extract iris information. In addition to providing information about an individual's identity, features extracted from these traits can also be explored to obtain other information such as the individual's gender, the influence of drug use, the use of contact lenses, spoofing, among others. This work presents a survey of the databases created for ocular recognition, detailing their protocols and how their images were acquired. We also describe and discuss the most popular ocular recognition competitions (contests), highlighting the submitted algorithms that achieved the best results using only iris trait and also fusing iris and periocular region information. Finally, we describe some relevant works applying deep learning techniques to ocular recognition and point out new challenges and future directions. Considering that there are a large number of ocular databases, and each one is usually designed for a specific problem, we believe this survey can provide a broad overview of the challenges in ocular biometrics.



### AdaFilter: Adaptive Filter Fine-tuning for Deep Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.09659v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09659v2)
- **Published**: 2019-11-21 18:39:01+00:00
- **Updated**: 2019-12-09 00:30:19+00:00
- **Authors**: Yunhui Guo, Yandong Li, Liqiang Wang, Tajana Rosing
- **Comment**: None
- **Journal**: None
- **Summary**: There is an increasing number of pre-trained deep neural network models. However, it is still unclear how to effectively use these models for a new task. Transfer learning, which aims to transfer knowledge from source tasks to a target task, is an effective solution to this problem. Fine-tuning is a popular transfer learning technique for deep neural networks where a few rounds of training are applied to the parameters of a pre-trained model to adapt them to a new task. Despite its popularity, in this paper, we show that fine-tuning suffers from several drawbacks. We propose an adaptive fine-tuning approach, called AdaFilter, which selects only a part of the convolutional filters in the pre-trained model to optimize on a per-example basis. We use a recurrent gated network to selectively fine-tune convolutional filters based on the activations of the previous layer. We experiment with 7 public image classification datasets and the results show that AdaFilter can reduce the average classification error of the standard fine-tuning by 2.54%.



### Adversarial Examples Improve Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.09665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09665v2)
- **Published**: 2019-11-21 18:53:23+00:00
- **Updated**: 2020-04-14 17:20:25+00:00
- **Authors**: Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan Yuille, Quoc V. Le
- **Comment**: CVPR 2020, models are available at
  https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet
- **Journal**: None
- **Summary**: Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples.   We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve significant improvements on ImageNet (+0.7%), ImageNet-C (+6.5%), ImageNet-A (+7.0%), Stylized-ImageNet (+4.8%). With an enhanced EfficientNet-B8, our method achieves the state-of-the-art 85.5% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images (~3000X more than ImageNet) and ~9.4X more parameters. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.



### Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller
- **Arxiv ID**: http://arxiv.org/abs/1911.09676v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09676v1)
- **Published**: 2019-11-21 18:59:29+00:00
- **Updated**: 2019-11-21 18:59:29+00:00
- **Authors**: Pratyusha Sharma, Deepak Pathak, Abhinav Gupta
- **Comment**: Accepted at NeurIPS 2019. Videos at
  https://pathak22.github.io/hierarchical-imitation/
- **Journal**: None
- **Summary**: We study a generalized setup for learning from demonstration to build an agent that can manipulate novel objects in unseen scenarios by looking at only a single video of human demonstration from a third-person perspective. To accomplish this goal, our agent should not only learn to understand the intent of the demonstrated third-person video in its context but also perform the intended task in its environment configuration. Our central insight is to enforce this structure explicitly during learning by decoupling what to achieve (intended task) from how to perform it (controller). We propose a hierarchical setup where a high-level module learns to generate a series of first-person sub-goals conditioned on the third-person video demonstration, and a low-level controller predicts the actions to achieve those sub-goals. Our agent acts from raw image observations without any access to the full state information. We show results on a real robotic platform using Baxter for the manipulation tasks of pouring and placing objects in a box. Project video and code are at https://pathak22.github.io/hierarchical-imitation/



### RefinedMPL: Refined Monocular PseudoLiDAR for 3D Object Detection in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1911.09712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09712v1)
- **Published**: 2019-11-21 19:21:32+00:00
- **Updated**: 2019-11-21 19:21:32+00:00
- **Authors**: Jean Marie Uwabeza Vianney, Shubhra Aich, Bingbing Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we strive for solving the ambiguities arisen by the astoundingly high density of raw PseudoLiDAR for monocular 3D object detection for autonomous driving. Without much computational overhead, we propose a supervised and an unsupervised sparsification scheme of PseudoLiDAR prior to 3D detection. Both the strategies assist the standard 3D detector gain better performance over the raw PseudoLiDAR baseline using only ~5% of its points on the KITTI object detection benchmark, thus making our monocular framework and LiDAR-based counterparts computationally equivalent (Figure 1). Moreover, our architecture agnostic refinements provide state-of-the-art results on KITTI3D test set for "Car" and "Pedestrian" categories with 54% relative improvement for "Pedestrian". Finally, exploratory analysis is performed on the discrepancy between monocular and LiDAR-based 3D detection frameworks to guide future endeavours.



### Fast Sparse ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1911.09723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09723v1)
- **Published**: 2019-11-21 19:48:14+00:00
- **Updated**: 2019-11-21 19:48:14+00:00
- **Authors**: Erich Elsen, Marat Dukhan, Trevor Gale, Karen Simonyan
- **Comment**: None
- **Journal**: None
- **Summary**: Historically, the pursuit of efficient inference has been one of the driving forces behind research into new deep learning architectures and building blocks. Some recent examples include: the squeeze-and-excitation module, depthwise separable convolutions in Xception, and the inverted bottleneck in MobileNet v2. Notably, in all of these cases, the resulting building blocks enabled not only higher efficiency, but also higher accuracy, and found wide adoption in the field. In this work, we further expand the arsenal of efficient building blocks for neural network architectures; but instead of combining standard primitives (such as convolution), we advocate for the replacement of these dense primitives with their sparse counterparts. While the idea of using sparsity to decrease the parameter count is not new, the conventional wisdom is that this reduction in theoretical FLOPs does not translate into real-world efficiency gains. We aim to correct this misconception by introducing a family of efficient sparse kernels for ARM and WebAssembly, which we open-source for the benefit of the community as part of the XNNPACK library. Equipped with our efficient implementation of sparse primitives, we show that sparse versions of MobileNet v1, MobileNet v2 and EfficientNet architectures substantially outperform strong dense baselines on the efficiency-accuracy curve. On Snapdragon 835 our sparse networks outperform their dense equivalents by $1.3-2.4\times$ -- equivalent to approximately one entire generation of MobileNet-family improvement. We hope that our findings will facilitate wider adoption of sparsity as a tool for creating efficient and accurate deep learning architectures.



### Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09737v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09737v2)
- **Published**: 2019-11-21 20:32:04+00:00
- **Updated**: 2020-04-01 04:19:08+00:00
- **Authors**: Saurabh Singh, Shankar Krishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Batch Normalization (BN) uses mini-batch statistics to normalize the activations during training, introducing dependence between mini-batch elements. This dependency can hurt the performance if the mini-batch size is too small, or if the elements are correlated. Several alternatives, such as Batch Renormalization and Group Normalization (GN), have been proposed to address this issue. However, they either do not match the performance of BN for large batches, or still exhibit degradation in performance for smaller batches, or introduce artificial constraints on the model architecture. In this paper we propose the Filter Response Normalization (FRN) layer, a novel combination of a normalization and an activation function, that can be used as a replacement for other normalizations and activations. Our method operates on each activation channel of each batch element independently, eliminating the dependency on other batch elements. Our method outperforms BN and other alternatives in a variety of settings for all batch sizes. FRN layer performs $\approx 0.7-1.0\%$ better than BN on top-1 validation accuracy with large mini-batch sizes for Imagenet classification using InceptionV3 and ResnetV2-50 architectures. Further, it performs $>1\%$ better than GN on the same problem in the small mini-batch size regime. For object detection problem on COCO dataset, FRN layer outperforms all other methods by at least $0.3-0.5\%$ in all batch size regimes.



### Rethinking Normalization and Elimination Singularity in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.09738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09738v1)
- **Published**: 2019-11-21 20:36:04+00:00
- **Updated**: 2019-11-21 20:36:04+00:00
- **Authors**: Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1903.10520
- **Journal**: None
- **Summary**: In this paper, we study normalization methods for neural networks from the perspective of elimination singularity. Elimination singularities correspond to the points on the training trajectory where neurons become consistently deactivated. They cause degenerate manifolds in the loss landscape which will slow down training and harm model performances. We show that channel-based normalizations (e.g. Layer Normalization and Group Normalization) are unable to guarantee a far distance from elimination singularities, in contrast with Batch Normalization which by design avoids models from getting too close to them. To address this issue, we propose BatchChannel Normalization (BCN), which uses batch knowledge to avoid the elimination singularities in the training of channel-normalized models. Unlike Batch Normalization, BCN is able to run in both large-batch and micro-batch training settings. The effectiveness of BCN is verified on many tasks, including image classification, object detection, instance segmentation, and semantic segmentation. The code is here: https://github.com/joe-siyuan-qiao/Batch-Channel-Normalization.



### Reinforcing an Image Caption Generator Using Off-Line Human Feedback
- **Arxiv ID**: http://arxiv.org/abs/1911.09753v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1911.09753v1)
- **Published**: 2019-11-21 21:26:28+00:00
- **Updated**: 2019-11-21 21:26:28+00:00
- **Authors**: Paul Hongsuck Seo, Piyush Sharma, Tomer Levinboim, Bohyung Han, Radu Soricut
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: Human ratings are currently the most accurate way to assess the quality of an image captioning model, yet most often the only used outcome of an expensive human rating evaluation is a few overall statistics over the evaluation dataset. In this paper, we show that the signal from instance-level human caption ratings can be leveraged to improve captioning models, even when the amount of caption ratings is several orders of magnitude less than the caption training data. We employ a policy gradient method to maximize the human ratings as rewards in an off-policy reinforcement learning setting, where policy gradients are estimated by samples from a distribution that focuses on the captions in a caption ratings dataset. Our empirical evidence indicates that the proposed method learns to generalize the human raters' judgments to a previously unseen set of images, as judged by a different set of human judges, and additionally on a different, multi-dimensional side-by-side human evaluation procedure.



### Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1911.09781v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09781v3)
- **Published**: 2019-11-21 23:05:28+00:00
- **Updated**: 2020-08-27 06:07:44+00:00
- **Authors**: Lu Jiang, Di Huang, Mason Liu, Weilong Yang
- **Comment**: published at ICML 2020
- **Journal**: None
- **Summary**: Performing controlled experiments on noisy data is essential in understanding deep learning across noise levels. Due to the lack of suitable datasets, previous research has only examined deep learning on controlled synthetic label noise, and real-world label noise has never been studied in a controlled setting. This paper makes three contributions. First, we establish the first benchmark of controlled real-world label noise from the web. This new benchmark enables us to study the web label noise in a controlled setting for the first time. The second contribution is a simple but effective method to overcome both synthetic and real noisy labels. We show that our method achieves the best result on our dataset as well as on two public benchmarks (CIFAR and WebVision). Third, we conduct the largest study by far into understanding deep neural networks trained on noisy labels across different noise levels, noise types, network architectures, and training settings. The data and code are released at the following link: http://www.lujiang.info/cnlw.html



### MIMAMO Net: Integrating Micro- and Macro-motion for Video Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.09784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.09784v1)
- **Published**: 2019-11-21 23:34:15+00:00
- **Updated**: 2019-11-21 23:34:15+00:00
- **Authors**: Didan Deng, Zhaokang Chen, Yuqian Zhou, Bertram Shi
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Spatial-temporal feature learning is of vital importance for video emotion recognition. Previous deep network structures often focused on macro-motion which extends over long time scales, e.g., on the order of seconds. We believe integrating structures capturing information about both micro- and macro-motion will benefit emotion prediction, because human perceive both micro- and macro-expressions. In this paper, we propose to combine micro- and macro-motion features to improve video emotion recognition with a two-stream recurrent network, named MIMAMO (Micro-Macro-Motion) Net. Specifically, smaller and shorter micro-motions are analyzed by a two-stream network, while larger and more sustained macro-motions can be well captured by a subsequent recurrent network. Assigning specific interpretations to the roles of different parts of the network enables us to make choice of parameters based on prior knowledge: choices that turn out to be optimal. One of the important innovations in our model is the use of interframe phase differences rather than optical flow as input to the temporal stream. Compared with the optical flow, phase differences require less computation and are more robust to illumination changes. Our proposed network achieves state of the art performance on two video emotion datasets, the OMG emotion dataset and the Aff-Wild dataset. The most significant gains are for arousal prediction, for which motion information is intuitively more informative. Source code is available at https://github.com/wtomin/MIMAMO-Net.



### ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring
- **Arxiv ID**: http://arxiv.org/abs/1911.09785v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.09785v2)
- **Published**: 2019-11-21 23:44:25+00:00
- **Updated**: 2020-02-13 23:14:46+00:00
- **Authors**: David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, Colin Raffel
- **Comment**: None
- **Journal**: None
- **Summary**: We improve the recently-proposed "MixMatch" semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of ground-truth labels. Augmentation anchoring feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring between $5\times$ and $16\times$ less data to reach the same accuracy. For example, on CIFAR-10 with 250 labeled examples we reach $93.73\%$ accuracy (compared to MixMatch's accuracy of $93.58\%$ with $4{,}000$ examples) and a median accuracy of $84.92\%$ with just four labels per class. We make our code and data open-source at https://github.com/google-research/remixmatch.



### Teaching Perception
- **Arxiv ID**: http://arxiv.org/abs/1911.11620v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.11620v1)
- **Published**: 2019-11-21 23:46:37+00:00
- **Updated**: 2019-11-21 23:46:37+00:00
- **Authors**: Jonathan Connell
- **Comment**: arXiv admin note: text overlap with arXiv:1911.09782
- **Journal**: None
- **Summary**: The visual world is very rich and generally too complex to perceive in its entirety. Yet only certain features are typically required to adequately perform some task in a given situation. Rather than hardwire-in decisions about when and what to sense, this paper describes a robotic system whose behavioral policy can be set by verbal instructions it receives. These capabilities are demonstrated in an associated video showing the fully implemented system guiding the perception of a physical robot in simple scenario. The structure and functioning of the underlying natural language based symbolic reasoning system is also discussed.




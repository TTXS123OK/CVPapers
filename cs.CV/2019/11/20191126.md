# Arxiv Papers in cs.CV on 2019-11-26
### Autolabeling 3D Objects with Differentiable Rendering of SDF Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/1911.11288v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11288v2)
- **Published**: 2019-11-26 00:11:49+00:00
- **Updated**: 2020-04-02 15:44:47+00:00
- **Authors**: Sergey Zakharov, Wadim Kehl, Arjun Bhargava, Adrien Gaidon
- **Comment**: CVPR 2020 (Oral). 8 pages + supplementary material. The first two
  authors contributed equally to this work
- **Journal**: None
- **Summary**: We present an automatic annotation pipeline to recover 9D cuboids and 3D shapes from pre-trained off-the-shelf 2D detectors and sparse LIDAR data. Our autolabeling method solves an ill-posed inverse problem by considering learned shape priors and optimizing geometric and physical parameters. To address this challenging problem, we apply a novel differentiable shape renderer to signed distance fields (SDF), leveraged together with normalized object coordinate spaces (NOCS). Initially trained on synthetic data to predict shape and coordinates, our method uses these predictions for projective and geometric alignment over real samples. Moreover, we also propose a curriculum learning strategy, iteratively retraining on samples of increasing difficulty in subsequent self-improving annotation rounds. Our experiments on the KITTI3D dataset show that we can recover a substantial amount of accurate cuboids, and that these autolabels can be used to train 3D vehicle detectors with state-of-the-art results.



### Efficient Saliency Maps for Explainable AI
- **Arxiv ID**: http://arxiv.org/abs/1911.11293v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.PF, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.11293v2)
- **Published**: 2019-11-26 00:32:23+00:00
- **Updated**: 2020-03-09 21:20:06+00:00
- **Authors**: T. Nathan Mundhenk, Barry Y. Chen, Gerald Friedland
- **Comment**: In submission to ECCV 2020
- **Journal**: None
- **Summary**: We describe an explainable AI saliency map method for use with deep convolutional neural networks (CNN) that is much more efficient than popular fine-resolution gradient methods. It is also quantitatively similar or better in accuracy. Our technique works by measuring information at the end of each network scale which is then combined into a single saliency map. We describe how saliency measures can be made more efficient by exploiting Saliency Map Order Equivalence. We visualize individual scale/layer contributions by using a Layer Ordered Visualization of Information. This provides an interesting comparison of scale information contributions within the network not provided by other saliency map methods. Using our method instead of Guided Backprop, coarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem to yield demonstrably superior results without sacrificing speed. This will make fine-resolution saliency methods feasible on resource limited platforms such as robots, cell phones, low-cost industrial devices, astronomy and satellite imagery.



### Motion-Based Generator Model: Unsupervised Disentanglement of Appearance, Trackable and Intrackable Motions in Dynamic Patterns
- **Arxiv ID**: http://arxiv.org/abs/1911.11294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11294v1)
- **Published**: 2019-11-26 00:32:34+00:00
- **Updated**: 2019-11-26 00:32:34+00:00
- **Authors**: Jianwen Xie, Ruiqi Gao, Zilong Zheng, Song-Chun Zhu, Ying Nian Wu
- **Comment**: None
- **Journal**: The Thirty-Fourth AAAI Conference on Artificial Intelligence 2020
- **Summary**: Dynamic patterns are characterized by complex spatial and motion patterns. Understanding dynamic patterns requires a disentangled representational model that separates the factorial components. A commonly used model for dynamic patterns is the state space model, where the state evolves over time according to a transition model and the state generates the observed image frames according to an emission model. To model the motions explicitly, it is natural for the model to be based on the motions or the displacement fields of the pixels. Thus in the emission model, we let the hidden state generate the displacement field, which warps the trackable component in the previous image frame to generate the next frame while adding a simultaneously emitted residual image to account for the change that cannot be explained by the deformation. The warping of the previous image is about the trackable part of the change of image frame, while the residual image is about the intrackable part of the image. We use a maximum likelihood algorithm to learn the model that iterates between inferring latent noise vectors that drive the transition model and updating the parameters given the inferred latent vectors. Meanwhile we adopt a regularization term to penalize the norms of the residual images to encourage the model to explain the change of image frames by trackable motion. Unlike existing methods on dynamic patterns, we learn our model in unsupervised setting without ground truth displacement fields. In addition, our model defines a notion of intrackability by the separation of warped component and residual component in each image frame. We show that our method can synthesize realistic dynamic pattern, and disentangling appearance, trackable and intrackable motions. The learned models are useful for motion transfer, and it is natural to adopt it to define and measure intrackability of a dynamic pattern.



### SRG: Snippet Relatedness-based Temporal Action Proposal Generator
- **Arxiv ID**: http://arxiv.org/abs/1911.11306v2
- **DOI**: 10.1109/TCSVT.2019.2953187
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11306v2)
- **Published**: 2019-11-26 01:59:54+00:00
- **Updated**: 2020-03-06 06:05:01+00:00
- **Authors**: Hyunjun Eun, Sumin Lee, Jinyoung Moon, Jongyoul Park, Chanho Jung, Changick Kim
- **Comment**: To appear in TCSVT
- **Journal**: None
- **Summary**: Recent temporal action proposal generation approaches have suggested integrating segment- and snippet score-based methodologies to produce proposals with high recall and accurate boundaries. In this paper, different from such a hybrid strategy, we focus on the potential of the snippet score-based approach. Specifically, we propose a new snippet score-based method, named Snippet Relatedness-based Generator (SRG), with a novel concept of "snippet relatedness". Snippet relatedness represents which snippets are related to a specific action instance. To effectively learn this snippet relatedness, we present "pyramid non-local operations" for locally and globally capturing long-range dependencies among snippets. By employing these components, SRG first produces a 2D relatedness score map that enables the generation of various temporal intervals reliably covering most action instances with high overlap. Then, SRG evaluates the action confidence scores of these temporal intervals and refines their boundaries to obtain temporal action proposals. On THUMOS-14 and ActivityNet-1.3 datasets, SRG outperforms state-of-the-art methods for temporal action proposal generation. Furthermore, compared to competing proposal generators, SRG leads to significant improvements in temporal action detection.



### Neural Graph Matching Network: Learning Lawler's Quadratic Assignment Problem with Extension to Hypergraph and Multiple-graph Matching
- **Arxiv ID**: http://arxiv.org/abs/1911.11308v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11308v3)
- **Published**: 2019-11-26 02:06:57+00:00
- **Updated**: 2021-05-06 17:18:34+00:00
- **Authors**: Runzhong Wang, Junchi Yan, Xiaokang Yang
- **Comment**: Accepted by TPAMI
- **Journal**: None
- **Summary**: Graph matching involves combinatorial optimization based on edge-to-edge affinity matrix, which can be generally formulated as Lawler's Quadratic Assignment Problem (QAP). This paper presents a QAP network directly learning with the affinity matrix (equivalently the association graph) whereby the matching problem is translated into a constrained vertex classification task. The association graph is learned by an embedding network for vertex classification, followed by Sinkhorn normalization and a cross-entropy loss for end-to-end learning. We further improve the embedding model on association graph by introducing Sinkhorn based matching-aware constraint, as well as dummy nodes to deal with unequal sizes of graphs. To our best knowledge, this is one of the first network to directly learn with the general Lawler's QAP. In contrast, recent deep matching methods focus on the learning of node/edge features in two graphs respectively. We also show how to extend our network to hypergraph matching, and matching of multiple graphs. Experimental results on both synthetic graphs and real-world images show its effectiveness. For pure QAP tasks on synthetic data and QAPLIB benchmark, our method can perform competitively and even surpass state-of-the-art graph matching and QAP solvers with notable less time cost. We provide a project homepage at http://thinklab.sjtu.edu.cn/project/NGM/index.html.



### Spatial-Aware GAN for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1911.11312v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11312v3)
- **Published**: 2019-11-26 02:56:34+00:00
- **Updated**: 2022-10-03 16:51:00+00:00
- **Authors**: Changgong Zhang, Fangneng Zhan
- **Comment**: Accepted to ICPR2020
- **Journal**: None
- **Summary**: The recent person re-identification research has achieved great success by learning from a large number of labeled person images. On the other hand, the learned models often experience significant performance drops when applied to images collected in a different environment. Unsupervised domain adaptation (UDA) has been investigated to mitigate this constraint, but most existing systems adapt images at pixel level only and ignore obvious discrepancies at spatial level. This paper presents an innovative UDA-based person re-identification network that is capable of adapting images at both spatial and pixel levels simultaneously. A novel disentangled cycle-consistency loss is designed which guides the learning of spatial-level and pixel-level adaptation in a collaborative manner. In addition, a novel multi-modal mechanism is incorporated which is capable of generating images of different geometry views and augmenting training images effectively. Extensive experiments over a number of public datasets show that the proposed UDA network achieves superior person re-identification performance as compared with the state-of-the-art.



### Calibrated Domain-Invariant Learning for Highly Generalizable Large Scale Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1911.11314v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11314v2)
- **Published**: 2019-11-26 03:08:28+00:00
- **Updated**: 2019-11-27 02:28:50+00:00
- **Authors**: Ye Yuan, Wuyang Chen, Tianlong Chen, Yang Yang, Zhou Ren, Zhangyang Wang, Gang Hua
- **Comment**: WACV 2020 accepted
- **Journal**: None
- **Summary**: Many real-world applications, such as city-scale traffic monitoring and control, requires large-scale re-identification. However, previous ReID methods often failed to address two limitations in existing ReID benchmarks, i.e., low spatiotemporal coverage and sample imbalance. Notwithstanding their demonstrated success in every single benchmark, they have difficulties in generalizing to unseen environments. As a result, these methods are less applicable in a large-scale setting due to poor generalization. In seek for a highly generalizable large-scale ReID method, we present an adversarial domain invariant feature learning framework (ADIN) that explicitly learns to separate identity-related features from challenging variations, where for the first time "free" annotations in ReID data such as video timestamp and camera index are utilized. Furthermore, we find that the imbalance of nuisance classes jeopardizes the adversarial training, and for mitigation we propose a calibrated adversarial loss that is attentive to nuisance distribution. Experiments on existing large-scale person vehicle ReID datasets demonstrate that ADIN learns more robust and generalizable representations, as evidenced by its outstanding direct transfer performance across datasets, which is a criterion that can better measure the generalizability of large-scale ReID methods/



### Learning Efficient Video Representation with Video Shuffle Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.11319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11319v1)
- **Published**: 2019-11-26 03:52:47+00:00
- **Updated**: 2019-11-26 03:52:47+00:00
- **Authors**: Pingchuan Ma, Yao Zhou, Yu Lu, Wei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: 3D CNN shows its strong ability in learning spatiotemporal representation in recent video recognition tasks. However, inflating 2D convolution to 3D inevitably introduces additional computational costs, making it cumbersome in practical deployment. We consider whether there is a way to equip the conventional 2D convolution with temporal vision no requiring expanding its kernel. To this end, we propose the video shuffle, a parameter-free plug-in component that efficiently reallocates the inputs of 2D convolution so that its receptive field can be extended to the temporal dimension. In practical, video shuffle firstly divides each frame feature into multiple groups and then aggregate the grouped features via temporal shuffle operation. This allows the following 2D convolution aggregate the global spatiotemporal features. The proposed video shuffle can be flexibly inserted into popular 2D CNNs, forming the Video Shuffle Networks (VSN). With a simple yet efficient implementation, VSN performs surprisingly well on temporal modeling benchmarks. In experiments, VSN not only gains non-trivial improvements on Kinetics and Moments in Time, but also achieves state-of-the-art performance on Something-Something-V1, Something-Something-V2 datasets.



### Progressive Retinex: Mutually Reinforced Illumination-Noise Perception Network for Low Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1911.11323v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11323v1)
- **Published**: 2019-11-26 03:56:45+00:00
- **Updated**: 2019-11-26 03:56:45+00:00
- **Authors**: Yang Wang, Yang Cao, Zheng-Jun Zha, Jing Zhang, Zhiwei Xiong, Wei Zhang, Feng Wu
- **Comment**: The 27th ACM International Conference on Multimedia (MM'19)
- **Journal**: None
- **Summary**: Contrast enhancement and noise removal are coupled problems for low-light image enhancement. The existing Retinex based methods do not take the coupling relation into consideration, resulting in under or over-smoothing of the enhanced images. To address this issue, this paper presents a novel progressive Retinex framework, in which illumination and noise of low-light image are perceived in a mutually reinforced manner, leading to noise reduction low-light enhancement results. Specifically, two fully pointwise convolutional neural networks are devised to model the statistical regularities of ambient light and image noise respectively, and to leverage them as constraints to facilitate the mutual learning process. The proposed method not only suppresses the interference caused by the ambiguity between tiny textures and image noises, but also greatly improves the computational efficiency. Moreover, to solve the problem of insufficient training data, we propose an image synthesis strategy based on camera imaging model, which generates color images corrupted by illumination-dependent noises. Experimental results on both synthetic and real low-light images demonstrate the superiority of our proposed approaches against the State-Of-The-Art (SOTA) low-light enhancement methods.



### Mis-classified Vector Guided Softmax Loss for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.00833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00833v1)
- **Published**: 2019-11-26 04:05:09+00:00
- **Updated**: 2019-11-26 04:05:09+00:00
- **Authors**: Xiaobo Wang, Shifeng Zhang, Shuo Wang, Tianyu Fu, Hailin Shi, Tao Mei
- **Comment**: Accepted by AAAI2020 as Oral presentation. arXiv admin note:
  substantial text overlap with arXiv:1812.11317
- **Journal**: None
- **Summary**: Face recognition has witnessed significant progress due to the advances of deep convolutional neural networks (CNNs), the central task of which is how to improve the feature discrimination. To this end, several margin-based (\textit{e.g.}, angular, additive and additive angular margins) softmax loss functions have been proposed to increase the feature margin between different classes. However, despite great achievements have been made, they mainly suffer from three issues: 1) Obviously, they ignore the importance of informative features mining for discriminative learning; 2) They encourage the feature margin only from the ground truth class, without realizing the discriminability from other non-ground truth classes; 3) The feature margin between different classes is set to be same and fixed, which may not adapt the situations very well. To cope with these issues, this paper develops a novel loss function, which adaptively emphasizes the mis-classified feature vectors to guide the discriminative feature learning. Thus we can address all the above issues and achieve more discriminative face features. To the best of our knowledge, this is the first attempt to inherit the advantages of feature margin and feature mining into a unified loss function. Experimental results on several benchmarks have demonstrated the effectiveness of our method over state-of-the-art alternatives.



### Super-Resolution for Practical Automated Plant Disease Diagnosis System
- **Arxiv ID**: http://arxiv.org/abs/1911.11341v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11341v1)
- **Published**: 2019-11-26 05:03:03+00:00
- **Updated**: 2019-11-26 05:03:03+00:00
- **Authors**: Quan Huu Cap, Hiroki Tani, Hiroyuki Uga, Satoshi Kagiwada, Hitoshi Iyatomi
- **Comment**: Published as a conference paper at CISS 2019, Baltimore, MD, USA
- **Journal**: None
- **Summary**: Automated plant diagnosis using images taken from a distance is often insufficient in resolution and degrades diagnostic accuracy since the important external characteristics of symptoms are lost. In this paper, we first propose an effective pre-processing method for improving the performance of automated plant disease diagnosis systems using super-resolution techniques. We investigate the efficiency of two different super-resolution methods by comparing the disease diagnostic performance on the practical original high-resolution, low-resolution, and super-resolved cucumber images. Our method generates super-resolved images that look very close to natural images with 4$\times$ upscaling factors and is capable of recovering the lost detailed symptoms, largely boosting the diagnostic performance. Our model improves the disease classification accuracy by 26.9% over the bicubic interpolation method of 65.6% and shows a small gap (3% lower) between the original result of 95.5%.



### Skeleton based Zero Shot Action Recognition in Joint Pose-Language Semantic Space
- **Arxiv ID**: http://arxiv.org/abs/1911.11344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11344v1)
- **Published**: 2019-11-26 05:10:47+00:00
- **Updated**: 2019-11-26 05:10:47+00:00
- **Authors**: Bhavan Jasani, Afshaan Mazagonwalla
- **Comment**: None
- **Journal**: None
- **Summary**: How does one represent an action? How does one describe an action that we have never seen before? Such questions are addressed by the Zero Shot Learning paradigm, where a model is trained on only a subset of classes and is evaluated on its ability to correctly classify an example from a class it has never seen before. In this work, we present a body pose based zero shot action recognition network and demonstrate its performance on the NTU RGB-D dataset. Our model learns to jointly encapsulate visual similarities based on pose features of the action performer as well as similarities in the natural language descriptions of the unseen action class names. We demonstrate how this pose-language semantic space encodes knowledge which allows our model to correctly predict actions not seen during training.



### Distraction-Aware Feature Learning for Human Attribute Recognition via Coarse-to-Fine Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/1911.11351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11351v1)
- **Published**: 2019-11-26 05:49:52+00:00
- **Updated**: 2019-11-26 05:49:52+00:00
- **Authors**: Mingda Wu, Di Huang, Yuanfang Guo, Yunhong Wang
- **Comment**: 8 pages, 5 figures, accepted by AAAI-20 as an oral presentation
- **Journal**: None
- **Summary**: Recently, Human Attribute Recognition (HAR) has become a hot topic due to its scientific challenges and application potentials, where localizing attributes is a crucial stage but not well handled. In this paper, we propose a novel deep learning approach to HAR, namely Distraction-aware HAR (Da-HAR). It enhances deep CNN feature learning by improving attribute localization through a coarse-to-fine attention mechanism. At the coarse step, a self-mask block is built to roughly discriminate and reduce distractions, while at the fine step, a masked attention branch is applied to further eliminate irrelevant regions. Thanks to this mechanism, feature learning is more accurate, especially when heavy occlusions and complex backgrounds exist. Extensive experiments are conducted on the WIDER-Attribute and RAP databases, and state-of-the-art results are achieved, demonstrating the effectiveness of the proposed approach.



### Semantic Bottleneck Scene Generation
- **Arxiv ID**: http://arxiv.org/abs/1911.11357v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11357v1)
- **Published**: 2019-11-26 06:01:09+00:00
- **Updated**: 2019-11-26 06:01:09+00:00
- **Authors**: Samaneh Azadi, Michael Tschannen, Eric Tzeng, Sylvain Gelly, Trevor Darrell, Mario Lucic
- **Comment**: None
- **Journal**: None
- **Summary**: Coupling the high-fidelity generation capabilities of label-conditional image synthesis methods with the flexibility of unconditional generative models, we propose a semantic bottleneck GAN model for unconditional synthesis of complex scenes. We assume pixel-wise segmentation labels are available during training and use them to learn the scene structure. During inference, our model first synthesizes a realistic segmentation layout from scratch, then synthesizes a realistic scene conditioned on that layout. For the former, we use an unconditional progressive segmentation generation network that captures the distribution of realistic semantic scene layouts. For the latter, we use a conditional segmentation-to-image synthesis network that captures the distribution of photo-realistic images conditioned on the semantic layout. When trained end-to-end, the resulting model outperforms state-of-the-art generative models in unsupervised image synthesis on two challenging domains in terms of the Frechet Inception Distance and user-study evaluations. Moreover, we demonstrate the generated segmentation maps can be used as additional training data to strongly improve recent segmentation-to-image synthesis networks.



### TextSLAM: Visual SLAM with Planar Text Features
- **Arxiv ID**: http://arxiv.org/abs/1912.05002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05002v2)
- **Published**: 2019-11-26 07:10:25+00:00
- **Updated**: 2020-05-15 15:16:21+00:00
- **Authors**: Boying Li, Danping Zou, Daniele Sartori, Ling Pei, Wenxian Yu
- **Comment**: Accepted by ICRA2020
- **Journal**: None
- **Summary**: We propose to integrate text objects in man-made scenes tightly into the visual SLAM pipeline. The key idea of our novel text-based visual SLAM is to treat each detected text as a planar feature which is rich of textures and semantic meanings. The text feature is compactly represented by three parameters and integrated into visual SLAM by adopting the illumination-invariant photometric error. We also describe important details involved in implementing a full pipeline of text-based visual SLAM. To our best knowledge, this is the first visual SLAM method tightly coupled with the text features. We tested our method in both indoor and outdoor environments. The results show that with text features, the visual SLAM system becomes more robust and produces much more accurate 3D text maps that could be useful for navigation and scene understanding in robotic or augmented reality applications.



### Text2FaceGAN: Face Generation from Fine Grained Textual Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1911.11378v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MM, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11378v1)
- **Published**: 2019-11-26 07:37:47+00:00
- **Updated**: 2019-11-26 07:37:47+00:00
- **Authors**: Osaid Rehman Nasir, Shailesh Kumar Jha, Manraj Singh Grover, Yi Yu, Ajit Kumar, Rajiv Ratn Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Powerful generative adversarial networks (GAN) have been developed to automatically synthesize realistic images from text. However, most existing tasks are limited to generating simple images such as flowers from captions. In this work, we extend this problem to the less addressed domain of face generation from fine-grained textual descriptions of face, e.g., "A person has curly hair, oval face, and mustache". We are motivated by the potential of automated face generation to impact and assist critical tasks such as criminal face reconstruction. Since current datasets for the task are either very small or do not contain captions, we generate captions for images in the CelebA dataset by creating an algorithm to automatically convert a list of attributes to a set of captions. We then model the highly multi-modal problem of text to face generation as learning the conditional distribution of faces (conditioned on text) in same latent space. We utilize the current state-of-the-art GAN (DC-GAN with GAN-CLS loss) for learning conditional multi-modality. The presence of more fine-grained details and variable length of the captions makes the problem easier for a user but more difficult to handle compared to the other text-to-image tasks. We flipped the labels for real and fake images and added noise in discriminator. Generated images for diverse textual descriptions show promising results. In the end, we show how the widely used inceptions score is not a good metric to evaluate the performance of generative models used for synthesizing faces from text.



### Content-based image retrieval speedup
- **Arxiv ID**: http://arxiv.org/abs/1911.11379v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11379v2)
- **Published**: 2019-11-26 07:40:06+00:00
- **Updated**: 2019-12-22 14:12:35+00:00
- **Authors**: Sadegh Fadaei, Abdolreza Rashno, Elyas Rashno
- **Comment**: None
- **Journal**: 5th Conference on Signal Processing and Intelligent Systems
  (ICSPIS2019)
- **Summary**: Content-based image retrieval (CBIR) is a task of retrieving images from their contents. Since retrieval process is a time-consuming task in large image databases, acceleration methods can be very useful. This paper presents a novel method to speed up CBIR systems. In the proposed method, first Zernike moments are extracted from query image and an interval is calculated for that query. Images in database which are out of the interval are ignored in retrieval process. Therefore, a database reduction occurs before retrieval which leads to speed up. It is shown that in reduced database, relevant images to query image are preserved and irrelevant images are throwed away. Therefore, the proposed method speed up retrieval process and preserve CBIR accuracy, simultaneously.



### Multi-Task Driven Feature Models for Thermal Infrared Tracking
- **Arxiv ID**: http://arxiv.org/abs/1911.11384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11384v1)
- **Published**: 2019-11-26 07:50:37+00:00
- **Updated**: 2019-11-26 07:50:37+00:00
- **Authors**: Qiao Liu, Xin Li, Zhenyu He, Nana Fan, Di Yuan, Wei Liu, Yonsheng Liang
- **Comment**: Thirty-Fourth AAAI Conference on Artifical Intelligence
- **Journal**: None
- **Summary**: Existing deep Thermal InfraRed (TIR) trackers usually use the feature models of RGB trackers for representation. However, these feature models learned on RGB images are neither effective in representing TIR objects nor taking fine-grained TIR information into consideration. To this end, we develop a multi-task framework to learn the TIR-specific discriminative features and fine-grained correlation features for TIR tracking. Specifically, we first use an auxiliary classification network to guide the generation of TIR-specific discriminative features for distinguishing the TIR objects belonging to different classes. Second, we design a fine-grained aware module to capture more subtle information for distinguishing the TIR objects belonging to the same class. These two kinds of features complement each other and recognize TIR objects in the levels of inter-class and intra-class respectively. These two feature models are learned using a multi-task matching framework and are jointly optimized on the TIR tracking task. In addition, we develop a large-scale TIR training dataset to train the network for adapting the model to the TIR domain. Extensive experimental results on three benchmarks show that the proposed algorithm achieves a relative gain of 10% over the baseline and performs favorably against the state-of-the-art methods. Codes and the proposed TIR dataset are available at {https://github.com/QiaoLiuHit/MMNet}.



### Efficient Attention Mechanism for Visual Dialog that can Handle All the Interactions between Multiple Inputs
- **Arxiv ID**: http://arxiv.org/abs/1911.11390v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11390v2)
- **Published**: 2019-11-26 08:10:02+00:00
- **Updated**: 2020-07-17 14:10:12+00:00
- **Authors**: Van-Quang Nguyen, Masanori Suganuma, Takayuki Okatani
- **Comment**: Accepted to ECCV 2020, 14 pages. Slight change in title
- **Journal**: None
- **Summary**: It has been a primary concern in recent studies of vision and language tasks to design an effective attention mechanism dealing with interactions between the two modalities. The Transformer has recently been extended and applied to several bi-modal tasks, yielding promising results. For visual dialog, it becomes necessary to consider interactions between three or more inputs, i.e., an image, a question, and a dialog history, or even its individual dialog components. In this paper, we present a neural architecture named Light-weight Transformer for Many Inputs (LTMI) that can efficiently deal with all the interactions between multiple such inputs in visual dialog. It has a block structure similar to the Transformer and employs the same design of attention computation, whereas it has only a small number of parameters, yet has sufficient representational power for the purpose. Assuming a standard setting of visual dialog, a layer built upon the proposed attention block has less than one-tenth of parameters as compared with its counterpart, a natural Transformer extension. The experimental results on the VisDial datasets validate the effectiveness of the proposed approach, showing improvements of the best NDCG score on the VisDial v1.0 dataset from 57.59 to 60.92 with a single model, from 64.47 to 66.53 with ensemble models, and even to 74.88 with additional finetuning. Our implementation code is available at https://github.com/davidnvq/visdial.



### A Two-stream End-to-End Deep Learning Network for Recognizing Atypical Visual Attention in Autism Spectrum Disorder
- **Arxiv ID**: http://arxiv.org/abs/1911.11393v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1911.11393v1)
- **Published**: 2019-11-26 08:19:47+00:00
- **Updated**: 2019-11-26 08:19:47+00:00
- **Authors**: Jin Xie, Longfei Wang, Paula Webster, Yang Yao, Jiayao Sun, Shuo Wang, Huihui Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Eye movements have been widely investigated to study the atypical visual attention in Autism Spectrum Disorder (ASD). The majority of these studies have been focused on limited eye movement features by statistical comparisons between ASD and Typically Developing (TD) groups, which make it difficult to accurately separate ASD from TD at the individual level. The deep learning technology has been highly successful in overcoming this issue by automatically extracting features important for classification through a data-driven learning process. However, there is still a lack of end-to-end deep learning framework for recognition of abnormal attention in ASD. In this study, we developed a novel two-stream deep learning network for this recognition based on 700 images and corresponding eye movement patterns of ASD and TD, and obtained an accuracy of 0.95, which was higher than the previous state-of-the-art. We next characterized contributions to the classification at the single image level and non-linearly integration of this single image level information during the classification. Moreover, we identified a group of pixel-level visual features within these images with greater impacts on the classification. Together, this two-stream deep learning network provides us a novel and powerful tool to recognize and understand abnormal visual attention in ASD.



### LaFIn: Generative Landmark Guided Face Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1911.11394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11394v1)
- **Published**: 2019-11-26 08:20:50+00:00
- **Updated**: 2019-11-26 08:20:50+00:00
- **Authors**: Yang Yang, Xiaojie Guo, Jiayi Ma, Lin Ma, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: It is challenging to inpaint face images in the wild, due to the large variation of appearance, such as different poses, expressions and occlusions. A good inpainting algorithm should guarantee the realism of output, including the topological structure among eyes, nose and mouth, as well as the attribute consistency on pose, gender, ethnicity, expression, etc. This paper studies an effective deep learning based strategy to deal with these issues, which comprises of a facial landmark predicting subnet and an image inpainting subnet. Concretely, given partial observation, the landmark predictor aims to provide the structural information (e.g. topological relationship and expression) of incomplete faces, while the inpaintor is to generate plausible appearance (e.g. gender and ethnicity) conditioned on the predicted landmarks. Experiments on the CelebA-HQ and CelebA datasets are conducted to reveal the efficacy of our design and, to demonstrate its superiority over state-of-the-art alternatives both qualitatively and quantitatively. In addition, we assume that high-quality completed faces together with their landmarks can be utilized as augmented data to further improve the performance of (any) landmark predictor, which is corroborated by experimental results on the 300W and WFLW datasets.



### Revisiting Image Aesthetic Assessment via Self-Supervised Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.11419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11419v1)
- **Published**: 2019-11-26 09:41:04+00:00
- **Updated**: 2019-11-26 09:41:04+00:00
- **Authors**: Kekai Sheng, Weiming Dong, Menglei Chai, Guohui Wang, Peng Zhou, Feiyue Huang, Bao-Gang Hu, Rongrong Ji, Chongyang Ma
- **Comment**: AAAI Conference on Artificial Intelligence, 2020, accepted
- **Journal**: Proceedings of AAAI Conference on Articial Intelligence 2020
- **Summary**: Visual aesthetic assessment has been an active research field for decades. Although latest methods have achieved promising performance on benchmark datasets, they typically rely on a large number of manual annotations including both aesthetic labels and related image attributes. In this paper, we revisit the problem of image aesthetic assessment from the self-supervised feature learning perspective. Our motivation is that a suitable feature representation for image aesthetic assessment should be able to distinguish different expert-designed image manipulations, which have close relationships with negative aesthetic effects. To this end, we design two novel pretext tasks to identify the types and parameters of editing operations applied to synthetic instances. The features from our pretext tasks are then adapted for a one-layer linear classifier to evaluate the performance in terms of binary aesthetic classification. We conduct extensive quantitative experiments on three benchmark datasets and demonstrate that our approach can faithfully extract aesthetics-aware features and outperform alternative pretext schemes. Moreover, we achieve comparable results to state-of-the-art supervised methods that use 10 million labels from ImageNet.



### Procrustes registration of two-dimensional statistical shape models without correspondences
- **Arxiv ID**: http://arxiv.org/abs/1911.11431v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11431v2)
- **Published**: 2019-11-26 10:01:28+00:00
- **Updated**: 2019-11-27 09:47:38+00:00
- **Authors**: Alma Eguizabal, Peter J. Schreier, JÃ¼rgen Schmidt
- **Comment**: None
- **Journal**: None
- **Summary**: Statistical shape models are a useful tool in image processing and computer vision. A Procrustres registration of the contours of the same shape is typically perform to align the training samples to learn the statistical shape model. A Procrustes registration between two contours with known correspondences is straightforward. However, these correspondences are not generally available. Manually placed landmarks are often used for correspondence in the design of statistical shape models. However, determining manual landmarks on the contours is time-consuming and often error-prone. One solution to simultaneously find correspondence and registration is the Iterative Closest Point (ICP) algorithm. However, ICP requires an initial position of the contours that is close to registration, and it is not robust against outliers. We propose a new strategy, based on Dynamic Time Warping, that efficiently solves the Procrustes registration problem without correspondences. We study the registration performance in a collection of different shape data sets and show that our technique outperforms competing techniques based on the ICP approach. Our strategy is applied to an ensemble of contours of the same shape as an extension of the generalized Procrustes analysis accounting for a lack of correspondence.



### "You might also like this model": Data Driven Approach for Recommending Deep Learning Models for Unknown Image Datasets
- **Arxiv ID**: http://arxiv.org/abs/1911.11433v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11433v2)
- **Published**: 2019-11-26 10:01:35+00:00
- **Updated**: 2020-05-20 20:45:57+00:00
- **Authors**: Ameya Prabhu, Riddhiman Dasgupta, Anush Sankaran, Srikanth Tamilselvam, Senthil Mani
- **Comment**: NeurIPS 2019, New in ML Group
- **Journal**: None
- **Summary**: For an unknown (new) classification dataset, choosing an appropriate deep learning architecture is often a recursive, time-taking, and laborious process. In this research, we propose a novel technique to recommend a suitable architecture from a repository of known models. Further, we predict the performance accuracy of the recommended architecture on the given unknown dataset, without the need for training the model. We propose a model encoder approach to learn a fixed length representation of deep learning architectures along with its hyperparameters, in an unsupervised fashion. We manually curate a repository of image datasets with corresponding known deep learning models and show that the predicted accuracy is a good estimator of the actual accuracy. We discuss the implications of the proposed approach for three benchmark images datasets and also the challenges in using the approach for text modality. To further increase the reproducibility of the proposed approach, the entire implementation is made publicly available along with the trained models.



### F3Net: Fusion, Feedback and Focus for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.11445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11445v1)
- **Published**: 2019-11-26 10:41:35+00:00
- **Updated**: 2019-11-26 10:41:35+00:00
- **Authors**: Jun Wei, Shuhui Wang, Qingming Huang
- **Comment**: Accepted by AAAI2020, https://github.com/weijun88/F3Net
- **Journal**: None
- **Summary**: Most of existing salient object detection models have achieved great progress by aggregating multi-level features extracted from convolutional neural networks. However, because of the different receptive fields of different convolutional layers, there exists big differences between features generated by these layers. Common feature fusion strategies (addition or concatenation) ignore these differences and may cause suboptimal solutions. In this paper, we propose the F3Net to solve above problem, which mainly consists of cross feature module (CFM) and cascaded feedback decoder (CFD) trained by minimizing a new pixel position aware loss (PPA). Specifically, CFM aims to selectively aggregate multi-level features. Different from addition and concatenation, CFM adaptively selects complementary components from input features before fusion, which can effectively avoid introducing too much redundant information that may destroy the original features. Besides, CFD adopts a multi-stage feedback mechanism, where features closed to supervision will be introduced to the output of previous layers to supplement them and eliminate the differences between features. These refined features will go through multiple similar iterations before generating the final saliency maps. Furthermore, different from binary cross entropy, the proposed PPA loss doesn't treat pixels equally, which can synthesize the local structure information of a pixel to guide the network to focus more on local details. Hard pixels from boundaries or error-prone parts will be given more attention to emphasize their importance. F3Net is able to segment salient object regions accurately and provide clear local details. Comprehensive experiments on five benchmark datasets demonstrate that F3Net outperforms state-of-the-art approaches on six evaluation metrics.



### Occluded Pedestrian Detection with Visible IoU and Box Sign Predictor
- **Arxiv ID**: http://arxiv.org/abs/1911.11449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11449v1)
- **Published**: 2019-11-26 10:52:19+00:00
- **Updated**: 2019-11-26 10:52:19+00:00
- **Authors**: Ruiqi Lu, Huimin Ma
- **Comment**: The 26th IEEE International Conference on Image Processing (ICIP)
- **Journal**: None
- **Summary**: Training a robust classifier and an accurate box regressor are difficult for occluded pedestrian detection. Traditionally adopted Intersection over Union (IoU) measurement does not consider the occluded region of the object and leads to improper training samples. To address such issue, a modification called visible IoU is proposed in this paper to explicitly incorporate the visible ratio in selecting samples. Then a newly designed box sign predictor is placed in parallel with box regressor to separately predict the moving direction of training samples. It leads to higher localization accuracy by introducing sign prediction loss during training and sign refining in testing. Following these novelties, we obtain state-of-the-art performance on CityPersons benchmark for occluded pedestrian detection.



### G-TAD: Sub-Graph Localization for Temporal Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.11462v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11462v2)
- **Published**: 2019-11-26 11:27:09+00:00
- **Updated**: 2020-04-02 21:48:21+00:00
- **Authors**: Mengmeng Xu, Chen Zhao, David S. Rojas, Ali Thabet, Bernard Ghanem
- **Comment**: Accepted by CVPR2020. 8 pages, 9 figures, 2 pages appendix
- **Journal**: None
- **Summary**: Temporal action detection is a fundamental yet challenging task in video understanding. Video context is a critical cue to effectively detect actions, but current works mainly focus on temporal context, while neglecting semantic context as well as other important context properties. In this work, we propose a graph convolutional network (GCN) model to adaptively incorporate multi-level semantic context into video features and cast temporal action detection as a sub-graph localization problem. Specifically, we formulate video snippets as graph nodes, snippet-snippet correlations as edges, and actions associated with context as target sub-graphs. With graph convolution as the basic operation, we design a GCN block called GCNeXt, which learns the features of each node by aggregating its context and dynamically updates the edges in the graph. To localize each sub-graph, we also design an SGAlign layer to embed each sub-graph into the Euclidean space. Extensive experiments show that G-TAD is capable of finding effective video context without extra supervision and achieves state-of-the-art performance on two detection benchmarks. On ActivityNet-1.3, it obtains an average mAP of 34.09%; on THUMOS14, it reaches 51.6% at IoU@0.5 when combined with a proposal processing method. G-TAD code is publicly available at https://github.com/frostinassiky/gtad.



### Domain-Aware Dynamic Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.13237v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.13237v1)
- **Published**: 2019-11-26 12:00:58+00:00
- **Updated**: 2019-11-26 12:00:58+00:00
- **Authors**: Tianyuan Zhang, Bichen Wu, Xin Wang, Joseph Gonzalez, Kurt Keutzer
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks with more parameters and FLOPs have higher capacity and generalize better to diverse domains. But to be deployed on edge devices, the model's complexity has to be constrained due to limited compute resource. In this work, we propose a method to improve the model capacity without increasing inference-time complexity. Our method is based on an assumption of data locality: for an edge device, within a short period of time, the input data to the device are sampled from a single domain with relatively low diversity. Therefore, it is possible to utilize a specialized, low-complexity model to achieve good performance in that input domain. To leverage this, we propose Domain-aware Dynamic Network (DDN), which is a high-capacity dynamic network in which each layer contains multiple weights. During inference, based on the input domain, DDN dynamically combines those weights into one single weight that specializes in the given domain. This way, DDN can keep the inference-time complexity low but still maintain a high capacity. Experiments show that without increasing the parameters, FLOPs, and actual latency, DDN achieves up to 2.6\% higher AP50 than a static network on the BDD100K object-detection benchmark.



### Using Depth for Pixel-Wise Detection of Adversarial Attacks in Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1911.11484v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11484v2)
- **Published**: 2019-11-26 12:12:34+00:00
- **Updated**: 2020-03-18 07:20:12+00:00
- **Authors**: Weizhe Liu, Mathieu Salzmann, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art methods for counting people in crowded scenes rely on deep networks to estimate crowd density. While effective, deep learning approaches are vulnerable to adversarial attacks, which, in a crowd-counting context, can lead to serious security issues. However, attack and defense mechanisms have been virtually unexplored in regression tasks, let alone for crowd density estimation.   In this paper, we investigate the effectiveness of existing attack strategies on crowd-counting networks, and introduce a simple yet effective pixel-wise detection mechanism. It builds on the intuition that, when attacking a multitask network, in our case estimating crowd density and scene depth, both outputs will be perturbed, and thus the second one can be used for detection purposes. We will demonstrate that this significantly outperforms heuristic and uncertainty-based strategies.



### Hearing Lips: Improving Lip Reading by Distilling Speech Recognizers
- **Arxiv ID**: http://arxiv.org/abs/1911.11502v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1911.11502v1)
- **Published**: 2019-11-26 13:05:07+00:00
- **Updated**: 2019-11-26 13:05:07+00:00
- **Authors**: Ya Zhao, Rui Xu, Xinchao Wang, Peng Hou, Haihong Tang, Mingli Song
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: Lip reading has witnessed unparalleled development in recent years thanks to deep learning and the availability of large-scale datasets. Despite the encouraging results achieved, the performance of lip reading, unfortunately, remains inferior to the one of its counterpart speech recognition, due to the ambiguous nature of its actuations that makes it challenging to extract discriminant features from the lip movement videos. In this paper, we propose a new method, termed as Lip by Speech (LIBS), of which the goal is to strengthen lip reading by learning from speech recognizers. The rationale behind our approach is that the features extracted from speech recognizers may provide complementary and discriminant clues, which are formidable to be obtained from the subtle movements of the lips, and consequently facilitate the training of lip readers. This is achieved, specifically, by distilling multi-granularity knowledge from speech recognizers to lip readers. To conduct this cross-modal knowledge distillation, we utilize an efficacious alignment scheme to handle the inconsistent lengths of the audios and videos, as well as an innovative filtering strategy to refine the speech recognizer's prediction. The proposed method achieves the new state-of-the-art performance on the CMLR and LRS2 datasets, outperforming the baseline by a margin of 7.66% and 2.75% in character error rate, respectively.



### WSOD with PSNet and Box Regression
- **Arxiv ID**: http://arxiv.org/abs/1911.11512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11512v1)
- **Published**: 2019-11-26 13:20:54+00:00
- **Updated**: 2019-11-26 13:20:54+00:00
- **Authors**: Sheng Yi, Xi Li, Huimin Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised object detection(WSOD) task uses only image-level annotations to train object detection task. WSOD does not require time-consuming instance-level annotations, so the study of this task has attracted more and more attention. Previous weakly supervised object detection methods iteratively update detectors and pseudo-labels, or use feature-based mask-out methods. Most of these methods do not generate complete and accurate proposals, often only the most discriminative parts of the object, or too many background areas. To solve this problem, we added the box regression module to the weakly supervised object detection network and proposed a proposal scoring network (PSNet) to supervise it. The box regression module modifies proposal to improve the IoU of proposal and ground truth. PSNet scores the proposal output from the box regression network and utilize the score to improve the box regression module. In addition, we take advantage of the PRS algorithm for generating a more accurate pseudo label to train the box regression module. Using these methods, we train the detector on the PASCAL VOC 2007 and 2012 and obtain significantly improved results.



### A Neural Rendering Framework for Free-Viewpoint Relighting
- **Arxiv ID**: http://arxiv.org/abs/1911.11530v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11530v2)
- **Published**: 2019-11-26 13:46:16+00:00
- **Updated**: 2020-06-13 15:55:08+00:00
- **Authors**: Zhang Chen, Anpei Chen, Guli Zhang, Chengyuan Wang, Yu Ji, Kiriakos N. Kutulakos, Jingyi Yu
- **Comment**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020
- **Summary**: We present a novel Relightable Neural Renderer (RNR) for simultaneous view synthesis and relighting using multi-view image inputs. Existing neural rendering (NR) does not explicitly model the physical rendering process and hence has limited capabilities on relighting. RNR instead models image formation in terms of environment lighting, object intrinsic attributes, and light transport function (LTF), each corresponding to a learnable component. In particular, the incorporation of a physically based rendering process not only enables relighting but also improves the quality of view synthesis. Comprehensive experiments on synthetic and real data show that RNR provides a practical and effective solution for conducting free-viewpoint relighting.



### Decoupling Features and Coordinates for Few-shot RGB Relocalization
- **Arxiv ID**: http://arxiv.org/abs/1911.11534v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11534v4)
- **Published**: 2019-11-26 13:57:39+00:00
- **Updated**: 2022-08-16 15:40:00+00:00
- **Authors**: Siyan Dong, Songyin Wu, Yixin Zhuang, Kai Xu, Shanghang Zhang, Baoquan Chen
- **Comment**: This is a very early initialization of a research project and
  contains some out-of-date results and errors. A later version with
  significant improvements has been published as a new paper. See
  arXiv:2208.06933
- **Journal**: None
- **Summary**: Cross-scene model adaption is crucial for camera relocalization in real scenarios. It is often preferable that a pre-learned model can be fast adapted to a novel scene with as few training samples as possible. The existing state-of-the-art approaches, however, can hardly support such few-shot scene adaption due to the entangling of image feature extraction and scene coordinate regression. To address this issue, we approach camera relocalization with a decoupled solution where feature extraction, coordinate regression, and pose estimation are performed separately. Our key insight is that feature encoder used for coordinate regression should be learned by removing the distracting factor of coordinate systems, such that feature encoder is learned from multiple scenes for general feature representation and more important, view-insensitive capability. With this feature prior, and combined with a coordinate regressor, few-shot observations in a new scene are much easier to connect with the 3D world than the one with existing integrated solution. Experiments have shown the superiority of our approach compared to the state-of-the-art methods, producing higher accuracy on several scenes with diverse visual appearance and viewpoint distribution.



### Image2StyleGAN++: How to Edit the Embedded Images?
- **Arxiv ID**: http://arxiv.org/abs/1911.11544v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1911.11544v2)
- **Published**: 2019-11-26 14:08:28+00:00
- **Updated**: 2020-08-07 00:38:59+00:00
- **Authors**: Rameen Abdal, Yipeng Qin, Peter Wonka
- **Comment**: CVPR 2020 " For the video, visit https://youtu.be/yd5WczbFt68 "
- **Journal**: None
- **Summary**: We propose Image2StyleGAN++, a flexible image editing framework with many applications. Our framework extends the recent Image2StyleGAN in three ways. First, we introduce noise optimization as a complement to the $W^+$ latent space embedding. Our noise optimization can restore high-frequency features in images and thus significantly improves the quality of reconstructed images, e.g. a big increase of PSNR from 20 dB to 45 dB. Second, we extend the global $W^+$ latent space embedding to enable local embeddings. Third, we combine embedding with activation tensor manipulation to perform high-quality local edits along with global semantic edits on images. Such edits motivate various high-quality image editing applications, e.g. image reconstruction, image inpainting, image crossover, local style transfer, image editing using scribbles, and attribute level feature transfer. Examples of the edited images are shown across the paper for visual inspection.



### Source Camera Verification from Strongly Stabilized Videos
- **Arxiv ID**: http://arxiv.org/abs/1912.05018v4
- **DOI**: 10.1109/TIFS.2020.3016830
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05018v4)
- **Published**: 2019-11-26 14:30:52+00:00
- **Updated**: 2020-07-22 08:43:28+00:00
- **Authors**: Enes Altinisik, Husrev Taha Sencar
- **Comment**: None
- **Journal**: None
- **Summary**: Image stabilization performed during imaging and/or post-processing poses one of the most significant challenges to photo-response non-uniformity based source camera attribution from videos. When performed digitally, stabilization involves cropping, warping, and inpainting of video frames to eliminate unwanted camera motion. Hence, successful attribution requires the inversion of these transformations in a blind manner. To address this challenge, we introduce a source camera verification method for videos that takes into account the spatially variant nature of stabilization transformations and assumes a larger degree of freedom in their search. Our method identifies transformations at a sub-frame level, incorporates a number of constraints to validate their correctness, and offers computational flexibility in the search for the correct transformation. The method also adopts a holistic approach in countering disruptive effects of other video generation steps, such as video coding and downsizing, for more reliable attribution. Tests performed on one public and two custom datasets show that the proposed method is able to verify the source of 23-30% of all videos that underwent stronger stabilization, depending on computation load, without a significant impact on false attribution.



### DDNet: Dual-path Decoder Network for Occlusion Relationship Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1911.11582v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11582v3)
- **Published**: 2019-11-26 14:47:28+00:00
- **Updated**: 2022-05-11 03:03:02+00:00
- **Authors**: Panhe Feng, Xuejing Kang, Lizhu Ye, Lei Zhu, Chunpeng Li, Anlong Ming
- **Comment**: The new one has been republished as arXiv:2108.05722
- **Journal**: None
- **Summary**: Occlusion relationship reasoning based on convolution neural networks consists of two subtasks: occlusion boundary extraction and occlusion orientation inference. Due to the essential differences between the two subtasks in the feature expression at the higher and lower stages, it is challenging to carry on them simultaneously in one network. To address this issue, we propose a novel Dual-path Decoder Network, which uniformly extracts occlusion information at higher stages and separates into two paths to recover boundary and occlusion orientation respectively in lower stages. Besides, considering the restriction of occlusion orientation presentation to occlusion orientation learning, we design a new orthogonal representation for occlusion orientation and proposed the Orthogonal Orientation Regression loss which can get rid of the unfitness between occlusion representation and learning and further prompt the occlusion orientation learning. Finally, we apply a multi-scale loss together with our proposed orientation regression loss to guide the boundary and orientation path learning respectively. Experiments demonstrate that our proposed method achieves state-of-the-art results on PIOD and BSDS ownership datasets.



### FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization
- **Arxiv ID**: http://arxiv.org/abs/1911.11680v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11680v2)
- **Published**: 2019-11-26 16:29:27+00:00
- **Updated**: 2020-10-02 00:32:02+00:00
- **Authors**: Xi Yin, Ying Tai, Yuge Huang, Xiaoming Liu
- **Comment**: None
- **Journal**: ACCV2020
- **Summary**: This paper studies face recognition (FR) and normalization in surveillance imagery. Surveillance FR is a challenging problem that has great values in law enforcement. Despite recent progress in conventional FR, less effort has been devoted to surveillance FR. To bridge this gap, we propose a Feature Adaptation Network (FAN) to jointly perform surveillance FR and normalization. Our face normalization mainly acts on the aspect of image resolution, closely related to face super-resolution. However, previous face super-resolution methods require paired training data with pixel-to-pixel correspondence, which is typically unavailable between real-world low-resolution and high-resolution faces. FAN can leverage both paired and unpaired data as we disentangle the features into identity and non-identity components and adapt the distribution of the identity features, which breaks the limit of current face super-resolution methods. We further propose a random scale augmentation scheme to learn resolution robust identity features, with advantages over previous fixed scale augmentation. Extensive experiments on LFW, WIDER FACE, QUML-SurvFace and SCface datasets have shown the effectiveness of our method on surveillance FR and normalization.



### Multi-Level Network for High-Speed Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.11686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11686v1)
- **Published**: 2019-11-26 16:42:46+00:00
- **Updated**: 2019-11-26 16:42:46+00:00
- **Authors**: Ying Huang, Jiankai Zhuang, Zengchang Qin
- **Comment**: 5 pages, published at ICIP 2019
- **Journal**: None
- **Summary**: In multi-person pose estimation, the left/right joint type discrimination is always a hard problem because of the similar appearance. Traditionally, we solve this problem by stacking multiple refinement modules to increase network's receptive fields and capture more global context, which can also increase a great amount of computation. In this paper, we propose a Multi-level Network (MLN) that learns to aggregate features from lower-level (left/right information), upper-level (localization information), joint-limb level (complementary information) and global-level (context) information for discrimination of joint type. Through feature reuse and its intra-relation, MLN can attain comparable performance to other conventional methods while runtime speed retains at 42.2 FPS.



### Revisiting Deep Architectures for Head Motion Prediction in 360Â° Videos
- **Arxiv ID**: http://arxiv.org/abs/1911.11702v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11702v3)
- **Published**: 2019-11-26 17:13:00+00:00
- **Updated**: 2021-04-14 16:13:35+00:00
- **Authors**: Miguel Fabian Romero Rondon, Lucile Sassatelli, Ramon Aparicio Pardo, Frederic Precioso
- **Comment**: None
- **Journal**: None
- **Summary**: We consider predicting the user's head motion in 360-degree videos, with 2 modalities only: the past user's positions and the video content (not knowing other users' traces). We make two main contributions. First, we re-examine existing deep-learning approaches for this problem and identify hidden flaws from a thorough root-cause analysis. Second, from the results of this analysis, we design a new proposal establishing state-of-the-art performance. First, re-assessing the existing methods that use both modalities, we obtain the surprising result that they all perform worse than baselines using the user's trajectory only. A root-cause analysis of the metrics, datasets and neural architectures shows in particular that (i) the content can inform the prediction for horizons longer than 2 to 3 sec. (existing methods consider shorter horizons), and that (ii) to compete with the baselines, it is necessary to have a recurrent unit dedicated to process the positions, but this is not sufficient. Second, from a re-examination of the problem supported with the concept of Structural-RNN, we design a new deep neural architecture, named TRACK. TRACK achieves state-of-the-art performance on all considered datasets and prediction horizons, outperforming competitors by up to 20 percent on focus-type videos and horizons 2-5 seconds. The entire framework (codes and datasets) is online and received an ACM reproducibility badge.



### Edge-Guided Occlusion Fading Reduction for a Light-Weighted Self-Supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.11705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11705v1)
- **Published**: 2019-11-26 17:19:19+00:00
- **Updated**: 2019-11-26 17:19:19+00:00
- **Authors**: Kuo-Shiuan Peng, Gregory Ditzler, Jerzy Rozenblit
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised monocular depth estimation methods generally suffer the occlusion fading issue due to the lack of supervision by the per pixel ground truth. Although a post-processing method was proposed by Godard et. al. to reduce the occlusion fading, the compensated results have a severe halo effect. In this paper, we propose a novel Edge-Guided post-processing to reduce the occlusion fading issue for self-supervised monocular depth estimation. We further introduce Atrous Spatial Pyramid Pooling (ASPP) into the network to reduce the computational costs and improve the inference performance. The proposed ASPP-based network is lighter, faster, and better than current commonly used depth estimation networks. This light-weight network only needs 8.1 million parameters and can achieve up to 40 frames per second for $256\times512$ input in the inference stage using a single nVIDIA GTX1080 GPU. The proposed network also outperforms the current state-of-the-art on the KITTI benchmarks. The ASPP-based network and Edge-Guided post-processing produce better results either quantitatively and qualitatively than the competitors.



### Imitation Learning of Robot Policies by Combining Language, Vision and Demonstration
- **Arxiv ID**: http://arxiv.org/abs/1911.11744v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11744v1)
- **Published**: 2019-11-26 18:27:51+00:00
- **Updated**: 2019-11-26 18:27:51+00:00
- **Authors**: Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Chitta Baral, Heni Ben Amor
- **Comment**: Accepted to the NeurIPS 2019 Workshop on Robot Learning: Control and
  Interaction in the Real World, Vancouver, Canada
- **Journal**: None
- **Summary**: In this work we propose a novel end-to-end imitation learning approach which combines natural language, vision, and motion information to produce an abstract representation of a task, which in turn is used to synthesize specific motion controllers at run-time. This multimodal approach enables generalization to a wide variety of environmental conditions and allows an end-user to direct a robot policy through verbal communication. We empirically validate our approach with an extensive set of simulations and show that it achieves a high task success rate over a variety of conditions while remaining amenable to probabilistic interpretability.



### Multi-person Spatial Interaction in a Large Immersive Display Using Smartphones as Touchpads
- **Arxiv ID**: http://arxiv.org/abs/1911.11751v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, H.5.2; H.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1911.11751v1)
- **Published**: 2019-11-26 18:39:24+00:00
- **Updated**: 2019-11-26 18:39:24+00:00
- **Authors**: Gyanendra Sharma, Richard J Radke
- **Comment**: 8 pages with references
- **Journal**: None
- **Summary**: In this paper, we present a multi-user interaction interface for a large immersive space that supports simultaneous screen interactions by combining (1) user input via personal smartphones and Bluetooth microphones, (2) spatial tracking via an overhead array of Kinect sensors, and (3) WebSocket interfaces to a webpage running on the large screen. Users are automatically, dynamically assigned personal and shared screen sub-spaces based on their tracked location with respect to the screen, and use a webpage on their personal smartphone for touchpad-type input. We report user experiments using our interaction framework that involve image selection and placement tasks, with the ultimate goal of realizing display-wall environments as viable, interactive workspaces with natural multimodal interfaces.



### Noise Robust Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.11776v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11776v2)
- **Published**: 2019-11-26 18:42:54+00:00
- **Updated**: 2020-03-31 16:54:37+00:00
- **Authors**: Takuhiro Kaneko, Tatsuya Harada
- **Comment**: Accepted to CVPR 2020. Project page:
  https://takuhirok.github.io/NR-GAN/
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are neural networks that learn data distributions through adversarial training. In intensive studies, recent GANs have shown promising results for reproducing training images. However, in spite of noise, they reproduce images with fidelity. As an alternative, we propose a novel family of GANs called noise robust GANs (NR-GANs), which can learn a clean image generator even when training images are noisy. In particular, NR-GANs can solve this problem without having complete noise information (e.g., the noise distribution type, noise amount, or signal-noise relationship). To achieve this, we introduce a noise generator and train it along with a clean image generator. However, without any constraints, there is no incentive to generate an image and noise separately. Therefore, we propose distribution and transformation constraints that encourage the noise generator to capture only the noise-specific components. In particular, considering such constraints under different assumptions, we devise two variants of NR-GANs for signal-independent noise and three variants of NR-GANs for signal-dependent noise. On three benchmark datasets, we demonstrate the effectiveness of NR-GANs in noise robust image generation. Furthermore, we show the applicability of NR-GANs in image denoising. Our code is available at https://github.com/takuhirok/NR-GAN/.



### MixNMatch: Multifactor Disentanglement and Encoding for Conditional Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1911.11758v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11758v3)
- **Published**: 2019-11-26 18:49:39+00:00
- **Updated**: 2020-04-13 17:56:13+00:00
- **Authors**: Yuheng Li, Krishna Kumar Singh, Utkarsh Ojha, Yong Jae Lee
- **Comment**: CVPR 2020 camera ready
- **Journal**: CVPR 2020
- **Summary**: We present MixNMatch, a conditional generative model that learns to disentangle and encode background, object pose, shape, and texture from real images with minimal supervision, for mix-and-match image generation. We build upon FineGAN, an unconditional generative model, to learn the desired disentanglement and image generator, and leverage adversarial joint image-code distribution matching to learn the latent factor encoders. MixNMatch requires bounding boxes during training to model background, but requires no other supervision. Through extensive experiments, we demonstrate MixNMatch's ability to accurately disentangle, encode, and combine multiple factors for mix-and-match image generation, including sketch2color, cartoon2img, and img2gif applications. Our code/models/demo can be found at https://github.com/Yuheng-Li/MixNMatch



### Password-conditioned Anonymization and Deanonymization with Face Identity Transformers
- **Arxiv ID**: http://arxiv.org/abs/1911.11759v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11759v4)
- **Published**: 2019-11-26 18:50:53+00:00
- **Updated**: 2020-09-30 15:52:50+00:00
- **Authors**: Xiuye Gu, Weixin Luo, Michael S. Ryoo, Yong Jae Lee
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Cameras are prevalent in our daily lives, and enable many useful systems built upon computer vision technologies such as smart cameras and home robots for service applications. However, there is also an increasing societal concern as the captured images/videos may contain privacy-sensitive information (e.g., face identity). We propose a novel face identity transformer which enables automated photo-realistic password-based anonymization as well as deanonymization of human faces appearing in visual data. Our face identity transformer is trained to (1) remove face identity information after anonymization, (2) make the recovery of the original face possible when given the correct password, and (3) return a wrong--but photo-realistic--face given a wrong password. Extensive experiments show that our approach enables multimodal password-conditioned face anonymizations and deanonymizations, without sacrificing privacy compared to existing anonymization approaches.



### SuperGlue: Learning Feature Matching with Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.11763v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11763v2)
- **Published**: 2019-11-26 18:57:21+00:00
- **Updated**: 2020-03-28 16:49:25+00:00
- **Authors**: Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich
- **Comment**: Oral at CVPR 2020, with appendix and link to publicly available code
- **Journal**: None
- **Summary**: This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at https://github.com/magicleap/SuperGluePretrainedNetwork.



### ViewAL: Active Learning with Viewpoint Entropy for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.11789v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11789v2)
- **Published**: 2019-11-26 19:00:16+00:00
- **Updated**: 2020-03-18 18:00:31+00:00
- **Authors**: Yawar Siddiqui, Julien Valentin, Matthias NieÃner
- **Comment**: CVPR2020, Video: https://youtu.be/tAGdx2j-X_g
- **Journal**: None
- **Summary**: We propose ViewAL, a novel active learning strategy for semantic segmentation that exploits viewpoint consistency in multi-view datasets. Our core idea is that inconsistencies in model predictions across viewpoints provide a very reliable measure of uncertainty and encourage the model to perform well irrespective of the viewpoint under which objects are observed. To incorporate this uncertainty measure, we introduce a new viewpoint entropy formulation, which is the basis of our active learning strategy. In addition, we propose uncertainty computations on a superpixel level, which exploits inherently localized signal in the segmentation task, directly lowering the annotation costs. This combination of viewpoint entropy and the use of superpixels allows to efficiently select samples that are highly informative for improving the network. We demonstrate that our proposed active learning strategy not only yields the best-performing models for the same amount of required labeled data, but also significantly reduces labeling effort. For instance, our method achieves 95% of maximum achievable network performance using only 7%, 17%, and 24% labeled data on SceneNet-RGBD, ScanNet, and Matterport3D, respectively. On these datasets, the best state-of-the-art method achieves the same performance with 14%, 27% and 33% labeled data. Finally, we demonstrate that labeling using superpixels yields the same quality of ground-truth compared to labeling whole images, but requires 25% less time.



### Multi-Object Portion Tracking in 4D Fluorescence Microscopy Imagery with Deep Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/1911.11808v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11808v1)
- **Published**: 2019-11-26 19:46:41+00:00
- **Updated**: 2019-11-26 19:46:41+00:00
- **Authors**: Yang Jiao, Mo Weng, Mei Yang
- **Comment**: 10 pages, 8 figures, CVPR 2019
- **Journal**: None
- **Summary**: 3D fluorescence microscopy of living organisms has increasingly become an essential and powerful tool in biomedical research and diagnosis. An exploding amount of imaging data has been collected, whereas efficient and effective computational tools to extract information from them are still lagging behind. This is largely due to the challenges in analyzing biological data. Interesting biological structures are not only small, but are often morphologically irregular and highly dynamic. Although tracking cells in live organisms has been studied for years, existing tracking methods for cells are not effective in tracking subcellular structures, such as protein complexes, which feature in continuous morphological changes including split and merge, in addition to fast migration and complex motion. In this paper, we first define the problem of multi-object portion tracking to model the protein object tracking process. A multi-object tracking method with portion matching is proposed based on 3D segmentation results. The proposed method distills deep feature maps from deep networks, then recognizes and matches object portions using an extended search. Experimental results confirm that the proposed method achieves 2.96% higher on consistent tracking accuracy and 35.48% higher on event identification accuracy than the state-of-art methods.



### Deep Template-based Object Instance Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.11822v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11822v3)
- **Published**: 2019-11-26 20:38:26+00:00
- **Updated**: 2020-11-15 03:26:20+00:00
- **Authors**: Jean-Philippe Mercier, Mathieu Garon, Philippe GiguÃ¨re, Jean-FranÃ§ois Lalonde
- **Comment**: None
- **Journal**: None
- **Summary**: Much of the focus in the object detection literature has been on the problem of identifying the bounding box of a particular class of object in an image. Yet, in contexts such as robotics and augmented reality, it is often necessary to find a specific object instance---a unique toy or a custom industrial part for example---rather than a generic object class. Here, applications can require a rapid shift from one object instance to another, thus requiring fast turnaround which affords little-to-no training time. What is more, gathering a dataset and training a model for every new object instance to be detected can be an expensive and time-consuming process. In this context, we propose a generic 2D object instance detection approach that uses example viewpoints of the target object at test time to retrieve its 2D location in RGB images, without requiring any additional training (i.e. fine-tuning) step. To this end, we present an end-to-end architecture that extracts global and local information of the object from its viewpoints. The global information is used to tune early filters in the backbone while local viewpoints are correlated with the input image. Our method offers an improvement of almost 30 mAP over the previous template matching methods on the challenging Occluded Linemod dataset (overall mAP of 50.7). Our experiments also show that our single generic model (not trained on any of the test objects) yields detection results that are on par with approaches that are trained specifically on the target objects.



### Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation
- **Arxiv ID**: http://arxiv.org/abs/1911.11834v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11834v2)
- **Published**: 2019-11-26 21:02:30+00:00
- **Updated**: 2020-04-02 12:56:20+00:00
- **Authors**: Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, Olga Russakovsky
- **Comment**: To appear in CVPR 2020
- **Journal**: None
- **Summary**: Computer vision models learn to perform a task by capturing relevant statistics from training data. It has been shown that models learn spurious age, gender, and race correlations when trained for seemingly unrelated tasks like activity recognition or image captioning. Various mitigation techniques have been presented to prevent models from utilizing or learning such biases. However, there has been little systematic comparison between these techniques. We design a simple but surprisingly effective visual recognition benchmark for studying bias mitigation. Using this benchmark, we provide a thorough analysis of a wide range of techniques. We highlight the shortcomings of popular adversarial training approaches for bias mitigation, propose a simple but similarly effective alternative to the inference-time Reducing Bias Amplification method of Zhao et al., and design a domain-independent training technique that outperforms all other methods. Finally, we validate our findings on the attribute classification task in the CelebA dataset, where attribute presence is known to be correlated with the gender of people in the image, and demonstrate that the proposed technique is effective at mitigating real-world gender bias.



### Compressed MRI Reconstruction Exploiting a Rotation-Invariant Total Variation Discretization
- **Arxiv ID**: http://arxiv.org/abs/1911.11854v5
- **DOI**: 10.1016/j.mri.2020.03.008
- **Categories**: **eess.IV**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1911.11854v5)
- **Published**: 2019-11-26 22:05:21+00:00
- **Updated**: 2020-04-21 07:03:20+00:00
- **Authors**: Erfan Ebrahim Esfahani, Alireza Hosseini
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the first-order method of Malitsky and Pock, we propose a new variational framework for compressed MR image reconstruction which introduces the application of a rotation-invariant discretization of total variation functional into MR imaging while exploiting BM3D frame as a sparsifying transform. In the first step, we provide theoretical and numerical analysis establishing the exceptional rotation-invariance property of this total variation functional and observe its superiority over other well-known variational regularization terms in both upright and rotated imaging setups. Thereupon, the proposed MRI reconstruction model is presented as a constrained optimization problem, however, we do not use conventional ADMM-type algorithms designed for constrained problems to obtain a solution, but rather we tailor the linesearch-equipped method of Malitsky and Pock to our model, which was originally proposed for unconstrained problems. As attested by numerical experiments, this framework significantly outperforms various state-of-the-art algorithms from variational methods to adaptive and learning approaches and in particular, it eliminates the stagnating behavior of a previous work on BM3D-MRI which compromised the solution beyond a certain iteration.



### Artificial Intelligence-Based Image Classification for Diagnosis of Skin Cancer: Challenges and Opportunities
- **Arxiv ID**: http://arxiv.org/abs/1911.11872v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1911.11872v3)
- **Published**: 2019-11-26 22:47:34+00:00
- **Updated**: 2020-06-20 18:17:48+00:00
- **Authors**: Manu Goyal, Thomas Knackstedt, Shaofeng Yan, Saeed Hassanpour
- **Comment**: AI Skin Cancer
- **Journal**: None
- **Summary**: Recently, there has been great interest in developing Artificial Intelligence (AI) enabled computer-aided diagnostics solutions for the diagnosis of skin cancer. With the increasing incidence of skin cancers, low awareness among a growing population, and a lack of adequate clinical expertise and services, there is an immediate need for AI systems to assist clinicians in this domain. A large number of skin lesion datasets are available publicly, and researchers have developed AI-based image classification solutions, particularly deep learning algorithms, to distinguish malignant skin lesions from benign lesions in different image modalities such as dermoscopic, clinical, and histopathology images. Despite the various claims of AI systems achieving higher accuracy than dermatologists in the classification of different skin lesions, these AI systems are still in the very early stages of clinical application in terms of being ready to aid clinicians in the diagnosis of skin cancers. In this review, we discuss advancements in the digital image-based AI solutions for the diagnosis of skin cancer, along with some challenges and future opportunities to improve these AI systems to support dermatologists and enhance their ability to diagnose skin cancer.




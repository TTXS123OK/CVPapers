# Arxiv Papers in cs.CV on 2019-11-12
### SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.04623v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04623v2)
- **Published**: 2019-11-12 00:44:10+00:00
- **Updated**: 2019-11-16 00:35:54+00:00
- **Authors**: Yan Wang, Wei-Lun Chao, Kilian Q. Weinberger, Laurens van der Maaten
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learners aim to recognize new object classes based on a small number of labeled training examples. To prevent overfitting, state-of-the-art few-shot learners use meta-learning on convolutional-network features and perform classification using a nearest-neighbor classifier. This paper studies the accuracy of nearest-neighbor baselines without meta-learning. Surprisingly, we find simple feature transformations suffice to obtain competitive few-shot learning accuracies. For example, we find that a nearest-neighbor classifier used in combination with mean-subtraction and L2-normalization outperforms prior results in three out of five settings on the miniImageNet dataset.



### Predicting Landslides Using Locally Aligned Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.04651v5
- **DOI**: 10.24963/ijcai.2020/462
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.04651v5)
- **Published**: 2019-11-12 03:21:53+00:00
- **Updated**: 2020-07-17 18:13:17+00:00
- **Authors**: Ainaz Hajimoradlou, Gioachino Roberti, David Poole
- **Comment**: Published in IJCAI 2020
- **Journal**: None
- **Summary**: Landslides, movement of soil and rock under the influence of gravity, are common phenomena that cause significant human and economic losses every year. Experts use heterogeneous features such as slope, elevation, land cover, lithology, rock age, and rock family to predict landslides. To work with such features, we adapted convolutional neural networks to consider relative spatial information for the prediction task. Traditional filters in these networks either have a fixed orientation or are rotationally invariant. Intuitively, the filters should orient uphill, but there is not enough data to learn the concept of uphill; instead, it can be provided as prior knowledge. We propose a model called Locally Aligned Convolutional Neural Network, LACNN, that follows the ground surface at multiple scales to predict possible landslide occurrence for a single point. To validate our method, we created a standardized dataset of georeferenced images consisting of the heterogeneous features as inputs, and compared our method to several baselines, including linear regression, a neural network, and a convolutional network, using log-likelihood error and Receiver Operating Characteristic curves on the test set. Our model achieves 2-7% improvement in terms of accuracy and 2-15% boost in terms of log likelihood compared to the other proposed baselines.



### Equalization Loss for Large Vocabulary Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.04692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04692v1)
- **Published**: 2019-11-12 06:04:35+00:00
- **Updated**: 2019-11-12 06:04:35+00:00
- **Authors**: Jingru Tan, Changbao Wang, Quanquan Li, Junjie Yan
- **Comment**: Technical Report. Winner of LVIS Challenge 2019
- **Journal**: None
- **Summary**: Recent object detection and instance segmentation tasks mainly focus on datasets with a relatively small set of categories, e.g. Pascal VOC with 20 classes and COCO with 80 classes. The new large vocabulary dataset LVIS brings new challenges to conventional methods. In this work, we propose an equalization loss to solve the long tail of rare categories problem. Combined with exploiting the data from detection datasets to alleviate the effect of missing-annotation problems during the training, our method achieves 5.1\% overall AP gain and 11.4\% AP gain of rare categories on LVIS benchmark without any bells and whistles compared to Mask R-CNN baseline. Finally we achieve 28.9 mask AP on the test-set of the LVIS and rank 1st place in LVIS Challenge 2019.



### WaveletKernelNet: An Interpretable Deep Neural Network for Industrial Intelligent Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1911.07925v3
- **DOI**: 10.1109/TSMC.2020.3048950
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1911.07925v3)
- **Published**: 2019-11-12 07:22:56+00:00
- **Updated**: 2020-05-06 04:37:47+00:00
- **Authors**: Tianfu Li, Zhibin Zhao, Chuang Sun, Li Cheng, Xuefeng Chen, Ruqiang Yan, Robert X. Gao
- **Comment**: None
- **Journal**: IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol.
  52, no. 4, pp. 2302-2312, April 2022
- **Summary**: Convolutional neural network (CNN), with ability of feature learning and nonlinear mapping, has demonstrated its effectiveness in prognostics and health management (PHM). However, explanation on the physical meaning of a CNN architecture has rarely been studied. In this paper, a novel wavelet driven deep neural network termed as WaveletKernelNet (WKN) is presented, where a continuous wavelet convolutional (CWConv) layer is designed to replace the first convolutional layer of the standard CNN. This enables the first CWConv layer to discover more meaningful filters. Furthermore, only the scale parameter and translation parameter are directly learned from raw data at this CWConv layer. This provides a very effective way to obtain a customized filter bank, specifically tuned for extracting defect-related impact component embedded in the vibration signal. In addition, three experimental verification using data from laboratory environment are carried out to verify effectiveness of the proposed method for mechanical fault diagnosis. The results show the importance of the designed CWConv layer and the output of CWConv layer is interpretable. Besides, it is found that WKN has fewer parameters, higher fault classification accuracy and faster convergence speed than standard CNN.



### Data-Free Point Cloud Network for 3D Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.04731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04731v1)
- **Published**: 2019-11-12 08:18:30+00:00
- **Updated**: 2019-11-12 08:18:30+00:00
- **Authors**: Ziyu Zhang, Feipeng Da, Yi Yu
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Point clouds-based Networks have achieved great attention in 3D object classification, segmentation and indoor scene semantic parsing. In terms of face recognition, 3D face recognition method which directly consume point clouds as input is still under study. Two main factors account for this: One is how to get discriminative face representations from 3D point clouds using deep network; the other is the lack of large 3D training dataset. To address these problems, a data-free 3D face recognition method is proposed only using synthesized unreal data from statistical 3D Morphable Model to train a deep point cloud network. To ease the inconsistent distribution between model data and real faces, different point sampling methods are used in train and test phase. In this paper, we propose a curvature-aware point sampling(CPS) strategy replacing the original furthest point sampling(FPS) to hierarchically down-sample feature-sensitive points which are crucial to pass and aggregate features deeply. A PointNet++ like Network is used to extract face features directly from point clouds. The experimental results show that the network trained on generated data generalizes well for real 3D faces. Fine tuning on a small part of FRGCv2.0 and Bosphorus, which include real faces in different poses and expressions, further improves recognition accuracy.



### Merging-ISP: Multi-Exposure High Dynamic Range Image Signal Processing
- **Arxiv ID**: http://arxiv.org/abs/1911.04762v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.04762v2)
- **Published**: 2019-11-12 09:53:50+00:00
- **Updated**: 2021-10-04 14:09:22+00:00
- **Authors**: Prashant Chaudhari, Franziska Schirrmacher, Andreas Maier, Christian Riess, Thomas Köhler
- **Comment**: Computational Photography, DAGM GCPR 2021
- **Journal**: None
- **Summary**: High dynamic range (HDR) imaging combines multiple images with different exposure times into a single high-quality image. The image signal processing pipeline (ISP) is a core component in digital cameras to perform these operations. It includes demosaicing of raw color filter array (CFA) data at different exposure times, alignment of the exposures, conversion to HDR domain, and exposure merging into an HDR image. Traditionally, such pipelines cascade algorithms that address these individual subtasks. However, cascaded designs suffer from error propagation, since simply combining multiple steps is not necessarily optimal for the entire imaging task. This paper proposes a multi-exposure HDR image signal processing pipeline (Merging-ISP) to jointly solve all these subtasks. Our pipeline is modeled by a deep neural network architecture. As such, it is end-to-end trainable, circumvents the use of hand-crafted and potentially complex algorithms, and mitigates error propagation. Merging-ISP enables direct reconstructions of HDR images of dynamic scenes from multiple raw CFA images with different exposures. We compare Merging-ISP against several state-of-the-art cascaded pipelines. The proposed method provides HDR reconstructions of high perceptual quality and it quantitatively outperforms competing ISPs by more than 1 dB in terms of PSNR.



### VITON-GAN: Virtual Try-on Image Generator Trained with Adversarial Loss
- **Arxiv ID**: http://arxiv.org/abs/1911.07926v1
- **DOI**: 10.2312/egp.20191043
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07926v1)
- **Published**: 2019-11-12 10:09:11+00:00
- **Updated**: 2019-11-12 10:09:11+00:00
- **Authors**: Shion Honda
- **Comment**: 2 pages, 4 figures. Accepted to Eurographics 2019 (Posters)
- **Journal**: Eurographics, 2019
- **Summary**: Generating a virtual try-on image from in-shop clothing images and a model person's snapshot is a challenging task because the human body and clothes have high flexibility in their shapes. In this paper, we develop a Virtual Try-on Generative Adversarial Network (VITON-GAN), that generates virtual try-on images using images of in-shop clothing and a model person. This method enhances the quality of the generated image when occlusion is present in a model person's image (e.g., arms crossed in front of the clothes) by adding an adversarial mechanism in the training pipeline.



### Grouping Capsules Based Different Types
- **Arxiv ID**: http://arxiv.org/abs/1911.04820v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.04820v1)
- **Published**: 2019-11-12 12:39:20+00:00
- **Updated**: 2019-11-12 12:39:20+00:00
- **Authors**: Qiang Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule network was introduced as a new architecture of neural networks, it encoding features as capsules to overcome the lacking of equivariant in the convolutional neural networks. It uses dynamic routing algorithm to train parameters in different capsule layers, but the dynamic routing algorithm need to be improved. In this paper, we propose a novel capsule network architecture and discussed the effect of initialization method of the coupling coefficient $c_{ij}$ on the model. First, we analyze the rate of change of the initial value of $c_{ij}$ when the dynamic routing algorithm iterates. The larger the initial value of $c_{ij}$, the better effect of the model. Then, we proposed improvement that training different types of capsules by grouping capsules based different types. And this improvement can adjust the initial value of $c_{ij}$ to make it more suitable. We experimented with our improvements on some computer vision datasets and achieved better results than the original capsule network



### Recognizing Facial Expressions of Occluded Faces using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.04852v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.04852v1)
- **Published**: 2019-11-12 13:53:56+00:00
- **Updated**: 2019-11-12 13:53:56+00:00
- **Authors**: Mariana-Iuliana Georgescu, Radu Tudor Ionescu
- **Comment**: Accepted at ICONIP 2019
- **Journal**: None
- **Summary**: In this paper, we present an approach based on convolutional neural networks (CNNs) for facial expression recognition in a difficult setting with severe occlusions. More specifically, our task is to recognize the facial expression of a person wearing a virtual reality (VR) headset which essentially occludes the upper part of the face. In order to accurately train neural networks for this setting, in which faces are severely occluded, we modify the training examples by intentionally occluding the upper half of the face. This forces the neural networks to focus on the lower part of the face and to obtain better accuracy rates than models trained on the entire faces. Our empirical results on two benchmark data sets, FER+ and AffectNet, show that our CNN models' predictions on lower-half faces are up to 13% higher than the baseline CNN models trained on entire faces, proving their suitability for the VR setting. Furthermore, our models' predictions on lower-half faces are no more than 10% under the baseline models' predictions on full faces, proving that there are enough clues in the lower part of the face to accurately predict facial expressions.



### Time-Dynamic Estimates of the Reliability of Deep Semantic Segmentation Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.05075v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05075v2)
- **Published**: 2019-11-12 13:55:50+00:00
- **Updated**: 2020-10-05 09:37:54+00:00
- **Authors**: Kira Maag, Matthias Rottmann, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: In the semantic segmentation of street scenes with neural networks, the reliability of predictions is of highest interest. The assessment of neural networks by means of uncertainties is a common ansatz to prevent safety issues. As in applications like automated driving, video streams of images are available, we present a time-dynamic approach to investigating uncertainties and assessing the prediction quality of neural networks. We track segments over time and gather aggregated metrics per segment, thus obtaining time series of metrics from which we assess prediction quality. This is done by either classifying between intersection over union equal to 0 and greater than 0 or predicting the intersection over union directly. We study different models for these two tasks and analyze the influence of the time series length on the predictive power of our metrics.



### Visual Dialogue State Tracking for Question Generation
- **Arxiv ID**: http://arxiv.org/abs/1911.07928v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07928v2)
- **Published**: 2019-11-12 15:54:55+00:00
- **Updated**: 2019-11-25 03:32:28+00:00
- **Authors**: Wei Pang, Xiaojie Wang
- **Comment**: 8 pages, 4 figures, Accept-Oral by AAAI-2020
- **Journal**: None
- **Summary**: GuessWhat?! is a visual dialogue task between a guesser and an oracle. The guesser aims to locate an object supposed by the oracle oneself in an image by asking a sequence of Yes/No questions. Asking proper questions with the progress of dialogue is vital for achieving successful final guess. As a result, the progress of dialogue should be properly represented and tracked. Previous models for question generation pay less attention on the representation and tracking of dialogue states, and therefore are prone to asking low quality questions such as repeated questions. This paper proposes visual dialogue state tracking (VDST) based method for question generation. A visual dialogue state is defined as the distribution on objects in the image as well as representations of objects. Representations of objects are updated with the change of the distribution on objects. An object-difference based attention is used to decode new question. The distribution on objects is updated by comparing the question-answer pair and objects. Experimental results on GuessWhat?! dataset show that our model significantly outperforms existing methods and achieves new state-of-the-art performance. It is also noticeable that our model reduces the rate of repeated questions from more than 50% to 21.9% compared with previous state-of-the-art methods.



### Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning
- **Arxiv ID**: http://arxiv.org/abs/1911.04967v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.04967v1)
- **Published**: 2019-11-12 15:58:23+00:00
- **Updated**: 2019-11-12 15:58:23+00:00
- **Authors**: Louis D. van Harten, Jelmer M. Wolterink, Joost J. C. Verhoeff, Ivana Išgum
- **Comment**: SPIE Medical Imaging 2020
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been widely and successfully used for medical image segmentation. However, CNNs are typically considered to require large numbers of dedicated expert-segmented training volumes, which may be limiting in practice. This work investigates whether clinically obtained segmentations which are readily available in picture archiving and communication systems (PACS) could provide a possible source of data to train a CNN for segmentation of organs-at-risk (OARs) in radiotherapy treatment planning. In such data, delineations of structures deemed irrelevant to the target clinical use may be lacking. To overcome this issue, we use multi-label instead of multi-class segmentation. We empirically assess how many clinical delineations would be sufficient to train a CNN for the segmentation of OARs and find that increasing the training set size beyond a limited number of images leads to sharply diminishing returns. Moreover, we find that by using multi-label segmentation, missing structures in the reference standard do not have a negative effect on overall segmentation accuracy. These results indicate that segmentations obtained in a clinical workflow can be used to train an accurate OAR segmentation model.



### Deep-Aligned Convolutional Neural Network for Skeleton-based Action Recognition and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.04969v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.04969v1)
- **Published**: 2019-11-12 16:00:56+00:00
- **Updated**: 2019-11-12 16:00:56+00:00
- **Authors**: Babak Hosseini, Romain Montagne, Barbara Hammer
- **Comment**: 19th IEEE International Conference on Data Mining (ICDM 2019)
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are deep learning frameworks which are well-known for their notable performance in classification tasks. Hence, many skeleton-based action recognition and segmentation (SBARS) algorithms benefit from them in their designs. However, a shortcoming of such applications is the general lack of spatial relationships between the input features in such data types. Besides, non-uniform temporal scalings is a common issue in skeleton-based data streams which leads to having different input sizes even within one specific action category. In this work, we propose a novel deep-aligned convolutional neural network (DACNN) to tackle the above challenges for the particular problem of SBARS. Our network is designed by introducing a new type of filters in the context of CNNs which are trained based on their alignments to the local subsequences in the inputs. These filters result in efficient predictions as well as learning interpretable patterns in the data. We empirically evaluate our framework on real-world benchmarks showing that the proposed DACNN algorithm obtains a competitive performance compared to the state-of-the-art while benefiting from a less complicated yet more interpretable model.



### Deep Variational Semi-Supervised Novelty Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.04971v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.04971v3)
- **Published**: 2019-11-12 16:03:50+00:00
- **Updated**: 2021-11-04 08:18:47+00:00
- **Authors**: Tal Daniel, Thanard Kurutach, Aviv Tamar
- **Comment**: NeurIPS 2021 Workshop on DGMs and Downstream Applications
- **Journal**: None
- **Summary**: In anomaly detection (AD), one seeks to identify whether a test sample is abnormal, given a data set of normal samples. A recent and promising approach to AD relies on deep generative models, such as variational autoencoders (VAEs), for unsupervised learning of the normal data distribution. In semi-supervised AD (SSAD), the data also includes a small sample of labeled anomalies. In this work, we propose two variational methods for training VAEs for SSAD. The intuitive idea in both methods is to train the encoder to `separate' between latent vectors for normal and outlier data. We show that this idea can be derived from principled probabilistic formulations of the problem, and propose simple and effective algorithms. Our methods can be applied to various data types, as we demonstrate on SSAD datasets ranging from natural images to astronomy and medicine, can be combined with any VAE model architecture, and are naturally compatible with ensembling. When comparing to state-of-the-art SSAD methods that are not specific to particular data types, we obtain marked improvement in outlier detection.



### Multi-hop Convolutions on Weighted Graphs
- **Arxiv ID**: http://arxiv.org/abs/1911.04978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.04978v1)
- **Published**: 2019-11-12 16:08:22+00:00
- **Updated**: 2019-11-12 16:08:22+00:00
- **Authors**: Qikui Zhu, Bo Du, Pingkun Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Graph Convolutional Networks (GCNs) have made significant advances in semi-supervised learning, especially for classification tasks. However, existing GCN based methods have two main drawbacks. First, to increase the receptive field and improve the representation capability of GCNs, larger kernels or deeper network architectures are used, which greatly increases the computational complexity and the number of parameters. Second, methods working on higher order graphs computed directly from adjacency matrices may alter the relationship between graph nodes, particularly for weighted graphs. In addition, the direct construction of higher-order graphs introduces redundant information, which may result in lower network performance. To address the above weaknesses, in this paper, we propose a new method of multi-hop convolutional network on weighted graphs. The proposed method consists of multiple convolutional branches, where each branch extracts node representation from a $k$-hop graph with small kernels. Such design systematically aggregates multi-scale contextual information without adding redundant information. Furthermore, to efficiently combine the extracted information from the multi-hop branches, an adaptive weight computation (AWC) layer is proposed. We demonstrate the superiority of our MultiHop in six publicly available datasets, including three citation network datasets and three medical image datasets. The experimental results show that our proposed MultiHop method achieves the highest classification accuracy and outperforms the state-of-the-art methods. The source code of this work have been released on GitHub (https://github.com/ahukui/Multi-hop-Convolutions-on-Weighted-Graphs).



### Automatic Online Quality Control of Synthetic CTs
- **Arxiv ID**: http://arxiv.org/abs/1911.04986v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.04986v1)
- **Published**: 2019-11-12 16:19:20+00:00
- **Updated**: 2019-11-12 16:19:20+00:00
- **Authors**: Louis D. van Harten, Jelmer M. Wolterink, Joost J. C. Verhoeff, Ivana Išgum
- **Comment**: SPIE Medical Imaging 2020
- **Journal**: None
- **Summary**: Accurate MR-to-CT synthesis is a requirement for MR-only workflows in radiotherapy (RT) treatment planning. In recent years, deep learning-based approaches have shown impressive results in this field. However, to prevent downstream errors in RT treatment planning, it is important that deep learning models are only applied to data for which they are trained and that generated synthetic CT (sCT) images do not contain severe errors. For this, a mechanism for online quality control should be in place. In this work, we use an ensemble of sCT generators and assess their disagreement as a measure of uncertainty of the results. We show that this uncertainty measure can be used for two kinds of online quality control. First, to detect input images that are outside the expected distribution of MR images. Second, to identify sCT images that were generated from suitable MR images but potentially contain errors. Such automatic online quality control for sCT generation is likely to become an integral part of MR-only RT workflows.



### Recursive Filter for Space-Variant Variance Reduction
- **Arxiv ID**: http://arxiv.org/abs/1911.04992v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, eess.IV, eess.SP, math.NA, I.4.3; I.4.4; G.3
- **Links**: [PDF](http://arxiv.org/pdf/1911.04992v2)
- **Published**: 2019-11-12 16:28:45+00:00
- **Updated**: 2019-11-13 19:09:51+00:00
- **Authors**: Alexander Zamyatin
- **Comment**: 12 pages, 13 figures
- **Journal**: None
- **Summary**: We propose a method to reduce non-uniform sample variance to a predetermined target level. The proposed space-variant filter can equalize variance of the non-stationary signal, or vary filtering strength based on image features, such as edges, etc., as shown by applications in this work. This approach computes variance reduction ratio at each point of the image, based on the given target variance. Then, a space-variant filter with matching variance reduction power is applied. A mathematical framework of atomic kernels is developed to facilitate stable and fast computation of the filter bank kernels. Recursive formulation allows using small kernel size, which makes the space-variant filter more suitable for fast parallel implementation. Despite the small kernel size, the recursive filter possesses strong variance reduction power. Filter accuracy is measured by the variance reduction against the target variance; testing demonstrated high accuracy of variance reduction of the recursive filter compared to the fixed-size filter. The proposed filter was applied to adaptive filtering in image reconstruction and edge-preserving denoising.



### Pose Guided Attention for Multi-label Fashion Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.05024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05024v1)
- **Published**: 2019-11-12 17:32:53+00:00
- **Updated**: 2019-11-12 17:32:53+00:00
- **Authors**: Beatriz Quintino Ferreira, João P. Costeira, Ricardo G. Sousa, Liang-Yan Gui, João P. Gomes
- **Comment**: Published at ICCV 2019 Workshop on Computer Vision for Fashion, Art
  and Design
- **Journal**: None
- **Summary**: We propose a compact framework with guided attention for multi-label classification in the fashion domain. Our visual semantic attention model (VSAM) is supervised by automatic pose extraction creating a discriminative feature space. VSAM outperforms the state of the art for an in-house dataset and performs on par with previous works on the DeepFashion dataset, even without using any landmark annotations. Additionally, we show that our semantic attention module brings robustness to large quantities of wrong annotations and provides more interpretable results.



### Visual cryptography in single-pixel imaging
- **Arxiv ID**: http://arxiv.org/abs/1911.05033v1
- **DOI**: 10.1364/OE.383240
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.05033v1)
- **Published**: 2019-11-12 17:49:50+00:00
- **Updated**: 2019-11-12 17:49:50+00:00
- **Authors**: Shuming Jiao, Jun Feng, Yang Gao, Ting Lei, Xiaocong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Two novel visual cryptography (VC) schemes are proposed by combining VC with single-pixel imaging (SPI) for the first time. It is pointed out that the overlapping of visual key images in VC is similar to the superposition of pixel intensities by a single-pixel detector in SPI. In the first scheme, QR-code VC is designed by using opaque sheets instead of transparent sheets. The secret image can be recovered when identical illumination patterns are projected onto multiple visual key images and a single detector is used to record the total light intensities. In the second scheme, the secret image is shared by multiple illumination pattern sequences and it can be recovered when the visual key patterns are projected onto identical items. The application of VC can be extended to more diversified scenarios by our proposed schemes.



### Trainable Spectrally Initializable Matrix Transformations in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.05045v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.05045v2)
- **Published**: 2019-11-12 18:06:52+00:00
- **Updated**: 2019-11-13 17:36:08+00:00
- **Authors**: Michele Alberti, Angela Botros, Narayan Schuez, Rolf Ingold, Marcus Liwicki, Mathias Seuret
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: In this work, we investigate the application of trainable and spectrally initializable matrix transformations on the feature maps produced by convolution operations. While previous literature has already demonstrated the possibility of adding static spectral transformations as feature processors, our focus is on more general trainable transforms. We study the transforms in various architectural configurations on four datasets of different nature: from medical (ColorectalHist, HAM10000) and natural (Flowers, ImageNet) images to historical documents (CB55) and handwriting recognition (GPDS). With rigorous experiments that control for the number of parameters and randomness, we show that networks utilizing the introduced matrix transformations outperform vanilla neural networks. The observed accuracy increases by an average of 2.2 across all datasets. In addition, we show that the benefit of spectral initialization leads to significantly faster convergence, as opposed to randomly initialized matrix transformations. The transformations are implemented as auto-differentiable PyTorch modules that can be incorporated into any neural network architecture. The entire code base is open-source.



### A convolutional neural network reaches optimal sensitivity for detecting some, but not all, patterns
- **Arxiv ID**: http://arxiv.org/abs/1911.05055v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05055v3)
- **Published**: 2019-11-12 18:26:02+00:00
- **Updated**: 2020-07-09 17:49:59+00:00
- **Authors**: Fabian H. Reith, Brian A. Wandell
- **Comment**: 22 pages, 8 figures, pre-print
- **Journal**: None
- **Summary**: We investigate the performance of modern convolutional neural networks (CNN) and a linear support vector machine (SVM) with respect to spatial contrast sensitivity. Specifically, we compare CNN sensitivity to that of a Bayesian ideal observer (IO) with the signal-known-exactly and noise known statistically. A ResNet-18 reaches optimal performance for harmonic patterns, as well as several classes of real world signals including faces. For these stimuli the CNN substantially outperforms the SVM. We further analyzed the case in which the signal might appear in one of multiple locations and found that CNN spatial sensitivity continues to match the IO. However, the CNN sensitivity was far below optimal at detecting certain complex texture patterns. These measurements show that CNNs can have very large performance differences when detecting the presence of spatial patterns. These differences may have a significant impact on the performance of an imaging system designed to detect low contrast spatial patterns.



### Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research
- **Arxiv ID**: http://arxiv.org/abs/1911.05063v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.05063v2)
- **Published**: 2019-11-12 18:47:37+00:00
- **Updated**: 2019-11-13 18:34:03+00:00
- **Authors**: Krishna Murthy Jatavallabhula, Edward Smith, Jean-Francois Lafleche, Clement Fuji Tsang, Artem Rozantsev, Wenzheng Chen, Tommy Xiang, Rev Lebaredian, Sanja Fidler
- **Comment**: Kaolin is available as open-source software at
  https://github.com/NVIDIAGameWorks/kaolin/
- **Journal**: None
- **Summary**: We present Kaolin, a PyTorch library aiming to accelerate 3D deep learning research. Kaolin provides efficient implementations of differentiable 3D modules for use in deep learning systems. With functionality to load and preprocess several popular 3D datasets, and native functions to manipulate meshes, pointclouds, signed distance functions, and voxel grids, Kaolin mitigates the need to write wasteful boilerplate code. Kaolin packages together several differentiable graphics modules including rendering, lighting, shading, and view warping. Kaolin also supports an array of loss functions and evaluation metrics for seamless evaluation and provides visualization functionality to render the 3D results. Importantly, we curate a comprehensive model zoo comprising many state-of-the-art 3D deep learning architectures, to serve as a starting point for future research endeavours. Kaolin is available as open-source software at https://github.com/NVIDIAGameWorks/kaolin/.



### Experience-Embedded Visual Foresight
- **Arxiv ID**: http://arxiv.org/abs/1911.05071v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.05071v2)
- **Published**: 2019-11-12 18:58:30+00:00
- **Updated**: 2019-11-17 14:45:06+00:00
- **Authors**: Lin Yen-Chen, Maria Bauza, Phillip Isola
- **Comment**: CoRL 2019. Project website: http://yenchenlin.me/evf/
- **Journal**: None
- **Summary**: Visual foresight gives an agent a window into the future, which it can use to anticipate events before they happen and plan strategic behavior. Although impressive results have been achieved on video prediction in constrained settings, these models fail to generalize when confronted with unfamiliar real-world objects. In this paper, we tackle the generalization problem via fast adaptation, where we train a prediction model to quickly adapt to the observed visual dynamics of a novel object. Our method, Experience-embedded Visual Foresight (EVF), jointly learns a fast adaptation module, which encodes observed trajectories of the new object into a vector embedding, and a visual prediction model, which conditions on this embedding to generate physically plausible predictions. For evaluation, we compare our method against baselines on video prediction and benchmark its utility on two real-world control tasks. We show that our method is able to quickly adapt to new visual dynamics and achieves lower error than the baselines when manipulating novel objects.



### Semi-Supervised Multi-Organ Segmentation through Quality Assurance Supervision
- **Arxiv ID**: http://arxiv.org/abs/1911.05113v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.05113v1)
- **Published**: 2019-11-12 19:35:58+00:00
- **Updated**: 2019-11-12 19:35:58+00:00
- **Authors**: Ho Hin Lee, Yucheng Tang, Olivia Tang, Yuchen Xu, Yunqiang Chen, Dashan Gao, Shizhong Han, Riqiang Gao, Michael R. Savona, Richard G. Abramson, Yuankai Huo, Bennett A. Landman
- **Comment**: 7 pages, 5 figures, Accepted by SPIE 2020: Medical Imaging
- **Journal**: None
- **Summary**: Human in-the-loop quality assurance (QA) is typically performed after medical image segmentation to ensure that the systems are performing as intended, as well as identifying and excluding outliers. By performing QA on large-scale, previously unlabeled testing data, categorical QA scores can be generatedIn this paper, we propose a semi-supervised multi-organ segmentation deep neural network consisting of a traditional segmentation model generator and a QA involved discriminator. A large-scale dataset of 2027 volumes are used to train the generator, whose 2-D montage images and segmentation mask with QA scores are used to train the discriminator. To generate the QA scores, the 2-D montage images were reviewed manually and coded 0 (success), 1 (errors consistent with published performance), and 2 (gross failure). Then, the ResNet-18 network was trained with 1623 montage images in equal distribution of all three code labels and achieved an accuracy 94% for classification predictions with 404 montage images withheld for the test cohort. To assess the performance of using the QA supervision, the discriminator was used as a loss function in a multi-organ segmentation pipeline. The inclusion of QA-loss function boosted performance on the unlabeled test dataset from 714 patients to 951 patients over the baseline model. Additionally, the number of failures decreased from 606 (29.90%) to 402 (19.83%). The contributions of the proposed method are threefold: We show that (1) the QA scores can be used as a loss function to perform semi-supervised learning for unlabeled data, (2) the well trained discriminator is learnt by QA score rather than traditional true/false, and (3) the performance of multi-organ segmentation on unlabeled datasets can be fine-tuned with more robust and higher accuracy than the original baseline method.



### Deep Multi-task Prediction of Lung Cancer and Cancer-free Progression from Censored Heterogenous Clinical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1911.05115v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05115v2)
- **Published**: 2019-11-12 19:39:22+00:00
- **Updated**: 2020-02-17 17:38:44+00:00
- **Authors**: Riqiang Gao, Lingfeng Li, Yucheng Tang, Sanja L. Antic, Alexis B. Paulson, Yuankai Huo, Kim L. Sandler, Pierre P. Massion, Bennett A. Landman
- **Comment**: Finalist for the 2020 Robert F. Wagner All Conference Best Student
  Paper Award, SPIE 2020 Medical Imaging. (Highest award in Image Processing
  section)
- **Journal**: None
- **Summary**: Annual low dose computed tomography (CT) lung screening is currently advised for individuals at high risk of lung cancer (e.g., heavy smokers between 55 and 80 years old). The recommended screening practice significantly reduces all-cause mortality, but the vast majority of screening results are negative for cancer. If patients at very low risk could be identified based on individualized, image-based biomarkers, the health care resources could be more efficiently allocated to higher risk patients and reduce overall exposure to ionizing radiation. In this work, we propose a multi-task (diagnosis and prognosis) deep convolutional neural network to improve the diagnostic accuracy over a baseline model while simultaneously estimating a personalized cancer-free progression time (CFPT). A novel Censored Regression Loss (CRL) is proposed to perform weakly supervised regression so that even single negative screening scans can provide small incremental value. Herein, we study 2287 scans from 1433 de-identified patients from the Vanderbilt Lung Screening Program (VLSP) and Molecular Characterization Laboratories (MCL) cohorts. Using five-fold cross-validation, we train a 3D attention-based network under two scenarios: (1) single-task learning with only classification, and (2) multi-task learning with both classification and regression. The single-task learning leads to a higher AUC compared with the Kaggle challenge winner pre-trained model (0.878 v. 0.856), and multi-task learning significantly improves the single-task one (AUC 0.895, p<0.01, McNemar test). In summary, the image-based predicted CFPT can be used in follow-up year lung cancer prediction and data assessment.



### Unsupervised Medical Image Segmentation with Adversarial Networks: From Edge Diagrams to Segmentation Maps
- **Arxiv ID**: http://arxiv.org/abs/1911.05140v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, I.4.6; I.2.10; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1911.05140v1)
- **Published**: 2019-11-12 20:56:33+00:00
- **Updated**: 2019-11-12 20:56:33+00:00
- **Authors**: Umaseh Sivanesan, Luis H. Braga, Ranil R. Sonnadara, Kiret Dhindsa
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: We develop and approach to unsupervised semantic medical image segmentation that extends previous work with generative adversarial networks. We use existing edge detection methods to construct simple edge diagrams, train a generative model to convert them into synthetic medical images, and construct a dataset of synthetic images with known segmentations using variations on extracted edge diagrams. This synthetic dataset is then used to train a supervised image segmentation model. We test our approach on a clinical dataset of kidney ultrasound images and the benchmark ISIC 2018 skin lesion dataset. We show that our unsupervised approach is more accurate than previous unsupervised methods, and performs reasonably compared to supervised image segmentation models. All code and trained models are available at https://github.com/kiretd/Unsupervised-MIseg.



### Scientific Image Restoration Anywhere
- **Arxiv ID**: http://arxiv.org/abs/1911.05878v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.05878v1)
- **Published**: 2019-11-12 21:33:14+00:00
- **Updated**: 2019-11-12 21:33:14+00:00
- **Authors**: Vibhatha Abeykoon, Zhengchun Liu, Rajkumar Kettimuthu, Geoffrey Fox, Ian Foster
- **Comment**: 6 pages, 8 figures, 1 table
- **Journal**: None
- **Summary**: The use of deep learning models within scientific experimental facilities frequently requires low-latency inference, so that, for example, quality control operations can be performed while data are being collected. Edge computing devices can be useful in this context, as their low cost and compact form factor permit them to be co-located with the experimental apparatus. Can such devices, with their limited resources, can perform neural network feed-forward computations efficiently and effectively? We explore this question by evaluating the performance and accuracy of a scientific image restoration model, for which both model input and output are images, on edge computing devices. Specifically, we evaluate deployments of TomoGAN, an image-denoising model based on generative adversarial networks developed for low-dose x-ray imaging, on the Google Edge TPU and NVIDIA Jetson. We adapt TomoGAN for edge execution, evaluate model inference performance, and propose methods to address the accuracy drop caused by model quantization. We show that these edge computing devices can deliver accuracy comparable to that of a full-fledged CPU or GPU model, at speeds that are more than adequate for use in the intended deployments, denoising a 1024 x 1024 image in less than a second. Our experiments also show that the Edge TPU models can provide 3x faster inference response than a CPU-based model and 1.5x faster than an edge GPU-based model. This combination of high speed and low cost permits image restoration anywhere.



### Pose estimation and bin picking for deformable products
- **Arxiv ID**: http://arxiv.org/abs/1911.05185v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05185v1)
- **Published**: 2019-11-12 23:09:55+00:00
- **Updated**: 2019-11-12 23:09:55+00:00
- **Authors**: Benjamin Joffe, Tevon Walker. Remi Gourdon, Konrad Ahlin
- **Comment**: None
- **Journal**: None
- **Summary**: Robotic systems in manufacturing applications commonly assume known object geometry and appearance. This simplifies the task for the 3D perception algorithms and allows the manipulation to be more deterministic. However, those approaches are not easily transferable to the agricultural and food domains due to the variability and deformability of natural food. We demonstrate an approach applied to poultry products that allows picking up a whole chicken from an unordered bin using a suction cup gripper, estimating its pose using a Deep Learning approach, and placing it in a canonical orientation where it can be further processed. Our robotic system was experimentally evaluated and is able to generalize to object variations and achieves high accuracy on bin picking and pose estimation tasks in a real-world environment.




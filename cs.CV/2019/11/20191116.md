# Arxiv Papers in cs.CV on 2019-11-16
### 3D Conditional Generative Adversarial Networks to enable large-scale seismic image enhancement
- **Arxiv ID**: http://arxiv.org/abs/1911.06932v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06932v1)
- **Published**: 2019-11-16 01:39:21+00:00
- **Updated**: 2019-11-16 01:39:21+00:00
- **Authors**: Praneet Dutta, Bruce Power, Adam Halpert, Carlos Ezequiel, Aravind Subramanian, Chanchal Chatterjee, Sindhu Hari, Kenton Prindle, Vishal Vaddina, Andrew Leach, Raj Domala, Laura Bandura, Massimo Mascaro
- **Comment**: To be Presented at the NeurIPS 2019, Second Workshop on Machine
  Learning and the Physicial Sciences, Vancouver, Canada
- **Journal**: None
- **Summary**: We propose GAN-based image enhancement models for frequency enhancement of 2D and 3D seismic images. Seismic imagery is used to understand and characterize the Earth's subsurface for energy exploration. Because these images often suffer from resolution limitations and noise contamination, our proposed method performs large-scale seismic volume frequency enhancement and denoising. The enhanced images reduce uncertainty and improve decisions about issues, such as optimal well placement, that often rely on low signal-to-noise ratio (SNR) seismic volumes. We explored the impact of adding lithology class information to the models, resulting in improved performance on PSNR and SSIM metrics over a baseline model with no conditional information.



### Unsupervised Representation Learning for Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.06939v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06939v4)
- **Published**: 2019-11-16 02:46:04+00:00
- **Updated**: 2020-04-03 14:30:02+00:00
- **Authors**: Yu Yu, Jean-Marc Odobez
- **Comment**: Paper accepted by CVPR 2020
- **Journal**: None
- **Summary**: Although automatic gaze estimation is very important to a large variety of application areas, it is difficult to train accurate and robust gaze models, in great part due to the difficulty in collecting large and diverse data (annotating 3D gaze is expensive and existing datasets use different setups). To address this issue, our main contribution in this paper is to propose an effective approach to learn a low dimensional gaze representation without gaze annotations, which to the best of our best knowledge, is the first work to do so. The main idea is to rely on a gaze redirection network and use the gaze representation difference of the input and target images (of the redirection network) as the redirection variable. A redirection loss in image domain allows the joint training of both the redirection network and the gaze representation network. In addition, we propose a warping field regularization which not only provides an explicit physical meaning to the gaze representations but also avoids redirection distortions. Promising results on few-shot gaze estimation (competitive results can be achieved with as few as <= 100 calibration samples), cross-dataset gaze estimation, gaze network pretraining, and another task (head pose estimation) demonstrate the validity of our framework.



### Dynamic Instance Normalization for Arbitrary Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1911.06953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06953v1)
- **Published**: 2019-11-16 04:03:52+00:00
- **Updated**: 2019-11-16 04:03:52+00:00
- **Authors**: Yongcheng Jing, Xiao Liu, Yukang Ding, Xinchao Wang, Errui Ding, Mingli Song, Shilei Wen
- **Comment**: Accepted to AAAI 2020 (Oral)
- **Journal**: None
- **Summary**: Prior normalization methods rely on affine transformations to produce arbitrary image style transfers, of which the parameters are computed in a pre-defined way. Such manually-defined nature eventually results in the high-cost and shared encoders for both style and content encoding, making style transfer systems cumbersome to be deployed in resource-constrained environments like on the mobile-terminal side. In this paper, we propose a new and generalized normalization module, termed as Dynamic Instance Normalization (DIN), that allows for flexible and more efficient arbitrary style transfers. Comprising an instance normalization and a dynamic convolution, DIN encodes a style image into learnable convolution parameters, upon which the content image is stylized. Unlike conventional methods that use shared complex encoders to encode content and style, the proposed DIN introduces a sophisticated style encoder, yet comes with a compact and lightweight content encoder for fast inference. Experimental results demonstrate that the proposed approach yields very encouraging results on challenging style patterns and, to our best knowledge, for the first time enables an arbitrary style transfer using MobileNet-based lightweight architecture, leading to a reduction factor of more than twenty in computational cost as compared to existing approaches. Furthermore, the proposed DIN provides flexible support for state-of-the-art convolutional operations, and thus triggers novel functionalities, such as uniform-stroke placement for non-natural images and automatic spatial-stroke control.



### On Space-spectrum Uncertainty Analysis for Coded Aperture Systems
- **Arxiv ID**: http://arxiv.org/abs/1911.06956v1
- **DOI**: 10.1364/OE.381154
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.06956v1)
- **Published**: 2019-11-16 04:56:33+00:00
- **Updated**: 2019-11-16 04:56:33+00:00
- **Authors**: Vishwanath Saragadam, Aswin Sankaranarayanan
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: We introduce and analyze the concept of space-spectrum uncertainty for certain commonly-used designs for spectrally programmable cameras. Our key finding states that, it is impossible to simultaneously capture high-resolution spatial images while programming the spectrum at high resolution. This phenomenon arises due to a Fourier relationship between the aperture used for obtaining spectrum and its corresponding diffraction blur in the (spatial) image. We show that the product of spatial and spectral standard deviations is lower bounded by {\lambda}/4{\pi}{\nu_0} femto square-meters, where {\nu_0} is the density of groves in the diffraction grating and {\lambda} is the wavelength of light. Experiments with a lab prototype for simultaneously measuring spectrum and image validate our findings and its implication for spectral filtering.



### Defensive Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.06968v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06968v2)
- **Published**: 2019-11-16 05:57:16+00:00
- **Updated**: 2023-08-25 11:19:54+00:00
- **Authors**: Wenbin Li, Lei Wang, Xingxing Zhang, Lei Qi, Jing Huo, Yang Gao, Jiebo Luo
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI) 2022
- **Journal**: None
- **Summary**: This paper investigates a new challenging problem called defensive few-shot learning in order to learn a robust few-shot model against adversarial attacks. Simply applying the existing adversarial defense methods to few-shot learning cannot effectively solve this problem. This is because the commonly assumed sample-level distribution consistency between the training and test sets can no longer be met in the few-shot setting. To address this situation, we develop a general defensive few-shot learning (DFSL) framework to answer the following two key questions: (1) how to transfer adversarial defense knowledge from one sample distribution to another? (2) how to narrow the distribution gap between clean and adversarial examples under the few-shot setting? To answer the first question, we propose an episode-based adversarial training mechanism by assuming a task-level distribution consistency to better transfer the adversarial defense knowledge. As for the second question, within each few-shot task, we design two kinds of distribution consistency criteria to narrow the distribution gap between clean and adversarial examples from the feature-wise and prediction-wise perspectives, respectively. Extensive experiments demonstrate that the proposed framework can effectively make the existing few-shot models robust against adversarial attacks. Code is available at https://github.com/WenbinLee/DefensiveFSL.git.



### BSP-Net: Generating Compact Meshes via Binary Space Partitioning
- **Arxiv ID**: http://arxiv.org/abs/1911.06971v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06971v6)
- **Published**: 2019-11-16 06:25:26+00:00
- **Updated**: 2020-12-07 20:02:19+00:00
- **Authors**: Zhiqin Chen, Andrea Tagliasacchi, Hao Zhang
- **Comment**: CVPR 2020 Best Student Paper Award. Project page:
  https://bsp-net.github.io, Code:
  https://github.com/czq142857/BSP-NET-original
- **Journal**: None
- **Summary**: Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only played a minor role in the deep learning revolution. Leading methods for learning generative models of shapes rely on implicit functions, and generate meshes only after expensive iso-surfacing routines. To overcome these challenges, we are inspired by a classical spatial data structure from computer graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core ingredient of BSP is an operation for recursive subdivision of space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net is unsupervised since no convex shape decompositions are needed for training. The network is trained to reconstruct a shape using a set of convexes obtained from a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can be easily extracted to form a polygon mesh, without any need for iso-surfacing. The generated meshes are compact (i.e., low-poly) and well suited to represent sharp geometry; they are guaranteed to be watertight and can be easily parameterized. We also show that the reconstruction quality by BSP-Net is competitive with state-of-the-art methods while using much fewer primitives. Code is available at https://github.com/czq142857/BSP-NET-original.



### Lightweight Residual Network for The Classification of Thyroid Nodules
- **Arxiv ID**: http://arxiv.org/abs/1911.08303v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.08303v1)
- **Published**: 2019-11-16 06:32:24+00:00
- **Updated**: 2019-11-16 06:32:24+00:00
- **Authors**: Ponugoti Nikhila, Sabari Nathan, Elmer Jeto Gomes Ataide, Alfredo Illanes, Dr. Michael Friebe, Srichandana Abbineni
- **Comment**: 1 Page , 1 Figure , IEEE EMBS
- **Journal**: None
- **Summary**: Ultrasound is a useful technique for diagnosing thyroid nodules. Benign and malignant nodules that automatically discriminate in the ultrasound pictures can provide diagnostic recommendations or, improve diagnostic accuracy in the absence of specialists. The main issue here is how to collect suitable features for this particular task. We suggest here a technique for extracting features from ultrasound pictures based on the Residual U-net. We attempt to introduce significant semantic characteristics to the classification. Our model gained 95% classification accuracy.



### Long Range 3D with Quadocular Thermal (LWIR) Camera
- **Arxiv ID**: http://arxiv.org/abs/1911.06975v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06975v2)
- **Published**: 2019-11-16 06:51:29+00:00
- **Updated**: 2019-11-20 01:01:15+00:00
- **Authors**: Andrey Filippov, Oleg Dzhimiev
- **Comment**: 10 pages, 5 figures; fixed abbreviations navigation, added pdf ToC
- **Journal**: None
- **Summary**: Long Wave Infrared (LWIR) cameras provide images regardles of the ambient illumination, they tolerate fog and are not blinded by the incoming car headlights. These features make LWIR cameras attractive for autonomous navigation, security and military applications. Thermal images can be used similarly to the visible range ones, including 3D scene reconstruction with two or more such cameras mounted on a rigid frame. There are two additional challenges for this spectral range: lower image resolution and lower contrast of the textures.   In this work, we demonstrate quadocular LWIR camera setup, calibration, image capturing and processing that result in long range 3D perception with 0.077 pix disparity error over 90% of the depth map. With low resolution (160 x 120) LWIR sensors we achieved 10% range accuracy at 28 m with 56 degrees horizontal field of view (HFoV) and 150 mm baseline. Scaled to the now-standard 640 x 512 resolution and 200 mm baseline suitable for head-mounted application the result would be 10% accuracy at 130 m.



### Grounding Human-to-Vehicle Advice for Self-driving Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1911.06978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06978v1)
- **Published**: 2019-11-16 07:15:01+00:00
- **Updated**: 2019-11-16 07:15:01+00:00
- **Authors**: Jinkyu Kim, Teruhisa Misu, Yi-Ting Chen, Ashish Tawari, John Canny
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019
- **Journal**: None
- **Summary**: Recent success suggests that deep neural control networks are likely to be a key component of self-driving vehicles. These networks are trained on large datasets to imitate human actions, but they lack semantic understanding of image contents. This makes them brittle and potentially unsafe in situations that do not match training data. Here, we propose to address this issue by augmenting training data with natural language advice from a human. Advice includes guidance about what to do and where to attend. We present the first step toward advice giving, where we train an end-to-end vehicle controller that accepts advice. The controller adapts the way it attends to the scene (visual attention) and the control (steering and speed). Attention mechanisms tie controller behavior to salient objects in the advice. We evaluate our model on a novel advisable driving dataset with manually annotated human-to-vehicle advice called Honda Research Institute-Advice Dataset (HAD). We show that taking advice improves the performance of the end-to-end network, while the network cues on a variety of visual features that are provided by advice. The dataset is available at https://usa.honda-ri.com/HAD.



### VLUC: An Empirical Benchmark for Video-Like Urban Computing on Citywide Crowd and Traffic Prediction
- **Arxiv ID**: http://arxiv.org/abs/1911.06982v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.06982v1)
- **Published**: 2019-11-16 07:38:44+00:00
- **Updated**: 2019-11-16 07:38:44+00:00
- **Authors**: Renhe Jiang, Zekun Cai, Zhaonan Wang, Chuang Yang, Zipei Fan, Xuan Song, Kota Tsubouchi, Ryosuke Shibasaki
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, massive urban human mobility data are being generated from mobile phones, car navigation systems, and traffic sensors. Predicting the density and flow of the crowd or traffic at a citywide level becomes possible by using the big data and cutting-edge AI technologies. It has been a very significant research topic with high social impact, which can be widely applied to emergency management, traffic regulation, and urban planning. In particular, by meshing a large urban area to a number of fine-grained mesh-grids, citywide crowd and traffic information in a continuous time period can be represented like a video, where each timestamp can be seen as one video frame. Based on this idea, a series of methods have been proposed to address video-like prediction for citywide crowd and traffic. In this study, we publish a new aggregated human mobility dataset generated from a real-world smartphone application and build a standard benchmark for such kind of video-like urban computing with this new dataset and the existing open datasets. We first comprehensively review the state-of-the-art works of literature and formulate the density and in-out flow prediction problem, then conduct a thorough performance assessment for those methods. With this benchmark, we hope researchers can easily follow up and quickly launch a new solution on this topic.



### Faster AutoAugment: Learning Augmentation Strategies using Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/1911.06987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06987v1)
- **Published**: 2019-11-16 08:10:59+00:00
- **Updated**: 2019-11-16 08:10:59+00:00
- **Authors**: Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, Hideki Nakayama
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as the differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented data and the original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior work without a performance drop.



### Automatic Design of CNNs via Differentiable Neural Architecture Search for PolSAR Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.06993v2
- **DOI**: 10.1109/TGRS.2020.2976694
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06993v2)
- **Published**: 2019-11-16 08:33:33+00:00
- **Updated**: 2019-11-19 11:13:32+00:00
- **Authors**: Hongwei Dong, Siyu Zhang, Bin Zou, Lamei Zhang
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, Volume 58,
  Issue 9, September 2020, Pages 6362-6375
- **Summary**: Convolutional neural networks (CNNs) have shown good performance in polarimetric synthetic aperture radar (PolSAR) image classification due to the automation of feature engineering. Excellent hand-crafted architectures of CNNs incorporated the wisdom of human experts, which is an important reason for CNN's success. However, the design of the architectures is a difficult problem, which needs a lot of professional knowledge as well as computational resources. Moreover, the architecture designed by hand might be suboptimal, because it is only one of thousands of unobserved but objective existed paths. Considering that the success of deep learning is largely due to its automation of the feature engineering process, how to design automatic architecture searching methods to replace the hand-crafted ones is an interesting topic. In this paper, we explore the application of neural architecture search (NAS) in PolSAR area for the first time. Different from the utilization of existing NAS methods, we propose a differentiable architecture search (DAS) method which is customized for PolSAR classification. The proposed DAS is equipped with a PolSAR tailored search space and an improved one-shot search strategy. By DAS, the weights parameters and architecture parameters (corresponds to the hyperparameters but not the topologies) can be optimized by stochastic gradient descent method during the training. The optimized architecture parameters should be transformed into corresponding CNN architecture and re-train to achieve high-precision PolSAR classification. In addition, complex-valued DAS is developed to take into account the characteristics of PolSAR images so as to further improve the performance. Experiments on three PolSAR benchmark datasets show that the CNNs obtained by searching have better classification performance than the hand-crafted ones.



### Regions of Interest Segmentation from LiDAR Point Cloud for Multirotor Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1911.06994v3
- **DOI**: 10.1109/ICUAS48674.2020.9214019
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.06994v3)
- **Published**: 2019-11-16 08:35:26+00:00
- **Updated**: 2020-05-04 06:58:55+00:00
- **Authors**: Geesara Prathap, Roman Fedorenko, Alexandr Klimchik
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel filter for segmenting the regions of interest from LiDAR 3D point cloud for multirotor aerial vehicles. It is specially targeted for real-time applications and works on sparse LiDAR point clouds without preliminary mapping. We use this filter as a crucial component of fast obstacle avoidance system for agriculture drone operating at low altitude. As the first step, each point cloud is transformed into a depth image and then identify places near to the vehicle (local maxima) by locating areas with high pixel densities. Afterwards, we merge the original depth image with identified locations after maximizing intensities of pixels in which local maxima were obtained. Next step is to calculate the range angle image that represents angles between two consecutive laser beams based on the improved depth image. Once the corresponding range angle image is constructed, smoothing is applied to reduce the noise. Finally, we find out connected components within the improved depth image while incorporating smoothed range angle image. This allows separating the regions of interest. The filter has been tested on various simulated environments as well as an actual drone and provides real-time performance. We make our source code, dataset available at https://github.com/GPrathap/hagen.git and real world experiment result can be found on the following link: https://www.youtube.com/watch?v=iHd_ZkhKPjc available online.



### Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game
- **Arxiv ID**: http://arxiv.org/abs/1911.06997v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.06997v2)
- **Published**: 2019-11-16 08:51:50+00:00
- **Updated**: 2020-01-08 18:00:44+00:00
- **Authors**: Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Linxiao Yang, Ngai-Man Cheung
- **Comment**: Accepted at NeurIPS 2019
- **Journal**: None
- **Summary**: Self-supervised (SS) learning is a powerful approach for representation learning using unlabeled data. Recently, it has been applied to Generative Adversarial Networks (GAN) training. Specifically, SS tasks were proposed to address the catastrophic forgetting issue in the GAN discriminator. In this work, we perform an in-depth analysis to understand how SS tasks interact with learning of generator. From the analysis, we identify issues of SS tasks which allow a severely mode-collapsed generator to excel the SS tasks. To address the issues, we propose new SS tasks based on a multi-class minimax game. The competition between our proposed SS tasks in the game encourages the generator to learn the data distribution and generate diverse samples. We provide both theoretical and empirical analysis to support that our proposed SS tasks have better convergence property. We conduct experiments to incorporate our proposed SS tasks into two different GAN baseline models. Our approach establishes state-of-the-art FID scores on CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet $32\times32$ and Stacked-MNIST datasets, outperforming existing works by considerable margins in some cases. Our unconditional GAN model approaches performance of conditional GAN without using labeled data. Our code: https://github.com/tntrung/msgan



### Revisiting Shadow Detection: A New Benchmark Dataset for Complex World
- **Arxiv ID**: http://arxiv.org/abs/1911.06998v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.06998v3)
- **Published**: 2019-11-16 08:54:09+00:00
- **Updated**: 2021-01-02 03:25:11+00:00
- **Authors**: Xiaowei Hu, Tianyu Wang, Chi-Wing Fu, Yitong Jiang, Qiong Wang, Pheng-Ann Heng
- **Comment**: Accepted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Shadow detection in general photos is a nontrivial problem, due to the complexity of the real world. Though recent shadow detectors have already achieved remarkable performance on various benchmark data, their performance is still limited for general real-world situations. In this work, we collected shadow images for multiple scenarios and compiled a new dataset of 10,500 shadow images, each with labeled ground-truth mask, for supporting shadow detection in the complex world. Our dataset covers a rich variety of scene categories, with diverse shadow sizes, locations, contrasts, and types. Further, we comprehensively analyze the complexity of the dataset, present a fast shadow detection network with a detail enhancement module to harvest shadow details, and demonstrate the effectiveness of our method to detect shadows in general situations.



### AETv2: AutoEncoding Transformations for Self-Supervised Representation Learning by Minimizing Geodesic Distances in Lie Groups
- **Arxiv ID**: http://arxiv.org/abs/1911.07004v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1911.07004v1)
- **Published**: 2019-11-16 09:58:58+00:00
- **Updated**: 2019-11-16 09:58:58+00:00
- **Authors**: Feng Lin, Haohang Xu, Houqiang Li, Hongkai Xiong, Guo-Jun Qi
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning by predicting transformations has demonstrated outstanding performances in both unsupervised and (semi-)supervised tasks. Among the state-of-the-art methods is the AutoEncoding Transformations (AET) by decoding transformations from the learned representations of original and transformed images. Both deterministic and probabilistic AETs rely on the Euclidean distance to measure the deviation of estimated transformations from their groundtruth counterparts. However, this assumption is questionable as a group of transformations often reside on a curved manifold rather staying in a flat Euclidean space. For this reason, we should use the geodesic to characterize how an image transform along the manifold of a transformation group, and adopt its length to measure the deviation between transformations. Particularly, we present to autoencode a Lie group of homography transformations PG(2) to learn image representations. For this, we make an estimate of the intractable Riemannian logarithm by projecting PG(2) to a subgroup of rotation transformations SO(3) that allows the closed-form expression of geodesic distances. Experiments demonstrate the proposed AETv2 model outperforms the previous version as well as the other state-of-the-art self-supervised models in multiple tasks.



### What Will Your Child Look Like? DNA-Net: Age and Gender Aware Kin Face Synthesizer
- **Arxiv ID**: http://arxiv.org/abs/1911.07014v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07014v1)
- **Published**: 2019-11-16 11:09:17+00:00
- **Updated**: 2019-11-16 11:09:17+00:00
- **Authors**: Pengyu Gao, Siyu Xia, Joseph Robinson, Junkang Zhang, Chao Xia, Ming Shao, Yun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual kinship recognition aims to identify blood relatives from facial images. Its practical application-- like in law-enforcement, video surveillance, automatic family album management, and more-- has motivated many researchers to put forth effort on the topic as of recent. In this paper, we focus on a new view of visual kinship technology: kin-based face generation. Specifically, we propose a two-stage kin-face generation model to predict the appearance of a child given a pair of parents. The first stage includes a deep generative adversarial autoencoder conditioned on ages and genders to map between facial appearance and high-level features. The second stage is our proposed DNA-Net, which serves as a transformation between the deep and genetic features based on a random selection process to fuse genes of a parent pair to form the genes of a child. We demonstrate the effectiveness of the proposed method quantitatively and qualitatively: quantitatively, pre-trained models and human subjects perform kinship verification on the generated images of children; qualitatively, we show photo-realistic face images of children that closely resemble the given pair of parents. In the end, experiments validate that the proposed model synthesizes convincing kin-faces using both subjective and objective standards.



### Effectively Unbiased FID and Inception Score and where to find them
- **Arxiv ID**: http://arxiv.org/abs/1911.07023v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07023v3)
- **Published**: 2019-11-16 12:54:05+00:00
- **Updated**: 2020-06-15 23:01:14+00:00
- **Authors**: Min Jin Chong, David Forsyth
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: This paper shows that two commonly used evaluation metrics for generative models, the Fr\'echet Inception Distance (FID) and the Inception Score (IS), are biased -- the expected value of the score computed for a finite sample set is not the true value of the score. Worse, the paper shows that the bias term depends on the particular model being evaluated, so model A may get a better score than model B simply because model A's bias term is smaller. This effect cannot be fixed by evaluating at a fixed number of samples. This means all comparisons using FID or IS as currently computed are unreliable.   We then show how to extrapolate the score to obtain an effectively bias-free estimate of scores computed with an infinite number of samples, which we term $\overline{\textrm{FID}}_\infty$ and $\overline{\textrm{IS}}_\infty$. In turn, this effectively bias-free estimate requires good estimates of scores with a finite number of samples. We show that using Quasi-Monte Carlo integration notably improves estimates of FID and IS for finite sample sets. Our extrapolated scores are simple, drop-in replacements for the finite sample scores. Additionally, we show that using low discrepancy sequence in GAN training offers small improvements in the resulting generator.



### S2DNAS:Transforming Static CNN Model for Dynamic Inference via Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1911.07033v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07033v2)
- **Published**: 2019-11-16 13:49:44+00:00
- **Updated**: 2019-12-17 02:54:18+00:00
- **Authors**: Zhihang Yuan, Bingzhe Wu, Zheng Liang, Shiwan Zhao, Weichen Bi, Guangyu Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, dynamic inference has emerged as a promising way to reduce the computational cost of deep convolutional neural network (CNN). In contrast to static methods (e.g. weight pruning), dynamic inference adaptively adjusts the inference process according to each input sample, which can considerably reduce the computational cost on "easy" samples while maintaining the overall model performance. In this paper, we introduce a general framework, S2DNAS, which can transform various static CNN models to support dynamic inference via neural architecture search. To this end, based on a given CNN model, we first generate a CNN architecture space in which each architecture is a multi-stage CNN generated from the given model using some predefined transformations. Then, we propose a reinforcement learning based approach to automatically search for the optimal CNN architecture in the generated space. At last, with the searched multi-stage network, we can perform dynamic inference by adaptively choosing a stage to evaluate for each sample. Unlike previous works that introduce irregular computations or complex controllers in the inference or re-design a CNN model from scratch, our method can generalize to most of the popular CNN architectures and the searched dynamic network can be directly deployed using existing deep learning frameworks in various hardware devices.



### Instance Shadow Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.07034v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07034v2)
- **Published**: 2019-11-16 14:06:05+00:00
- **Updated**: 2020-06-09 08:38:33+00:00
- **Authors**: Tianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng, Chi-Wing Fu
- **Comment**: Accepted to CVPR 2020
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  pp. 1880-1889, 2020
- **Summary**: Instance shadow detection is a brand new problem, aiming to find shadow instances paired with object instances. To approach it, we first prepare a new dataset called SOBA, named after Shadow-OBject Association, with 3,623 pairs of shadow and object instances in 1,000 photos, each with individual labeled masks. Second, we design LISA, named after Light-guided Instance Shadow-object Association, an end-to-end framework to automatically predict the shadow and object instances, together with the shadow-object associations and light direction. Then, we pair up the predicted shadow and object instances, and match them with the predicted shadow-object associations to generate the final results. In our evaluations, we formulate a new metric named the shadow-object average precision to measure the performance of our results. Further, we conducted various experiments and demonstrate our method's applicability on light direction estimation and photo editing.



### Quality Assessment of DIBR-synthesized views: An Overview
- **Arxiv ID**: http://arxiv.org/abs/1911.07036v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.07036v2)
- **Published**: 2019-11-16 14:13:56+00:00
- **Updated**: 2021-04-27 08:18:10+00:00
- **Authors**: Shishun Tian, Lu Zhang, Wenbin Zou, Xia Li, Ting Su, Luce Morin, Olivier Deforges
- **Comment**: None
- **Journal**: None
- **Summary**: The Depth-Image-Based-Rendering (DIBR) is one of the main fundamental technique to generate new views in 3D video applications, such as Multi-View Videos (MVV), Free-Viewpoint Videos (FVV) and Virtual Reality (VR). However, the quality assessment of DIBR-synthesized views is quite different from the traditional 2D images/videos. In recent years, several efforts have been made towards this topic, but there {is a lack of} detailed survey in {the} literature. In this paper, we provide a comprehensive survey on various current approaches for DIBR-synthesized views. The current accessible datasets of DIBR-synthesized views are firstly reviewed{, followed} by a summary analysis of the representative state-of-the-art objective metrics. Then, the performances of different objective metrics are evaluated and discussed on all available datasets. Finally, we discuss the potential challenges and suggest possible directions for future research.



### Automatic Annotation of Hip Anatomy in Fluoroscopy for Robust and Efficient 2D/3D Registration
- **Arxiv ID**: http://arxiv.org/abs/1911.07042v2
- **DOI**: 10.1007/s11548-020-02162-7
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07042v2)
- **Published**: 2019-11-16 14:58:00+00:00
- **Updated**: 2020-03-18 15:12:51+00:00
- **Authors**: Robert Grupp, Mathias Unberath, Cong Gao, Rachel Hegeman, Ryan Murphy, Clayton Alexander, Yoshito Otake, Benjamin McArthur, Mehran Armand, Russell Taylor
- **Comment**: Revised article to address reviewer comments. Accepted to IPCAI 2020.
  Supplementary video at https://youtu.be/5AwGlNkcp9o and dataset/code at
  https://github.com/rg2/DeepFluoroLabeling-IPCAI2020
- **Journal**: International Journal of Computer Assisted Radiology and Surgery
  15 (2020) 759-769
- **Summary**: Fluoroscopy is the standard imaging modality used to guide hip surgery and is therefore a natural sensor for computer-assisted navigation. In order to efficiently solve the complex registration problems presented during navigation, human-assisted annotations of the intraoperative image are typically required. This manual initialization interferes with the surgical workflow and diminishes any advantages gained from navigation. We propose a method for fully automatic registration using annotations produced by a neural network. Neural networks are trained to simultaneously segment anatomy and identify landmarks in fluoroscopy. Training data is obtained using an intraoperatively incompatible 2D/3D registration of hip anatomy. Ground truth 2D labels are established using projected 3D annotations. Intraoperative registration couples an intensity-based strategy with annotations inferred by the network and requires no human assistance. Ground truth labels were obtained in 366 fluoroscopic images across 6 cadaveric specimens. In a leave-one-subject-out experiment, networks obtained mean dice coefficients for left and right hemipelves, left and right femurs of 0.86, 0.87, 0.90, and 0.84. The mean 2D landmark error was 5.0 mm. The pelvis was registered within 1 degree for 86% of the images when using the proposed intraoperative approach with an average runtime of 7 seconds. In comparison, an intensity-only approach without manual initialization, registered the pelvis to 1 degree in 18% of images. We have created the first accurately annotated, non-synthetic, dataset of hip fluoroscopy. By using these annotations as training data for neural networks, state of the art performance in fluoroscopic segmentation and landmark localization was achieved. Integrating these annotations allows for a robust, fully automatic, and efficient intraoperative registration during fluoroscopic navigation of the hip.



### A method for detecting text of arbitrary shapes in natural scenes that improves text spotting
- **Arxiv ID**: http://arxiv.org/abs/1911.07046v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07046v3)
- **Published**: 2019-11-16 15:23:32+00:00
- **Updated**: 2020-05-28 03:04:47+00:00
- **Authors**: Qitong Wang, Yi Zheng, Margrit Betke
- **Comment**: Accepted by IEEE CVPR-W 2020
- **Journal**: None
- **Summary**: Understanding the meaning of text in images of natural scenes like highway signs or store front emblems is particularly challenging if the text is foreshortened in the image or the letters are artistically distorted. We introduce a pipeline-based text spotting framework that can both detect and recognize text in various fonts, shapes, and orientations in natural scene images with complicated backgrounds. The main contribution of our work is the text detection component, which we call UHT, short for UNet, Heatmap, and Textfill. UHT uses a UNet to compute heatmaps for candidate text regions and a textfill algorithm to produce tight polygonal boundaries around each word in the candidate text. Our method trains the UNet with groundtruth heatmaps that we obtain from text bounding polygons provided by groundtruth annotations. Our text spotting framework, called UHTA, combines UHT with the state-of-the-art text recognition system ASTER. Experiments on four challenging and public scene-text-detection datasets (Total-Text, SCUT-CTW1500, MSRA-TD500, and COCO-Text) show the effectiveness and generalization ability of UHT in detecting not only multilingual (potentially rotated) straight but also curved text in scripts of multiple languages. Our experimental results of UHTA on the Total-Text dataset show that UHTA outperforms four state-of-the-art text spotting frameworks by at least 9.1 percent points in the F-measure, which suggests that UHTA may be used as a complete text detection and recognition system in real applications.



### All-In-One: Facial Expression Transfer, Editing and Recognition Using A Single Network
- **Arxiv ID**: http://arxiv.org/abs/1911.07050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07050v1)
- **Published**: 2019-11-16 15:58:16+00:00
- **Updated**: 2019-11-16 15:58:16+00:00
- **Authors**: Kamran Ali, Charles E. Hughes
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper, we present a unified architecture known as Transfer-Editing and Recognition Generative Adversarial Network (TER-GAN) which can be used: 1. to transfer facial expressions from one identity to another identity, known as Facial Expression Transfer (FET), 2. to transform the expression of a given image to a target expression, while preserving the identity of the image, known as Facial Expression Editing (FEE), and 3. to recognize the facial expression of a face image, known as Facial Expression Recognition (FER). In TER-GAN, we combine the capabilities of generative models to generate synthetic images, while learning important information about the input images during the reconstruction process. More specifically, two encoders are used in TER-GAN to encode identity and expression information from two input images, and a synthetic expression image is generated by the decoder part of TER-GAN. To improve the feature disentanglement and extraction process, we also introduce a novel expression consistency loss and an identity consistency loss which exploit extra expression and identity information from generated images. Experimental results show that the proposed method can be used for efficient facial expression transfer, facial expression editing and facial expression recognition. In order to evaluate the proposed technique and to compare our results with state-of-the-art methods, we have used the Oulu-CASIA dataset for our experiments.



### Maintaining Discrimination and Fairness in Class Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.07053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07053v1)
- **Published**: 2019-11-16 16:05:53+00:00
- **Updated**: 2019-11-16 16:05:53+00:00
- **Authors**: Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, Shutao Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been applied in class incremental learning, which aims to solve common real-world problems of learning new classes continually. One drawback of standard DNNs is that they are prone to catastrophic forgetting. Knowledge distillation (KD) is a commonly used technique to alleviate this problem. In this paper, we demonstrate it can indeed help the model to output more discriminative results within old classes. However, it cannot alleviate the problem that the model tends to classify objects into new classes, causing the positive effect of KD to be hidden and limited. We observed that an important factor causing catastrophic forgetting is that the weights in the last fully connected (FC) layer are highly biased in class incremental learning. In this paper, we propose a simple and effective solution motivated by the aforementioned observations to address catastrophic forgetting. Firstly, we utilize KD to maintain the discrimination within old classes. Then, to further maintain the fairness between old classes and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after normal training process. Unlike previous work, WA does not require any extra parameters or a validation set in advance, as it utilizes the information provided by the biased weights themselves. The proposed method is evaluated on ImageNet-1000, ImageNet-100, and CIFAR-100 under various settings. Experimental results show that the proposed method can effectively alleviate catastrophic forgetting and significantly outperform state-of-the-art methods.



### Tigris: Architecture and Algorithms for 3D Perception in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1911.07841v3
- **DOI**: 10.1145/3352460.3358259
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07841v3)
- **Published**: 2019-11-16 16:54:53+00:00
- **Updated**: 2019-11-21 01:42:42+00:00
- **Authors**: Tiancheng Xu, Boyuan Tian, Yuhao Zhu
- **Comment**: Published at MICRO-52 (52nd IEEE/ACM International Symposium on
  Microarchitecture); Tiancheng Xu and Boyuan Tian are co-primary authors
- **Journal**: None
- **Summary**: Machine perception applications are increasingly moving toward manipulating and processing 3D point cloud. This paper focuses on point cloud registration, a key primitive of 3D data processing widely used in high-level tasks such as odometry, simultaneous localization and mapping, and 3D reconstruction. As these applications are routinely deployed in energy-constrained environments, real-time and energy-efficient point cloud registration is critical.   We present Tigris, an algorithm-architecture co-designed system specialized for point cloud registration. Through an extensive exploration of the registration pipeline design space, we find that, while different design points make vastly different trade-offs between accuracy and performance, KD-tree search is a common performance bottleneck, and thus is an ideal candidate for architectural specialization. While KD-tree search is inherently sequential, we propose an acceleration-amenable data structure and search algorithm that exposes different forms of parallelism of KD-tree search in the context of point cloud registration. The co-designed accelerator systematically exploits the parallelism while incorporating a set of architectural techniques that further improve the accelerator efficiency. Overall, Tigris achieves 77.2$\times$ speedup and 7.4$\times$ power reduction in KD-tree search over an RTX 2080 Ti GPU, which translates to a 41.7% registration performance improvements and 3.0$\times$ power reduction.



### ResUNet++: An Advanced Architecture for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.07067v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07067v1)
- **Published**: 2019-11-16 18:04:17+00:00
- **Updated**: 2019-11-16 18:04:17+00:00
- **Authors**: Debesh Jha, Pia H. Smedsrud, Michael A. Riegler, Dag Johansen, Thomas de Lange, Pal Halvorsen, Havard D. Johansen
- **Comment**: 7 pages, 3 figures, 21st IEEE International Symposium on Multimedia
- **Journal**: None
- **Summary**: Accurate computer-aided polyp detection and segmentation during colonoscopy examinations can help endoscopists resect abnormal tissue and thereby decrease chances of polyps growing into cancer. Towards developing a fully automated model for pixel-wise polyp segmentation, we propose ResUNet++, which is an improved ResUNet architecture for colonoscopic image segmentation. Our experimental evaluations show that the suggested architecture produces good segmentation results on publicly available datasets. Furthermore, ResUNet++ significantly outperforms U-Net and ResUNet, two key state-of-the-art deep learning architectures, by achieving high evaluation scores with a dice coefficient of 81.33%, and a mean Intersection over Union (mIoU) of 79.27% for the Kvasir-SEG dataset and a dice coefficient of 79.55%, and a mIoU of 79.62% with CVC-612 dataset.



### Sensory Optimization: Neural Networks as a Model for Understanding and Creating Art
- **Arxiv ID**: http://arxiv.org/abs/1911.07068v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07068v1)
- **Published**: 2019-11-16 18:10:00+00:00
- **Updated**: 2019-11-16 18:10:00+00:00
- **Authors**: Owain Evans
- **Comment**: 27 pages. Web version with high-resolution images:
  https://owainevans.github.io/visual_aesthetics/sensory-optimization.html
- **Journal**: None
- **Summary**: This article is about the cognitive science of visual art. Artists create physical artifacts (such as sculptures or paintings) which depict people, objects, and events. These depictions are usually stylized rather than photo-realistic. How is it that humans are able to understand and create stylized representations? Does this ability depend on general cognitive capacities or an evolutionary adaptation for art? What role is played by learning and culture?   Machine Learning can shed light on these questions. It's possible to train convolutional neural networks (CNNs) to recognize objects without training them on any visual art. If such CNNs can generalize to visual art (by creating and understanding stylized representations), then CNNs provide a model for how humans could understand art without innate adaptations or cultural learning. I argue that Deep Dream and Style Transfer show that CNNs can create a basic form of visual art, and that humans could create art by similar processes. This suggests that artists make art by optimizing for effects on the human object-recognition system. Physical artifacts are optimized to evoke real-world objects for this system (e.g. to evoke people or landscapes) and to serve as superstimuli for this system.



### Kvasir-SEG: A Segmented Polyp Dataset
- **Arxiv ID**: http://arxiv.org/abs/1911.07069v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07069v1)
- **Published**: 2019-11-16 18:13:43+00:00
- **Updated**: 2019-11-16 18:13:43+00:00
- **Authors**: Debesh Jha, Pia H. Smedsrud, Michael A. Riegler, Pl Halvorsen, Thomas de Lange, Dag Johansen, Hvard D. Johansen
- **Comment**: 12 pages, 4 figures, 26TH INTERNATIONAL CONFERENCE ON MULTIMEDIA
  MODELING
- **Journal**: None
- **Summary**: Pixel-wise image segmentation is a highly demanding task in medical-image analysis. In practice, it is difficult to find annotated medical images with corresponding segmentation masks. In this paper, we present Kvasir-SEG: an open-access dataset of gastrointestinal polyp images and corresponding segmentation masks, manually annotated by a medical doctor and then verified by an experienced gastroenterologist. Moreover, we also generated the bounding boxes of the polyp regions with the help of segmentation masks. We demonstrate the use of our dataset with a traditional segmentation approach and a modern deep-learning based Convolutional Neural Network (CNN) approach. The dataset will be of value for researchers to reproduce results and compare methods. By adding segmentation masks to the Kvasir dataset, which only provide frame-wise annotations, we enable multimedia and computer vision researchers to contribute in the field of polyp segmentation and automatic analysis of colonoscopy images.



### Unsupervised Deep Metric Learning via Auxiliary Rotation Loss
- **Arxiv ID**: http://arxiv.org/abs/1911.07072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07072v1)
- **Published**: 2019-11-16 18:28:45+00:00
- **Updated**: 2019-11-16 18:28:45+00:00
- **Authors**: Xuefei Cao, Bor-Chun Chen, Ser-Nam Lim
- **Comment**: None
- **Journal**: None
- **Summary**: Deep metric learning is an important area due to its applicability to many domains such as image retrieval and person re-identification. The main drawback of such models is the necessity for labeled data. In this work, we propose to generate pseudo-labels for deep metric learning directly from clustering assignment and we introduce unsupervised deep metric learning (UDML) regularized by a self-supervision (SS) task. In particular, we propose to regularize the training process by predicting image rotations. Our method (UDML-SS) jointly learns discriminative embeddings, unsupervised clustering assignments of the embeddings, as well as a self-supervised pretext task. UDML-SS iteratively cluster embeddings using traditional clustering algorithm (e.g., k-means), and sampling training pairs based on the cluster assignment for metric learning, while optimizing self-supervised pretext task in a multi-task fashion. The role of self-supervision is to stabilize the training process and encourages the model to learn meaningful feature representations that are not distorted due to unreliable clustering assignments. The proposed method performs well on standard benchmarks for metric learning, where it outperforms current state-of-the-art approaches by a large margin and it also shows competitive performance with various metric learning loss functions.



### Signed Input Regularization
- **Arxiv ID**: http://arxiv.org/abs/1911.07086v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07086v3)
- **Published**: 2019-11-16 19:56:43+00:00
- **Updated**: 2019-12-11 05:09:35+00:00
- **Authors**: Saeid Asgari Taghanaki, Kumar Abhishek, Ghassan Hamarneh
- **Comment**: None
- **Journal**: None
- **Summary**: Over-parameterized deep models usually over-fit to a given training distribution, which makes them sensitive to small changes and out-of-distribution samples at inference time, leading to low generalization performance. To this end, several model-based and randomized data-dependent regularization methods are applied, such as data augmentation, which prevents a model from memorizing the training distribution. Instead of the random transformation of the input images, we propose SIGN, a new regularization method, which modifies the input variables using a linear transformation by estimating each variable's contribution to the final prediction. Our proposed technique maps the input data to a new manifold where the less important variables are de-emphasized. To test the effectiveness of the proposed idea and compare it with other competing methods, we design several test scenarios, such as classification performance, uncertainty, out-of-distribution, and robustness analyses. We compare the methods using three different datasets and four models. We find that SIGN encourages more compact class representations, which results in the model's robustness to random corruptions and out-of-distribution samples while also simultaneously achieving superior performance on normal data compared to other competing methods. Our experiments also demonstrate the successful transferability of the SIGN samples from one model to another.



### Liver Steatosis Segmentation with Deep Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/1911.07088v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07088v1)
- **Published**: 2019-11-16 20:04:09+00:00
- **Updated**: 2019-11-16 20:04:09+00:00
- **Authors**: Xiaoyuan Guo, Fusheng Wang, George Teodorou, Alton B. Farris, Jun Kong
- **Comment**: 4 pages
- **Journal**: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2019) Venice, Italy, April 8-11, 2019
- **Summary**: Liver steatosis is known as the abnormal accumulation of lipids within cells. An accurate quantification of steatosis area within the liver histopathological microscopy images plays an important role in liver disease diagnosis and trans-plantation assessment. Such a quantification analysis often requires a precise steatosis segmentation that is challenging due to abundant presence of highly overlapped steatosis droplets. In this paper, a deep learning model Mask-RCNN is used to segment the steatosis droplets in clumps. Extended from Faster R-CNN, Mask-RCNN can predict object masks in addition to bounding box detection. With transfer learning, the resulting model is able to segment overlapped steatosis regions at 75.87% by Average Precision, 60.66% by Recall,65.88% by F1-score, and 76.97% by Jaccard index, promising to support liver disease diagnosis and allograft rejection prediction in future clinical practice.



### SMART: Skeletal Motion Action Recognition aTtack
- **Arxiv ID**: http://arxiv.org/abs/1911.07107v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07107v3)
- **Published**: 2019-11-16 22:25:29+00:00
- **Updated**: 2020-03-10 13:12:04+00:00
- **Authors**: He Wang, Feixiang He, Zhexi Peng, Yongliang Yang, Tianjia Shao, Kun Zhou, David Hogg
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attack has inspired great interest in computer vision, by showing that classification-based solutions are prone to imperceptible attack in many tasks. In this paper, we propose a method, SMART, to attack action recognizers which rely on 3D skeletal motions. Our method involves an innovative perceptual loss which ensures the imperceptibility of the attack. Empirical studies demonstrate that SMART is effective in both white-box and black-box scenarios. Its generalizability is evidenced on a variety of action recognizers and datasets. Its versatility is shown in different attacking strategies. Its deceitfulness is proven in extensive perceptual studies. Finally, SMART shows that adversarial attack on 3D skeletal motion, one type of time-series data, is significantly different from traditional adversarial attack problems.




# Arxiv Papers in cs.CV on 2019-11-25
### Reducing the Human Effort in Developing PET-CT Registration
- **Arxiv ID**: http://arxiv.org/abs/1911.10657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10657v1)
- **Published**: 2019-11-25 01:36:33+00:00
- **Updated**: 2019-11-25 01:36:33+00:00
- **Authors**: Teaghan O'Briain, Kyong Hwan Jin, Hongyoon Choi, Erika Chin, Magdalena Bazalova-Carter, Kwang Moo Yi
- **Comment**: None
- **Journal**: None
- **Summary**: We aim to reduce the tedious nature of developing and evaluating methods for aligning PET-CT scans from multiple patient visits. Current methods for registration rely on correspondences that are created manually by medical experts with 3D manipulation, or assisted alignments done by utilizing mutual information across CT scans that may not be consistent when transferred to the PET images. Instead, we propose to label multiple key points across several 2D slices, which we then fit a key curve to. This removes the need for creating manual alignments in 3D and makes the labelling process easier. We use these key curves to define an error metric for the alignments that can be computed efficiently. While our metric is non-differentiable, we further show that we can utilize it during the training of our deep model via a novel method. Specifically, instead of relying on detailed geometric labels -- e.g., manual 3D alignments -- we use synthetically generated deformations of real data. To incorporate robustness to changes that occur between visits other than geometric changes, we enforce consistency across visits in the deep network's internal representations. We demonstrate the potential of our method via qualitative and quantitative experiments.



### GAC-GAN: A General Method for Appearance-Controllable Human Video Motion Transfer
- **Arxiv ID**: http://arxiv.org/abs/1911.10672v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1911.10672v2)
- **Published**: 2019-11-25 02:56:47+00:00
- **Updated**: 2020-02-09 15:52:47+00:00
- **Authors**: Dongxu Wei, Xiaowei Xu, Haibin Shen, Kejie Huang
- **Comment**: paper under review
- **Journal**: None
- **Summary**: Human video motion transfer has a wide range of applications in multimedia, computer vision and graphics. Recently, due to the rapid development of Generative Adversarial Networks (GANs), there has been significant progress in the field. However, almost all existing GAN-based works are prone to address the mapping from human motions to video scenes, with scene appearances are encoded individually in the trained models. Therefore, each trained model can only generate videos with a specific scene appearance, new models are required to be trained to generate new appearances. Besides, existing works lack the capability of appearance control. For example, users have to provide video records of wearing new clothes or performing in new backgrounds to enable clothes or background changing in their synthetic videos, which greatly limits the application flexibility. In this paper, we propose GAC-GAN, a general method for appearance-controllable human video motion transfer. To enable general-purpose appearance synthesis, we propose to include appearance information in the conditioning inputs. Thus, once trained, our model can generate new appearances by altering the input appearance information. To achieve appearance control, we first obtain the appearance-controllable conditioning inputs and then utilize a two-stage GAC-GAN to generate the corresponding appearance-controllable outputs, where we utilize an ACGAN loss and a shadow extraction module for output foreground and background appearance control respectively. We further build a solo dance dataset containing a large number of dance videos for training and evaluation. Experimental results show that, our proposed GAC-GAN can not only support appearance-controllable human video motion transfer but also achieve higher video quality than state-of-art methods.



### Attribute Restoration Framework for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1911.10676v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10676v3)
- **Published**: 2019-11-25 03:06:43+00:00
- **Updated**: 2020-12-12 07:50:28+00:00
- **Authors**: Chaoqin Huang, Fei Ye, Jinkun Cao, Maosen Li, Ya Zhang, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: With the recent advances in deep neural networks, anomaly detection in multimedia has received much attention in the computer vision community. While reconstruction-based methods have recently shown great promise for anomaly detection, the information equivalence among input and supervision for reconstruction tasks can not effectively force the network to learn semantic feature embeddings. We here propose to break this equivalence by erasing selected attributes from the original data and reformulate it as a restoration task, where the normal and the anomalous data are expected to be distinguishable based on restoration errors. Through forcing the network to restore the original image, the semantic feature embeddings related to the erased attributes are learned by the network. During testing phases, because anomalous data are restored with the attribute learned from the normal data, the restoration error is expected to be large. Extensive experiments have demonstrated that the proposed method significantly outperforms several state-of-the-arts on multiple benchmark datasets, especially on ImageNet, increasing the AUROC of the top-performing baseline by 10.1%. We also evaluate our method on a real-world anomaly detection dataset MVTec AD and a video anomaly detection dataset ShanghaiTech.



### Image-based table recognition: data, model, and evaluation
- **Arxiv ID**: http://arxiv.org/abs/1911.10683v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10683v5)
- **Published**: 2019-11-25 03:25:03+00:00
- **Updated**: 2020-03-04 02:38:07+00:00
- **Authors**: Xu Zhong, Elaheh ShafieiBavani, Antonio Jimeno Yepes
- **Comment**: None
- **Journal**: None
- **Summary**: Important information that relates to a specific topic in a document is often organized in tabular format to assist readers with information retrieval and comparison, which may be difficult to provide in natural language. However, tabular data in unstructured digital documents, e.g., Portable Document Format (PDF) and images, are difficult to parse into structured machine-readable format, due to complexity and diversity in their structure and style. To facilitate image-based table recognition with deep learning, we develop the largest publicly available table recognition dataset PubTabNet (https://github.com/ibm-aur-nlp/PubTabNet), containing 568k table images with corresponding structured HTML representation. PubTabNet is automatically generated by matching the XML and PDF representations of the scientific articles in PubMed Central Open Access Subset (PMCOA). We also propose a novel attention-based encoder-dual-decoder (EDD) architecture that converts images of tables into HTML code. The model has a structure decoder which reconstructs the table structure and helps the cell decoder to recognize cell content. In addition, we propose a new Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition, which more appropriately captures multi-hop cell misalignment and OCR errors than the pre-established metric. The experiments demonstrate that the EDD model can accurately recognize complex tables solely relying on the image representation, outperforming the state-of-the-art by 9.7% absolute TEDS score.



### Zero-Shot Imitating Collaborative Manipulation Plans from YouTube Cooking Videos
- **Arxiv ID**: http://arxiv.org/abs/1911.10686v5
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10686v5)
- **Published**: 2019-11-25 03:41:00+00:00
- **Updated**: 2022-09-26 05:44:56+00:00
- **Authors**: Hejia Zhang, Jie Zhong, Stefanos Nikolaidis
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: People often watch videos on the web to learn how to cook new recipes, assemble furniture or repair a computer. We wish to enable robots with the very same capability. This is challenging; there is a large variation in manipulation actions and some videos even involve multiple persons, who collaborate by sharing and exchanging objects and tools. Furthermore, the learned representations need to be general enough to be transferable to robotic systems. On the other hand, previous work has shown that the space of human manipulation actions has a linguistic, hierarchical structure that relates actions to manipulated objects and tools. Building upon this theory of language for action, we propose a system for understanding and executing demonstrated action sequences from full-length, real-world cooking videos on the web. The system takes as input a new, previously unseen cooking video annotated with object labels and bounding boxes, and outputs a collaborative manipulation action plan for one or more robotic arms. We demonstrate performance of the system in a standardized dataset of 100 YouTube cooking videos, as well as in six full-length Youtube videos that include collaborative actions between two participants. We compare our system with a baseline system that consists of a state-of-the-art action detection baseline and show our system achieves higher action detection accuracy. We additionally propose an open-source platform for executing the learned plans in a simulation environment as well as with an actual robotic arm.



### Rethinking Softmax with Cross-Entropy: Neural Network Classifier as Mutual Information Estimator
- **Arxiv ID**: http://arxiv.org/abs/1911.10688v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.10688v4)
- **Published**: 2019-11-25 03:44:34+00:00
- **Updated**: 2020-09-17 04:47:27+00:00
- **Authors**: Zhenyue Qin, Dongwoo Kim, Tom Gedeon
- **Comment**: None
- **Journal**: None
- **Summary**: Mutual information is widely applied to learn latent representations of observations, whilst its implication in classification neural networks remain to be better explained. We show that optimising the parameters of classification neural networks with softmax cross-entropy is equivalent to maximising the mutual information between inputs and labels under the balanced data assumption. Through experiments on synthetic and real datasets, we show that softmax cross-entropy can estimate mutual information approximately. When applied to image classification, this relation helps approximate the point-wise mutual information between an input image and a label without modifying the network structure. To this end, we propose infoCAM, informative class activation map, which highlights regions of the input image that are the most relevant to a given label based on differences in information. The activation map helps localise the target object in an input image. Through experiments on the semi-supervised object localisation task with two real-world datasets, we evaluate the effectiveness of our information-theoretic approach.



### Mitigate Bias in Face Recognition using Skewness-Aware Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.10692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10692v1)
- **Published**: 2019-11-25 03:58:11+00:00
- **Updated**: 2019-11-25 03:58:11+00:00
- **Authors**: Mei Wang, Weihong Deng
- **Comment**: None
- **Journal**: CVPR 2020: 9322-9331
- **Summary**: Racial equality is an important theme of international human rights law, but it has been largely obscured when the overall face recognition accuracy is pursued blindly. More facts indicate racial bias indeed degrades the fairness of recognition system and the error rates on non-Caucasians are usually much higher than Caucasians. To encourage fairness, we introduce the idea of adaptive margin to learn balanced performance for different races based on large margin losses. A reinforcement learning based race balance network (RL-RBN) is proposed. We formulate the process of finding the optimal margins for non-Caucasians as a Markov decision process and employ deep Q-learning to learn policies for an agent to select appropriate margin by approximating the Q-value function. Guided by the agent, the skewness of feature scatter between races can be reduced. Besides, we provide two ethnicity aware training datasets, called BUPT-Globalface and BUPT-Balancedface dataset, which can be utilized to study racial bias from both data and algorithm aspects. Extensive experiments on RFW database show that RL-RBN successfully mitigates racial bias and learns more balanced performance for different races.



### When NAS Meets Robustness: In Search of Robust Architectures against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1911.10695v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.10695v3)
- **Published**: 2019-11-25 04:14:02+00:00
- **Updated**: 2020-03-26 01:37:40+00:00
- **Authors**: Minghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, Dahua Lin
- **Comment**: CVPR 2020. First two authors contributed equally
- **Journal**: None
- **Summary**: Recent advances in adversarial attacks uncover the intrinsic vulnerability of modern deep neural networks. Since then, extensive efforts have been devoted to enhancing the robustness of deep networks via specialized learning algorithms and loss functions. In this work, we take an architectural perspective and investigate the patterns of network architectures that are resilient to adversarial attacks. To obtain the large number of networks needed for this study, we adopt one-shot neural architecture search, training a large network for once and then finetuning the sub-networks sampled therefrom. The sampled architectures together with the accuracies they achieve provide a rich basis for our study. Our "robust architecture Odyssey" reveals several valuable observations: 1) densely connected patterns result in improved robustness; 2) under computational budget, adding convolution operations to direct connection edge is effective; 3) flow of solution procedure (FSP) matrix is a good indicator of network robustness. Based on these observations, we discover a family of robust architectures (RobNets). On various datasets, including CIFAR, SVHN, Tiny-ImageNet, and ImageNet, RobNets exhibit superior robustness performance to other widely used architectures. Notably, RobNets substantially improve the robust accuracy (~5% absolute gains) under both white-box and black-box attacks, even with fewer parameter numbers. Code is available at https://github.com/gmh14/RobNets.



### Natural Image Manipulation for Autoregressive Models Using Fisher Scores
- **Arxiv ID**: http://arxiv.org/abs/1912.05015v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05015v2)
- **Published**: 2019-11-25 05:31:30+00:00
- **Updated**: 2020-05-11 03:21:32+00:00
- **Authors**: Wilson Yan, Jonathan Ho, Pieter Abbeel
- **Comment**: None
- **Journal**: None
- **Summary**: Deep autoregressive models are one of the most powerful models that exist today which achieve state-of-the-art bits per dim. However, they lie at a strict disadvantage when it comes to controlled sample generation compared to latent variable models. Latent variable models such as VAEs and normalizing flows allow meaningful semantic manipulations in latent space, which autoregressive models do not have. In this paper, we propose using Fisher scores as a method to extract embeddings from an autoregressive model to use for interpolation and show that our method provides more meaningful sample manipulation compared to alternate embeddings such as network activations.



### Prototype Rectification for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.10713v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10713v4)
- **Published**: 2019-11-25 06:04:29+00:00
- **Updated**: 2020-07-13 06:59:51+00:00
- **Authors**: Jinlu Liu, Liang Song, Yongqiang Qin
- **Comment**: ECCV 2020 Oral
- **Journal**: None
- **Summary**: Few-shot learning requires to recognize novel classes with scarce labeled data. Prototypical network is useful in existing researches, however, training on narrow-size distribution of scarce data usually tends to get biased prototypes. In this paper, we figure out two key influencing factors of the process: the intra-class bias and the cross-class bias. We then propose a simple yet effective approach for prototype rectification in transductive setting. The approach utilizes label propagation to diminish the intra-class bias and feature shifting to diminish the cross-class bias. We also conduct theoretical analysis to derive its rationality as well as the lower bound of the performance. Effectiveness is shown on three few-shot benchmarks. Notably, our approach achieves state-of-the-art performance on both miniImageNet (70.31% on 1-shot and 81.89% on 5-shot) and tieredImageNet (78.74% on 1-shot and 86.92% on 5-shot).



### Cascaded Detail-Preserving Networks for Super-Resolution of Document Images
- **Arxiv ID**: http://arxiv.org/abs/1911.10714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10714v1)
- **Published**: 2019-11-25 06:19:41+00:00
- **Updated**: 2019-11-25 06:19:41+00:00
- **Authors**: Zhichao Fu, Yu Kong, Yingbin Zheng, Hao Ye, Wenxin Hu, Jing Yang, Liang He
- **Comment**: None
- **Journal**: None
- **Summary**: The accuracy of OCR is usually affected by the quality of the input document image and different kinds of marred document images hamper the OCR results. Among these scenarios, the low-resolution image is a common and challenging case. In this paper, we propose the cascaded networks for document image super-resolution. Our model is composed by the Detail-Preserving Networks with small magnification. The loss function with perceptual terms is designed to simultaneously preserve the original patterns and enhance the edge of the characters. These networks are trained with the same architecture and different parameters and then assembled into a pipeline model with a larger magnification. The low-resolution images can upscale gradually by passing through each Detail-Preserving Network until the final high-resolution images. Through extensive experiments on two scanning document image datasets, we demonstrate that the proposed approach outperforms recent state-of-the-art image super-resolution methods, and combining it with standard OCR system lead to signification improvements on the recognition results.



### GBCNs: Genetic Binary Convolutional Networks for Enhancing the Performance of 1-bit DCNNs
- **Arxiv ID**: http://arxiv.org/abs/1911.11634v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11634v2)
- **Published**: 2019-11-25 06:44:20+00:00
- **Updated**: 2020-01-24 17:20:50+00:00
- **Authors**: Chunlei Liu, Wenrui Ding, Yuan Hu, Baochang Zhang, Jianzhuang Liu, Guodong Guo
- **Comment**: We withdraw it for error in Fig. 4. Yuan Hu, who is supervised by
  Chunlei Liu, forgot to set "tensorboard" for exp. Chunlei knew this and they
  thought ok regress several recorded real points on old data for Fig. 4. Max.
  accuracy is right. But made error due to insufficient communication. Hu and
  Liu are responsible for it. Other authors were not told details. We will
  correct code github.com/liuchunlei0430
- **Journal**: None
- **Summary**: Training 1-bit deep convolutional neural networks (DCNNs) is one of the most challenging problems in computer vision, because it is much easier to get trapped into local minima than conventional DCNNs. The reason lies in that the binarized kernels and activations of 1-bit DCNNs cause a significant accuracy loss and training inefficiency. To address this problem, we propose Genetic Binary Convolutional Networks (GBCNs) to optimize 1-bit DCNNs, by introducing a new balanced Genetic Algorithm (BGA) to improve the representational ability in an end-to-end framework. The BGA method is proposed to modify the binary process of GBCNs to alleviate the local minima problem, which can significantly improve the performance of 1-bit DCNNs. We develop a new BGA module that is generic and flexible, and can be easily incorporated into existing DCNNs, such asWideResNets and ResNets. Extensive experiments on the object classification tasks (CIFAR, ImageNet) validate the effectiveness of the proposed method. To highlight, our method shows strong generalization on the object recognition task, i.e., face recognition, facial and person re-identification.



### AOP: An Anti-overfitting Pretreatment for Practical Image-based Plant Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1911.10727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10727v1)
- **Published**: 2019-11-25 06:59:08+00:00
- **Updated**: 2019-11-25 06:59:08+00:00
- **Authors**: Takumi Saikawa, Quan Huu Cap, Satoshi Kagiwada, Hiroyuki Uga, Hitoshi Iyatomi
- **Comment**: To appear in the IEEE BigData 2019 Workshop on Big Food and Nutrition
  Data Management and Analysis
- **Journal**: None
- **Summary**: In image-based plant diagnosis, clues related to diagnosis are often unclear, and the other factors such as image backgrounds often have a significant impact on the final decision. As a result, overfitting due to latent similarities in the dataset often occurs, and the diagnostic performance on real unseen data (e,g. images from other farms) is usually dropped significantly. However, this problem has not been sufficiently explored, since many systems have shown excellent diagnostic performance due to the bias caused by the similarities in the dataset. In this study, we investigate this problem with experiments using more than 50,000 images of cucumber leaves, and propose an anti-overfitting pretreatment (AOP) for realizing practical image-based plant diagnosis systems. The AOP detects the area of interest (leaf, fruit etc.) and performs brightness calibration as a preprocessing step. The experimental results demonstrate that our AOP can improve the accuracy of diagnosis for unknown test images from different farms by 12.2% in a practical setting.



### Point Cloud Processing via Recurrent Set Encoding
- **Arxiv ID**: http://arxiv.org/abs/1911.10729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10729v1)
- **Published**: 2019-11-25 07:04:40+00:00
- **Updated**: 2019-11-25 07:04:40+00:00
- **Authors**: Pengxiang Wu, Chao Chen, Jingru Yi, Dimitris Metaxas
- **Comment**: AAAI2019
- **Journal**: None
- **Summary**: We present a new permutation-invariant network for 3D point cloud processing. Our network is composed of a recurrent set encoder and a convolutional feature aggregator. Given an unordered point set, the encoder firstly partitions its ambient space into parallel beams. Points within each beam are then modeled as a sequence and encoded into subregional geometric features by a shared recurrent neural network (RNN). The spatial layout of the beams is regular, and this allows the beam features to be further fed into an efficient 2D convolutional neural network (CNN) for hierarchical feature aggregation. Our network is effective at spatial feature learning, and competes favorably with the state-of-the-arts (SOTAs) on a number of benchmarks. Meanwhile, it is significantly more efficient compared to the SOTAs.



### Nearest Neighbor Sampling of Point Sets using Rays
- **Arxiv ID**: http://arxiv.org/abs/1911.10737v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T09, 65D19 (Primary) 68T07, 65D40 (Secondary), G.0; G.2.3; G.3; I.2.10; I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/1911.10737v4)
- **Published**: 2019-11-25 07:31:54+00:00
- **Updated**: 2023-05-29 23:16:24+00:00
- **Authors**: Liangchen Liu, Louis Ly, Colin Macdonald, Yen-Hsi Richard Tsai
- **Comment**: 48 pages, 14 figures, submitted to CAMC special issue, additional
  content added
- **Journal**: None
- **Summary**: We propose a new framework for the sampling, compression, and analysis of distributions of point sets and other geometric objects embedded in Euclidean spaces. Our approach involves the construction of a tensor called the RaySense sketch, which captures the nearest neighbors from the underlying geometry of points along a set of rays. We explore various operations that can be performed on the RaySense sketch, leading to different properties and potential applications. Statistical information about the data set can be extracted from the sketch, independent of the ray set. Line integrals on point sets can be efficiently computed using the sketch. We also present several examples illustrating applications of the proposed strategy in practical scenarios.



### Empirical Study of Easy and Hard Examples in CNN Training
- **Arxiv ID**: http://arxiv.org/abs/1911.10739v1
- **DOI**: 10.1007/978-3-030-36808-1_20
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10739v1)
- **Published**: 2019-11-25 07:32:04+00:00
- **Updated**: 2019-11-25 07:32:04+00:00
- **Authors**: Ikki Kishida, Hideki Nakayama
- **Comment**: Accepted to ICONIP 2019
- **Journal**: ICONIP 2019
- **Summary**: Deep Neural Networks (DNNs) generalize well despite their massive size and capability of memorizing all examples. There is a hypothesis that DNNs start learning from simple patterns and the hypothesis is based on the existence of examples that are consistently well-classified at the early training stage (i.e., easy examples) and examples misclassified (i.e., hard examples). Easy examples are the evidence that DNNs start learning from specific patterns and there is a consistent learning process. It is important to know how DNNs learn patterns and obtain generalization ability, however, properties of easy and hard examples are not thoroughly investigated (e.g., contributions to generalization and visual appearances). In this work, we study the similarities of easy and hard examples respectively for different Convolutional Neural Network (CNN) architectures, assessing how those examples contribute to generalization. Our results show that easy examples are visually similar to each other and hard examples are visually diverse, and both examples are largely shared across different CNN architectures. Moreover, while hard examples tend to contribute more to generalization than easy examples, removing a large number of easy examples leads to poor generalization. By analyzing those results, we hypothesize that biases in a dataset and Stochastic Gradient Descent (SGD) are the reasons why CNNs have consistent easy and hard examples. Furthermore, we show that large scale classification datasets can be efficiently compressed by using easiness proposed in this work.



### Deep Image-to-Video Adaptation and Fusion Networks for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.10751v1
- **DOI**: 10.1109/TIP.2019.2957930
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10751v1)
- **Published**: 2019-11-25 08:02:17+00:00
- **Updated**: 2019-11-25 08:02:17+00:00
- **Authors**: Yang Liu, Zhaoyang Lu, Jing Li, Tao Yang, Chao Yao
- **Comment**: Accepted by IEEE Transactions on Image Processing, codes can be found
  at https://yangliu9208.github.io/DIVAFN/
- **Journal**: None
- **Summary**: Existing deep learning methods for action recognition in videos require a large number of labeled videos for training, which is labor-intensive and time-consuming. For the same action, the knowledge learned from different media types, e.g., videos and images, may be related and complementary. However, due to the domain shifts and heterogeneous feature representations between videos and images, the performance of classifiers trained on images may be dramatically degraded when directly deployed to videos. In this paper, we propose a novel method, named Deep Image-to-Video Adaptation and Fusion Networks (DIVAFN), to enhance action recognition in videos by transferring knowledge from images using video keyframes as a bridge. The DIVAFN is a unified deep learning model, which integrates domain-invariant representations learning and cross-modal feature fusion into a unified optimization framework. Specifically, we design an efficient cross-modal similarities metric to reduce the modality shift among images, keyframes and videos. Then, we adopt an autoencoder architecture, whose hidden layer is constrained to be the semantic representations of the action class names. In this way, when the autoencoder is adopted to project the learned features from different domains to the same space, more compact, informative and discriminative representations can be obtained. Finally, the concatenation of the learned semantic feature representations from these three autoencoders are used to train the classifier for action recognition in videos. Comprehensive experiments on four real-world datasets show that our method outperforms some state-of-the-art domain adaptation and action recognition methods.



### Fast and Incremental Loop Closure Detection Using Proximity Graphs
- **Arxiv ID**: http://arxiv.org/abs/1911.10752v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10752v1)
- **Published**: 2019-11-25 08:03:59+00:00
- **Updated**: 2019-11-25 08:03:59+00:00
- **Authors**: Shan An, Guangfu Che, Fangru Zhou, Xianglong Liu, Xin Ma, Yu Chen
- **Comment**: 8 pages, 6 figures, IROS 2019
- **Journal**: None
- **Summary**: Visual loop closure detection, which can be considered as an image retrieval task, is an important problem in SLAM (Simultaneous Localization and Mapping) systems. The frequently used bag-of-words (BoW) models can achieve high precision and moderate recall. However, the requirement for lower time costs and fewer memory costs for mobile robot applications is not well satisfied. In this paper, we propose a novel loop closure detection framework titled `FILD' (Fast and Incremental Loop closure Detection), which focuses on an on-line and incremental graph vocabulary construction for fast loop closure detection. The global and local features of frames are extracted using the Convolutional Neural Networks (CNN) and SURF on the GPU, which guarantee extremely fast extraction speeds. The graph vocabulary construction is based on one type of proximity graph, named Hierarchical Navigable Small World (HNSW) graphs, which is modified to adapt to this specific application. In addition, this process is coupled with a novel strategy for real-time geometrical verification, which only keeps binary hash codes and significantly saves on memory usage. Extensive experiments on several publicly available datasets show that the proposed approach can achieve fairly good recall at 100\% precision compared to other state-of-the-art methods. The source code can be downloaded at https://github.com/AnshanTJU/FILD for further studies.



### Regularized Fine-grained Meta Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/1911.10771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10771v1)
- **Published**: 2019-11-25 08:55:46+00:00
- **Updated**: 2019-11-25 08:55:46+00:00
- **Authors**: Rui Shao, Xiangyuan Lan, Pong C. Yuen
- **Comment**: Accepted by AAAI 2020. Codes are available at
  https://github.com/rshaojimmy/AAAI2020-RFMetaFAS
- **Journal**: None
- **Summary**: Face presentation attacks have become an increasingly critical concern when face recognition is widely applied. Many face anti-spoofing methods have been proposed, but most of them ignore the generalization ability to unseen attacks. To overcome the limitation, this work casts face anti-spoofing as a domain generalization (DG) problem, and attempts to address this problem by developing a new meta-learning framework called Regularized Fine-grained Meta-learning. To let our face anti-spoofing model generalize well to unseen attacks, the proposed framework trains our model to perform well in the simulated domain shift scenarios, which is achieved by finding generalized learning directions in the meta-learning process. Specifically, the proposed framework incorporates the domain knowledge of face anti-spoofing as the regularization so that meta-learning is conducted in the feature space regularized by the supervision of domain knowledge. This enables our model more likely to find generalized learning directions with the regularized meta-learning for face anti-spoofing task. Besides, to further enhance the generalization ability of our model, the proposed framework adopts a fine-grained learning strategy that simultaneously conducts meta-learning in a variety of domain shift scenarios in each iteration. Extensive experiments on four public datasets validate the effectiveness of the proposed method.



### Fine-grained Attention and Feature-sharing Generative Adversarial Networks for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1911.10773v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10773v2)
- **Published**: 2019-11-25 09:03:15+00:00
- **Updated**: 2020-12-24 14:21:22+00:00
- **Authors**: Yitong Yan, Chuangchuang Liu, Changyou Chen, Xianfang Sun, Longcun Jin, Xiang Zhou
- **Comment**: 15 pages, 14 figures, 7 tables
- **Journal**: None
- **Summary**: The traditional super-resolution methods that aim to minimize the mean square error usually produce the images with over-smoothed and blurry edges, due to the lose of high-frequency details. In this paper, we propose two novel techniques in the generative adversarial networks to produce photo-realistic images for image super-resolution. Firstly, instead of producing a single score to discriminate images between real and fake, we propose a variant, called Fine-grained Attention Generative Adversarial Network for image super-resolution (FASRGAN), to discriminate each pixel between real and fake. FASRGAN adopts a Unet-like network as the discriminator with two outputs: an image score and an image score map. The score map has the same spatial size as the HR/SR images, serving as the fine-grained attention to represent the degree of reconstruction difficulty for each pixel. Secondly, instead of using different networks for the generator and the discriminator in the SR problem, we use a feature-sharing network (Fs-SRGAN) for both the generator and the discriminator. By network sharing, certain information is shared between the generator and the discriminator, which in turn can improve the ability of producing high-quality images. Quantitative and visual comparisons with the state-of-the-art methods on the benchmark datasets demonstrate the superiority of our methods. The application of super-resolution images to object recognition further proves that the proposed methods endow the power to reconstruction capabilities and the excellent super-resolution effects.



### Estimating People Flows to Better Count Them in Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/1911.10782v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.10782v4)
- **Published**: 2019-11-25 09:34:40+00:00
- **Updated**: 2020-07-15 16:59:04+00:00
- **Authors**: Weizhe Liu, Mathieu Salzmann, Pascal Fua
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Modern methods for counting people in crowded scenes rely on deep networks to estimate people densities in individual images. As such, only very few take advantage of temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames.   In this paper, we advocate estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing. This enables us to impose much stronger constraints encoding the conservation of the number of people. As a result, it significantly boosts performance without requiring a more complex architecture. Furthermore, it also enables us to exploit the correlation between people flow and optical flow to further improve the results.   We will demonstrate that we consistently outperform state-of-the-art methods on five benchmark datasets.



### Matrix Normal PCA for Interpretable Dimension Reduction and Graphical Noise Modeling
- **Arxiv ID**: http://arxiv.org/abs/1911.10796v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.10796v2)
- **Published**: 2019-11-25 09:59:33+00:00
- **Updated**: 2021-01-05 15:01:38+00:00
- **Authors**: Chihao Zhang, Kuo Gai, Shihua Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Principal component analysis (PCA) is one of the most widely used dimension reduction and multivariate statistical techniques. From a probabilistic perspective, PCA seeks a low-dimensional representation of data in the presence of independent identical Gaussian noise. Probabilistic PCA (PPCA) and its variants have been extensively studied for decades. Most of them assume the underlying noise follows a certain independent identical distribution. However, the noise in the real world is usually complicated and structured. To address this challenge, some variants of PCA for data with non-IID noise have been proposed. However, most of the existing methods only assume that the noise is correlated in the feature space while there may exist two-way structured noise. To this end, we propose a powerful and intuitive PCA method (MN-PCA) through modeling the graphical noise by the matrix normal distribution, which enables us to explore the structure of noise in both the feature space and the sample space. MN-PCA obtains a low-rank representation of data and the structure of noise simultaneously. And it can be explained as approximating data over the generalized Mahalanobis distance. We develop two algorithms to solve this model: one maximizes the regularized likelihood, the other exploits the Wasserstein distance, which is more robust. Extensive experiments on various data demonstrate their effectiveness.



### Generalized Adaptation for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.10807v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10807v3)
- **Published**: 2019-11-25 10:18:37+00:00
- **Updated**: 2020-06-02 06:24:39+00:00
- **Authors**: Liang Song, Jinlu Liu, Yongqiang Qin
- **Comment**: We found a bug in the script in reexamining some of this work. We
  decide to withdraw for further modification
- **Journal**: None
- **Summary**: Many Few-Shot Learning research works have two stages: pre-training base model and adapting to novel model. In this paper, we propose to use closed-form base learner, which constrains the adapting stage with pre-trained base model to get better generalized novel model. Following theoretical analysis proves its rationality as well as indication of how to train a well-generalized base model. We then conduct experiments on four benchmarks and achieve state-of-the-art performance in all cases. Notably, we achieve the accuracy of 87.75% on 5-shot miniImageNet which approximately outperforms existing methods by 10%.



### Deep Image Deraining Via Intrinsic Rainy Image Priors and Multi-scale Auxiliary Decoding
- **Arxiv ID**: http://arxiv.org/abs/1911.10810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10810v1)
- **Published**: 2019-11-25 10:20:15+00:00
- **Updated**: 2019-11-25 10:20:15+00:00
- **Authors**: Yinglong Wang, Chao Ma, Bing Zeng
- **Comment**: 11 figures, 5 table
- **Journal**: None
- **Summary**: Different rain models and novel network structures have been proposed to remove rain streaks from single rainy images. In this work, we bring attention to the intrinsic priors and multi-scale features of the rainy images, and develop several intrinsic loss functions to train a CNN deraining network. We first study the sparse priors of rainy images, which have been verified to preserve unbroken edges in image decomposition. However, its mathematical formulation usually leads to an intractable solution, we propose quasi-sparsity priors to decrease complexity, so that our network can be trained under the supervision of sparse properties of rainy images. Quasi-sparsity supervises network training in different gradient domain which is still ill-posed to decompose a rainy image into rain layer and background layer. We develop another $L_1$ loss based on the intrinsic low-value property of rain layer to restore image contents together with the commonly-used $L_1$ similarity loss. Multi-scale features are further explored via a multi-scale auxiliary decoding structure to show which kinds of features contribute the most to the deraining task, and the corresponding multi-scale auxiliary loss improves the deraining performance further. In our network, more efficient group convolution and feature sharing are utilized to obtain an one order of magnitude improvement in network running speed. The proposed deraining method performs favorably against state-of-the-art deraining approaches.



### Discriminative training of conditional random fields with probably submodular constraints
- **Arxiv ID**: http://arxiv.org/abs/1911.10819v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.10819v1)
- **Published**: 2019-11-25 10:38:05+00:00
- **Updated**: 2019-11-25 10:38:05+00:00
- **Authors**: Maxim Berman, Matthew B. Blaschko
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Problems of segmentation, denoising, registration and 3D reconstruction are often addressed with the graph cut algorithm. However, solving an unconstrained graph cut problem is NP-hard. For tractable optimization, pairwise potentials have to fulfill the submodularity inequality. In our learning paradigm, pairwise potentials are created as the dot product of a learned vector w with positive feature vectors. In order to constrain such a model to remain tractable, previous approaches have enforced the weight vector to be positive for pairwise potentials in which the labels differ, and set pairwise potentials to zero in the case that the label remains the same. Such constraints are sufficient to guarantee that the resulting pairwise potentials satisfy the submodularity inequality. However, we show that such an approach unnecessarily restricts the capacity of the learned models. Guaranteeing submodularity for all possible inputs, no matter how improbable, reduces inference error to effectively zero, but increases model error. In contrast, we relax the requirement of guaranteed submodularity to solutions that are probably approximately submodular. We show that the conceptually simple strategy of enforcing submodularity on the training examples guarantees with low sample complexity that test images will also yield submodular pairwise potentials. Results are presented in the binary and muticlass settings, showing substantial improvement from the resulting increased model capacity.



### Binarized Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1911.10862v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10862v2)
- **Published**: 2019-11-25 12:25:05+00:00
- **Updated**: 2020-02-11 11:50:28+00:00
- **Authors**: Hanlin Chen, Li'an Zhuo, Baochang Zhang, Xiawu Zheng, Jianzhuang Liu, David Doermann, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Neural architecture search (NAS) can have a significant impact in computer vision by automatically designing optimal neural network architectures for various tasks. A variant, binarized neural architecture search (BNAS), with a search space of binarized convolutions, can produce extremely compressed models. Unfortunately, this area remains largely unexplored. BNAS is more challenging than NAS due to the learning inefficiency caused by optimization requirements and the huge architecture space. To address these issues, we introduce channel sampling and operation space reduction into a differentiable NAS to significantly reduce the cost of searching. This is accomplished through a performance-based strategy used to abandon less potential operations. Two optimization methods for binarized neural networks are used to validate the effectiveness of our BNAS. Extensive experiments demonstrate that the proposed BNAS achieves a performance comparable to NAS on both CIFAR and ImageNet databases. An accuracy of $96.53\%$ vs. $97.22\%$ is achieved on the CIFAR-10 dataset, but with a significantly compressed model, and a $40\%$ faster search than the state-of-the-art PC-DARTS.



### End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances
- **Arxiv ID**: http://arxiv.org/abs/1911.10868v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.10868v2)
- **Published**: 2019-11-25 12:34:26+00:00
- **Updated**: 2020-03-16 14:44:13+00:00
- **Authors**: Marin Toromanoff, Emilie Wirbel, Fabien Moutarde
- **Comment**: Accepted at main conference of CVPR 2020
- **Journal**: None
- **Summary**: Reinforcement Learning (RL) aims at learning an optimal behavior policy from its own experiments and not rule-based control methods. However, there is no RL algorithm yet capable of handling a task as difficult as urban driving. We present a novel technique, coined implicit affordances, to effectively leverage RL for urban driving thus including lane keeping, pedestrians and vehicles avoidance, and traffic light detection. To our knowledge we are the first to present a successful RL agent handling such a complex task especially regarding the traffic light detection. Furthermore, we have demonstrated the effectiveness of our method by winning the Camera Only track of the CARLA challenge.



### Learning New Tricks from Old Dogs -- Inter-Species, Inter-Tissue Domain Adaptation for Mitotic Figure Assessment
- **Arxiv ID**: http://arxiv.org/abs/1911.10873v1
- **DOI**: 10.1007/978-3-658-29267-6_1
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.10873v1)
- **Published**: 2019-11-25 12:45:33+00:00
- **Updated**: 2019-11-25 12:45:33+00:00
- **Authors**: Marc Aubreville, Christof A. Bertram, Samir Jabari, Christian Marzahl, Robert Klopfleisch, Andreas Maier
- **Comment**: 5 pages, submission to BVM 2020
- **Journal**: Bildverarbeitung f\"ur die Medizin 2020. Informatik Aktuell. p.
  1-7
- **Summary**: For histopathological tumor assessment, the count of mitotic figures per area is an important part of prognostication. Algorithmic approaches - such as for mitotic figure identification - have significantly improved in recent times, potentially allowing for computer-augmented or fully automatic screening systems in the future. This trend is further supported by whole slide scanning microscopes becoming available in many pathology labs and could soon become a standard imaging tool.   For an application in broader fields of such algorithms, the availability of mitotic figure data sets of sufficient size for the respective tissue type and species is an important precondition, that is, however, rarely met. While algorithmic performance climbed steadily for e.g. human mammary carcinoma, thanks to several challenges held in the field, for most tumor types, data sets are not available.   In this work, we assess domain transfer of mitotic figure recognition using domain adversarial training on four data sets, two from dogs and two from humans. We were able to show that domain adversarial training considerably improves accuracy when applying mitotic figure classification learned from the canine on the human data sets (up to +12.8% in accuracy) and is thus a helpful method to transfer knowledge from existing data sets to new tissue types and species.



### ColorFool: Semantic Adversarial Colorization
- **Arxiv ID**: http://arxiv.org/abs/1911.10891v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10891v2)
- **Published**: 2019-11-25 13:10:29+00:00
- **Updated**: 2020-04-12 09:40:31+00:00
- **Authors**: Ali Shahin Shamsabadi, Ricardo Sanchez-Matilla, Andrea Cavallaro
- **Comment**: Conference on Computer Vision and Pattern Recognition (CVPR2020)
- **Journal**: None
- **Summary**: Adversarial attacks that generate small L_p-norm perturbations to mislead classifiers have limited success in black-box settings and with unseen classifiers. These attacks are also not robust to defenses that use denoising filters and to adversarial training procedures. Instead, adversarial attacks that generate unrestricted perturbations are more robust to defenses, are generally more successful in black-box settings and are more transferable to unseen classifiers. However, unrestricted perturbations may be noticeable to humans. In this paper, we propose a content-based black-box adversarial attack that generates unrestricted perturbations by exploiting image semantics to selectively modify colors within chosen ranges that are perceived as natural by humans. We show that the proposed approach, ColorFool, outperforms in terms of success rate, robustness to defense frameworks and transferability, five state-of-the-art adversarial attacks on two different tasks, scene and object classification, when attacking three state-of-the-art deep neural networks using three standard datasets. The source code is available at https://github.com/smartcameras/ColorFool.



### Sub-frame Appearance and 6D Pose Estimation of Fast Moving Objects
- **Arxiv ID**: http://arxiv.org/abs/1911.10927v1
- **DOI**: 10.1109/CVPR42600.2020.00681
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10927v1)
- **Published**: 2019-11-25 14:13:25+00:00
- **Updated**: 2019-11-25 14:13:25+00:00
- **Authors**: Denys Rozumnyi, Jan Kotera, Filip Sroubek, Jiri Matas
- **Comment**: None
- **Journal**: 2020 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)
- **Summary**: We propose a novel method that tracks fast moving objects, mainly non-uniform spherical, in full 6 degrees of freedom, estimating simultaneously their 3D motion trajectory, 3D pose and object appearance changes with a time step that is a fraction of the video frame exposure time. The sub-frame object localization and appearance estimation allows realistic temporal super-resolution and precise shape estimation. The method, called TbD-3D (Tracking by Deblatting in 3D) relies on a novel reconstruction algorithm which solves a piece-wise deblurring and matting problem. The 3D rotation is estimated by minimizing the reprojection error. As a second contribution, we present a new challenging dataset with fast moving objects that change their appearance and distance to the camera. High speed camera recordings with zero lag between frame exposures were used to generate videos with different frame rates annotated with ground-truth trajectory and pose.



### PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/1911.10949v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.10949v3)
- **Published**: 2019-11-25 14:43:05+00:00
- **Updated**: 2020-04-24 01:38:40+00:00
- **Authors**: Rundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, Baoquan Chen
- **Comment**: Accepted to CVPR 2020. Code available at
  https://github.com/ChrisWu1997/PQ-NET
- **Journal**: None
- **Summary**: We introduce PQ-NET, a deep neural network which represents and generates 3D shapes via sequential part assembly. The input to our network is a 3D shape segmented into parts, where each part is first encoded into a feature representation using a part autoencoder. The core component of PQ-NET is a sequence-to-sequence or Seq2Seq autoencoder which encodes a sequence of part features into a latent vector of fixed size, and the decoder reconstructs the 3D shape, one part at a time, resulting in a sequential assembly. The latent space formed by the Seq2Seq encoder encodes both part structure and fine part geometry. The decoder can be adapted to perform several generative tasks including shape autoencoding, interpolation, novel shape generation, and single-view 3D reconstruction, where the generated shapes are all composed of meaningful parts.



### Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video
- **Arxiv ID**: http://arxiv.org/abs/1911.10967v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.10967v2)
- **Published**: 2019-11-25 15:10:20+00:00
- **Updated**: 2020-07-20 01:58:19+00:00
- **Authors**: Miao Liu, Siyu Tang, Yin Li, James Rehg
- **Comment**: None
- **Journal**: None
- **Summary**: We address the challenging task of anticipating human-object interaction in first person videos. Most existing methods ignore how the camera wearer interacts with the objects, or simply consider body motion as a separate modality. In contrast, we observe that the international hand movement reveals critical information about the future activity. Motivated by this, we adopt intentional hand movement as a future representation and propose a novel deep network that jointly models and predicts the egocentric hand motion, interaction hotspots and future action. Specifically, we consider the future hand motion as the motor attention, and model this attention using latent variables in our deep model. The predicted motor attention is further used to characterise the discriminative spatial-temporal visual features for predicting actions and interaction hotspots. We present extensive experiments demonstrating the benefit of the proposed joint model. Importantly, our model produces new state-of-the-art results for action anticipation on both EGTEA Gaze+ and the EPIC-Kitchens datasets. Our project page is available at https://aptx4869lm.github.io/ForecastingHOI/



### Microscopy Image Restoration with Deep Wiener-Kolmogorov filters
- **Arxiv ID**: http://arxiv.org/abs/1911.10989v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.10989v3)
- **Published**: 2019-11-25 15:36:05+00:00
- **Updated**: 2020-05-14 09:15:55+00:00
- **Authors**: Valeriya Pronina, Filippos Kokkinos, Dmitry V. Dylov, Stamatios Lefkimmiatis
- **Comment**: Updated version
- **Journal**: None
- **Summary**: Microscopy is a powerful visualization tool in biology, enabling the study of cells, tissues, and the fundamental biological processes; yet, the observed images typically suffer from blur and background noise. In this work, we propose a unifying framework of algorithms for Gaussian image deblurring and denoising. These algorithms are based on deep learning techniques for the design of learnable regularizers integrated into the Wiener-Kolmogorov filter. Our extensive experimentation line showcases that the proposed approach achieves a superior quality of image reconstruction and surpasses the solutions that rely either on deep learning or on optimization schemes alone. Augmented with the variance stabilizing transformation, the proposed reconstruction pipeline can also be successfully applied to the problem of Poisson image deblurring, surpassing the state-of-the-art methods. Moreover, several variants of the proposed framework demonstrate competitive performance at low computational complexity, which is of high importance for real-time imaging applications.



### Improving land cover segmentation across satellites using domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/1912.05000v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05000v2)
- **Published**: 2019-11-25 15:41:14+00:00
- **Updated**: 2020-04-01 14:34:21+00:00
- **Authors**: Nadir Bengana, Janne Heikkil
- **Comment**: 12 pages, Transaction
- **Journal**: None
- **Summary**: Land use and land cover mapping are essential to various fields of study, including forestry, agriculture, and urban management. Using earth observation satellites both facilitate and accelerate the task. Lately, deep learning methods have proven to be excellent at automating the mapping via semantic image segmentation. However, because deep neural networks require large amounts of labeled data, it is not easy to exploit the full potential of satellite imagery. Additionally, the land cover tends to differ in appearance from one region to another; therefore, having labeled data from one location does not necessarily help in mapping others. Furthermore, satellite images come in various multispectral bands (the bands could range from RGB to over twelve bands). In this paper, we aim at using domain adaptation to solve the aforementioned problems. We applied a well-performing domain adaptation approach on datasets we have built using RGB images from Sentinel-2, WorldView-2, and Pleiades-1 satellites with Corine Land Cover as ground-truth labels. We have also used the DeepGlobe land cover dataset. Experiments show a significant improvement over results obtained without the use of domain adaptation. In some cases, an improvement of over 20% MIoU. At times it even manages to correct errors in the ground-truth labels.



### Event Recognition with Automatic Album Detection based on Sequential Processing, Neural Attention and Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1911.11010v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T10 (Primary)
- **Links**: [PDF](http://arxiv.org/pdf/1911.11010v2)
- **Published**: 2019-11-25 15:58:18+00:00
- **Updated**: 2020-01-15 05:44:26+00:00
- **Authors**: Andrey V. Savchenko
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper a new formulation of event recognition task is examined: it is required to predict event categories in a gallery of images, for which albums (groups of photos corresponding to a single event) are unknown. We propose the novel two-stage approach. At first, features are extracted in each photo using the pre-trained convolutional neural network. These features are classified individually. The scores of the classifier are used to group sequential photos into several clusters. Finally, the features of photos in each group are aggregated into a single descriptor using neural attention mechanism. This algorithm is optionally extended to improve the accuracy for classification of each image in an album. In contrast to conventional fine-tuning of convolutional neural networks (CNN) we proposed to use image captioning, i.e., generative model that converts images to textual descriptions. They are one-hot encoded and summarized into sparse feature vector suitable for learning of arbitrary classifier. Experimental study with Photo Event Collection and Multi-Label Curation of Flickr Events Dataset demonstrates that our approach is 9-20% more accurate than event recognition on single photos. Moreover, proposed method has 13-16% lower error rate than classification of groups of photos obtained with hierarchical clustering. It is experimentally shown that the image captions trained on Conceptual Captions dataset can be classified more accurately than the features from object detector, though they both are obviously not as rich as the CNN-based features. However, it is possible to combine our approach with conventional CNNs in an ensemble to provide the state-of-the-art results for several event datasets.



### Deep Decomposition Learning for Inverse Imaging Problems
- **Arxiv ID**: http://arxiv.org/abs/1911.11028v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11028v3)
- **Published**: 2019-11-25 16:24:13+00:00
- **Updated**: 2020-07-16 21:47:03+00:00
- **Authors**: Dongdong Chen, Mike E. Davies
- **Comment**: To appear in ECCV 2020
- **Journal**: None
- **Summary**: Deep learning is emerging as a new paradigm for solving inverse imaging problems. However, the deep learning methods often lack the assurance of traditional physics-based methods due to the lack of physical information considerations in neural network training and deploying. The appropriate supervision and explicit calibration by the information of the physic model can enhance the neural network learning and its practical performance. In this paper, inspired by the geometry that data can be decomposed by two components from the null-space of the forward operator and the range space of its pseudo-inverse, we train neural networks to learn the two components and therefore learn the decomposition, i.e. we explicitly reformulate the neural network layers as learning range-nullspace decomposition functions with reference to the layer inputs, instead of learning unreferenced functions. We empirically show that the proposed framework demonstrates superior performance over recent deep residual learning, unrolled learning and nullspace learning on tasks including compressive sensing medical imaging and natural image super-resolution. Our code is available at https://github.com/edongdongchen/DDN.



### Gating Revisited: Deep Multi-layer RNNs That Can Be Trained
- **Arxiv ID**: http://arxiv.org/abs/1911.11033v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11033v4)
- **Published**: 2019-11-25 16:35:51+00:00
- **Updated**: 2021-03-06 12:26:10+00:00
- **Authors**: Mehmet Ozgur Turkoglu, Stefano D'Aronco, Jan Dirk Wegner, Konrad Schindler
- **Comment**: To appear in TPAMI (accepted March 2021)
- **Journal**: None
- **Summary**: We propose a new STAckable Recurrent cell (STAR) for recurrent neural networks (RNNs), which has fewer parameters than widely used LSTM and GRU while being more robust against vanishing or exploding gradients. Stacking recurrent units into deep architectures suffers from two major limitations: (i) many recurrent cells (e.g., LSTMs) are costly in terms of parameters and computation resources; and (ii) deep RNNs are prone to vanishing or exploding gradients during training. We investigate the training of multi-layer RNNs and examine the magnitude of the gradients as they propagate through the network in the "vertical" direction. We show that, depending on the structure of the basic recurrent unit, the gradients are systematically attenuated or amplified. Based on our analysis we design a new type of gated cell that better preserves gradient magnitude. We validate our design on a large number of sequence modelling tasks and demonstrate that the proposed STAR cell allows to build and train deeper recurrent architectures, ultimately leading to improved performance while being computationally more efficient.



### Radius Adaptive Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1911.11079v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11079v1)
- **Published**: 2019-11-25 17:35:04+00:00
- **Updated**: 2019-11-25 17:35:04+00:00
- **Authors**: Meisam Rakhshanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) is widely used in computer vision applications. In the networks that deal with images, CNNs are the most time-consuming layer of the networks. Usually, the solution to address the computation cost is to decrease the number of trainable parameters. This solution usually comes with the cost of dropping the accuracy. Another problem with this technique is that usually the cost of memory access is not taken into account which results in insignificant speedup gain. The number of operations and memory access in a standard convolution layer is independent of the input content, which makes it limited for certain accelerations. We propose a simple modification to a standard convolution to bridge this gap. We propose an adaptive convolution that adopts different kernel sizes (or radii) based on the content. The network can learn and select the proper radius based on the input content in a soft decision manner. Our proposed radius-adaptive convolutional neural network (RACNN) has a similar number of weights to a standard one, yet, results show it can reach higher speeds. The code has been made available at: https://github.com/meisamrf/racnn.



### Robust Feature-Based Point Registration Using Directional Mixture Model
- **Arxiv ID**: http://arxiv.org/abs/1912.05016v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.05016v1)
- **Published**: 2019-11-25 17:37:54+00:00
- **Updated**: 2019-11-25 17:37:54+00:00
- **Authors**: Saman Fahandezh-Saadi, Di Wang, Masayoshi Tomizuka
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a robust probabilistic point registration method for estimating the rigid transformation (i.e. rotation matrix and translation vector) between two pointcloud dataset. The method improves the robustness of point registration and consequently the robot localization in the presence of outliers in the pointclouds which always occurs due to occlusion, dynamic objects, and sensor errors. The framework models the point registration task based on directional statistics on a unit sphere. In particular, a Kent distribution mixture model is adopted and the process of point registration has been carried out in the two phases of Expectation-Maximization algorithm. The proposed method has been evaluated on the pointcloud dataset from LiDAR sensors in an indoor environment.



### Improving Feature Attribution through Input-specific Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/1911.11081v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11081v2)
- **Published**: 2019-11-25 17:40:37+00:00
- **Updated**: 2020-03-09 17:39:42+00:00
- **Authors**: Ashkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian Rupprecht, Seong Tae Kim, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Attributing the output of a neural network to the contribution of given input elements is a way of shedding light on the black-box nature of neural networks. Due to the complexity of current network architectures, current gradient-based attribution methods provide very noisy or coarse results. We propose to prune a neural network for a given single input to keep only neurons that highly contribute to the prediction. We show that by input-specific pruning, network gradients change from reflecting local (noisy) importance information to global importance. Our proposed method is efficient and generates fine-grained attribution maps. We further provide a theoretical justification of the pruning approach relating it to perturbations and validate it through a novel experimental setup. Our method is evaluated by multiple benchmarks: sanity checks, pixel perturbation, and Remove-and-Retrain (ROAR). These benchmarks evaluate the method from different perspectives and our method performs better than other methods across all evaluations.



### StructEdit: Learning Structural Shape Variations
- **Arxiv ID**: http://arxiv.org/abs/1911.11098v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1911.11098v1)
- **Published**: 2019-11-25 18:08:04+00:00
- **Updated**: 2019-11-25 18:08:04+00:00
- **Authors**: Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to encode differences in the geometry and (topological) structure of the shapes of ordinary objects is key to generating semantically plausible variations of a given shape, transferring edits from one shape to another, and many other applications in 3D content creation. The common approach of encoding shapes as points in a high-dimensional latent feature space suggests treating shape differences as vectors in that space. Instead, we treat shape differences as primary objects in their own right and propose to encode them in their own latent space. In a setting where the shapes themselves are encoded in terms of fine-grained part hierarchies, we demonstrate that a separate encoding of shape deltas or differences provides a principled way to deal with inhomogeneities in the shape space due to different combinatorial part structures, while also allowing for compactness in the representation, as well as edit abstraction and transfer. Our approach is based on a conditional variational autoencoder for encoding and decoding shape deltas, conditioned on a source shape. We demonstrate the effectiveness and robustness of our approach in multiple shape modification and generation tasks, and provide comparison and ablation studies on the PartNet dataset, one of the largest publicly available 3D datasets.



### Phase Contrast Microscopy Cell PopulationSegmentation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1911.11111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11111v1)
- **Published**: 2019-11-25 18:20:40+00:00
- **Updated**: 2019-11-25 18:20:40+00:00
- **Authors**: Lin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Phase contrast microscopy (PCM) has been widely used in biomedicine research, which allows users to observe objectives without staining or killing them. One important related research is to employ PCM to monitor live cells. How to segment cell populations in obtained PCM images gains more and more attention as its a critical step for downstream applications, such as cell tracking, cell classification and others. Many papers have been published to deal with this problem from different perspectives. In this paper we aim to present a comprehensive review on the development of PCM cell population segmentation.



### Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1911.11130v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11130v2)
- **Published**: 2019-11-25 18:56:12+00:00
- **Updated**: 2020-03-31 03:57:25+00:00
- **Authors**: Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi
- **Comment**: CVPR 2020 Oral. Project page: https://elliottwu.com/projects/unsup3d/
- **Journal**: None
- **Summary**: We propose a method to learn 3D deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination. In order to disentangle these components without supervision, we use the fact that many object categories have, at least in principle, a symmetric structure. We show that reasoning about illumination allows us to exploit the underlying object symmetry even if the appearance is not symmetric due to shading. Furthermore, we model objects that are probably, but not certainly, symmetric by predicting a symmetry probability map, learned end-to-end with the other components of the model. Our experiments show that this method can recover very accurately the 3D shape of human faces, cat faces and cars from single-view images, without any supervision or a prior shape model. On benchmarks, we demonstrate superior accuracy compared to another method that uses supervision at the level of 2D image correspondences.



### Scaling Out-of-Distribution Detection for Real-World Settings
- **Arxiv ID**: http://arxiv.org/abs/1911.11132v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11132v4)
- **Published**: 2019-11-25 18:58:23+00:00
- **Updated**: 2022-05-15 16:44:03+00:00
- **Authors**: Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, Dawn Song
- **Comment**: ICML 2022; The Species dataset and code are available at
  https://github.com/hendrycks/anomaly-seg
- **Journal**: None
- **Summary**: Detecting out-of-distribution examples is important for safety-critical machine learning applications such as detecting novel biological phenomena and self-driving cars. However, existing research mainly focuses on simple small-scale settings. To set the stage for more realistic out-of-distribution detection, we depart from small-scale settings and explore large-scale multiclass and multi-label settings with high-resolution images and thousands of classes. To make future work in real-world settings possible, we create new benchmarks for three large-scale settings. To test ImageNet multiclass anomaly detectors, we introduce the Species dataset containing over 700,000 images and over a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL VOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies. We conduct extensive experiments in these more realistic settings for out-of-distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large-scale multi-class, multi-label, and segmentation tasks, establishing a simple new baseline for future work.



### Rigging the Lottery: Making All Tickets Winners
- **Arxiv ID**: http://arxiv.org/abs/1911.11134v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11134v3)
- **Published**: 2019-11-25 18:58:53+00:00
- **Updated**: 2021-07-23 14:12:42+00:00
- **Authors**: Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, Erich Elsen
- **Comment**: Published in Proceedings of the 37th International Conference on
  Machine Learning. Code can be found in github.com/google-research/rigl
- **Journal**: Proceedings of the 37th International Conference on Machine
  Learning (2020) 471-481
- **Summary**: Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static. Code used in our work can be found in github.com/google-research/rigl.



### Real-Time Object Tracking via Meta-Learning: Efficient Model Adaptation and One-Shot Channel Pruning
- **Arxiv ID**: http://arxiv.org/abs/1911.11170v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11170v3)
- **Published**: 2019-11-25 19:09:01+00:00
- **Updated**: 2019-12-04 08:38:46+00:00
- **Authors**: Ilchae Jung, Kihyun You, Hyeonwoo Noh, Minsu Cho, Bohyung Han
- **Comment**: 9 pages, 5 figures, AAAI 2020 accepted
- **Journal**: None
- **Summary**: We propose a novel meta-learning framework for real-time object tracking with efficient model adaptation and channel pruning. Given an object tracker, our framework learns to fine-tune its model parameters in only a few iterations of gradient-descent during tracking while pruning its network channels using the target ground-truth at the first frame. Such a learning problem is formulated as a meta-learning task, where a meta-tracker is trained by updating its meta-parameters for initial weights, learning rates, and pruning masks through carefully designed tracking simulations. The integrated meta-tracker greatly improves tracking performance by accelerating the convergence of online learning and reducing the cost of feature computation. Experimental evaluation on the standard datasets demonstrates its outstanding accuracy and speed compared to the state-of-the-art methods.



### Structured Multi-Hashing for Model Compression
- **Arxiv ID**: http://arxiv.org/abs/1911.11177v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11177v1)
- **Published**: 2019-11-25 19:21:25+00:00
- **Updated**: 2019-11-25 19:21:25+00:00
- **Authors**: Elad Eban, Yair Movshovitz-Attias, Hao Wu, Mark Sandler, Andrew Poon, Yerlan Idelbayev, Miguel A. Carreira-Perpinan
- **Comment**: Elad and Yair contributed equally to the paper. They jointly proposed
  the idea of structured-multi-hashing. Elad: Wrote most of the code and ran
  most of the experiments Yair: Main contributor to the manuscript Hao: Coding
  and experiments Yerlan: Coding and experiments Miguel: advised Yerlan about
  optimization and model compression Mark:experiments Andrew: experiments
- **Journal**: None
- **Summary**: Despite the success of deep neural networks (DNNs), state-of-the-art models are too large to deploy on low-resource devices or common server configurations in which multiple models are held in memory. Model compression methods address this limitation by reducing the memory footprint, latency, or energy consumption of a model with minimal impact on accuracy. We focus on the task of reducing the number of learnable variables in the model. In this work we combine ideas from weight hashing and dimensionality reductions resulting in a simple and powerful structured multi-hashing method based on matrix products that allows direct control of model size of any deep network and is trained end-to-end. We demonstrate the strength of our approach by compressing models from the ResNet, EfficientNet, and MobileNet architecture families. Our method allows us to drastically decrease the number of variables while maintaining high accuracy. For instance, by applying our approach to EfficentNet-B4 (16M parameters) we reduce it to to the size of B0 (5M parameters), while gaining over 3% in accuracy over B0 baseline. On the commonly used benchmark CIFAR10 we reduce the ResNet32 model by 75% with no loss in quality, and are able to do a 10x compression while still achieving above 90% accuracy.



### Oops! Predicting Unintentional Action in Video
- **Arxiv ID**: http://arxiv.org/abs/1911.11206v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11206v1)
- **Published**: 2019-11-25 20:15:31+00:00
- **Updated**: 2019-11-25 20:15:31+00:00
- **Authors**: Dave Epstein, Boyuan Chen, Carl Vondrick
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: From just a short glance at a video, we can often tell whether a person's action is intentional or not. Can we train a model to recognize this? We introduce a dataset of in-the-wild videos of unintentional action, as well as a suite of tasks for recognizing, localizing, and anticipating its onset. We train a supervised neural network as a baseline and analyze its performance compared to human consistency on the tasks. We also investigate self-supervised representations that leverage natural signals in our dataset, and show the effectiveness of an approach that uses the intrinsic speed of video to perform competitively with highly-supervised pretraining. However, a significant gap between machine and human performance remains. The project website is available at https://oops.cs.columbia.edu



### Automatic Post-Stroke Lesion Segmentation on MR Images using 3D Residual Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1911.11209v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11209v1)
- **Published**: 2019-11-25 20:20:58+00:00
- **Updated**: 2019-11-25 20:20:58+00:00
- **Authors**: Naofumi Tomita, Steven Jiang, Matthew E. Maeder, Saeed Hassanpour
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we demonstrate the feasibility and performance of deep residual neural networks for volumetric segmentation of irreversibly damaged brain tissue lesions on T1-weighted MRI scans for chronic stroke patients. A total of 239 T1-weighted MRI scans of chronic ischemic stroke patients from a public dataset were retrospectively analyzed by 3D deep convolutional segmentation models with residual learning, using a novel zoom-in&out strategy. Dice similarity coefficient (DSC), Average symmetric surface distance (ASSD), and Hausdorff distance (HD) of the identified lesions were measured by using the manual tracing of lesions as the reference standard. Bootstrapping was employed for all metrics to estimate 95% confidence intervals. The models were assessed on the test set of 31 scans. The average DSC was 0.64 (0.51-0.76) with a median of 0.78. ASSD and HD were 3.6 mm (1.7-6.2 mm) and 20.4 mm (10.0-33.3 mm), respectively. To the best of our knowledge, this performance is the highest achieved on this public dataset. The latest deep learning architecture and techniques were applied for 3D segmentation on MRI scans and demonstrated to be effective for volumetric segmentation of chronic ischemic stroke lesions.



### One Man's Trash is Another Man's Treasure: Resisting Adversarial Examples by Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1911.11219v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11219v2)
- **Published**: 2019-11-25 20:33:59+00:00
- **Updated**: 2019-11-27 21:10:06+00:00
- **Authors**: Chang Xiao, Changxi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Modern image classification systems are often built on deep neural networks, which suffer from adversarial examples--images with deliberately crafted, imperceptible noise to mislead the network's classification. To defend against adversarial examples, a plausible idea is to obfuscate the network's gradient with respect to the input image. This general idea has inspired a long line of defense methods. Yet, almost all of them have proven vulnerable. We revisit this seemingly flawed idea from a radically different perspective. We embrace the omnipresence of adversarial examples and the numerical procedure of crafting them, and turn this harmful attacking process into a useful defense mechanism. Our defense method is conceptually simple: before feeding an input image for classification, transform it by finding an adversarial example on a pre-trained external model. We evaluate our method against a wide range of possible attacks. On both CIFAR-10 and Tiny ImageNet datasets, our method is significantly more robust than state-of-the-art methods. Particularly, in comparison to adversarial training, our method offers lower training cost as well as stronger robustness.



### Shape Reconstruction by Learning Differentiable Surface Representations
- **Arxiv ID**: http://arxiv.org/abs/1911.11227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11227v1)
- **Published**: 2019-11-25 20:51:18+00:00
- **Updated**: 2019-11-25 20:51:18+00:00
- **Authors**: Jan Bednarik, Shaifali Parashar, Erhan Gundogdu, Mathieu Salzmann, Pascal Fua
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Generative models that produce point clouds have emerged as a powerful tool to represent 3D surfaces, and the best current ones rely on learning an ensemble of parametric representations. Unfortunately, they offer no control over the deformations of the surface patches that form the ensemble and thus fail to prevent them from either overlapping or collapsing into single points or lines. As a consequence, computing shape properties such as surface normals and curvatures becomes difficult and unreliable.   In this paper, we show that we can exploit the inherent differentiability of deep networks to leverage differential surface properties during training so as to prevent patch collapse and strongly reduce patch overlap. Furthermore, this lets us reliably compute quantities such as surface normals and curvatures. We will demonstrate on several tasks that this yields more accurate surface reconstructions than the state-of-the-art methods in terms of normals estimation and amount of collapsed and overlapped patches.



### Identifying Model Weakness with Adversarial Examiner
- **Arxiv ID**: http://arxiv.org/abs/1911.11230v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11230v1)
- **Published**: 2019-11-25 21:04:49+00:00
- **Updated**: 2019-11-25 21:04:49+00:00
- **Authors**: Michelle Shu, Chenxi Liu, Weichao Qiu, Alan Yuille
- **Comment**: To appear in AAAI-20
- **Journal**: None
- **Summary**: Machine learning models are usually evaluated according to the average case performance on the test set. However, this is not always ideal, because in some sensitive domains (e.g. autonomous driving), it is the worst case performance that matters more. In this paper, we are interested in systematic exploration of the input data space to identify the weakness of the model to be evaluated. We propose to use an adversarial examiner in the testing stage. Different from the existing strategy to always give the same (distribution of) test data, the adversarial examiner will dynamically select the next test data to hand out based on the testing history so far, with the goal being to undermine the model's performance. This sequence of test data not only helps us understand the current model, but also serves as constructive feedback to help improve the model in the next iteration. We conduct experiments on ShapeNet object classification. We show that our adversarial examiner can successfully put more emphasis on the weakness of the model, preventing performance estimates from being overly optimistic.



### RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1911.11236v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.11236v3)
- **Published**: 2019-11-25 21:15:52+00:00
- **Updated**: 2020-05-01 21:29:02+00:00
- **Authors**: Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, Andrew Markham
- **Comment**: CVPR 2020 Oral. Code and data are available at:
  https://github.com/QingyongHu/RandLA-Net
- **Journal**: None
- **Summary**: We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200X faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI.



### Learning to Learn Words from Visual Scenes
- **Arxiv ID**: http://arxiv.org/abs/1911.11237v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11237v3)
- **Published**: 2019-11-25 21:19:31+00:00
- **Updated**: 2020-07-12 21:19:49+00:00
- **Authors**: Ddac Surs, Dave Epstein, Heng Ji, Shih-Fu Chang, Carl Vondrick
- **Comment**: 26 pages, 12 figures
- **Journal**: European Conference on Computer Vision (ECCV), 2020
- **Summary**: Language acquisition is the process of learning words from the surrounding scene. We introduce a meta-learning framework that learns how to learn word representations from unconstrained scenes. We leverage the natural compositional structure of language to create training episodes that cause a meta-learner to learn strong policies for language acquisition. Experiments on two datasets show that our approach is able to more rapidly acquire novel words as well as more robustly generalize to unseen compositions, significantly outperforming established baselines. A key advantage of our approach is that it is data efficient, allowing representations to be learned from scratch without language pre-training. Visualizations and analysis suggest visual information helps our approach learn a rich cross-modal representation from minimal examples. Project webpage is available at https://expert.cs.columbia.edu/



### Translation Insensitive CNNs
- **Arxiv ID**: http://arxiv.org/abs/1911.11238v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.11238v1)
- **Published**: 2019-11-25 21:22:06+00:00
- **Updated**: 2019-11-25 21:22:06+00:00
- **Authors**: Ganesh Sundaramoorthi, Timothy E. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem that state-of-the-art Convolution Neural Networks (CNN) classifiers are not invariant to small shifts. The problem can be solved by the removal of sub-sampling operations such as stride and max pooling, but at a cost of severely degraded training and test efficiency. We present a novel usage of Gaussian-Hermite basis to efficiently approximate arbitrary filters within the CNN framework to obtain translation invariance. This is shown to be invariant to small shifts, and preserves the efficiency of training. Further, to improve efficiency in memory usage as well as computational speed, we show that it is still possible to sub-sample with this approach and retain a weaker form of invariance that we call \emph{translation insensitivity}, which leads to stability with respect to shifts. We prove these claims analytically and empirically. Our analytic methods further provide a framework for understanding any architecture in terms of translation insensitivity, and provide guiding principles for design.



### A Novel Visual Fault Detection and Classification System for Semiconductor Manufacturing Using Stacked Hybrid Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.11250v5
- **DOI**: 10.1109/ETFA.2019.8869311
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11250v5)
- **Published**: 2019-11-25 21:58:28+00:00
- **Updated**: 2021-03-18 23:22:01+00:00
- **Authors**: Tobias Schlosser, Frederik Beuth, Michael Friedrich, Danny Kowerko
- **Comment**: Accepted for: 2019 IEEE 24th International Conference on Emerging
  Technologies and Factory Automation (ETFA); the latest versions of this
  contribution cover minor typo corrections
- **Journal**: None
- **Summary**: Automated visual inspection in the semiconductor industry aims to detect and classify manufacturing defects utilizing modern image processing techniques. While an earliest possible detection of defect patterns allows quality control and automation of manufacturing chains, manufacturers benefit from an increased yield and reduced manufacturing costs. Since classical image processing systems are limited in their ability to detect novel defect patterns, and machine learning approaches often involve a tremendous amount of computational effort, this contribution introduces a novel deep neural network based hybrid approach. Unlike classical deep neural networks, a multi-stage system allows the detection and classification of the finest structures in pixel size within high-resolution imagery. Consisting of stacked hybrid convolutional neural networks (SH-CNN) and inspired by current approaches of visual attention, the realized system draws the focus over the level of detail from its structures to more task-relevant areas of interest. The results of our test environment show that the SH-CNN outperforms current approaches of learning-based automated visual inspection, whereas a distinction depending on the level of detail enables the elimination of defect patterns in earlier stages of the manufacturing process.



### Hexagonal Image Processing in the Context of Machine Learning: Conception of a Biologically Inspired Hexagonal Deep Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/1911.11251v7
- **DOI**: 10.1109/ICMLA.2019.00300
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.11251v7)
- **Published**: 2019-11-25 21:58:31+00:00
- **Updated**: 2021-03-18 23:21:56+00:00
- **Authors**: Tobias Schlosser, Michael Friedrich, Danny Kowerko
- **Comment**: Accepted for: 2019 18th IEEE International Conference on Machine
  Learning and Applications (ICMLA); the latest versions of this contribution
  cover minor typo corrections
- **Journal**: None
- **Summary**: Inspired by the human visual perception system, hexagonal image processing in the context of machine learning deals with the development of image processing systems that combine the advantages of evolutionary motivated structures based on biological models. While conventional state-of-the-art image processing systems of recording and output devices almost exclusively utilize square arranged methods, their hexagonal counterparts offer a number of key advantages that can benefit both researchers and users. This contribution serves as a general application-oriented approach the synthesis of the therefore designed hexagonal image processing framework, called Hexnet, the processing steps of hexagonal image transformation, and dependent methods. The results of our created test environment show that the realized framework surpasses current approaches of hexagonal image processing systems, while hexagonal artificial neural networks can benefit from the implemented hexagonal architecture. As hexagonal lattice format based deep neural networks, also called H-DNN, can be compared to their square counterparts by transforming classical square lattice based data sets into their hexagonal representation, they can also result in a reduction of trainable parameters as well as result in increased training and test rates.



### Deeply Shape-guided Cascade for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.11263v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.11263v4)
- **Published**: 2019-11-25 22:44:46+00:00
- **Updated**: 2021-03-27 08:24:03+00:00
- **Authors**: Hao Ding, Siyuan Qiao, Alan Yuille, Wei Shen
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: The key to a successful cascade architecture for precise instance segmentation is to fully leverage the relationship between bounding box detection and mask segmentation across multiple stages. Although modern instance segmentation cascades achieve leading performance, they mainly make use of a unidirectional relationship, i.e., mask segmentation can benefit from iteratively refined bounding box detection. In this paper, we investigate an alternative direction, i.e., how to take the advantage of precise mask segmentation for bounding box detection in a cascade architecture. We propose a Deeply Shape-guided Cascade (DSC) for instance segmentation, which iteratively imposes the shape guidances extracted from mask prediction at the previous stage on bounding box detection at current stage. It forms a bi-directional relationship between the two tasks by introducing three key components: (1) Initial shape guidance: A mask-supervised Region Proposal Network (mPRN) with the ability to generate class-agnostic masks; (2) Explicit shape guidance: A mask-guided region-of-interest (RoI) feature extractor, which employs mask segmentation at previous stage to focus feature extraction at current stage within a region aligned well with the shape of the instance-of-interest rather than a rectangular RoI; (3) Implicit shape guidance: A feature fusion operation which feeds intermediate mask features at previous stage to the bounding box head at current stage. Experimental results show that DSC outperforms the state-of-the-art instance segmentation cascade, Hybrid Task Cascade (HTC), by a large margin and achieves 51.8 box AP and 45.5 mask AP on COCO test-dev. The code is released at: https://github.com/hding2455/DSC.




# Arxiv Papers in cs.CV on 2019-02-05
### Active Image Synthesis for Efficient Labeling
- **Arxiv ID**: http://arxiv.org/abs/1902.01522v4
- **DOI**: 10.1109/TPAMI.2020.2993221
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01522v4)
- **Published**: 2019-02-05 02:49:09+00:00
- **Updated**: 2020-08-04 15:04:32+00:00
- **Authors**: Jialei Chen, Yujia Xie, Kan Wang, Chuck Zhang, Mani A. Vannan, Ben Wang, Zhen Qian
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2020
- **Summary**: The great success achieved by deep neural networks attracts increasing attention from the manufacturing and healthcare communities. However, the limited availability of data and high costs of data collection are the major challenges for the applications in those fields. We propose in this work AISEL, an active image synthesis method for efficient labeling to improve the performance of the small-data learning tasks. Specifically, a complementary AISEL dataset is generated, with labels actively acquired via a physics-based method to incorporate underlining physical knowledge at hand. An important component of our AISEL method is the bidirectional generative invertible network (GIN), which can extract interpretable features from the training images and generate physically meaningful virtual images. Our AISEL method then efficiently samples virtual images not only further exploits the uncertain regions, but also explores the entire image space. We then discuss the interpretability of GIN both theoretically and experimentally, demonstrating clear visual improvements over the benchmarks. Finally, we demonstrate the effectiveness of our AISEL framework on aortic stenosis application, in which our method lower the labeling cost by $90\%$ while achieving a $15\%$ improvement in prediction accuracy.



### A Practical Maximum Clique Algorithm for Matching with Pairwise Constraints
- **Arxiv ID**: http://arxiv.org/abs/1902.01534v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1902.01534v2)
- **Published**: 2019-02-05 04:06:26+00:00
- **Updated**: 2020-02-27 05:13:59+00:00
- **Authors**: Álvaro Parra, Tat-Jun Chin, Frank Neumann, Tobias Friedrich, Maximilian Katzmann
- **Comment**: Code and demo program are available in the supplementary material. 9
  pages
- **Journal**: None
- **Summary**: A popular paradigm for 3D point cloud registration is by extracting 3D keypoint correspondences, then estimating the registration function from the correspondences using a robust algorithm. However, many existing 3D keypoint techniques tend to produce large proportions of erroneous correspondences or outliers, which significantly increases the cost of robust estimation. An alternative approach is to directly search for the subset of correspondences that are pairwise consistent, without optimising the registration function. This gives rise to the combinatorial problem of matching with pairwise constraints. In this paper, we propose a very efficient maximum clique algorithm to solve matching with pairwise constraints. Our technique combines tree searching with efficient bounding and pruning based on graph colouring. We demonstrate that, despite the theoretical intractability, many real problem instances can be solved exactly and quickly (seconds to minutes) with our algorithm, which makes our approach an excellent alternative to standard robust techniques for 3D registration.



### SASSE: Scalable and Adaptable 6-DOF Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1902.01549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01549v1)
- **Published**: 2019-02-05 05:13:54+00:00
- **Updated**: 2019-02-05 05:13:54+00:00
- **Authors**: Huu Le, Tuan Hoang, Qianggong Zhang, Thanh-Toan Do, Anders Eriksson, Michael Milford
- **Comment**: None
- **Journal**: None
- **Summary**: Visual localization has become a key enabling component of many place recognition and SLAM systems. Contemporary research has primarily focused on improving accuracy and precision-recall type metrics, with relatively little attention paid to a system's absolute storage scaling characteristics, its flexibility to adapt to available computational resources, and its longevity with respect to easily incorporating newly learned or hand-crafted image descriptors. Most significantly, improvement in one of these aspects typically comes at the cost of others: for example, a snapshot-based system that achieves sub-linear storage cost typically provides no metric pose estimation, or, a highly accurate pose estimation technique is often ossified in adapting to recent advances in appearance-invariant features. In this paper, we present a novel 6-DOF localization system that for the first time simultaneously achieves all the three characteristics: significantly sub-linear storage growth, agnosticism to image descriptors, and customizability to available storage and computational resources. The key features of our method are developed based on a novel adaptation of multiple-label learning, together with effective dimensional reduction and learning techniques that enable simple and efficient optimization. We evaluate our system on several large benchmarking datasets and provide detailed comparisons to state-of-the-art systems. The proposed method demonstrates competitive accuracy with existing pose estimation methods while achieving better sub-linear storage scaling, significantly reduced absolute storage requirements, and faster training and deployment speeds.



### Revisiting a single-stage method for face detection
- **Arxiv ID**: http://arxiv.org/abs/1902.01559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01559v1)
- **Published**: 2019-02-05 06:02:20+00:00
- **Updated**: 2019-02-05 06:02:20+00:00
- **Authors**: Nguyen Van Quang, Hiromasa Fujihara
- **Comment**: None
- **Journal**: None
- **Summary**: Although accurate, two-stage face detectors usually require more inference time than single-stage detectors do. This paper proposes a simple yet effective single-stage model for real-time face detection with a prominently high accuracy. We build our single-stage model on the top of the ResNet-101 backbone and analyze some problems with the baseline single-stage detector in order to design several strategies for reducing the false positive rate. The design leverages the context information from the deeper layers in order to increase recall rate while maintaining a low false positive rate. In addition, we reduce the detection time by an improved inference procedure for decoding outputs faster. The inference time of a VGA ($640{\times}480$) image was only approximately 26 ms with a Titan X GPU. The effectiveness of our proposed method was evaluated on several face detection benchmarks (Wider Face, AFW, Pascal Face, and FDDB). The experiments show that our method achieved competitive results on these popular datasets with a faster runtime than the current best two-stage practices.



### EasyLabel: A Semi-Automatic Pixel-wise Object Annotation Tool for Creating Robotic RGB-D Datasets
- **Arxiv ID**: http://arxiv.org/abs/1902.01626v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.01626v2)
- **Published**: 2019-02-05 10:14:12+00:00
- **Updated**: 2019-03-01 07:09:34+00:00
- **Authors**: Markus Suchi, Timothy Patten, David Fischinger, Markus Vincze
- **Comment**: 7 pages, 8 figures, ICRA2019, Draft
- **Journal**: None
- **Summary**: Developing robot perception systems for recognizing objects in the real-world requires computer vision algorithms to be carefully scrutinized with respect to the expected operating domain. This demands large quantities of ground truth data to rigorously evaluate the performance of algorithms. This paper presents the EasyLabel tool for easily acquiring high quality ground truth annotation of objects at the pixel-level in densely cluttered scenes. In a semi-automatic process, complex scenes are incrementally built and EasyLabel exploits depth change to extract precise object masks at each step. We use this tool to generate the Object Cluttered Indoor Dataset (OCID) that captures diverse settings of objects, background, context, sensor to scene distance, viewpoint angle and lighting conditions. OCID is used to perform a systematic comparison of existing object segmentation methods. The baseline comparison supports the need for pixel- and object-wise annotation to progress robot vision towards realistic applications. This insight reveals the usefulness of EasyLabel and OCID to better understand the challenges that robots face in the real-world.   Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.



### DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1902.01654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01654v1)
- **Published**: 2019-02-05 12:05:17+00:00
- **Updated**: 2019-02-05 12:05:17+00:00
- **Authors**: Guillaume Michel, Mohammed Amine Alaoui, Alice Lebois, Amal Feriani, Mehdi Felhi
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic search of neural network architectures is a standing research topic. In addition to the fact that it presents a faster alternative to hand-designed architectures, it can improve their efficiency and for instance generate Convolutional Neural Networks (CNN) adapted for mobile devices. In this paper, we present a multi-objective neural architecture search method to find a family of CNN models with the best accuracy and computational resources tradeoffs, in a search space inspired by the state-of-the-art findings in neural search. Our work, called Dvolver, evolves a population of architectures and iteratively improves an approximation of the optimal Pareto front. Applying Dvolver on the model accuracy and on the number of floating points operations as objective functions, we are able to find, in only 2.5 days, a set of competitive mobile models on ImageNet. Amongst these models one architecture has the same Top-1 accuracy on ImageNet as NASNet-A mobile with 8% less floating point operations and another one has a Top-1 accuracy of 75.28% on ImageNet exceeding by 0.28% the best MobileNetV2 model for the same computational resources.



### Reduce Noise in Computed Tomography Image using Adaptive Gaussian Filter
- **Arxiv ID**: http://arxiv.org/abs/1902.05985v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.05985v1)
- **Published**: 2019-02-05 13:02:00+00:00
- **Updated**: 2019-02-05 13:02:00+00:00
- **Authors**: Rini Mayasari, Nono Heryana
- **Comment**: None
- **Journal**: None
- **Summary**: One image processing application that is very helpful for humans is to improve image quality, poor image quality makes the image more difficult to interpret because the information conveyed by the image is reduced. In the process of the acquisition of medical images, the resulting image has decreased quality (degraded) due to external factors and medical equipment used. For this reason, it is necessary to have an image processing process to improve the quality of medical images, so that later it is expected to help facilitate medical personnel in analyzing and translating medical images, which will lead to an improvement in the quality of diagnosis. In this study, an analysis will be carried out to improve the quality of medical images with noise reduction with the Gaussian Filter Method. Next, it is carried out, and tested against medical images, in this case, the lung photo image. The test image is given noise in the form of impulse salt & pepper and adaptive Gaussian then analyzed its performance qualitatively by comparing the output filter image, noise image, and the original image by naked eye.



### 6D Object Pose Estimation without PnP
- **Arxiv ID**: http://arxiv.org/abs/1902.01728v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01728v2)
- **Published**: 2019-02-05 15:04:36+00:00
- **Updated**: 2019-02-06 12:40:06+00:00
- **Authors**: Jin Liu, Sheng He
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an efficient end-to-end algorithm to tackle the problem of estimating the 6D pose of objects from a single RGB image. Our system trains a fully convolutional network to regress the 3D rotation and the 3D translation in region layer. On this basis, a special layer, Collinear Equation Layer, is added next to region layer to output the 2D projections of the 3D bounding boxs corners. In the back propagation stage, the 6D pose network are adjusted according to the error of the 2D projections. In the detection phase, we directly output the position and pose through the region layer. Besides, we introduce a novel and concise representation of 3D rotation to make the regression more precise and easier. Experiments show that our method outperforms base-line and state of the art methods both at accuracy and efficiency. In the LineMod dataset, our algorithm achieves less than 18 ms/object on a GeForce GTX 1080Ti GPU, while the translational error and rotational error are less than 1.67 cm and 2.5 degree.



### An RNN-based IMM Filter Surrogate
- **Arxiv ID**: http://arxiv.org/abs/1902.01739v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01739v2)
- **Published**: 2019-02-05 15:21:53+00:00
- **Updated**: 2019-04-26 07:48:38+00:00
- **Authors**: Stefan Becker, Ronny Hug, Wolfgang Hübner, Michael Arens
- **Comment**: Accepted at Scandinavian Conference on Image Analysis (SCIA) 2019
- **Journal**: None
- **Summary**: The problem of varying dynamics of tracked objects, such as pedestrians, is traditionally tackled with approaches like the Interacting Multiple Model (IMM) filter using a Bayesian formulation. By following the current trend towards using deep neural networks, in this paper an RNN-based IMM filter surrogate is presented. Similar to an IMM filter solution, the presented RNN-based model assigns a probability value to a performed dynamic and, based on them, puts out a multi-modal distribution over future pedestrian trajectories. The evaluation is done on synthetic data, reflecting prototypical pedestrian maneuvers.



### Deep Convolutional Generative Adversarial Networks Based Flame Detection in Video
- **Arxiv ID**: http://arxiv.org/abs/1902.01824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01824v1)
- **Published**: 2019-02-05 17:53:42+00:00
- **Updated**: 2019-02-05 17:53:42+00:00
- **Authors**: Süleyman Aslan, Uğur Güdükbay, B. Uğur Töreyin, A. Enis Çetin
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time flame detection is crucial in video based surveillance systems. We propose a vision-based method to detect flames using Deep Convolutional Generative Adversarial Neural Networks (DCGANs). Many existing supervised learning approaches using convolutional neural networks do not take temporal information into account and require substantial amount of labeled data. In order to have a robust representation of sequences with and without flame, we propose a two-stage training of a DCGAN exploiting spatio-temporal flame evolution. Our training framework includes the regular training of a DCGAN with real spatio-temporal images, namely, temporal slice images, and noise vectors, and training the discriminator separately using the temporal flame images without the generator. Experimental results show that the proposed method effectively detects flame in video with negligible false positive rates in real-time.



### Face Alignment using a 3D Deeply-initialized Ensemble of Regression Trees
- **Arxiv ID**: http://arxiv.org/abs/1902.01831v2
- **DOI**: 10.1016/j.cviu.2019.102846
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01831v2)
- **Published**: 2019-02-05 18:07:17+00:00
- **Updated**: 2019-12-13 10:15:08+00:00
- **Authors**: Roberto Valle, José M. Buenaposada, Antonio Valdés, Luis Baumela
- **Comment**: Accepted Version to Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Face alignment algorithms locate a set of landmark points in images of faces taken in unrestricted situations. State-of-the-art approaches typically fail or lose accuracy in the presence of occlusions, strong deformations, large pose variations and ambiguous configurations. In this paper we present 3DDE, a robust and efficient face alignment algorithm based on a coarse-to-fine cascade of ensembles of regression trees. It is initialized by robustly fitting a 3D face model to the probability maps produced by a convolutional neural network. With this initialization we address self-occlusions and large face rotations. Further, the regressor implicitly imposes a prior face shape on the solution, addressing occlusions and ambiguous face configurations. Its coarse-to-fine structure tackles the combinatorial explosion of parts deformation. In the experiments performed, 3DDE improves the state-of-the-art in 300W, COFW, AFLW and WFLW data sets. Finally, we perform cross-dataset experiments that reveal the existence of a significant data set bias in these benchmarks.



### Disguised-Nets: Image Disguising for Privacy-preserving Outsourced Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.01878v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.01878v2)
- **Published**: 2019-02-05 19:20:02+00:00
- **Updated**: 2019-04-19 04:31:54+00:00
- **Authors**: Sagar Sharma, Keke Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning model developers often use cloud GPU resources to experiment with large data and models that need expensive setups. However, this practice raises privacy concerns. Adversaries may be interested in: 1) personally identifiable information or objects encoded in the training images, and 2) the models trained with sensitive data to launch model-based attacks. Learning deep neural networks (DNN) from encrypted data is still impractical due to the large training data and the expensive learning process. A few recent studies have tried to provide efficient, practical solutions to protect data privacy in outsourced deep-learning. However, we find out that they are vulnerable under certain attacks. In this paper, we specifically identify two types of unique attacks on outsourced deep-learning: 1) the visual re-identification attack on the training data, and 2) the class membership attack on the learned models, which can break existing privacy-preserving solutions. We develop an image disguising approach to address these attacks and design a suite of methods to evaluate the levels of attack resilience for a privacy-preserving solution for outsourced deep learning. The experimental results show that our image-disguising mechanisms can provide a high level of protection against the two attacks while still generating high-quality DNN models for image classification.



### Multi-Kernel Prediction Networks for Denoising of Burst Images
- **Arxiv ID**: http://arxiv.org/abs/1902.05392v2
- **DOI**: 10.1109/ICIP.2019.8803335
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05392v2)
- **Published**: 2019-02-05 22:29:09+00:00
- **Updated**: 2021-03-11 14:57:46+00:00
- **Authors**: Talmaj Marinč, Vignesh Srinivasan, Serhan Gül, Cornelius Hellge, Wojciech Samek
- **Comment**: 5 pages, 4 figures
- **Journal**: 2019 IEEE International Conference on Image Processing (ICIP), pp.
  2404-2408
- **Summary**: In low light or short-exposure photography the image is often corrupted by noise. While longer exposure helps reduce the noise, it can produce blurry results due to the object and camera motion. The reconstruction of a noise-less image is an ill posed problem. Recent approaches for image denoising aim to predict kernels which are convolved with a set of successively taken images (burst) to obtain a clear image. We propose a deep neural network based approach called Multi-Kernel Prediction Networks (MKPN) for burst image denoising. MKPN predicts kernels of not just one size but of varying sizes and performs fusion of these different kernels resulting in one kernel per pixel. The advantages of our method are two fold: (a) the different sized kernels help in extracting different information from the image which results in better reconstruction and (b) kernel fusion assures retaining of the extracted information while maintaining computational efficiency. Experimental results reveal that MKPN outperforms state-of-the-art on our synthetic datasets with different noise levels.



### Technical Considerations for Semantic Segmentation in MRI using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.01977v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.01977v1)
- **Published**: 2019-02-05 23:39:17+00:00
- **Updated**: 2019-02-05 23:39:17+00:00
- **Authors**: Arjun D. Desai, Garry E. Gold, Brian A. Hargreaves, Akshay S. Chaudhari
- **Comment**: Submitted to Magnetic Resonance in Medicine
- **Journal**: None
- **Summary**: High-fidelity semantic segmentation of magnetic resonance volumes is critical for estimating tissue morphometry and relaxation parameters in both clinical and research applications. While manual segmentation is accepted as the gold-standard, recent advances in deep learning and convolutional neural networks (CNNs) have shown promise for efficient automatic segmentation of soft tissues. However, due to the stochastic nature of deep learning and the multitude of hyperparameters in training networks, predicting network behavior is challenging. In this paper, we quantify the impact of three factors associated with CNN segmentation performance: network architecture, training loss functions, and training data characteristics. We evaluate the impact of these variations on the segmentation of femoral cartilage and propose potential modifications to CNN architectures and training protocols to train these models with confidence.



### Deep Learning for Bridge Load Capacity Estimation in Post-Disaster and -Conflict Zones
- **Arxiv ID**: http://arxiv.org/abs/1902.05391v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05391v1)
- **Published**: 2019-02-05 23:44:17+00:00
- **Updated**: 2019-02-05 23:44:17+00:00
- **Authors**: Arya Pamuncak, Weisi Guo, Ahmed Soliman Khaled, Irwanda Laory
- **Comment**: None
- **Journal**: None
- **Summary**: Many post-disaster and -conflict regions do not have sufficient data on their transportation infrastructure assets, hindering both mobility and reconstruction. In particular, as the number of aging and deteriorating bridges increase, it is necessary to quantify their load characteristics in order to inform maintenance and prevent failure. The load carrying capacity and the design load are considered as the main aspects of any civil structures. Human examination can be costly and slow when expertise is lacking in challenging scenarios. In this paper, we propose to employ deep learning as method to estimate the load carrying capacity from crowd sourced images. A new convolutional neural network architecture is trained on data from over 6000 bridges, which will benefit future research and applications. We tackle significant variations in the dataset (e.g. class interval, image completion, image colour) and quantify their impact on the prediction accuracy, precision, recall and F1 score. Finally, practical optimisation is performed by converting multiclass classification into binary classification to achieve a promising field use performance.




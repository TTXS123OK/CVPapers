# Arxiv Papers in cs.CV on 2019-02-18
### Automatic Segmentation of Pulmonary Lobes Using a Progressive Dense V-Network
- **Arxiv ID**: http://arxiv.org/abs/1902.06362v1
- **DOI**: 10.1007/978-3-030-00889-5_32
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06362v1)
- **Published**: 2019-02-18 00:37:22+00:00
- **Updated**: 2019-02-18 00:37:22+00:00
- **Authors**: Abdullah-Al-Zubaer Imran, Ali Hatamizadeh, Shilpa P. Ananth, Xiaowei Ding, Demetri Terzopoulos, Nima Tajbakhsh
- **Comment**: None
- **Journal**: None
- **Summary**: Reliable and automatic segmentation of lung lobes is important for diagnosis, assessment, and quantification of pulmonary diseases. The existing techniques are prohibitively slow, undesirably rely on prior (airway/vessel) segmentation, and/or require user interactions for optimal results. This work presents a reliable, fast, and fully automated lung lobe segmentation based on a progressive dense V-network (PDV-Net). The proposed method can segment lung lobes in one forward pass of the network, with an average runtime of 2 seconds using 1 Nvidia Titan XP GPU, eliminating the need for any prior atlases, lung segmentation or any subsequent user intervention. We evaluated our model using 84 chest CT scans from the LIDC and 154 pathological cases from the LTRC datasets. Our model achieved a Dice score of $0.939 \pm 0.02$ for the LIDC test set and $0.950 \pm 0.01$ for the LTRC test set, significantly outperforming a 2D U-net model and a 3D dense V-net. We further evaluated our model against 55 cases from the LOLA11 challenge, obtaining an average Dice score of 0.935---a performance level competitive to the best performing team with an average score of 0.938. Our extensive robustness analyses also demonstrate that our model can reliably segment both healthy and pathological lung lobes in CT scans from different vendors, and that our model is robust against configurations of CT scan reconstruction.



### PointIT: A Fast Tracking Framework Based on 3D Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.06379v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06379v1)
- **Published**: 2019-02-18 02:39:08+00:00
- **Updated**: 2019-02-18 02:39:08+00:00
- **Authors**: Yuan Wang, Yang Yu, Ming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently most popular tracking frameworks focus on 2D image sequences. They seldom track the 3D object in point clouds. In this paper, we propose PointIT, a fast, simple tracking method based on 3D on-road instance segmentation. Firstly, we transform 3D LiDAR data into the spherical image with the size of 64 x 512 x 4 and feed it into instance segment model to get the predicted instance mask for each class. Then we use MobileNet as our primary encoder instead of the original ResNet to reduce the computational complexity. Finally, we extend the Sort algorithm with this instance framework to realize tracking in the 3D LiDAR point cloud data. The model is trained on the spherical images dataset with the corresponding instance label masks which are provided by KITTI 3D Object Track dataset. According to the experiment results, our network can achieve on Average Precision (AP) of 0.617 and the performance of multi-tracking task has also been improved.



### Single-shot Channel Pruning Based on Alternating Direction Method of Multipliers
- **Arxiv ID**: http://arxiv.org/abs/1902.06382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06382v1)
- **Published**: 2019-02-18 03:07:59+00:00
- **Updated**: 2019-02-18 03:07:59+00:00
- **Authors**: Chengcheng Li, Zi Wang, Xiangyang Wang, Hairong Qi
- **Comment**: Submitted to ICIP 2019
- **Journal**: None
- **Summary**: Channel pruning has been identified as an effective approach to constructing efficient network structures. Its typical pipeline requires iterative pruning and fine-tuning. In this work, we propose a novel single-shot channel pruning approach based on alternating direction methods of multipliers (ADMM), which can eliminate the need for complex iterative pruning and fine-tuning procedure and achieve a target compression ratio with only one run of pruning and fine-tuning. To the best of our knowledge, this is the first study of single-shot channel pruning. The proposed method introduces filter-level sparsity during training and can achieve competitive performance with a simple heuristic pruning criterion (L1-norm). Extensive evaluations have been conducted with various widely-used benchmark architectures and image datasets for object classification purpose. The experimental results on classification accuracy show that the proposed method can outperform state-of-the-art network pruning works under various scenarios.



### Periocular Recognition in the Wild with Orthogonal Combination of Local Binary Coded Pattern in Dual-stream Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1902.06383v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.06383v2)
- **Published**: 2019-02-18 03:08:04+00:00
- **Updated**: 2019-03-19 05:25:05+00:00
- **Authors**: Leslie Ching Ow Tiong, Andrew Beng Jin Teoh, Yunli Lee
- **Comment**: Accepted in International Conference On Biometrics 2019
- **Journal**: None
- **Summary**: In spite of the advancements made in the periocular recognition, the dataset and periocular recognition in the wild remains a challenge. In this paper, we propose a multilayer fusion approach by means of a pair of shared parameters (dual-stream) convolutional neural network where each network accepts RGB data and a novel colour-based texture descriptor, namely Orthogonal Combination-Local Binary Coded Pattern (OC-LBCP) for periocular recognition in the wild. Specifically, two distinct late-fusion layers are introduced in the dual-stream network to aggregate the RGB data and OC-LBCP. Thus, the network beneficial from this new feature of the late-fusion layers for accuracy performance gain. We also introduce and share a new dataset for periocular in the wild, namely Ethnic-ocular dataset for benchmarking. The proposed network has also been assessed on one publicly available dataset, namely UBIPr. The proposed network outperforms several competing approaches on these datasets.



### Speeding up convolutional networks pruning with coarse ranking
- **Arxiv ID**: http://arxiv.org/abs/1902.06385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06385v1)
- **Published**: 2019-02-18 03:13:16+00:00
- **Updated**: 2019-02-18 03:13:16+00:00
- **Authors**: Zi Wang, Chengcheng Li, Dali Wang, Xiangyang Wang, Hairong Qi
- **Comment**: Submitted to ICIP 2019
- **Journal**: None
- **Summary**: Channel-based pruning has achieved significant successes in accelerating deep convolutional neural network, whose pipeline is an iterative three-step procedure: ranking, pruning and fine-tuning. However, this iterative procedure is computationally expensive. In this study, we present a novel computationally efficient channel pruning approach based on the coarse ranking that utilizes the intermediate results during fine-tuning to rank the importance of filters, built upon state-of-the-art works with data-driven ranking criteria. The goal of this work is not to propose a single improved approach built upon a specific channel pruning method, but to introduce a new general framework that works for a series of channel pruning methods. Various benchmark image datasets (CIFAR-10, ImageNet, Birds-200, and Flowers-102) and network architectures (AlexNet and VGG-16) are utilized to evaluate the proposed approach for object classification purpose. Experimental results show that the proposed method can achieve almost identical performance with the corresponding state-of-the-art works (baseline) while our ranking time is negligibly short. In specific, with the proposed method, 75% and 54% of the total computation time for the whole pruning procedure can be reduced for AlexNet on CIFAR-10, and for VGG-16 on ImageNet, respectively. Our approach would significantly facilitate pruning practice, especially on resource-constrained platforms.



### 2017 Robotic Instrument Segmentation Challenge
- **Arxiv ID**: http://arxiv.org/abs/1902.06426v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06426v2)
- **Published**: 2019-02-18 07:08:36+00:00
- **Updated**: 2019-02-21 17:01:02+00:00
- **Authors**: Max Allan, Alex Shvets, Thomas Kurmann, Zichen Zhang, Rahul Duggal, Yun-Hsuan Su, Nicola Rieke, Iro Laina, Niveditha Kalavakonda, Sebastian Bodenstedt, Luis Herrera, Wenqi Li, Vladimir Iglovikov, Huoling Luo, Jian Yang, Danail Stoyanov, Lena Maier-Hein, Stefanie Speidel, Mahdi Azizian
- **Comment**: None
- **Journal**: None
- **Summary**: In mainstream computer vision and machine learning, public datasets such as ImageNet, COCO and KITTI have helped drive enormous improvements by enabling researchers to understand the strengths and limitations of different algorithms via performance comparison. However, this type of approach has had limited translation to problems in robotic assisted surgery as this field has never established the same level of common datasets and benchmarking methods. In 2015 a sub-challenge was introduced at the EndoVis workshop where a set of robotic images were provided with automatically generated annotations from robot forward kinematics. However, there were issues with this dataset due to the limited background variation, lack of complex motion and inaccuracies in the annotation. In this work we present the results of the 2017 challenge on robotic instrument segmentation which involved 10 teams participating in binary, parts and type based segmentation of articulated da Vinci robotic instruments.



### SEGAN: Structure-Enhanced Generative Adversarial Network for Compressed Sensing MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1902.06455v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06455v2)
- **Published**: 2019-02-18 08:28:50+00:00
- **Updated**: 2019-03-05 07:47:19+00:00
- **Authors**: Zhongnian Li, Tao Zhang, Peng Wan, Daoqiang Zhang
- **Comment**: 9 pages,5 figures, Proceedings of the Association for the Advancement
  of Artificial Intelligence (AAAI-19)
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are powerful tools for reconstructing Compressed Sensing Magnetic Resonance Imaging (CS-MRI). However most recent works lack exploration of structure information of MRI images that is crucial for clinical diagnosis. To tackle this problem, we propose the Structure-Enhanced GAN (SEGAN) that aims at restoring structure information at both local and global scale. SEGAN defines a new structure regularization called Patch Correlation Regularization (PCR) which allows for efficient extraction of structure information. In addition, to further enhance the ability to uncover structure information, we propose a novel generator SU-Net by incorporating multiple-scale convolution filters into each layer. Besides, we theoretically analyze the convergence of stochastic factors contained in training process. Experimental results show that SEGAN is able to learn target structure information and achieves state-of-the-art performance for CS-MRI reconstruction.



### Robust Structured Group Local Sparse Tracker Using Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1902.07668v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07668v2)
- **Published**: 2019-02-18 08:43:51+00:00
- **Updated**: 2020-03-30 06:06:59+00:00
- **Authors**: Mohammadreza Javanmardi, Amir Hossein Farzaneh, Xiaojun Qi
- **Comment**: This submission is similar version of Structured Group Local Sparse
  Tracker arXiv:1902.06182
- **Journal**: None
- **Summary**: Sparse representation has recently been successfully applied in visual tracking. It utilizes a set of templates to represent target candidates and find the best one with the minimum reconstruction error as the tracking result. In this paper, we propose a robust deep features-based structured group local sparse tracker (DF-SGLST), which exploits the deep features of local patches inside target candidates and represents them by a set of templates in the particle filter framework. Unlike the conventional local sparse trackers, the proposed optimization model in DF-SGLST employs a group-sparsity regularization term to seamlessly adopt local and spatial information of the target candidates and attain the spatial layout structure among them. To solve the optimization model, we propose an efficient and fast numerical algorithm that consists of two subproblems with the closed-form solutions. Different evaluations in terms of success and precision on the benchmarks of challenging image sequences (e.g., OTB50 and OTB100) demonstrate the superior performance of the proposed tracker against several state-of-the-art trackers.



### Stable Topological Summaries for Analyzing the Organization of Cells in a Packed Tissue
- **Arxiv ID**: http://arxiv.org/abs/1902.06467v5
- **DOI**: 10.3390/math9151723
- **Categories**: **cs.CV**, 68T10, 92B99, 65D18, 94A17, 55N99, 5504
- **Links**: [PDF](http://arxiv.org/pdf/1902.06467v5)
- **Published**: 2019-02-18 09:03:07+00:00
- **Updated**: 2021-05-18 12:33:52+00:00
- **Authors**: N. Atienza, M. J. Jimenez, M. Soriano-Trigueros
- **Comment**: None
- **Journal**: None
- **Summary**: We use Topological Data Analysis tools for studying the inner organization of cells in segmented images of epithelial tissues. More specifically, for each segmented image, we compute different persistence barcodes, which codify lifetime of homology classes (persistent homology) along different filtrations (increasing nested sequences of simplicial complexes) that are built from the regions representing the cells in the tissue. We use a complete and well-grounded set of numerical variables over those persistence barcodes, also known as topological summaries. A novel combination of normalization methods for both, the set of input segmented images and the produced barcodes, allows to prove stability results for those variables with respect to small changes in the input, as well as invariance to image scale. Our study provides new insights to this problem, such as a possible novel indicator for the development of the drosophila wing disc tissue or the importance of centroids distribution to differentiate some tissues from their CVT-path counterpart (a mathematical model of epithelia based on Voronoi diagrams). We also show how the use of topological summaries may improve the classification accuracy of epithelial images using Random Forests algorithm.



### Structural Recurrent Neural Network for Traffic Speed Prediction
- **Arxiv ID**: http://arxiv.org/abs/1902.06506v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.06506v1)
- **Published**: 2019-02-18 10:49:04+00:00
- **Updated**: 2019-02-18 10:49:04+00:00
- **Authors**: Youngjoo Kim, Peng Wang, Lyudmila Mihaylova
- **Comment**: Accepted and revised, to be presented in International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP) on May 2019
- **Journal**: None
- **Summary**: Deep neural networks have recently demonstrated the traffic prediction capability with the time series data obtained by sensors mounted on road segments. However, capturing spatio-temporal features of the traffic data often requires a significant number of parameters to train, increasing computational burden. In this work we demonstrate that embedding topological information of the road network improves the process of learning traffic features. We use a graph of a vehicular road network with recurrent neural networks (RNNs) to infer the interaction between adjacent road segments as well as the temporal dynamics. The topology of the road network is converted into a spatio-temporal graph to form a structural RNN (SRNN). The proposed approach is validated over traffic speed data from the road network of the city of Santander in Spain. The experiment shows that the graph-based method outperforms the state-of-the-art methods based on spatio-temporal images, requiring much fewer parameters to train.



### Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology
- **Arxiv ID**: http://arxiv.org/abs/1902.06543v2
- **DOI**: 10.1016/j.media.2019.101544
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06543v2)
- **Published**: 2019-02-18 12:36:58+00:00
- **Updated**: 2020-04-15 14:03:56+00:00
- **Authors**: David Tellez, Geert Litjens, Peter Bandi, Wouter Bulten, John-Melle Bokhorst, Francesco Ciompi, Jeroen van der Laak
- **Comment**: Accepted in the Medical Image Analysis journal
- **Journal**: None
- **Summary**: Stain variation is a phenomenon observed when distinct pathology laboratories stain tissue slides that exhibit similar but not identical color appearance. Due to this color shift between laboratories, convolutional neural networks (CNNs) trained with images from one lab often underperform on unseen images from the other lab. Several techniques have been proposed to reduce the generalization error, mainly grouped into two categories: stain color augmentation and stain color normalization. The former simulates a wide variety of realistic stain variations during training, producing stain-invariant CNNs. The latter aims to match training and test color distributions in order to reduce stain variation. For the first time, we compared some of these techniques and quantified their effect on CNN classification performance using a heterogeneous dataset of hematoxylin and eosin histopathology images from 4 organs and 9 pathology laboratories. Additionally, we propose a novel unsupervised method to perform stain color normalization using a neural network. Based on our experimental results, we provide practical guidelines on how to use stain color augmentation and stain color normalization in future computational pathology applications.



### LocalNorm: Robust Image Classification through Dynamically Regularized Normalization
- **Arxiv ID**: http://arxiv.org/abs/1902.06550v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.06550v3)
- **Published**: 2019-02-18 12:58:23+00:00
- **Updated**: 2019-03-04 10:21:13+00:00
- **Authors**: Bojian Yin, Siebren Schaafsma, Henk Corporaal, H. Steven Scholte, Sander M. Bohte
- **Comment**: 14 pages, 17 figures
- **Journal**: None
- **Summary**: While modern convolutional neural networks achieve outstanding accuracy on many image classification tasks, they are, compared to humans, much more sensitive to image degradation. Here, we describe a variant of Batch Normalization, LocalNorm, that regularizes the normalization layer in the spirit of Dropout while dynamically adapting to the local image intensity and contrast at test-time. We show that the resulting deep neural networks are much more resistant to noise-induced image degradation, improving accuracy by up to three times, while achieving the same or slightly better accuracy on non-degraded classical benchmarks. In computational terms, LocalNorm adds negligible training cost and little or no cost at inference time, and can be applied to already-trained networks in a straightforward manner.



### MetaGrasp: Data Efficient Grasping by Affordance Interpreter Network
- **Arxiv ID**: http://arxiv.org/abs/1902.06554v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.06554v2)
- **Published**: 2019-02-18 13:05:36+00:00
- **Updated**: 2019-02-23 01:25:51+00:00
- **Authors**: Junhao Cai, Hui Cheng, Zhanpeng Zhang, Jingcheng Su
- **Comment**: 7 pages, 10 figures, IEEE International Conference on Robotics and
  Automation 2019
- **Journal**: None
- **Summary**: Data-driven approach for grasping shows significant advance recently. But these approaches usually require much training data. To increase the efficiency of grasping data collection, this paper presents a novel grasp training system including the whole pipeline from data collection to model inference. The system can collect effective grasp sample with a corrective strategy assisted by antipodal grasp rule, and we design an affordance interpreter network to predict pixelwise grasp affordance map. We define graspability, ungraspability and background as grasp affordances. The key advantage of our system is that the pixel-level affordance interpreter network trained with only a small number of grasp samples under antipodal rule can achieve significant performance on totally unseen objects and backgrounds. The training sample is only collected in simulation. Extensive qualitative and quantitative experiments demonstrate the accuracy and robustness of our proposed approach. In the real-world grasp experiments, we achieve a grasp success rate of 93% on a set of household items and 91% on a set of adversarial items with only about 6,300 simulated samples. We also achieve 87% accuracy in clutter scenario. Although the model is trained using only RGB image, when changing the background textures, it also performs well and can achieve even 94% accuracy on the set of adversarial objects, which outperforms current state-of-the-art methods.



### Decomposing multispectral face images into diffuse and specular shading and biophysical parameters
- **Arxiv ID**: http://arxiv.org/abs/1902.06557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06557v1)
- **Published**: 2019-02-18 13:09:35+00:00
- **Updated**: 2019-02-18 13:09:35+00:00
- **Authors**: Sarah Alotaibi, William A. P. Smith
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel biophysical and dichromatic reflectance model that efficiently characterises spectral skin reflectance. We show how to fit the model to multispectral face images enabling high quality estimation of diffuse and specular shading as well as biophysical parameter maps (melanin and haemoglobin). Our method works from a single image without requiring complex controlled lighting setups yet provides quantitatively accurate reconstructions and qualitatively convincing decomposition and editing.



### A Generative Map for Image-based Camera Localization
- **Arxiv ID**: http://arxiv.org/abs/1902.11124v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.11124v4)
- **Published**: 2019-02-18 13:18:36+00:00
- **Updated**: 2019-04-16 15:59:29+00:00
- **Authors**: Mingpan Guo, Stefan Matthes, Jiaojiao Ye, Hao Shen
- **Comment**: typo fixes
- **Journal**: None
- **Summary**: In image-based camera localization systems, information about the environment is usually stored in some representation, which can be referred to as a map. Conventionally, most maps are built upon hand-crafted features. Recently, neural networks have attracted attention as a data-driven map representation, and have shown promising results in visual localization. However, these neural network maps are generally hard to interpret by human. A readable map is not only accessible to humans, but also provides a way to be verified when the ground truth pose is unavailable. To tackle this problem, we propose Generative Map, a new framework for learning human-readable neural network maps, by combining a generative model with the Kalman filter, which also allows it to incorporate additional sensor information such as stereo visual odometry. For evaluation, we use real world images from the 7-Scenes and Oxford RobotCar datasets. We demonstrate that our Generative Map can be queried with a pose of interest from the test sequence to predict an image, which closely resembles the true scene. For localization, we show that Generative Map achieves comparable performance with current regression models. Moreover, our framework is trained completely from scratch, unlike regression models which rely on large ImageNet pretrained networks.



### Object Recognition under Multifarious Conditions: A Reliability Analysis and A Feature Similarity-based Performance Estimation
- **Arxiv ID**: http://arxiv.org/abs/1902.06585v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1902.06585v2)
- **Published**: 2019-02-18 14:27:25+00:00
- **Updated**: 2019-05-06 07:36:56+00:00
- **Authors**: Dogancan Temel, Jinsol Lee, Ghassan AlRegib
- **Comment**: 5 pages, 3 figures, 1 table
- **Journal**: IEEE International Conference on Image Processing, Taipei, Taiwan,
  2019
- **Summary**: In this paper, we investigate the reliability of online recognition platforms, Amazon Rekognition and Microsoft Azure, with respect to changes in background, acquisition device, and object orientation. We focus on platforms that are commonly used by the public to better understand their real-world performances. To assess the variation in recognition performance, we perform a controlled experiment by changing the acquisition conditions one at a time. We use three smartphones, one DSLR, and one webcam to capture side views and overhead views of objects in a living room, an office, and photo studio setups. Moreover, we introduce a framework to estimate the recognition performance with respect to backgrounds and orientations. In this framework, we utilize both handcrafted features based on color, texture, and shape characteristics and data-driven features obtained from deep neural networks. Experimental results show that deep learning-based image representations can estimate the recognition performance variation with a Spearman's rank-order correlation of 0.94 under multifarious acquisition conditions.



### Contextual Encoder-Decoder Network for Visual Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/1902.06634v3
- **DOI**: 10.1016/j.neunet.2020.05.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06634v3)
- **Published**: 2019-02-18 16:15:25+00:00
- **Updated**: 2020-07-07 11:35:47+00:00
- **Authors**: Alexander Kroner, Mario Senden, Kurt Driessens, Rainer Goebel
- **Comment**: Accepted Manuscript
- **Journal**: Neural Networks, 2020, Volume 129, Pages 261-270, ISSN 0893-6080
- **Summary**: Predicting salient regions in natural images requires the detection of objects that are present in a scene. To develop robust representations for this challenging task, high-level visual features at multiple spatial scales must be extracted and augmented with contextual information. However, existing models aimed at explaining human fixation maps do not incorporate such a mechanism explicitly. Here we propose an approach based on a convolutional neural network pre-trained on a large-scale image classification task. The architecture forms an encoder-decoder structure and includes a module with multiple convolutional layers at different dilation rates to capture multi-scale features in parallel. Moreover, we combine the resulting representations with global scene information for accurately predicting visual saliency. Our model achieves competitive and consistent results across multiple evaluation metrics on two public saliency benchmarks and we demonstrate the effectiveness of the suggested approach on five datasets and selected examples. Compared to state of the art approaches, the network is based on a lightweight image classification backbone and hence presents a suitable choice for applications with limited computational resources, such as (virtual) robotic systems, to estimate human fixations across complex natural scenes.



### Generative Adversarial Networks Synthesize Realistic OCT Images of the Retina
- **Arxiv ID**: http://arxiv.org/abs/1902.06676v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.06676v1)
- **Published**: 2019-02-18 17:47:53+00:00
- **Updated**: 2019-02-18 17:47:53+00:00
- **Authors**: Stephen G. Odaibo, M. D., M. S., M. S.
- **Comment**: None
- **Journal**: None
- **Summary**: We report, to our knowledge, the first end-to-end application of Generative Adversarial Networks (GANs) towards the synthesis of Optical Coherence Tomography (OCT) images of the retina. Generative models have gained recent attention for the increasingly realistic images they can synthesize, given a sampling of a data type. In this paper, we apply GANs to a sampling distribution of OCTs of the retina. We observe the synthesis of realistic OCT images depicting recognizable pathology such as macular holes, choroidal neovascular membranes, myopic degeneration, cystoid macular edema, and central serous retinopathy amongst others. This represents the first such report of its kind. Potential applications of this new technology include for surgical simulation, for treatment planning, for disease prognostication, and for accelerating the development of new drugs and surgical procedures to treat retinal disease.



### HybridSN: Exploring 3D-2D CNN Feature Hierarchy for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.06701v3
- **DOI**: 10.1109/LGRS.2019.2918719
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06701v3)
- **Published**: 2019-02-18 18:14:26+00:00
- **Updated**: 2019-07-03 06:05:46+00:00
- **Authors**: Swalpa Kumar Roy, Gopal Krishna, Shiv Ram Dubey, Bidyut B. Chaudhuri
- **Comment**: Published in IEEE Geoscience and Remote Sensing Letters
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) classification is widely used for the analysis of remotely sensed images. Hyperspectral imagery includes varying bands of images. Convolutional Neural Network (CNN) is one of the most frequently used deep learning based methods for visual data processing. The use of CNN for HSI classification is also visible in recent works. These approaches are mostly based on 2D CNN. Whereas, the HSI classification performance is highly dependent on both spatial and spectral information. Very few methods have utilized the 3D CNN because of increased computational complexity. This letter proposes a Hybrid Spectral Convolutional Neural Network (HybridSN) for HSI classification. Basically, the HybridSN is a spectral-spatial 3D-CNN followed by spatial 2D-CNN. The 3D-CNN facilitates the joint spatial-spectral feature representation from a stack of spectral bands. The 2D-CNN on top of the 3D-CNN further learns more abstract level spatial representation. Moreover, the use of hybrid CNNs reduces the complexity of the model compared to 3D-CNN alone. To test the performance of this hybrid approach, very rigorous HSI classification experiments are performed over Indian Pines, Pavia University and Salinas Scene remote sensing datasets. The results are compared with the state-of-the-art hand-crafted as well as end-to-end deep learning based methods. A very satisfactory performance is obtained using the proposed HybridSN for HSI classification. The source code can be found at \url{https://github.com/gokriznastic/HybridSN}.



### 3D Scene Reconstruction with Multi-layer Depth and Epipolar Transformers
- **Arxiv ID**: http://arxiv.org/abs/1902.06729v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06729v2)
- **Published**: 2019-02-18 18:55:22+00:00
- **Updated**: 2019-08-27 17:25:32+00:00
- **Authors**: Daeyun Shin, Zhile Ren, Erik B. Sudderth, Charless C. Fowlkes
- **Comment**: Accepted at ICCV 2019. Paper title changed. Project web page:
  https://research.dshin.org/iccv19/multi-layer-depth
- **Journal**: None
- **Summary**: We tackle the problem of automatically reconstructing a complete 3D model of a scene from a single RGB image. This challenging task requires inferring the shape of both visible and occluded surfaces. Our approach utilizes viewer-centered, multi-layer representation of scene geometry adapted from recent methods for single object shape completion. To improve the accuracy of view-centered representations for complex scenes, we introduce a novel "Epipolar Feature Transformer" that transfers convolutional network features from an input view to other virtual camera viewpoints, and thus better covers the 3D scene geometry. Unlike existing approaches that first detect and localize objects in 3D, and then infer object shape using category-specific models, our approach is fully convolutional, end-to-end differentiable, and avoids the resolution and memory limitations of voxel representations. We demonstrate the advantages of multi-layer depth representations and epipolar feature transformers on the reconstruction of a large database of indoor scenes.



### DIViS: Domain Invariant Visual Servoing for Collision-Free Goal Reaching
- **Arxiv ID**: http://arxiv.org/abs/1902.05947v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1902.05947v1)
- **Published**: 2019-02-18 18:57:05+00:00
- **Updated**: 2019-02-18 18:57:05+00:00
- **Authors**: Fereshteh Sadeghi
- **Comment**: Supplementary videos: https://fsadeghi.github.io/DIViS
- **Journal**: None
- **Summary**: Robots should understand both semantics and physics to be functional in the real world. While robot platforms provide means for interacting with the physical world they cannot autonomously acquire object-level semantics without needing human. In this paper, we investigate how to minimize human effort and intervention to teach robots perform real world tasks that incorporate semantics. We study this question in the context of visual servoing of mobile robots and propose DIViS, a Domain Invariant policy learning approach for collision free Visual Servoing. DIViS incorporates high level semantics from previously collected static human-labeled datasets and learns collision free servoing entirely in simulation and without any real robot data. However, DIViS can directly be deployed on a real robot and is capable of servoing to the user-specified object categories while avoiding collisions in the real world. DIViS is not constrained to be queried by the final view of goal but rather is robust to servo to image goals taken from initial robot view with high occlusions without this impairing its ability to maintain a collision free path. We show the generalization capability of DIViS on real mobile robots in more than 90 real world test scenarios with various unseen object goals in unstructured environments. DIViS is compared to prior approaches via real world experiments and rigorous tests in simulation. For supplementary videos, see: \href{https://fsadeghi.github.io/DIViS}{https://fsadeghi.github.io/DIViS}



### Democratisation of Usable Machine Learning in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1902.06804v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.06804v1)
- **Published**: 2019-02-18 21:22:45+00:00
- **Updated**: 2019-02-18 21:22:45+00:00
- **Authors**: Raymond Bond, Ansgar Koene, Alan Dix, Jennifer Boger, Maurice D. Mulvenna, Mykola Galushka, Bethany Waterhouse Bradley, Fiona Browne, Hui Wang, Alexander Wong
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Many industries are now investing heavily in data science and automation to replace manual tasks and/or to help with decision making, especially in the realm of leveraging computer vision to automate many monitoring, inspection, and surveillance tasks. This has resulted in the emergence of the 'data scientist' who is conversant in statistical thinking, machine learning (ML), computer vision, and computer programming. However, as ML becomes more accessible to the general public and more aspects of ML become automated, applications leveraging computer vision are increasingly being created by non-experts with less opportunity for regulatory oversight. This points to the overall need for more educated responsibility for these lay-users of usable ML tools in order to mitigate potentially unethical ramifications. In this paper, we undertake a SWOT analysis to study the strengths, weaknesses, opportunities, and threats of building usable ML tools for mass adoption for important areas leveraging ML such as computer vision. The paper proposes a set of data science literacy criteria for educating and supporting lay-users in the responsible development and deployment of ML applications.



### FreeLabel: A Publicly Available Annotation Tool based on Freehand Traces
- **Arxiv ID**: http://arxiv.org/abs/1902.06806v2
- **DOI**: 10.1109/WACV.2019.00010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06806v2)
- **Published**: 2019-02-18 21:47:39+00:00
- **Updated**: 2019-03-11 15:34:45+00:00
- **Authors**: Philipe A. Dias, Zhou Shen, Amy Tabb, Henry Medeiros
- **Comment**: Accepted and presented at 2019 IEEE Winter Conference on Applications
  of Computer Vision (WACV). 10 pages
- **Journal**: 2019 IEEE Winter Conference on Applications of Computer Vision
  (WACV)
- **Summary**: Large-scale annotation of image segmentation datasets is often prohibitively expensive, as it usually requires a huge number of worker hours to obtain high-quality results. Abundant and reliable data has been, however, crucial for the advances on image understanding tasks achieved by deep learning models. In this paper, we introduce FreeLabel, an intuitive open-source web interface that allows users to obtain high-quality segmentation masks with just a few freehand scribbles, in a matter of seconds. The efficacy of FreeLabel is quantitatively demonstrated by experimental results on the PASCAL dataset as well as on a dataset from the agricultural domain. Designed to benefit the computer vision community, FreeLabel can be used for both crowdsourced or private annotation and has a modular structure that can be easily adapted for any image dataset.



### Motion Equivariant Networks for Event Cameras with the Temporal Normalization Transform
- **Arxiv ID**: http://arxiv.org/abs/1902.06820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06820v1)
- **Published**: 2019-02-18 22:21:59+00:00
- **Updated**: 2019-02-18 22:21:59+00:00
- **Authors**: Alex Zihao Zhu, Ziyun Wang, Kostas Daniilidis
- **Comment**: 8 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: In this work, we propose a novel transformation for events from an event camera that is equivariant to optical flow under convolutions in the 3-D spatiotemporal domain. Events are generated by changes in the image, which are typically due to motion, either of the camera or the scene. As a result, different motions result in a different set of events. For learning based tasks based on a static scene such as classification which directly use the events, we must either rely on the learning method to learn the underlying object distinct from the motion, or to memorize all possible motions for each object with extensive data augmentation. Instead, we propose a novel transformation of the input event data which normalizes the $x$ and $y$ positions by the timestamp of each event. We show that this transformation generates a representation of the events that is equivariant to this motion when the optical flow is constant, allowing a deep neural network to learn the classification task without the need for expensive data augmentation. We test our method on the event based N-MNIST dataset, as well as a novel dataset N-MOVING-MNIST, with significantly more variety in motion compared to the standard N-MNIST dataset. In all sequences, we demonstrate that our transformed network is able to achieve similar or better performance compared to a network with a standard volumetric event input, and performs significantly better when the test set has a larger set of motions than seen at training.



### Low-bit Quantization of Neural Networks for Efficient Inference
- **Arxiv ID**: http://arxiv.org/abs/1902.06822v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.06822v2)
- **Published**: 2019-02-18 22:28:34+00:00
- **Updated**: 2019-03-25 08:12:15+00:00
- **Authors**: Yoni Choukroun, Eli Kravchik, Fan Yang, Pavel Kisilev
- **Comment**: None
- **Journal**: None
- **Summary**: Recent machine learning methods use increasingly large deep neural networks to achieve state of the art results in various tasks. The gains in performance come at the cost of a substantial increase in computation and storage requirements. This makes real-time implementations on limited resources hardware a challenging task. One popular approach to address this challenge is to perform low-bit precision computations via neural network quantization. However, aggressive quantization generally entails a severe penalty in terms of accuracy, and often requires retraining of the network, or resorting to higher bit precision quantization. In this paper, we formalize the linear quantization task as a Minimum Mean Squared Error (MMSE) problem for both weights and activations, allowing low-bit precision inference without the need for full network retraining. The main contributions of our approach are the optimizations of the constrained MSE problem at each layer of the network, the hardware aware partitioning of the network parameters, and the use of multiple low precision quantized tensors for poorly approximated layers. The proposed approach allows 4 bits integer (INT4) quantization for deployment of pretrained models on limited hardware resources. Multiple experiments on various network architectures show that the suggested method yields state of the art results with minimal loss of tasks accuracy.



### Commodity RGB-D Sensors: Data Acquisition
- **Arxiv ID**: http://arxiv.org/abs/1902.06835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06835v1)
- **Published**: 2019-02-18 23:17:19+00:00
- **Updated**: 2019-02-18 23:17:19+00:00
- **Authors**: Michael Zollh√∂fer
- **Comment**: Contributed chapter to a book on "RGB-D Image Analysis and
  Processing"
- **Journal**: None
- **Summary**: Over the past ten years we have seen a democratization of range sensing technology. While previously range sensors have been highly expensive and only accessible to a few domain experts, such sensors are nowadays ubiquitous and can even be found in the latest generation of mobile devices, e.g., current smartphones. This democratization of range sensing technology was started with the release of the Microsoft Kinect, and since then many different commodity range sensors followed its lead, such as the Primesense Carmine, Asus Xtion Pro, and the Structure Sensor from Occipital. The availability of cheap range sensing technology led to a big leap in research, especially in the context of more powerful static and dynamic reconstruction techniques, starting from 3D scanning applications, such as KinectFusion, to highly accurate face and body tracking approaches. In this chapter, we have a detailed look into the different types of existing range sensors. We discuss the two fundamental types of commodity range sensing techniques in detail, namely passive and active sensing, and we explore the principles these technologies are based on. Our focus is on modern active commodity range sensors based on time-of-flight and structured light. We conclude by discussing the noise characteristics, working ranges, and types of errors made by the different sensing modalities.



### SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color
- **Arxiv ID**: http://arxiv.org/abs/1902.06838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06838v1)
- **Published**: 2019-02-18 23:28:30+00:00
- **Updated**: 2019-02-18 23:28:30+00:00
- **Authors**: Youngjoo Jo, Jongyoul Park
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel image editing system that generates images as the user provides free-form mask, sketch and color as an input. Our system consist of a end-to-end trainable convolutional network. Contrary to the existing methods, our system wholly utilizes free-form user input with color and shape. This allows the system to respond to the user's sketch and color input, using it as a guideline to generate an image. In our particular work, we trained network with additional style loss which made it possible to generate realistic results, despite large portions of the image being removed. Our proposed network architecture SC-FEGAN is well suited to generate high quality synthetic image using intuitive user inputs.




# Arxiv Papers in cs.CV on 2019-02-20
### Human Motion Prediction via Learning Local Structure Representations and Temporal Dependencies
- **Arxiv ID**: http://arxiv.org/abs/1902.07367v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07367v2)
- **Published**: 2019-02-20 01:39:23+00:00
- **Updated**: 2019-05-07 19:37:11+00:00
- **Authors**: Xiao Guo, Jongmoo Choi
- **Comment**: Accepted by AAAI19; Updated with the open source link
- **Journal**: None
- **Summary**: Human motion prediction from motion capture data is a classical problem in the computer vision, and conventional methods take the holistic human body as input. These methods ignore the fact that, in various human activities, different body components (limbs and the torso) have distinctive characteristics in terms of the moving pattern. In this paper, we argue local representations on different body components should be learned separately and, based on such idea, propose a network, Skeleton Network (SkelNet), for long-term human motion prediction. Specifically, at each time-step, local structure representations of input (human body) are obtained via SkelNet's branches of component-specific layers, then the shared layer uses local spatial representations to predict the future human pose. Our SkelNet is the first to use local structure representations for predicting the human motion. Then, for short-term human motion prediction, we propose the second network, named as Skeleton Temporal Network (Skel-TNet). Skel-TNet consists of three components: SkelNet and a Recurrent Neural Network, they have advantages in learning spatial and temporal dependencies for predicting human motion, respectively; a feed-forward network that outputs the final estimation. Our methods achieve promising results on the Human3.6M dataset and the CMU motion capture dataset.



### Learning Transferable Self-attentive Representations for Action Recognition in Untrimmed Videos with Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/1902.07370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07370v1)
- **Published**: 2019-02-20 01:58:42+00:00
- **Updated**: 2019-02-20 01:58:42+00:00
- **Authors**: Xiao-Yu Zhang, Haichao Shi, Changsheng Li, Kai Zheng, Xiaobin Zhu, Lixin Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition in videos has attracted a lot of attention in the past decade. In order to learn robust models, previous methods usually assume videos are trimmed as short sequences and require ground-truth annotations of each video frame/sequence, which is quite costly and time-consuming. In this paper, given only video-level annotations, we propose a novel weakly supervised framework to simultaneously locate action frames as well as recognize actions in untrimmed videos. Our proposed framework consists of two major components. First, for action frame localization, we take advantage of the self-attention mechanism to weight each frame, such that the influence of background frames can be effectively eliminated. Second, considering that there are trimmed videos publicly available and also they contain useful information to leverage, we present an additional module to transfer the knowledge from trimmed videos for improving the classification performance in untrimmed ones. Extensive experiments are conducted on two benchmark datasets (i.e., THUMOS14 and ActivityNet1.3), and experimental results clearly corroborate the efficacy of our method.



### An Autoencoder-based Learned Image Compressor: Description of Challenge Proposal by NCTU
- **Arxiv ID**: http://arxiv.org/abs/1902.07385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07385v1)
- **Published**: 2019-02-20 03:16:55+00:00
- **Updated**: 2019-02-20 03:16:55+00:00
- **Authors**: David Alexandre, Chih-Peng Chang, Wen-Hsiao Peng, Hsueh-Ming Hang
- **Comment**: Published in CVPR 2018: Workshop And Challenge On Learned Image
  Compression
- **Journal**: None
- **Summary**: We propose a lossy image compression system using the deep-learning autoencoder structure to participate in the Challenge on Learned Image Compression (CLIC) 2018. Our autoencoder uses the residual blocks with skip connections to reduce the correlation among image pixels and condense the input image into a set of feature maps, a compact representation of the original image. The bit allocation and bitrate control are implemented by using the importance maps and quantizer. The importance maps are generated by a separate neural net in the encoder. The autoencoder and the importance net are trained jointly based on minimizing a weighted sum of mean squared error, MS-SSIM, and a rate estimate. Our aim is to produce reconstructed images with good subjective quality subject to the 0.15 bits-per-pixel constraint.



### Deep Learning Based Video System for Accurate and Real-Time Parking Measurement
- **Arxiv ID**: http://arxiv.org/abs/1902.07401v1
- **DOI**: 10.1109/jiot.2019.2902887
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07401v1)
- **Published**: 2019-02-20 04:49:13+00:00
- **Updated**: 2019-02-20 04:49:13+00:00
- **Authors**: Bill Yang Cai, Ricardo Alvarez, Michelle Sit, Fábio Duarte, Carlo Ratti
- **Comment**: Accepted for publication in IEEE Internet of Things Journal, Special
  Issue on Enabling a Smart City: IoT Meets AI
- **Journal**: None
- **Summary**: Parking spaces are costly to build, parking payments are difficult to enforce, and drivers waste an excessive amount of time searching for empty lots. Accurate quantification would inform developers and municipalities in space allocation and design, while real-time measurements would provide drivers and parking enforcement with information that saves time and resources. In this paper, we propose an accurate and real-time video system for future Internet of Things (IoT) and smart cities applications. Using recent developments in deep convolutional neural networks (DCNNs) and a novel vehicle tracking filter, we combine information across multiple image frames in a video sequence to remove noise introduced by occlusions and detection failures. We demonstrate that our system achieves higher accuracy than pure image-based instance segmentation, and is comparable in performance to industry benchmark systems that utilize more expensive sensors such as radar. Furthermore, our system shows significant potential in its scalability to a city-wide scale and also in the richness of its output that goes beyond traditional binary occupancy statistics.



### A Novel Euler's Elastica based Segmentation Approach for Noisy Images via using the Progressive Hedging Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1902.07402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07402v1)
- **Published**: 2019-02-20 04:50:42+00:00
- **Updated**: 2019-02-20 04:50:42+00:00
- **Authors**: Lu Tan, Ling Li, Wanquan Liu, Jie Sun, Min Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Euler's Elastica based unsupervised segmentation models have strong capability of completing the missing boundaries for existing objects in a clean image, but they are not working well for noisy images. This paper aims to establish a Euler's Elastica based approach that properly deals with random noises to improve the segmentation performance for noisy images. We solve the corresponding optimization problem via using the progressive hedging algorithm (PHA) with a step length suggested by the alternating direction method of multipliers (ADMM). Technically, all the simplified convex versions of the subproblems derived from the major framework of PHA can be obtained by using the curvature weighted approach and the convex relaxation method. Then an alternating optimization strategy is applied with the merits of using some powerful accelerating techniques including the fast Fourier transform (FFT) and generalized soft threshold formulas. Extensive experiments have been conducted on both synthetic and real images, which validated some significant gains of the proposed segmentation models and demonstrated the advantages of the developed algorithm.



### Motion Corrected Multishot MRI Reconstruction Using Generative Networks with Sensitivity Encoding
- **Arxiv ID**: http://arxiv.org/abs/1902.07430v6
- **DOI**: 10.1038/s41598-020-61705-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07430v6)
- **Published**: 2019-02-20 07:23:28+00:00
- **Updated**: 2020-03-12 02:09:20+00:00
- **Authors**: Muhammad Usman, Muhammad Umar Farooq, Siddique Latif, Muhammad Asim, Junaid Qadir
- **Comment**: This paper has been published in Scientific Reports Journal
- **Journal**: None
- **Summary**: Multishot Magnetic Resonance Imaging (MRI) is a promising imaging modality that can produce a high-resolution image with relatively less data acquisition time. The downside of multishot MRI is that it is very sensitive to subject motion and even small amounts of motion during the scan can produce artifacts in the final MR image that may cause misdiagnosis. Numerous efforts have been made to address this issue; however, all of these proposals are limited in terms of how much motion they can correct and the required computational time. In this paper, we propose a novel generative networks based conjugate gradient SENSE (CG-SENSE) reconstruction framework for motion correction in multishot MRI. The proposed framework first employs CG-SENSE reconstruction to produce the motion-corrupted image and then a generative adversarial network (GAN) is used to correct the motion artifacts. The proposed method has been rigorously evaluated on synthetically corrupted data on varying degrees of motion, numbers of shots, and encoding trajectories. Our analyses (both quantitative as well as qualitative/visual analysis) establishes that the proposed method significantly robust and outperforms state-of-the-art motion correction techniques and also reduces severalfold of computational times.



### Dynamic Matrix Decomposition for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.07438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07438v1)
- **Published**: 2019-02-20 07:39:45+00:00
- **Updated**: 2019-02-20 07:39:45+00:00
- **Authors**: Abdul Basit
- **Comment**: None
- **Journal**: None
- **Summary**: Designing a technique for the automatic analysis of different actions in videos in order to detect the presence of interested activities is of high significance nowadays. In this paper, we explore a robust and dynamic appearance technique for the purpose of identifying different action activities. We also exploit a low-rank and structured sparse matrix decomposition (LSMD) method to better model these activities.. Our method is effective in encoding localized spatio-temporal features which enables the analysis of local motion taking place in the video. Our proposed model use adjacent frame differences as the input to the method thereby forcing it to capture the changes occurring in the video. The performance of our model is tested on a benchmark dataset in terms of detection accuracy. Results achieved with our model showed the promising capability of our model in detecting action activities.



### Long-Bone Fracture Detection using Artificial Neural Networks based on Line Features of X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/1902.07458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07458v1)
- **Published**: 2019-02-20 08:51:41+00:00
- **Updated**: 2019-02-20 08:51:41+00:00
- **Authors**: Alice Yi Yang, Ling Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Two line-based fracture detection scheme are developed and discussed, namely Standard line-based fracture detection and Adaptive Differential Parameter Optimized (ADPO) line-based fracture detection. The purpose for the two line-based fracture detection schemes is to detect fractured lines from X-ray images using extracted features based on recognised patterns to differentiate fractured lines from non-fractured lines. The difference between the two schemes is the detection of detailed lines. The ADPO scheme optimizes the parameters of the Probabilistic Hough Transform, such that granule lines within the fractured regions are detected, whereas the Standard scheme is unable to detect them. The lines are detected using the Probabilistic Hough Function, in which the detected lines are a representation of the image edge objects. The lines are given in the form of points, (x,y), which includes the starting and ending point. Based on the given line points, 13 features are extracted from each line, as a summary of line information. These features are used for fracture and non-fracture classification of the detected lines. The classification is carried out by the Artificial Neural Network (ANN). There are two evaluations that are employed to evaluate both the entirety of the system and the ANN. The Standard Scheme is capable of achieving an average accuracy of 74.25%, whilst the ADPO scheme achieved an average accuracy of 74.4%. The ADPO scheme is opted for over the Standard scheme, however it can be further improved with detected contours and its extracted features.



### DNNVM : End-to-End Compiler Leveraging Heterogeneous Optimizations on FPGA-based CNN Accelerators
- **Arxiv ID**: http://arxiv.org/abs/1902.07463v2
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.07463v2)
- **Published**: 2019-02-20 09:30:17+00:00
- **Updated**: 2019-07-25 09:41:31+00:00
- **Authors**: Yu Xing, Shuang Liang, Lingzhi Sui, Xijie Jia, Jiantao Qiu, Xin Liu, Yushun Wang, Yu Wang, Yi Shan
- **Comment**: 18 pages, 9 figures, 5 tables
- **Journal**: None
- **Summary**: The convolutional neural network (CNN) has become a state-of-the-art method for several artificial intelligence domains in recent years. The increasingly complex CNN models are both computation-bound and I/O-bound. FPGA-based accelerators driven by custom instruction set architecture (ISA) achieve a balance between generality and efficiency, but there is much on them left to be optimized. We propose the full-stack compiler DNNVM, which is an integration of optimizers for graphs, loops and data layouts, and an assembler, a runtime supporter and a validation environment. The DNNVM works in the context of deep learning frameworks and transforms CNN models into the directed acyclic graph: XGraph. Based on XGraph, we transform the optimization challenges for both the data layout and pipeline into graph-level problems. DNNVM enumerates all potentially profitable fusion opportunities by a heuristic subgraph isomorphism algorithm to leverage pipeline and data layout optimizations, and searches for the best choice of execution strategies of the whole computing graph. On the Xilinx ZU2 @330 MHz and ZU9 @330 MHz, we achieve equivalently state-of-the-art performance on our benchmarks by na\"ive implementations without optimizations, and the throughput is further improved up to 1.26x by leveraging heterogeneous optimizations in DNNVM. Finally, with ZU9 @330 MHz, we achieve state-of-the-art performance for VGG and ResNet50. We achieve a throughput of 2.82 TOPs/s and an energy efficiency of 123.7 GOPs/s/W for VGG. Additionally, we achieve 1.38 TOPs/s for ResNet50 and 1.41 TOPs/s for GoogleNet.



### Dual-modality seq2seq network for audio-visual event localization
- **Arxiv ID**: http://arxiv.org/abs/1902.07473v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07473v2)
- **Published**: 2019-02-20 09:48:30+00:00
- **Updated**: 2020-08-06 07:08:41+00:00
- **Authors**: Yan-Bo Lin, Yu-Jhe Li, Yu-Chiang Frank Wang
- **Comment**: Accepted in ICASSP 2019
- **Journal**: None
- **Summary**: Audio-visual event localization requires one to identify theevent which is both visible and audible in a video (eitherat a frame or video level). To address this task, we pro-pose a deep neural network named Audio-Visual sequence-to-sequence dual network (AVSDN). By jointly taking bothaudio and visual features at each time segment as inputs, ourproposed model learns global and local event information ina sequence to sequence manner, which can be realized in ei-ther fully supervised or weakly supervised settings. Empiricalresults confirm that our proposed method performs favorablyagainst recent deep learning approaches in both settings.



### Spatially-Adaptive Filter Units for Compact and Efficient Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.07474v2
- **DOI**: 10.1007/s11263-019-01282-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07474v2)
- **Published**: 2019-02-20 09:49:55+00:00
- **Updated**: 2020-02-06 18:11:44+00:00
- **Authors**: Domen Tabernik, Matej Kristan, Aleš Leonardis
- **Comment**: Accepted for publication in International Journal of Computer Vision,
  Jan 02 2020
- **Journal**: None
- **Summary**: Convolutional neural networks excel in a number of computer vision tasks. One of their most crucial architectural elements is the effective receptive field size, that has to be manually set to accommodate a specific task. Standard solutions involve large kernels, down/up-sampling and dilated convolutions. These require testing a variety of dilation and down/up-sampling factors and result in non-compact representations and excessive number of parameters. We address this issue by proposing a new convolution filter composed of displaced aggregation units (DAU). DAUs learn spatial displacements and adapt the receptive field sizes of individual convolution filters to a given problem, thus eliminating the need for hand-crafted modifications. DAUs provide a seamless substitution of convolutional filters in existing state-of-the-art architectures, which we demonstrate on AlexNet, ResNet50, ResNet101, DeepLab and SRN-DeblurNet. The benefits of this design are demonstrated on a variety of computer vision tasks and datasets, such as image classification (ILSVRC 2012), semantic segmentation (PASCAL VOC 2011, Cityscape) and blind image de-blurring (GOPRO). Results show that DAUs efficiently allocate parameters resulting in up to four times more compact networks at similar or better performance.



### An efficient solution for semantic segmentation: ShuffleNet V2 with atrous separable convolutions
- **Arxiv ID**: http://arxiv.org/abs/1902.07476v2
- **DOI**: 10.1007/978-3-030-20205-7_4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07476v2)
- **Published**: 2019-02-20 09:50:47+00:00
- **Updated**: 2019-04-03 17:18:58+00:00
- **Authors**: Sercan Türkmen, Janne Heikkilä
- **Comment**: 12 pages, 6 figures, 5 tables
- **Journal**: None
- **Summary**: Assigning a label to each pixel in an image, namely semantic segmentation, has been an important task in computer vision, and has applications in autonomous driving, robotic navigation, localization, and scene understanding. Fully convolutional neural networks have proved to be a successful solution for the task over the years but most of the work being done focuses primarily on accuracy. In this paper, we present a computationally efficient approach to semantic segmentation, while achieving a high mean intersection over union (mIOU), 70.33% on Cityscapes challenge. The network proposed is capable of running real-time on mobile devices. In addition, we make our code and model weights publicly available.



### Dense 3D Visual Mapping via Semantic Simplification
- **Arxiv ID**: http://arxiv.org/abs/1902.07511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07511v1)
- **Published**: 2019-02-20 11:23:44+00:00
- **Updated**: 2019-02-20 11:23:44+00:00
- **Authors**: Luca Morreale, Andrea Romanoni, Matteo Matteucci
- **Comment**: 7 pages; accepted at International Conference on Robotics and
  Automation (IEEE) 2019
- **Journal**: None
- **Summary**: Dense 3D visual mapping estimates as many as possible pixel depths, for each image. This results in very dense point clouds that often contain redundant and noisy information, especially for surfaces that are roughly planar, for instance, the ground or the walls in the scene. In this paper we leverage on semantic image segmentation to discriminate which regions of the scene require simplification and which should be kept at high level of details. We propose four different point cloud simplification methods which decimate the perceived point cloud by relying on class-specific local and global statistics still maintaining more points in the proximity of class boundaries to preserve the infra-class edges and discontinuities. 3D dense model is obtained by fusing the point clouds in a 3D Delaunay Triangulation to deal with variable point cloud density. In the experimental evaluation we have shown that, by leveraging on semantics, it is possible to simplify the model and diminish the noise affecting the point clouds.



### Patch-based Output Space Adversarial Learning for Joint Optic Disc and Cup Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.07519v1
- **DOI**: 10.1109/TMI.2019.2899910
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07519v1)
- **Published**: 2019-02-20 11:37:59+00:00
- **Updated**: 2019-02-20 11:37:59+00:00
- **Authors**: Shujun Wang, Lequan Yu, Xin Yang, Chi-Wing Fu, Pheng-Ann Heng
- **Comment**: IEEE Transactions on Medical Imaging (In press)
- **Journal**: None
- **Summary**: Glaucoma is a leading cause of irreversible blindness. Accurate segmentation of the optic disc (OD) and cup (OC) from fundus images is beneficial to glaucoma screening and diagnosis. Recently, convolutional neural networks demonstrate promising progress in joint OD and OC segmentation. However, affected by the domain shift among different datasets, deep networks are severely hindered in generalizing across different scanners and institutions. In this paper, we present a novel patchbased Output Space Adversarial Learning framework (pOSAL) to jointly and robustly segment the OD and OC from different fundus image datasets. We first devise a lightweight and efficient segmentation network as a backbone. Considering the specific morphology of OD and OC, a novel morphology-aware segmentation loss is proposed to guide the network to generate accurate and smooth segmentation. Our pOSAL framework then exploits unsupervised domain adaptation to address the domain shift challenge by encouraging the segmentation in the target domain to be similar to the source ones. Since the whole-segmentationbased adversarial loss is insufficient to drive the network to capture segmentation details, we further design the pOSAL in a patch-based fashion to enable fine-grained discrimination on local segmentation details. We extensively evaluate our pOSAL framework and demonstrate its effectiveness in improving the segmentation performance on three public retinal fundus image datasets, i.e., Drishti-GS, RIM-ONE-r3, and REFUGE. Furthermore, our pOSAL framework achieved the first place in the OD and OC segmentation tasks in MICCAI 2018 Retinal Fundus Glaucoma Challenge.



### Dynamic Cell Imaging in PET with Optimal Transport Regularization
- **Arxiv ID**: http://arxiv.org/abs/1902.07521v2
- **DOI**: 10.1109/TMI.2019.2953773
- **Categories**: **cs.CV**, eess.IV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1902.07521v2)
- **Published**: 2019-02-20 11:45:14+00:00
- **Updated**: 2019-11-22 09:28:19+00:00
- **Authors**: Bernhard Schmitzer, Klaus P. Schäfers, Benedikt Wirth
- **Comment**: Revised version, to appear in IEEE Trans Med Imaging. Supplementary
  material attached as last page
- **Journal**: None
- **Summary**: We propose a novel dynamic image reconstruction method from PET listmode data that could be particularly suited to tracking single or small numbers of cells. In contrast to conventional PET reconstruction our method combines the information from all detected events not only to reconstruct the dynamic evolution of the radionuclide distribution, but also to improve the reconstruction at each single time point by enforcing temporal consistency. This is achieved via optimal transport regularization where in principle, among all possible temporally evolving radionuclide distributions consistent with the PET measurement, the one is chosen with least kinetic motion energy. The reconstruction is found by convex optimization so that there is no dependence on the initialization of the method. We study its behaviour on simulated data of a human PET system and demonstrate its robustness even in settings with very low radioactivity. In contrast to previously reported cell tracking algorithms, our technique is oblivious to the number of tracked cells. Without any additional complexity one or multiple cells can be reconstructed, and the model automatically determines the number of particles. For instance, four radiolabelled cells moving at a velocity of 3.1 mm/s and a PET recorded count rate of 1.1 cps (for each cell) could be simultaneously tracked with a tracking accuracy of 5.3 mm inside a simulated human body.



### TomoGAN: Low-Dose Synchrotron X-Ray Tomography with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.07582v5
- **DOI**: 10.1364/JOSAA.375595
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07582v5)
- **Published**: 2019-02-20 14:55:37+00:00
- **Updated**: 2019-12-30 17:41:08+00:00
- **Authors**: Zhengchun Liu, Tekin Bicer, Rajkumar Kettimuthu, Doga Gursoy, Francesco De Carlo, Ian Foster
- **Comment**: None
- **Journal**: None
- **Summary**: Synchrotron-based x-ray tomography is a noninvasive imaging technique that allows for reconstructing the internal structure of materials at high spatial resolutions from tens of micrometers to a few nanometers. In order to resolve sample features at smaller length scales, however, a higher radiation dose is required. Therefore, the limitation on the achievable resolution is set primarily by noise at these length scales. We present \TOMOGAN{}, a denoising technique based on generative adversarial networks, for improving the quality of reconstructed images for low-dose imaging conditions. We evaluate our approach in two photon-budget-limited experimental conditions: (1) sufficient number of low-dose projections (based on Nyquist sampling), and (2) insufficient or limited number of high-dose projections. In both cases the angular sampling is assumed to be isotropic, and the photon budget throughout the experiment is fixed based on the maximum allowable radiation dose on the sample. Evaluation with both simulated and experimental datasets shows that our approach can significantly reduce noise in reconstructed images, improving the structural similarity score of simulation and experimental data from 0.18 to 0.9 and from 0.18 to 0.41, respectively. Furthermore, the quality of the reconstructed images with filtered back projection followed by our denoising approach exceeds that of reconstructions with the simultaneous iterative reconstruction technique, showing the computational superiority of our approach.



### Sparsity Constrained Distributed Unmixing of Hyperspectral Data
- **Arxiv ID**: http://arxiv.org/abs/1902.07593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07593v1)
- **Published**: 2019-02-20 15:16:06+00:00
- **Updated**: 2019-02-20 15:16:06+00:00
- **Authors**: Sara Khoshsokhan, Roozbeh Rajabi, Hadi Zayyani
- **Comment**: one column, 17 pages, 10 figures, journal
- **Journal**: None
- **Summary**: Spectral unmixing (SU) is a technique to characterize mixed pixels in hyperspectral images measured by remote sensors. Most of the spectral unmixing algorithms are developed using the linear mixing models. To estimate endmembers and fractional abundance matrices in a blind problem, nonnegative matrix factorization (NMF) and its developments are widely used in the SU problem. One of the constraints which was added to NMF is sparsity, that was regularized by Lq norm. In this paper, a new algorithm based on distributed optimization is suggested for spectral unmixing. In the proposed algorithm, a network including single-node clusters is employed. Each pixel in the hyperspectral images is considered as a node in this network. The sparsity constrained distributed unmixing is optimized with diffusion least mean p-power (LMP) strategy, and then the update equations for fractional abundance and signature matrices are obtained. Afterwards the proposed algorithm is analyzed for different values of LMP power and Lq norms. Simulation results based on defined performance metrics illustrate the advantage of the proposed algorithm in spectral unmixing of hyperspectral data compared with other methods.



### Point cloud denoising based on tensor Tucker decomposition
- **Arxiv ID**: http://arxiv.org/abs/1902.07602v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07602v2)
- **Published**: 2019-02-20 15:41:25+00:00
- **Updated**: 2019-05-16 07:12:42+00:00
- **Authors**: Jianze Li, Xiao-Ping Zhang, Tuan Tran
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: In this paper, we propose a new algorithm for point cloud denoising based on the tensor Tucker decomposition. We first represent the local surface patches of a noisy point cloud to be matrices by their distances to a reference point, and stack the similar patch matrices to be a 3rd order tensor. Then we use the Tucker decomposition to compress this patch tensor to be a core tensor of smaller size. We consider this core tensor as the frequency domain and remove the noise by manipulating the hard thresholding. Finally, all the fibers of the denoised patch tensor are placed back, and the average is taken if there are more than one estimators overlapped. The experimental evaluation shows that the proposed algorithm outperforms the state-of-the-art graph Laplacian regularized (GLR) algorithm when the Gaussian noise is high ($\sigma=0.1$), and the GLR algorithm is better in lower noise cases ($\sigma=0.04, 0.05, 0.08$).



### advertorch v0.1: An Adversarial Robustness Toolbox based on PyTorch
- **Arxiv ID**: http://arxiv.org/abs/1902.07623v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.07623v1)
- **Published**: 2019-02-20 16:18:37+00:00
- **Updated**: 2019-02-20 16:18:37+00:00
- **Authors**: Gavin Weiguang Ding, Luyu Wang, Xiaomeng Jin
- **Comment**: None
- **Journal**: None
- **Summary**: advertorch is a toolbox for adversarial robustness research. It contains various implementations for attacks, defenses and robust training methods. advertorch is built on PyTorch (Paszke et al., 2017), and leverages the advantages of the dynamic computational graph to provide concise and efficient reference implementations. The code is licensed under the LGPL license and is open sourced at https://github.com/BorealisAI/advertorch .



### Filtering Point Targets via Online Learning of Motion Models
- **Arxiv ID**: http://arxiv.org/abs/1902.07630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07630v1)
- **Published**: 2019-02-20 16:36:52+00:00
- **Updated**: 2019-02-20 16:36:52+00:00
- **Authors**: Mehryar Emambakhsh, Alessandro Bay, Eduard Vazquez
- **Comment**: arXiv admin note: text overlap with arXiv:1806.06594
- **Journal**: None
- **Summary**: Filtering point targets in highly cluttered and noisy data frames can be very challenging, especially for complex target motions. Fixed motion models can fail to provide accurate predictions, while learning based algorithm can be difficult to design (due to the variable number of targets), slow to train and dependent on separate train/test steps. To address these issues, this paper proposes a multi-target filtering algorithm which learns the motion models, on the fly, using a recurrent neural network with a long short-term memory architecture, as a regression block. The target state predictions are then corrected using a novel data association algorithm, with a low computational complexity. The proposed algorithm is evaluated over synthetic and real point target filtering scenarios, demonstrating a remarkable performance over highly cluttered data sequences.



### Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system
- **Arxiv ID**: http://arxiv.org/abs/1902.07651v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.07651v3)
- **Published**: 2019-02-20 17:06:00+00:00
- **Updated**: 2019-10-17 15:46:21+00:00
- **Authors**: Victor Boutin, Angelo Franciosini, Frederic Chavane, Franck Ruffier, Laurent Perrinet
- **Comment**: None
- **Journal**: None
- **Summary**: Both neurophysiological and psychophysical experiments have pointed out the crucial role of recurrent and feedback connections to process context-dependent information in the early visual cortex. While numerous models have accounted for feedback effects at either neural or representational level, none of them were able to bind those two levels of analysis. Is it possible to describe feedback effects at both levels using the same model? We answer this question by combining Predictive Coding (PC) and Sparse Coding (SC) into a hierarchical and convolutional framework. In this Sparse Deep Predictive Coding (SDPC) model, the SC component models the internal recurrent processing within each layer, and the PC component describes the interactions between layers using feedforward and feedback connections. Here, we train a 2-layered SDPC on two different databases of images, and we interpret it as a model of the early visual system (V1 & V2). We first demonstrate that once the training has converged, SDPC exhibits oriented and localized receptive fields in V1 and more complex features in V2. Second, we analyze the effects of feedback on the neural organization beyond the classical receptive field of V1 neurons using interaction maps. These maps are similar to association fields and reflect the Gestalt principle of good continuation. We demonstrate that feedback signals reorganize interaction maps and modulate neural activity to promote contour integration. Third, we demonstrate at the representational level that the SDPC feedback connections are able to overcome noise in input images. Therefore, the SDPC captures the association field principle at the neural level which results in better disambiguation of blurred images at the representational level.



### On the effect of age perception biases for real age regression
- **Arxiv ID**: http://arxiv.org/abs/1902.07653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07653v1)
- **Published**: 2019-02-20 17:07:44+00:00
- **Updated**: 2019-02-20 17:07:44+00:00
- **Authors**: Julio C. S. Jacques Junior, Cagri Ozcinar, Marina Marjanovic, Xavier Baró, Gholamreza Anbarjafari, Sergio Escalera
- **Comment**: Accepted in the 14th IEEE International Conference on Automatic Face
  and Gesture Recognition (FG 2019)
- **Journal**: None
- **Summary**: Automatic age estimation from facial images represents an important task in computer vision. This paper analyses the effect of gender, age, ethnic, makeup and expression attributes of faces as sources of bias to improve deep apparent age prediction. Following recent works where it is shown that apparent age labels benefit real age estimation, rather than direct real to real age regression, our main contribution is the integration, in an end-to-end architecture, of face attributes for apparent age prediction with an additional loss for real age regression. Experimental results on the APPA-REAL dataset indicate the proposed network successfully take advantage of the adopted attributes to improve both apparent and real age estimation. Our model outperformed a state-of-the-art architecture proposed to separately address apparent and real age regression. Finally, we present preliminary results and discussion of a proof of concept application using the proposed model to regress the apparent age of an individual based on the gender of an external observer.



### Knowledge-based Analysis for Mortality Prediction from CT Images
- **Arxiv ID**: http://arxiv.org/abs/1902.07687v2
- **DOI**: 10.1109/JBHI.2019.2946066
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07687v2)
- **Published**: 2019-02-20 18:15:57+00:00
- **Updated**: 2019-11-23 20:34:12+00:00
- **Authors**: Hengtao Guo, Uwe Kruger, Ge Wang, Mannudeep K. Kalra, Pingkun Yan
- **Comment**: Accepted for publication in IEEE Journal of Biomedical and Health
  Informatics (JBHI)
- **Journal**: IEEE Journal of Biomedical and Health Informatics, 2019
- **Summary**: Recent studies have highlighted the high correlation between cardiovascular diseases (CVD) and lung cancer, and both are associated with significant morbidity and mortality. Low-Dose CT (LCDT) scans have led to significant improvements in the accuracy of lung cancer diagnosis and thus the reduction of cancer deaths. However, the high correlation between lung cancer and CVD has not been well explored for mortality prediction. This paper introduces a knowledge-based analytical method using deep convolutional neural network (CNN) for all-cause mortality prediction. The underlying approach combines structural image features extracted from CNNs, based on LDCT volume in different scale, and clinical knowledge obtained from quantitative measurements, to comprehensively predict the mortality risk of lung cancer screening subjects. The introduced method is referred to here as the Knowledge-based Analysis of Mortality Prediction Network, or KAMP-Net. It constitutes a collaborative framework that utilizes both imaging features and anatomical information, instead of completely relying on automatic feature extraction. Our work demonstrates the feasibility of incorporating quantitative clinical measurements to assist CNNs in all-cause mortality prediction from chest LDCT images. The results of this study confirm that radiologist defined features are an important complement to CNNs to achieve a more comprehensive feature extraction. Thus, the proposed KAMP-Net has shown to achieve a superior performance when compared to other methods. Our code is available at https://github.com/DIAL-RPI/KAMP-Net.



### Adversarial Augmentation for Enhancing Classification of Mammography Images
- **Arxiv ID**: http://arxiv.org/abs/1902.07762v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.07762v1)
- **Published**: 2019-02-20 20:13:24+00:00
- **Updated**: 2019-02-20 20:13:24+00:00
- **Authors**: Lukas Jendele, Ondrej Skopek, Anton S. Becker, Ender Konukoglu
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised deep learning relies on the assumption that enough training data is available, which presents a problem for its application to several fields, like medical imaging. On the example of a binary image classification task (breast cancer recognition), we show that pretraining a generative model for meaningful image augmentation helps enhance the performance of the resulting classifier. By augmenting the data, performance on downstream classification tasks could be improved even with a relatively small training set. We show that this "adversarial augmentation" yields promising results compared to classical image augmentation on the example of breast cancer classification.



### Dense Depth Estimation in Monocular Endoscopy with Self-supervised Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/1902.07766v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.07766v2)
- **Published**: 2019-02-20 20:25:22+00:00
- **Updated**: 2019-10-30 02:28:19+00:00
- **Authors**: Xingtong Liu, Ayushi Sinha, Masaru Ishii, Gregory D. Hager, Austin Reiter, Russell H. Taylor, Mathias Unberath
- **Comment**: Accepted to IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: We present a self-supervised approach to training convolutional neural networks for dense depth estimation from monocular endoscopy data without a priori modeling of anatomy or shading. Our method only requires monocular endoscopic videos and a multi-view stereo method, e.g., structure from motion, to supervise learning in a sparse manner. Consequently, our method requires neither manual labeling nor patient computed tomography (CT) scan in the training and application phases. In a cross-patient experiment using CT scans as groundtruth, the proposed method achieved submillimeter mean residual error. In a comparison study to recent self-supervised depth estimation methods designed for natural video on in vivo sinus endoscopy data, we demonstrate that the proposed approach outperforms the previous methods by a large margin. The source code for this work is publicly available online at https://github.com/lppllppl920/EndoscopyDepthEstimation-Pytorch.



### Perceptual Quality-preserving Black-Box Attack against Deep Learning Image Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1902.07776v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07776v3)
- **Published**: 2019-02-20 20:57:40+00:00
- **Updated**: 2020-09-23 07:37:46+00:00
- **Authors**: Diego Gragnaniello, Francesco Marra, Giovanni Poggi, Luisa Verdoliva
- **Comment**: 8 pages, journal
- **Journal**: None
- **Summary**: Deep neural networks provide unprecedented performance in all image classification problems, taking advantage of huge amounts of data available for training. Recent studies, however, have shown their vulnerability to adversarial attacks, spawning an intense research effort in this field. With the aim of building better systems, new countermeasures and stronger attacks are proposed by the day. On the attacker's side, there is growing interest for the realistic black-box scenario, in which the user has no access to the neural network parameters. The problem is to design efficient attacks which mislead the neural network without compromising image quality. In this work, we propose to perform the black-box attack along a low-distortion path, so as to improve both the attack efficiency and the perceptual quality of the adversarial image. Numerical experiments on real-world systems prove the effectiveness of the proposed approach, both in benchmark classification tasks and in key applications in biometrics and forensics.



### Class-independent sequential full image segmentation, using a convolutional net that finds a segment within an attention region, given a pointer pixel within this segment
- **Arxiv ID**: http://arxiv.org/abs/1902.07810v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07810v2)
- **Published**: 2019-02-20 23:35:49+00:00
- **Updated**: 2019-02-23 12:02:27+00:00
- **Authors**: Sagi Eppel
- **Comment**: None
- **Journal**: None
- **Summary**: This work examines the use of a fully convolutional net (FCN) to find an image segment, given a pixel within this segment region. The net receives an image, a point in the image and a region of interest (RoI ) mask. The net output is a binary mask of the segment in which the point is located. The region where the segment can be found is contained within the input RoI mask. Full image segmentation can be achieved by running this net sequentially, region-by-region on the image, and stitching the output segments into a single segmentation map. This simple method addresses two major challenges of image segmentation: 1) Segmentation of unknown categories that were not included in the training set. 2) Segmentation of both individual object instances (things) and non-objects (stuff), such as sky and vegetation. Hence, if the pointer pixel is located within a person in a group, the net will output a mask that covers that individual person; if the pointer point is located within the sky region, the net returns the region of the sky in the image. This is true even if no example for sky or person appeared in the training set. The net was tested and trained on the COCO panoptic dataset and achieved 67% IOU for segmentation of familiar classes (that were part of the net training set) and 53% IOU for segmentation of unfamiliar classes (that were not included in the training).




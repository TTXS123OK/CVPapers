# Arxiv Papers in cs.CV on 2024-11-08
### Hierarchical Visual Feature Aggregation for OCR-Free Document Understanding
- **Arxiv ID**: http://arxiv.org/abs/2411.05254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05254v1)
- **Published**: 2024-11-08 00:58:12+00:00
- **Updated**: 2024-11-08 00:58:12+00:00
- **Authors**: Jaeyoo Park, Jin Young Choi, Jeonghyung Park, Bohyung Han
- **Comment**: NeurIPS 2024
- **Journal**: None
- **Summary**: We present a novel OCR-free document understanding framework based on pretrained Multimodal Large Language Models (MLLMs). Our approach employs multi-scale visual features to effectively handle various font sizes within document images. To address the increasing costs of considering the multi-scale visual inputs for MLLMs, we propose the Hierarchical Visual Feature Aggregation (HVFA) module, designed to reduce the number of input tokens to LLMs. Leveraging a feature pyramid with cross-attentive pooling, our approach effectively manages the trade-off between information loss and efficiency without being affected by varying document image sizes. Furthermore, we introduce a novel instruction tuning task, which facilitates the model's text-reading capability by learning to predict the relative positions of input text, eventually minimizing the risk of truncated text caused by the limited capacity of LLMs. Comprehensive experiments validate the effectiveness of our approach, demonstrating superior performance in various document understanding tasks.



### Decoding Report Generators: A Cyclic Vision-Language Adapter for Counterfactual Explanations
- **Arxiv ID**: http://arxiv.org/abs/2411.05261v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05261v1)
- **Published**: 2024-11-08 01:46:11+00:00
- **Updated**: 2024-11-08 01:46:11+00:00
- **Authors**: Yingying Fang, Zihao Jin, Shaojie Guo, Jinda Liu, Yijian Gao, Junzhi Ning, Zhiling Yue, Zhi Li, Simon LF Walsh, Guang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite significant advancements in report generation methods, a critical limitation remains: the lack of interpretability in the generated text. This paper introduces an innovative approach to enhance the explainability of text generated by report generation models. Our method employs cyclic text manipulation and visual comparison to identify and elucidate the features in the original content that influence the generated text. By manipulating the generated reports and producing corresponding images, we create a comparative framework that highlights key attributes and their impact on the text generation process. This approach not only identifies the image features aligned to the generated text but also improves transparency but also provides deeper insights into the decision-making mechanisms of the report generation models. Our findings demonstrate the potential of this method to significantly enhance the interpretability and transparency of AI-generated reports.



### Image Decomposition: Theory, Numerical Schemes, and Performance Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2411.05265v1
- **DOI**: 10.1016/s1076-5670(09)00008-1
- **Categories**: **eess.IV**, cs.CV, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/2411.05265v1)
- **Published**: 2024-11-08 01:47:53+00:00
- **Updated**: 2024-11-08 01:47:53+00:00
- **Authors**: Jerome Gilles
- **Comment**: None
- **Journal**: Advances in Imaging and Electron Physics, Chapter 3, Vol.158, 2009
- **Summary**: This paper describes the many image decomposition models that allow to separate structures and textures or structures, textures, and noise. These models combined a total variation approach with different adapted functional spaces such as Besov or Contourlet spaces or a special oscillating function space based on the work of Yves Meyer. We propose a method to evaluate the performance of such algorithms to enhance understanding of the behavior of these models.



### Cancer-Net SCa-Synth: An Open Access Synthetically Generated 2D Skin Lesion Dataset for Skin Cancer Classification
- **Arxiv ID**: http://arxiv.org/abs/2411.05269v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05269v1)
- **Published**: 2024-11-08 02:04:21+00:00
- **Updated**: 2024-11-08 02:04:21+00:00
- **Authors**: Chi-en Amy Tai, Oustan Ding, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: In the United States, skin cancer ranks as the most commonly diagnosed cancer, presenting a significant public health issue due to its high rates of occurrence and the risk of serious complications if not caught early. Recent advancements in dataset curation and deep learning have shown promise in quick and accurate detection of skin cancer. However, current open-source datasets have significant class imbalances which impedes the effectiveness of these deep learning models. In healthcare, generative artificial intelligence (AI) models have been employed to create synthetic data, addressing data imbalance in datasets by augmenting underrepresented classes and enhancing the overall quality and performance of machine learning models. In this paper, we build on top of previous work by leveraging new advancements in generative AI, notably Stable Diffusion and DreamBooth. We introduce Cancer-Net SCa-Synth, an open access synthetically generated 2D skin lesion dataset for skin cancer classification. Further analysis on the data effectiveness by comparing the ISIC 2020 test set performance for training with and without these synthetic images for a simple model highlights the benefits of leveraging synthetic data to improve performance. Cancer-Net SCa-Synth is publicly available at https://github.com/catai9/Cancer-Net-SCa-Synth as part of a global open-source initiative for accelerating machine learning for cancer care.



### SimpleBEV: Improved LiDAR-Camera Fusion Architecture for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2411.05292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2411.05292v1)
- **Published**: 2024-11-08 02:51:39+00:00
- **Updated**: 2024-11-08 02:51:39+00:00
- **Authors**: Yun Zhao, Zhan Gong, Peiru Zheng, Hong Zhu, Shaohua Wu
- **Comment**: None
- **Journal**: None
- **Summary**: More and more research works fuse the LiDAR and camera information to improve the 3D object detection of the autonomous driving system. Recently, a simple yet effective fusion framework has achieved an excellent detection performance, fusing the LiDAR and camera features in a unified bird's-eye-view (BEV) space. In this paper, we propose a LiDAR-camera fusion framework, named SimpleBEV, for accurate 3D object detection, which follows the BEV-based fusion framework and improves the camera and LiDAR encoders, respectively. Specifically, we perform the camera-based depth estimation using a cascade network and rectify the depth results with the depth information derived from the LiDAR points. Meanwhile, an auxiliary branch that implements the 3D object detection using only the camera-BEV features is introduced to exploit the camera information during the training phase. Besides, we improve the LiDAR feature extractor by fusing the multi-scaled sparse convolutional features. Experimental results demonstrate the effectiveness of our proposed method. Our method achieves 77.6\% NDS accuracy on the nuScenes dataset, showcasing superior performance in the 3D object detection track.



### Adaptive Whole-Body PET Image Denoising Using 3D Diffusion Models with ControlNet
- **Arxiv ID**: http://arxiv.org/abs/2411.05302v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2411.05302v1)
- **Published**: 2024-11-08 03:06:47+00:00
- **Updated**: 2024-11-08 03:06:47+00:00
- **Authors**: Boxiao Yu, Kuang Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Positron Emission Tomography (PET) is a vital imaging modality widely used in clinical diagnosis and preclinical research but faces limitations in image resolution and signal-to-noise ratio due to inherent physical degradation factors. Current deep learning-based denoising methods face challenges in adapting to the variability of clinical settings, influenced by factors such as scanner types, tracer choices, dose levels, and acquisition times. In this work, we proposed a novel 3D ControlNet-based denoising method for whole-body PET imaging. We first pre-trained a 3D Denoising Diffusion Probabilistic Model (DDPM) using a large dataset of high-quality normal-dose PET images. Following this, we fine-tuned the model on a smaller set of paired low- and normal-dose PET images, integrating low-dose inputs through a 3D ControlNet architecture, thereby making the model adaptable to denoising tasks in diverse clinical settings. Experimental results based on clinical PET datasets show that the proposed framework outperformed other state-of-the-art PET image denoising methods both in visual quality and quantitative metrics. This plug-and-play approach allows large diffusion models to be fine-tuned and adapted to PET images from diverse acquisition protocols.



### Revisiting Network Perturbation for Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2411.05307v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2411.05307v1)
- **Published**: 2024-11-08 03:23:39+00:00
- **Updated**: 2024-11-08 03:23:39+00:00
- **Authors**: Sien Li, Tao Wang, Ruizhe Hu, Wenxi Liu
- **Comment**: Accepted by PRCV2024
- **Journal**: None
- **Summary**: In semi-supervised semantic segmentation (SSS), weak-to-strong consistency regularization techniques are widely utilized in recent works, typically combined with input-level and feature-level perturbations. However, the integration between weak-to-strong consistency regularization and network perturbation has been relatively rare. We note several problems with existing network perturbations in SSS that may contribute to this phenomenon. By revisiting network perturbations, we introduce a new approach for network perturbation to expand the existing weak-to-strong consistency regularization for unlabeled data. Additionally, we present a volatile learning process for labeled data, which is uncommon in existing research. Building upon previous work that includes input-level and feature-level perturbations, we present MLPMatch (Multi-Level-Perturbation Match), an easy-to-implement and efficient framework for semi-supervised semantic segmentation. MLPMatch has been validated on the Pascal VOC and Cityscapes datasets, achieving state-of-the-art performance. Code is available from https://github.com/LlistenL/MLPMatch.



### ZOPP: A Framework of Zero-shot Offboard Panoptic Perception for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2411.05311v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2411.05311v1)
- **Published**: 2024-11-08 03:52:32+00:00
- **Updated**: 2024-11-08 03:52:32+00:00
- **Authors**: Tao Ma, Hongbin Zhou, Qiusheng Huang, Xuemeng Yang, Jianfei Guo, Bo Zhang, Min Dou, Yu Qiao, Botian Shi, Hongsheng Li
- **Comment**: Accepted by NeurIPS 2024
- **Journal**: None
- **Summary**: Offboard perception aims to automatically generate high-quality 3D labels for autonomous driving (AD) scenes. Existing offboard methods focus on 3D object detection with closed-set taxonomy and fail to match human-level recognition capability on the rapidly evolving perception tasks. Due to heavy reliance on human labels and the prevalence of data imbalance and sparsity, a unified framework for offboard auto-labeling various elements in AD scenes that meets the distinct needs of perception tasks is not being fully explored. In this paper, we propose a novel multi-modal Zero-shot Offboard Panoptic Perception (ZOPP) framework for autonomous driving scenes. ZOPP integrates the powerful zero-shot recognition capabilities of vision foundation models and 3D representations derived from point clouds. To the best of our knowledge, ZOPP represents a pioneering effort in the domain of multi-modal panoptic perception and auto labeling for autonomous driving scenes. We conduct comprehensive empirical studies and evaluations on Waymo open dataset to validate the proposed ZOPP on various perception tasks. To further explore the usability and extensibility of our proposed ZOPP, we also conduct experiments in downstream applications. The results further demonstrate the great potential of our ZOPP for real-world scenarios.



### A Real-time Face Mask Detection and Social Distancing System for COVID-19 using Attention-InceptionV3 Model
- **Arxiv ID**: http://arxiv.org/abs/2411.05312v1
- **DOI**: 10.38032/jea.2022.01.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05312v1)
- **Published**: 2024-11-08 04:00:05+00:00
- **Updated**: 2024-11-08 04:00:05+00:00
- **Authors**: Abdullah Al Asif, Farhana Chowdhury Tisha
- **Comment**: None
- **Journal**: Journal of Engineering Advancements Vol. 03(01) 2022, pp 001-005
- **Summary**: One of the deadliest pandemics is now happening in the current world due to COVID-19. This contagious virus is spreading like wildfire around the whole world. To minimize the spreading of this virus, World Health Organization (WHO) has made protocols mandatory for wearing face masks and maintaining 6 feet physical distance. In this paper, we have developed a system that can detect the proper maintenance of that distance and people are properly using masks or not. We have used the customized attention-inceptionv3 model in this system for the identification of those two components. We have used two different datasets along with 10,800 images including both with and without Face Mask images. The training accuracy has been achieved 98% and validation accuracy 99.5%. The system can conduct a precision value of around 98.2% and the frame rate per second (FPS) was 25.0. So, with this system, we can identify high-risk areas with the highest possibility of the virus spreading zone. This may help authorities to take necessary steps to locate those risky areas and alert the local people to ensure proper precautions in no time.



### Rate-aware Compression for NeRF-based Volumetric Video
- **Arxiv ID**: http://arxiv.org/abs/2411.05322v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2411.05322v1)
- **Published**: 2024-11-08 04:29:14+00:00
- **Updated**: 2024-11-08 04:29:14+00:00
- **Authors**: Zhiyu Zhang, Guo Lu, Huanxiong Liang, Zhengxue Cheng, Anni Tang, Li Song
- **Comment**: Accepted by ACM MM 2024 (Oral)
- **Journal**: None
- **Summary**: The neural radiance fields (NeRF) have advanced the development of 3D volumetric video technology, but the large data volumes they involve pose significant challenges for storage and transmission. To address these problems, the existing solutions typically compress these NeRF representations after the training stage, leading to a separation between representation training and compression. In this paper, we try to directly learn a compact NeRF representation for volumetric video in the training stage based on the proposed rate-aware compression framework. Specifically, for volumetric video, we use a simple yet effective modeling strategy to reduce temporal redundancy for the NeRF representation. Then, during the training phase, an implicit entropy model is utilized to estimate the bitrate of the NeRF representation. This entropy model is then encoded into the bitstream to assist in the decoding of the NeRF representation. This approach enables precise bitrate estimation, thereby leading to a compact NeRF representation. Furthermore, we propose an adaptive quantization strategy and learn the optimal quantization step for the NeRF representations. Finally, the NeRF representation can be optimized by using the rate-distortion trade-off. Our proposed compression framework can be used for different representations and experimental results demonstrate that our approach significantly reduces the storage size with marginal distortion and achieves state-of-the-art rate-distortion performance for volumetric video on the HumanRF and ReRF datasets. Compared to the previous state-of-the-art method TeTriRF, we achieved an approximately -80% BD-rate on the HumanRF dataset and -60% BD-rate on the ReRF dataset.



### SASWISE-UE: Segmentation and Synthesis with Interpretable Scalable Ensembles for Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2411.05324v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ME, 62P10, 68T01, 68T37, 92C50
- **Links**: [PDF](http://arxiv.org/pdf/2411.05324v1)
- **Published**: 2024-11-08 04:37:55+00:00
- **Updated**: 2024-11-08 04:37:55+00:00
- **Authors**: Weijie Chen, Alan McMillan
- **Comment**: 16 pages, 12 figures, 5 tables
- **Journal**: None
- **Summary**: This paper introduces an efficient sub-model ensemble framework aimed at enhancing the interpretability of medical deep learning models, thus increasing their clinical applicability. By generating uncertainty maps, this framework enables end-users to evaluate the reliability of model outputs. We developed a strategy to develop diverse models from a single well-trained checkpoint, facilitating the training of a model family. This involves producing multiple outputs from a single input, fusing them into a final output, and estimating uncertainty based on output disagreements. Implemented using U-Net and UNETR models for segmentation and synthesis tasks, this approach was tested on CT body segmentation and MR-CT synthesis datasets. It achieved a mean Dice coefficient of 0.814 in segmentation and a Mean Absolute Error of 88.17 HU in synthesis, improved from 89.43 HU by pruning. Additionally, the framework was evaluated under corruption and undersampling, maintaining correlation between uncertainty and error, which highlights its robustness. These results suggest that the proposed approach not only maintains the performance of well-trained models but also enhances interpretability through effective uncertainty estimation, applicable to both convolutional and transformer models in a range of imaging tasks.



### A Quality-Centric Framework for Generic Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2411.05335v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05335v1)
- **Published**: 2024-11-08 05:14:46+00:00
- **Updated**: 2024-11-08 05:14:46+00:00
- **Authors**: Wentang Song, Zhiyuan Yan, Yuzhen Lin, Taiping Yao, Changsheng Chen, Shen Chen, Yandan Zhao, Shouhong Ding, Bin Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the generalization issue in deepfake detection by harnessing forgery quality in training data. Generally, the forgery quality of different deepfakes varies: some have easily recognizable forgery clues, while others are highly realistic. Existing works often train detectors on a mix of deepfakes with varying forgery qualities, potentially leading detectors to short-cut the easy-to-spot artifacts from low-quality forgery samples, thereby hurting generalization performance. To tackle this issue, we propose a novel quality-centric framework for generic deepfake detection, which is composed of a Quality Evaluator, a low-quality data enhancement module, and a learning pacing strategy that explicitly incorporates forgery quality into the training process. The framework is inspired by curriculum learning, which is designed to gradually enable the detector to learn more challenging deepfake samples, starting with easier samples and progressing to more realistic ones. We employ both static and dynamic assessments to assess the forgery quality, combining their scores to produce a final rating for each training sample. The rating score guides the selection of deepfake samples for training, with higher-rated samples having a higher probability of being chosen. Furthermore, we propose a novel frequency data augmentation method specifically designed for low-quality forgery samples, which helps to reduce obvious forgery traces and improve their overall realism. Extensive experiments show that our method can be applied in a plug-and-play manner and significantly enhance the generalization performance.



### Enhancing Visual Classification using Comparative Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2411.05357v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05357v2)
- **Published**: 2024-11-08 06:28:02+00:00
- **Updated**: 2024-11-11 02:24:36+00:00
- **Authors**: Hankyeol Lee, Gawon Seo, Wonseok Choi, Geunyoung Jung, Kyungwoo Song, Jiyoung Jung
- **Comment**: Accepted by WACV 2025
- **Journal**: None
- **Summary**: The performance of vision-language models (VLMs), such as CLIP, in visual classification tasks, has been enhanced by leveraging semantic knowledge from large language models (LLMs), including GPT. Recent studies have shown that in zero-shot classification tasks, descriptors incorporating additional cues, high-level concepts, or even random characters often outperform those using only the category name. In many classification tasks, while the top-1 accuracy may be relatively low, the top-5 accuracy is often significantly higher. This gap implies that most misclassifications occur among a few similar classes, highlighting the model's difficulty in distinguishing between classes with subtle differences. To address this challenge, we introduce a novel concept of comparative descriptors. These descriptors emphasize the unique features of a target class against its most similar classes, enhancing differentiation. By generating and integrating these comparative descriptors into the classification framework, we refine the semantic focus and improve classification accuracy. An additional filtering process ensures that these descriptors are closer to the image embeddings in the CLIP space, further enhancing performance. Our approach demonstrates improved accuracy and robustness in visual classification tasks by addressing the specific challenge of subtle inter-class differences.



### Agricultural Landscape Understanding At Country-Scale
- **Arxiv ID**: http://arxiv.org/abs/2411.05359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2411.05359v1)
- **Published**: 2024-11-08 06:29:02+00:00
- **Updated**: 2024-11-08 06:29:02+00:00
- **Authors**: Radhika Dua, Nikita Saxena, Aditi Agarwal, Alex Wilson, Gaurav Singh, Hoang Tran, Ishan Deshpande, Amandeep Kaur, Gaurav Aggarwal, Chandan Nath, Arnab Basu, Vishal Batchu, Sharath Holla, Bindiya Kurle, Olana Missura, Rahul Aggarwal, Shubhika Garg, Nishi Shah, Avneet Singh, Dinesh Tewari, Agata Dondzik, Bharat Adsul, Milind Sohoni, Asim Rama Praveen, Aaryan Dangi, Lisan Kadivar, E Abhishek, Niranjan Sudhansu, Kamlakar Hattekar, Sameer Datar, Musty Krishna Chaithanya, Anumas Ranjith Reddy, Aashish Kumar, Betala Laxmi Tirumala, Alok Talekar
- **Comment**: 34 pages, 7 tables, 15 figs
- **Journal**: None
- **Summary**: Agricultural landscapes are quite complex, especially in the Global South where fields are smaller, and agricultural practices are more varied. In this paper we report on our progress in digitizing the agricultural landscape (natural and man-made) in our study region of India. We use high resolution imagery and a UNet style segmentation model to generate the first of its kind national-scale multi-class panoptic segmentation output. Through this work we have been able to identify individual fields across 151.7M hectares, and delineating key features such as water resources and vegetation. We share how this output was validated by our team and externally by downstream users, including some sample use cases that can lead to targeted data driven decision making. We believe this dataset will contribute towards digitizing agriculture by generating the foundational baselayer.



### From Transparent to Opaque: Rethinking Neural Implicit Surfaces with $α$-NeuS
- **Arxiv ID**: http://arxiv.org/abs/2411.05362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05362v1)
- **Published**: 2024-11-08 06:36:31+00:00
- **Updated**: 2024-11-08 06:36:31+00:00
- **Authors**: Haoran Zhang, Junkai Deng, Xuhui Chen, Fei Hou, Wencheng Wang, Hong Qin, Chen Qian, Ying He
- **Comment**: None
- **Journal**: NeurIPS 2024
- **Summary**: Traditional 3D shape reconstruction techniques from multi-view images, such as structure from motion and multi-view stereo, primarily focus on opaque surfaces. Similarly, recent advances in neural radiance fields and its variants also primarily address opaque objects, encountering difficulties with the complex lighting effects caused by transparent materials. This paper introduces $\alpha$-NeuS, a new method for simultaneously reconstructing thin transparent objects and opaque objects based on neural implicit surfaces (NeuS). Our method leverages the observation that transparent surfaces induce local extreme values in the learned distance fields during neural volumetric rendering, contrasting with opaque surfaces that align with zero level sets. Traditional iso-surfacing algorithms such as marching cubes, which rely on fixed iso-values, are ill-suited for this data. We address this by taking the absolute value of the distance field and developing an optimization method that extracts level sets corresponding to both non-negative local minima and zero iso-values. We prove that the reconstructed surfaces are unbiased for both transparent and opaque objects. To validate our approach, we construct a benchmark that includes both real-world and synthetic scenes, demonstrating its practical utility and effectiveness. Our data and code are publicly available at https://github.com/728388808/alpha-NeuS.



### Advancing Meteorological Forecasting: AI-based Approach to Synoptic Weather Map Analysis
- **Arxiv ID**: http://arxiv.org/abs/2411.05384v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2411.05384v1)
- **Published**: 2024-11-08 07:46:50+00:00
- **Updated**: 2024-11-08 07:46:50+00:00
- **Authors**: Yo-Hwan Choi, Seon-Yu Kang, Minjong Cheon
- **Comment**: None
- **Journal**: None
- **Summary**: As global warming increases the complexity of weather patterns; the precision of weather forecasting becomes increasingly important. Our study proposes a novel preprocessing method and convolutional autoencoder model developed to improve the interpretation of synoptic weather maps. These are critical for meteorologists seeking a thorough understanding of weather conditions. This model could recognize historical synoptic weather maps that nearly match current atmospheric conditions, marking a significant step forward in modern technology in meteorological forecasting. This comprises unsupervised learning models like VQ-VQE, as well as supervised learning models like VGG16, VGG19, Xception, InceptionV3, and ResNet50 trained on the ImageNet dataset, as well as research into newer models like EfficientNet and ConvNeXt. Our findings proved that, while these models perform well in various settings, their ability to identify comparable synoptic weather maps has certain limits. Our research, motivated by the primary goal of significantly increasing meteorologists' efficiency in labor-intensive tasks, discovered that cosine similarity is the most effective metric, as determined by a combination of quantitative and qualitative assessments to accurately identify relevant historical weather patterns. This study broadens our understanding by shifting the emphasis from numerical precision to practical application, ensuring that our model is effective in theory practical, and accessible in the complex and dynamic field of meteorology.



### AuthFormer: Adaptive Multimodal biometric authentication transformer for middle-aged and elderly people
- **Arxiv ID**: http://arxiv.org/abs/2411.05395v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05395v1)
- **Published**: 2024-11-08 08:21:08+00:00
- **Updated**: 2024-11-08 08:21:08+00:00
- **Authors**: Yang rui, Meng ling-tao, Zhang qiu-yu
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal biometric authentication methods address the limitations of unimodal biometric technologies in security, robustness, and user adaptability. However, most existing methods depend on fixed combinations and numbers of biometric modalities, which restricts flexibility and adaptability in real-world applications. To overcome these challenges, we propose an adaptive multimodal biometric authentication model, AuthFormer, tailored for elderly users. AuthFormer is trained on the LUTBIO multimodal biometric database, containing biometric data from elderly individuals. By incorporating a cross-attention mechanism and a Gated Residual Network (GRN), the model improves adaptability to physiological variations in elderly users. Experiments show that AuthFormer achieves an accuracy of 99.73%. Additionally, its encoder requires only two layers to perform optimally, reducing complexity compared to traditional Transformer-based models.



### POC-SLT: Partial Object Completion with SDF Latent Transformers
- **Arxiv ID**: http://arxiv.org/abs/2411.05419v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2411.05419v1)
- **Published**: 2024-11-08 09:13:20+00:00
- **Updated**: 2024-11-08 09:13:20+00:00
- **Authors**: Faezeh Zakeri, Raphael Braun, Lukas Ruppert, Henrik P. A. Lensch
- **Comment**: None
- **Journal**: None
- **Summary**: 3D geometric shape completion hinges on representation learning and a deep understanding of geometric data. Without profound insights into the three-dimensional nature of the data, this task remains unattainable. Our work addresses this challenge of 3D shape completion given partial observations by proposing a transformer operating on the latent space representing Signed Distance Fields (SDFs). Instead of a monolithic volume, the SDF of an object is partitioned into smaller high-resolution patches leading to a sequence of latent codes. The approach relies on a smooth latent space encoding learned via a variational autoencoder (VAE), trained on millions of 3D patches. We employ an efficient masked autoencoder transformer to complete partial sequences into comprehensive shapes in latent space. Our approach is extensively evaluated on partial observations from ShapeNet and the ABC dataset where only fractions of the objects are given. The proposed POC-SLT architecture compares favorably with several baseline state-of-the-art methods, demonstrating a significant improvement in 3D shape completion, both qualitatively and quantitatively.



### WeatherGFM: Learning A Weather Generalist Foundation Model via In-context Learning
- **Arxiv ID**: http://arxiv.org/abs/2411.05420v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2411.05420v1)
- **Published**: 2024-11-08 09:14:19+00:00
- **Updated**: 2024-11-08 09:14:19+00:00
- **Authors**: Xiangyu Zhao, Zhiwang Zhou, Wenlong Zhang, Yihao Liu, Xiangyu Chen, Junchao Gong, Hao Chen, Ben Fei, Shiqi Chen, Wanli Ouyang, Xiao-Ming Wu, Lei Bai
- **Comment**: None
- **Journal**: None
- **Summary**: The Earth's weather system encompasses intricate weather data modalities and diverse weather understanding tasks, which hold significant value to human life. Existing data-driven models focus on single weather understanding tasks (e.g., weather forecasting). Although these models have achieved promising results, they fail to tackle various complex tasks within a single and unified model. Moreover, the paradigm that relies on limited real observations for a single scenario hinders the model's performance upper bound. In response to these limitations, we draw inspiration from the in-context learning paradigm employed in state-of-the-art visual foundation models and large language models. In this paper, we introduce the first generalist weather foundation model (WeatherGFM), designed to address a wide spectrum of weather understanding tasks in a unified manner. More specifically, we initially unify the representation and definition of the diverse weather understanding tasks. Subsequently, we devised weather prompt formats to manage different weather data modalities, namely single, multiple, and temporal modalities. Finally, we adopt a visual prompting question-answering paradigm for the training of unified weather understanding tasks. Extensive experiments indicate that our WeatherGFM can effectively handle up to ten weather understanding tasks, including weather forecasting, super-resolution, weather image translation, and post-processing. Our method also showcases generalization ability on unseen tasks.



### VISTA: Visual Integrated System for Tailored Automation in Math Problem Generation Using LLM
- **Arxiv ID**: http://arxiv.org/abs/2411.05423v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2411.05423v1)
- **Published**: 2024-11-08 09:15:56+00:00
- **Updated**: 2024-11-08 09:15:56+00:00
- **Authors**: Jeongwoo Lee, Kwangsuk Park, Jihyeon Park
- **Comment**: Accepted at NeurIPS 2024 Workshop on Large Foundation Models for
  Educational Assessment (FM-Assess)
- **Journal**: None
- **Summary**: Generating accurate and consistent visual aids is a critical challenge in mathematics education, where visual representations like geometric shapes and functions play a pivotal role in enhancing student comprehension. This paper introduces a novel multi-agent framework that leverages Large Language Models (LLMs) to automate the creation of complex mathematical visualizations alongside coherent problem text. Our approach not only simplifies the generation of precise visual aids but also aligns these aids with the problem's core mathematical concepts, improving both problem creation and assessment. By integrating multiple agents, each responsible for distinct tasks such as numeric calculation, geometry validation, and visualization, our system delivers mathematically accurate and contextually relevant problems with visual aids. Evaluation across Geometry and Function problem types shows that our method significantly outperforms basic LLMs in terms of text coherence, consistency, relevance and similarity, while maintaining the essential geometrical and functional integrity of the original problems. Although some challenges remain in ensuring consistent visual outputs, our framework demonstrates the immense potential of LLMs in transforming the way educators generate and utilize visual aids in math education.



### Comparative Study of Probabilistic Atlas and Deep Learning Approaches for Automatic Brain Tissue Segmentation from MRI Using N4 Bias Field Correction and Anisotropic Diffusion Pre-processing Techniques
- **Arxiv ID**: http://arxiv.org/abs/2411.05456v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05456v1)
- **Published**: 2024-11-08 10:07:03+00:00
- **Updated**: 2024-11-08 10:07:03+00:00
- **Authors**: Mohammad Imran Hossain, Muhammad Zain Amin, Daniel Tweneboah Anyimadu, Taofik Ahmed Suleiman
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic brain tissue segmentation from Magnetic Resonance Imaging (MRI) images is vital for accurate diagnosis and further analysis in medical imaging. Despite advancements in segmentation techniques, a comprehensive comparison between traditional statistical methods and modern deep learning approaches using pre-processing techniques like N4 Bias Field Correction and Anisotropic Diffusion remains underexplored. This study provides a comparative analysis of various segmentation models, including Probabilistic ATLAS, U-Net, nnU-Net, and LinkNet, enhanced with these pre-processing techniques to segment brain tissues (white matter (WM), grey matter (GM) and cerebrospinal fluid (CSF)) on the Internet Brain Segmentation Repository (IBSR18) dataset. Our results demonstrate that the 3D nnU-Net model outperforms others, achieving the highest mean Dice Coefficient score (0.937 +- 0.012), while the 2D nnU-Net model recorded the lowest mean Hausdorff Distance (5.005 +- 0.343 mm) and the lowest mean Absolute Volumetric Difference (3.695 +- 2.931 mm) across five unseen test samples. The findings highlight the superiority of nnU-Net models in brain tissue segmentation, particularly when combined with N4 Bias Field Correction and Anisotropic Diffusion pre-processing techniques. Our implemented code can be accessed via GitHub.



### Improving image synthesis with diffusion-negative sampling
- **Arxiv ID**: http://arxiv.org/abs/2411.05473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05473v1)
- **Published**: 2024-11-08 10:58:09+00:00
- **Updated**: 2024-11-08 10:58:09+00:00
- **Authors**: Alakh Desai, Nuno Vasconcelos
- **Comment**: None
- **Journal**: None
- **Summary**: For image generation with diffusion models (DMs), a negative prompt n can be used to complement the text prompt p, helping define properties not desired in the synthesized image. While this improves prompt adherence and image quality, finding good negative prompts is challenging. We argue that this is due to a semantic gap between humans and DMs, which makes good negative prompts for DMs appear unintuitive to humans. To bridge this gap, we propose a new diffusion-negative prompting (DNP) strategy. DNP is based on a new procedure to sample images that are least compliant with p under the distribution of the DM, denoted as diffusion-negative sampling (DNS). Given p, one such image is sampled, which is then translated into natural language by the user or a captioning model, to produce the negative prompt n*. The pair (p, n*) is finally used to prompt the DM. DNS is straightforward to implement and requires no training. Experiments and human evaluations show that DNP performs well both quantitatively and qualitatively and can be easily combined with several DM variants.



### Do Histopathological Foundation Models Eliminate Batch Effects? A Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/2411.05489v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2411.05489v1)
- **Published**: 2024-11-08 11:39:03+00:00
- **Updated**: 2024-11-08 11:39:03+00:00
- **Authors**: Jonah Kömen, Hannah Marienwald, Jonas Dippel, Julius Hense
- **Comment**: Accepted to AIM-FM Workshop @ NeurIPS'24
- **Journal**: None
- **Summary**: Deep learning has led to remarkable advancements in computational histopathology, e.g., in diagnostics, biomarker prediction, and outcome prognosis. Yet, the lack of annotated data and the impact of batch effects, e.g., systematic technical data differences across hospitals, hamper model robustness and generalization. Recent histopathological foundation models -- pretrained on millions to billions of images -- have been reported to improve generalization performances on various downstream tasks. However, it has not been systematically assessed whether they fully eliminate batch effects. In this study, we empirically show that the feature embeddings of the foundation models still contain distinct hospital signatures that can lead to biased predictions and misclassifications. We further find that the signatures are not removed by stain normalization methods, dominate distances in feature space, and are evident across various principal components. Our work provides a novel perspective on the evaluation of medical foundation models, paving the way for more robust pretraining strategies and downstream predictors.



### Tightly-Coupled, Speed-aided Monocular Visual-Inertial Localization in Topological Map
- **Arxiv ID**: http://arxiv.org/abs/2411.05497v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2411.05497v1)
- **Published**: 2024-11-08 11:55:27+00:00
- **Updated**: 2024-11-08 11:55:27+00:00
- **Authors**: Chanuk Yang, Hayeon O, Kunsoo Huh
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel algorithm for vehicle speed-aided monocular visual-inertial localization using a topological map. The proposed system aims to address the limitations of existing methods that rely heavily on expensive sensors like GPS and LiDAR by leveraging relatively inexpensive camera-based pose estimation. The topological map is generated offline from LiDAR point clouds and includes depth images, intensity images, and corresponding camera poses. This map is then used for real-time localization through correspondence matching between current camera images and the stored topological images. The system employs an Iterated Error State Kalman Filter (IESKF) for optimized pose estimation, incorporating correspondence among images and vehicle speed measurements to enhance accuracy. Experimental results using both open dataset and our collected data in challenging scenario, such as tunnel, demonstrate the proposed algorithm's superior performance in topological map generation and localization tasks.



### FGGP: Fixed-Rate Gradient-First Gradual Pruning
- **Arxiv ID**: http://arxiv.org/abs/2411.05500v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05500v1)
- **Published**: 2024-11-08 12:02:25+00:00
- **Updated**: 2024-11-08 12:02:25+00:00
- **Authors**: Lingkai Zhu, Can Deniz Bezek, Orcun Goksel
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the increasing size of deep learning models and their growing demand for computational resources have drawn significant attention to the practice of pruning neural networks, while aiming to preserve their accuracy. In unstructured gradual pruning, which sparsifies a network by gradually removing individual network parameters until a targeted network sparsity is reached, recent works show that both gradient and weight magnitudes should be considered. In this work, we show that such mechanism, e.g., the order of prioritization and selection criteria, is essential. We introduce a gradient-first magnitude-next strategy for choosing the parameters to prune, and show that a fixed-rate subselection criterion between these steps works better, in contrast to the annealing approach in the literature. We validate this on CIFAR-10 dataset, with multiple randomized initializations on both VGG-19 and ResNet-50 network backbones, for pruning targets of 90, 95, and 98% sparsity and for both initially dense and 50% sparse networks. Our proposed fixed-rate gradient-first gradual pruning (FGGP) approach outperforms its state-of-the-art alternatives in most of the above experimental settings, even occasionally surpassing the upperbound of corresponding dense network results, and having the highest ranking across the considered experimental settings.



### Towards Scalable Foundation Models for Digital Dermatology
- **Arxiv ID**: http://arxiv.org/abs/2411.05514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2411.05514v1)
- **Published**: 2024-11-08 12:19:20+00:00
- **Updated**: 2024-11-08 12:19:20+00:00
- **Authors**: Fabian Gröger, Philippe Gottfrois, Ludovic Amruthalingam, Alvaro Gonzalez-Jimenez, Simone Lionetti, Luis R. Soenksen-Martinez, Alexander A. Navarini, Marc Pouly
- **Comment**: Findings paper presented at Machine Learning for Health (ML4H)
  symposium 2024, December 15-16, 2024, Vancouver, Canada, 11 pages
- **Journal**: None
- **Summary**: The growing demand for accurate and equitable AI models in digital dermatology faces a significant challenge: the lack of diverse, high-quality labeled data. In this work, we investigate the potential of domain-specific foundation models for dermatology in addressing this challenge. We utilize self-supervised learning (SSL) techniques to pre-train models on a dataset of over 240,000 dermatological images from public and private collections. Our study considers several SSL methods and compares the resulting foundation models against domain-agnostic models like those pre-trained on ImageNet and state-of-the-art models such as MONET across 12 downstream tasks. Unlike previous research, we emphasize the development of smaller models that are more suitable for resource-limited clinical settings, facilitating easier adaptation to a broad range of use cases. Results show that models pre-trained in this work not only outperform general-purpose models but also approach the performance of models 50 times larger on clinically relevant diagnostic tasks. To promote further research in this direction, we publicly release both the training code and the foundation models, which can benefit clinicians in dermatological applications.



### Alignment of 3D woodblock geometrical models and 2D orthographic projection image
- **Arxiv ID**: http://arxiv.org/abs/2411.05524v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2411.05524v1)
- **Published**: 2024-11-08 12:30:41+00:00
- **Updated**: 2024-11-08 12:30:41+00:00
- **Authors**: Minh DUc Nguyen, Cong Thuong Le, Trong Lam Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: The accurate alignment of 3D woodblock geometrical models with 2D orthographic projection images presents a significant challenge in the digital preservation of Vietnamese cultural heritage. This paper proposes a unified image processing algorithm to address this issue, enhancing the registration quality between 3D woodblock models and their 2D representations. The method includes determining the plane of the 3D character model, establishing a transformation matrix to align this plane with the 2D printed image plane, and creating a parallel-projected depth map for precise alignment. This process minimizes disocclusions and ensures that character shapes and strokes are correctly positioned. Experimental results highlight the importance of structure-based comparisons to optimize alignment for large-scale Han-Nom character datasets. The proposed approach, combining density-based and structure-based methods, demonstrates improved registration performance, offering an effective normalization scheme for digital heritage preservation.



### Towards Lifelong Few-Shot Customization of Text-to-Image Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2411.05544v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05544v1)
- **Published**: 2024-11-08 12:58:48+00:00
- **Updated**: 2024-11-08 12:58:48+00:00
- **Authors**: Nan Song, Xiaofeng Yang, Ze Yang, Guosheng Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Lifelong few-shot customization for text-to-image diffusion aims to continually generalize existing models for new tasks with minimal data while preserving old knowledge. Current customization diffusion models excel in few-shot tasks but struggle with catastrophic forgetting problems in lifelong generations. In this study, we identify and categorize the catastrophic forgetting problems into two folds: relevant concepts forgetting and previous concepts forgetting. To address these challenges, we first devise a data-free knowledge distillation strategy to tackle relevant concepts forgetting. Unlike existing methods that rely on additional real data or offline replay of original concept data, our approach enables on-the-fly knowledge distillation to retain the previous concepts while learning new ones, without accessing any previous data. Second, we develop an In-Context Generation (ICGen) paradigm that allows the diffusion model to be conditioned upon the input vision context, which facilitates the few-shot generation and mitigates the issue of previous concepts forgetting. Extensive experiments show that the proposed Lifelong Few-Shot Diffusion (LFS-Diffusion) method can produce high-quality and accurate images while maintaining previously learned knowledge.



### DeepArUco++: Improved detection of square fiducial markers in challenging lighting conditions
- **Arxiv ID**: http://arxiv.org/abs/2411.05552v1
- **DOI**: 10.1016/j.imavis.2024.105313
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05552v1)
- **Published**: 2024-11-08 13:18:31+00:00
- **Updated**: 2024-11-08 13:18:31+00:00
- **Authors**: Rafael Berral-Soler, Rafael Muñoz-Salinas, Rafael Medina-Carnicer, Manuel J. Marín-Jiménez
- **Comment**: None
- **Journal**: Image and Vision Computing. Elsevier BV, p. 105313, Oct. 2024
- **Summary**: Fiducial markers are a computer vision tool used for object pose estimation and detection. These markers are highly useful in fields such as industry, medicine and logistics. However, optimal lighting conditions are not always available,and other factors such as blur or sensor noise can affect image quality. Classical computer vision techniques that precisely locate and decode fiducial markers often fail under difficult illumination conditions (e.g. extreme variations of lighting within the same frame). Hence, we propose DeepArUco++, a deep learning-based framework that leverages the robustness of Convolutional Neural Networks to perform marker detection and decoding in challenging lighting conditions. The framework is based on a pipeline using different Neural Network models at each step, namely marker detection, corner refinement and marker decoding. Additionally, we propose a simple method for generating synthetic data for training the different models that compose the proposed pipeline, and we present a second, real-life dataset of ArUco markers in challenging lighting conditions used to evaluate our system. The developed method outperforms other state-of-the-art methods in such tasks and remains competitive even when testing on the datasets used to develop those methods. Code available in GitHub: https://github.com/AVAuco/deeparuco/



### A Nerf-Based Color Consistency Method for Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2411.05557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T07, I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2411.05557v1)
- **Published**: 2024-11-08 13:26:07+00:00
- **Updated**: 2024-11-08 13:26:07+00:00
- **Authors**: Zongcheng Zuo, Yuanxiang Li, Tongtong Zhang
- **Comment**: 4 pages, 4 figures, The International Geoscience and Remote Sensing
  Symposium (IGARSS2023)
- **Journal**: None
- **Summary**: Due to different seasons, illumination, and atmospheric conditions, the photometric of the acquired image varies greatly, which leads to obvious stitching seams at the edges of the mosaic image. Traditional methods can be divided into two categories, one is absolute radiation correction and the other is relative radiation normalization. We propose a NeRF-based method of color consistency correction for multi-view images, which weaves image features together using implicit expressions, and then re-illuminates feature space to generate a fusion image with a new perspective. We chose Superview-1 satellite images and UAV images with large range and time difference for the experiment. Experimental results show that the synthesize image generated by our method has excellent visual effect and smooth color transition at the edges.



### Training objective drives the consistency of representational similarity across datasets
- **Arxiv ID**: http://arxiv.org/abs/2411.05561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05561v1)
- **Published**: 2024-11-08 13:35:45+00:00
- **Updated**: 2024-11-08 13:35:45+00:00
- **Authors**: Laure Ciernik, Lorenz Linhardt, Marco Morik, Jonas Dippel, Simon Kornblith, Lukas Muttenthaler
- **Comment**: 26 pages
- **Journal**: None
- **Summary**: The Platonic Representation Hypothesis claims that recent foundation models are converging to a shared representation space as a function of their downstream task performance, irrespective of the objectives and data modalities used to train these models. Representational similarity is generally measured for individual datasets and is not necessarily consistent across datasets. Thus, one may wonder whether this convergence of model representations is confounded by the datasets commonly used in machine learning. Here, we propose a systematic way to measure how representational similarity between models varies with the set of stimuli used to construct the representations. We find that the objective function is the most crucial factor in determining the consistency of representational similarities across datasets. Specifically, self-supervised vision models learn representations whose relative pairwise similarities generalize better from one dataset to another compared to those of image classification or image-text models. Moreover, the correspondence between representational similarities and the models' task behavior is dataset-dependent, being most strongly pronounced for single-domain datasets. Our work provides a framework for systematically measuring similarities of model representations across datasets and linking those similarities to differences in task behavior.



### Open-set object detection: towards unified problem formulation and benchmarking
- **Arxiv ID**: http://arxiv.org/abs/2411.05564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05564v1)
- **Published**: 2024-11-08 13:40:01+00:00
- **Updated**: 2024-11-08 13:40:01+00:00
- **Authors**: Hejer Ammar, Nikita Kiselov, Guillaume Lapouge, Romaric Audigier
- **Comment**: Accepted at ECCV 2024 Workshop: "The 3rd Workshop for
  Out-of-Distribution Generalization in Computer Vision Foundation Models"
- **Journal**: None
- **Summary**: In real-world applications where confidence is key, like autonomous driving, the accurate detection and appropriate handling of classes differing from those used during training are crucial. Despite the proposal of various unknown object detection approaches, we have observed widespread inconsistencies among them regarding the datasets, metrics, and scenarios used, alongside a notable absence of a clear definition for unknown objects, which hampers meaningful evaluation. To counter these issues, we introduce two benchmarks: a unified VOC-COCO evaluation, and the new OpenImagesRoad benchmark which provides clear hierarchical object definition besides new evaluation metrics. Complementing the benchmark, we exploit recent self-supervised Vision Transformers performance, to improve pseudo-labeling-based OpenSet Object Detection (OSOD), through OW-DETR++. State-of-the-art methods are extensively evaluated on the proposed benchmarks. This study provides a clear problem definition, ensures consistent evaluations, and draws new conclusions about effectiveness of OSOD strategies.



### Predicting Stroke through Retinal Graphs and Multimodal Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2411.05597v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05597v1)
- **Published**: 2024-11-08 14:40:56+00:00
- **Updated**: 2024-11-08 14:40:56+00:00
- **Authors**: Yuqing Huang, Bastian Wittmann, Olga Demler, Bjoern Menze, Neda Davoudi
- **Comment**: Accepted as oral paper at ML-CDS workshop, MICCAI 2024
- **Journal**: None
- **Summary**: Early identification of stroke is crucial for intervention, requiring reliable models. We proposed an efficient retinal image representation together with clinical information to capture a comprehensive overview of cardiovascular health, leveraging large multimodal datasets for new medical insights. Our approach is one of the first contrastive frameworks that integrates graph and tabular data, using vessel graphs derived from retinal images for efficient representation. This method, combined with multimodal contrastive learning, significantly enhances stroke prediction accuracy by integrating data from multiple sources and using contrastive learning for transfer learning. The self-supervised learning techniques employed allow the model to learn effectively from unlabeled data, reducing the dependency on large annotated datasets. Our framework showed an AUROC improvement of 3.78% from supervised to self-supervised approaches. Additionally, the graph-level representation approach achieved superior performance to image encoders while significantly reducing pre-training and fine-tuning runtimes. These findings indicate that retinal images are a cost-effective method for improving cardiovascular disease predictions and pave the way for future research into retinal and cerebral vessel connections and the use of graph-based retinal vessel representations.



### Efficient Audio-Visual Fusion for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/2411.05603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05603v1)
- **Published**: 2024-11-08 14:47:28+00:00
- **Updated**: 2024-11-08 14:47:28+00:00
- **Authors**: Mahrukh Awan, Asmar Nadeem, Armin Mustafa
- **Comment**: CVMP Short Paper
- **Journal**: None
- **Summary**: We present Attend-Fusion, a novel and efficient approach for audio-visual fusion in video classification tasks. Our method addresses the challenge of exploiting both audio and visual modalities while maintaining a compact model architecture. Through extensive experiments on the YouTube-8M dataset, we demonstrate that our Attend-Fusion achieves competitive performance with significantly reduced model complexity compared to larger baseline models.



### A Two-Step Concept-Based Approach for Enhanced Interpretability and Trust in Skin Lesion Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2411.05609v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05609v1)
- **Published**: 2024-11-08 14:52:42+00:00
- **Updated**: 2024-11-08 14:52:42+00:00
- **Authors**: Cristiano Patrício, Luís F. Teixeira, João C. Neves
- **Comment**: Preprint submitted for review
- **Journal**: None
- **Summary**: The main challenges hindering the adoption of deep learning-based systems in clinical settings are the scarcity of annotated data and the lack of interpretability and trust in these systems. Concept Bottleneck Models (CBMs) offer inherent interpretability by constraining the final disease prediction on a set of human-understandable concepts. However, this inherent interpretability comes at the cost of greater annotation burden. Additionally, adding new concepts requires retraining the entire system. In this work, we introduce a novel two-step methodology that addresses both of these challenges. By simulating the two stages of a CBM, we utilize a pretrained Vision Language Model (VLM) to automatically predict clinical concepts, and a Large Language Model (LLM) to generate disease diagnoses based on the predicted concepts. We validate our approach on three skin lesion datasets, demonstrating that it outperforms traditional CBMs and state-of-the-art explainable methods, all without requiring any training and utilizing only a few annotated examples. The code is available at https://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.



### SynDroneVision: A Synthetic Dataset for Image-Based Drone Detection
- **Arxiv ID**: http://arxiv.org/abs/2411.05633v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2411.05633v1)
- **Published**: 2024-11-08 15:22:49+00:00
- **Updated**: 2024-11-08 15:22:49+00:00
- **Authors**: Tamara R. Lenhard, Andreas Weinmann, Kai Franke, Tobias Koch
- **Comment**: Accepted at the 2025 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV)
- **Journal**: None
- **Summary**: Developing robust drone detection systems is often constrained by the limited availability of large-scale annotated training data and the high costs associated with real-world data collection. However, leveraging synthetic data generated via game engine-based simulations provides a promising and cost-effective solution to overcome this issue. Therefore, we present SynDroneVision, a synthetic dataset specifically designed for RGB-based drone detection in surveillance applications. Featuring diverse backgrounds, lighting conditions, and drone models, SynDroneVision offers a comprehensive training foundation for deep learning algorithms. To evaluate the dataset's effectiveness, we perform a comparative analysis across a selection of recent YOLO detection models. Our findings demonstrate that SynDroneVision is a valuable resource for real-world data enrichment, achieving notable enhancements in model performance and robustness, while significantly reducing the time and costs of real-world data acquisition. SynDroneVision will be publicly released upon paper acceptance.



### Video RWKV:Video Action Recognition Based RWKV
- **Arxiv ID**: http://arxiv.org/abs/2411.05636v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05636v1)
- **Published**: 2024-11-08 15:30:10+00:00
- **Updated**: 2024-11-08 15:30:10+00:00
- **Authors**: Zhuowen Yin, Chengru Li, Xingbo Dong
- **Comment**: None
- **Journal**: None
- **Summary**: To address the challenges of high computational costs and long-distance dependencies in exist ing video understanding methods, such as CNNs and Transformers, this work introduces RWKV to the video domain in a novel way. We propose a LSTM CrossRWKV (LCR) framework, designed for spatiotemporal representation learning to tackle the video understanding task. Specifically, the proposed linear complexity LCR incorporates a novel Cross RWKV gate to facilitate interaction be tween current frame edge information and past features, enhancing the focus on the subject through edge features and globally aggregating inter-frame features over time. LCR stores long-term mem ory for video processing through an enhanced LSTM recurrent execution mechanism. By leveraging the Cross RWKV gate and recurrent execution, LCR effectively captures both spatial and temporal features. Additionally, the edge information serves as a forgetting gate for LSTM, guiding long-term memory management.Tube masking strategy reduces redundant information in food and reduces overfitting.These advantages enable LSTM CrossRWKV to set a new benchmark in video under standing, offering a scalable and efficient solution for comprehensive video analysis. All code and models are publicly available.



### Online-LoRA: Task-free Online Continual Learning via Low Rank Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2411.05663v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05663v1)
- **Published**: 2024-11-08 16:04:16+00:00
- **Updated**: 2024-11-08 16:04:16+00:00
- **Authors**: Xiwen Wei, Guihong Li, Radu Marculescu
- **Comment**: WACV 2025
- **Journal**: None
- **Summary**: Catastrophic forgetting is a significant challenge in online continual learning (OCL), especially for non-stationary data streams that do not have well-defined task boundaries. This challenge is exacerbated by the memory constraints and privacy concerns inherent in rehearsal buffers. To tackle catastrophic forgetting, in this paper, we introduce Online-LoRA, a novel framework for task-free OCL. Online-LoRA allows to finetune pre-trained Vision Transformer (ViT) models in real-time to address the limitations of rehearsal buffers and leverage pre-trained models' performance benefits. As the main contribution, our approach features a novel online weight regularization strategy to identify and consolidate important model parameters. Moreover, Online-LoRA leverages the training dynamics of loss values to enable the automatic recognition of the data distribution shifts. Extensive experiments across many task-free OCL scenarios and benchmark datasets (including CIFAR-100, ImageNet-R, ImageNet-S, CUB-200 and CORe50) demonstrate that Online-LoRA can be robustly adapted to various ViT architectures, while achieving better performance compared to SOTA methods. Our code will be publicly available at: https://github.com/Christina200/Online-LoRA-official.git.



### Tell What You Hear From What You See -- Video to Audio Generation Through Text
- **Arxiv ID**: http://arxiv.org/abs/2411.05679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2411.05679v1)
- **Published**: 2024-11-08 16:29:07+00:00
- **Updated**: 2024-11-08 16:29:07+00:00
- **Authors**: Xiulong Liu, Kun Su, Eli Shlizerman
- **Comment**: NeurIPS 2024
- **Journal**: None
- **Summary**: The content of visual and audio scenes is multi-faceted such that a video can be paired with various audio and vice-versa. Thereby, in video-to-audio generation task, it is imperative to introduce steering approaches for controlling the generated audio. While Video-to-Audio generation is a well-established generative task, existing methods lack such controllability. In this work, we propose VATT, a multi-modal generative framework that takes a video and an optional text prompt as input, and generates audio and optional textual description of the audio. Such a framework has two advantages: i) Video-to-Audio generation process can be refined and controlled via text which complements the context of visual information, and ii) The model can suggest what audio to generate for the video by generating audio captions. VATT consists of two key modules: VATT Converter, a LLM that is fine-tuned for instructions and includes a projection layer that maps video features to the LLM vector space; and VATT Audio, a transformer that generates audio tokens from visual frames and from optional text prompt using iterative parallel decoding. The audio tokens are converted to a waveform by pretrained neural codec. Experiments show that when VATT is compared to existing video-to-audio generation methods in objective metrics, it achieves competitive performance when the audio caption is not provided. When the audio caption is provided as a prompt, VATT achieves even more refined performance (lowest KLD score of 1.41). Furthermore, subjective studies show that VATT Audio has been chosen as preferred generated audio than audio generated by existing methods. VATT enables controllable video-to-audio generation through text as well as suggesting text prompts for videos through audio captions, unlocking novel applications such as text-guided video-to-audio generation and video-to-audio captioning.



### Autoregressive Adaptive Hypergraph Transformer for Skeleton-based Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2411.05692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05692v1)
- **Published**: 2024-11-08 16:45:52+00:00
- **Updated**: 2024-11-08 16:45:52+00:00
- **Authors**: Abhisek Ray, Ayush Raj, Maheshkumar H. Kolekar
- **Comment**: Accepted to WACV 2025
- **Journal**: None
- **Summary**: Extracting multiscale contextual information and higher-order correlations among skeleton sequences using Graph Convolutional Networks (GCNs) alone is inadequate for effective action classification. Hypergraph convolution addresses the above issues but cannot harness the long-range dependencies. Transformer proves to be effective in capturing these dependencies and making complex contextual features accessible. We propose an Autoregressive Adaptive HyperGraph Transformer (AutoregAd-HGformer) model for in-phase (autoregressive and discrete) and out-phase (adaptive) hypergraph generation. The vector quantized in-phase hypergraph equipped with powerful autoregressive learned priors produces a more robust and informative representation suitable for hyperedge formation. The out-phase hypergraph generator provides a model-agnostic hyperedge learning technique to align the attributes with input skeleton embedding. The hybrid (supervised and unsupervised) learning in AutoregAd-HGformer explores the action-dependent feature along spatial, temporal, and channel dimensions. The extensive experimental results and ablation study indicate the superiority of our model over state-of-the-art hypergraph architectures on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.



### Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc Explainability in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2411.05698v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05698v1)
- **Published**: 2024-11-08 16:52:52+00:00
- **Updated**: 2024-11-08 16:52:52+00:00
- **Authors**: Antonio De Santis, Riccardo Campi, Matteo Bianchi, Marco Brambilla
- **Comment**: Preprint currently under review
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have seen significant performance improvements in recent years. However, due to their size and complexity, they function as black-boxes, leading to transparency concerns. State-of-the-art saliency methods generate local explanations that highlight the area in the input image where a class is identified but cannot explain how a concept of interest contributes to the prediction, which is essential for bias mitigation. On the other hand, concept-based methods, such as TCAV (Testing with Concept Activation Vectors), provide insights into how sensitive is the network to a concept, but cannot compute its attribution in a specific prediction nor show its location within the input image. This paper introduces a novel post-hoc explainability framework, Visual-TCAV, which aims to bridge the gap between these methods by providing both local and global explanations for CNN-based image classification. Visual-TCAV uses Concept Activation Vectors (CAVs) to generate saliency maps that show where concepts are recognized by the network. Moreover, it can estimate the attribution of these concepts to the output of any class using a generalization of Integrated Gradients. This framework is evaluated on popular CNN architectures, with its validity further confirmed via experiments where ground truth for explanations is known, and a comparison with TCAV. Our code will be made available soon.



### Image inpainting enhancement by replacing the original mask with a self-attended region from the input image
- **Arxiv ID**: http://arxiv.org/abs/2411.05705v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2411.05705v1)
- **Published**: 2024-11-08 17:04:05+00:00
- **Updated**: 2024-11-08 17:04:05+00:00
- **Authors**: Kourosh Kiani, Razieh Rastgoo, Alireza Chaji, Sergio Escalera
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting, the process of restoring missing or corrupted regions of an image by reconstructing pixel information, has recently seen considerable advancements through deep learning-based approaches. In this paper, we introduce a novel deep learning-based pre-processing methodology for image inpainting utilizing the Vision Transformer (ViT). Our approach involves replacing masked pixel values with those generated by the ViT, leveraging diverse visual patches within the attention matrix to capture discriminative spatial features. To the best of our knowledge, this is the first instance of such a pre-processing model being proposed for image inpainting tasks. Furthermore, we show that our methodology can be effectively applied using the pre-trained ViT model with pre-defined patch size. To evaluate the generalization capability of the proposed methodology, we provide experimental results comparing our approach with four standard models across four public datasets, demonstrating the efficacy of our pre-processing technique in enhancing inpainting performance.



### Image2Text2Image: A Novel Framework for Label-Free Evaluation of Image-to-Text Generation with Text-to-Image Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2411.05706v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2411.05706v1)
- **Published**: 2024-11-08 17:07:01+00:00
- **Updated**: 2024-11-08 17:07:01+00:00
- **Authors**: Jia-Hong Huang, Hongyi Zhu, Yixian Shen, Stevan Rudinac, Evangelos Kanoulas
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2408.01723
- **Journal**: None
- **Summary**: Evaluating the quality of automatically generated image descriptions is a complex task that requires metrics capturing various dimensions, such as grammaticality, coverage, accuracy, and truthfulness. Although human evaluation provides valuable insights, its cost and time-consuming nature pose limitations. Existing automated metrics like BLEU, ROUGE, METEOR, and CIDEr attempt to fill this gap, but they often exhibit weak correlations with human judgment. To address this challenge, we propose a novel evaluation framework called Image2Text2Image, which leverages diffusion models, such as Stable Diffusion or DALL-E, for text-to-image generation. In the Image2Text2Image framework, an input image is first processed by a selected image captioning model, chosen for evaluation, to generate a textual description. Using this generated description, a diffusion model then creates a new image. By comparing features extracted from the original and generated images, we measure their similarity using a designated similarity metric. A high similarity score suggests that the model has produced a faithful textual description, while a low score highlights discrepancies, revealing potential weaknesses in the model's performance. Notably, our framework does not rely on human-annotated reference captions, making it a valuable tool for assessing image captioning models. Extensive experiments and human evaluations validate the efficacy of our proposed Image2Text2Image evaluation framework. The code and dataset will be published to support further research in the community.



### Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream
- **Arxiv ID**: http://arxiv.org/abs/2411.05712v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2411.05712v1)
- **Published**: 2024-11-08 17:13:53+00:00
- **Updated**: 2024-11-08 17:13:53+00:00
- **Authors**: Abdulkadir Gokce, Martin Schrimpf
- **Comment**: 9 pages for the main paper, 20 pages in total. 6 main figures and 10
  supplementary figures. Code, model weights, and benchmark results can be
  accessed at https://github.com/epflneuroailab/scaling-primate-vvs
- **Journal**: None
- **Summary**: When trained on large-scale object classification datasets, certain artificial neural network models begin to approximate core object recognition (COR) behaviors and neural response patterns in the primate visual ventral stream (VVS). While recent machine learning advances suggest that scaling model size, dataset size, and compute resources improve task performance, the impact of scaling on brain alignment remains unclear. In this study, we explore scaling laws for modeling the primate VVS by systematically evaluating over 600 models trained under controlled conditions on benchmarks spanning V1, V2, V4, IT and COR behaviors. We observe that while behavioral alignment continues to scale with larger models, neural alignment saturates. This observation remains true across model architectures and training datasets, even though models with stronger inductive bias and datasets with higher-quality images are more compute-efficient. Increased scaling is especially beneficial for higher-level visual areas, where small models trained on few samples exhibit only poor alignment. Finally, we develop a scaling recipe, indicating that a greater proportion of compute should be allocated to data samples over model size. Our results suggest that while scaling alone might suffice for alignment with human core object recognition behavior, it will not yield improved models of the brain's visual ventral stream with current architectures and datasets, highlighting the need for novel strategies in building brain-like models.



### STARS: Sensor-agnostic Transformer Architecture for Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2411.05714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2411.05714v1)
- **Published**: 2024-11-08 17:16:02+00:00
- **Updated**: 2024-11-08 17:16:02+00:00
- **Authors**: Ethan King, Jaime Rodriguez, Diego Llanes, Timothy Doster, Tegan Emerson, James Koch
- **Comment**: None
- **Journal**: None
- **Summary**: We present a sensor-agnostic spectral transformer as the basis for spectral foundation models. To that end, we introduce a Universal Spectral Representation (USR) that leverages sensor meta-data, such as sensing kernel specifications and sensing wavelengths, to encode spectra obtained from any spectral instrument into a common representation, such that a single model can ingest data from any sensor. Furthermore, we develop a methodology for pre-training such models in a self-supervised manner using a novel random sensor-augmentation and reconstruction pipeline to learn spectral features independent of the sensing paradigm. We demonstrate that our architecture can learn sensor independent spectral features that generalize effectively to sensors not seen during training. This work sets the stage for training foundation models that can both leverage and be effective for the growing diversity of spectral data.



### PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for View-Adaptive Rendering
- **Arxiv ID**: http://arxiv.org/abs/2411.05731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05731v1)
- **Published**: 2024-11-08 17:42:02+00:00
- **Updated**: 2024-11-08 17:42:02+00:00
- **Authors**: Junxi Jin, Xiulai Li, Haiping Huang, Lianjun Liu, Yujie Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in structured 3D Gaussians for view-adaptive rendering, particularly through methods like Scaffold-GS, have demonstrated promising results in neural scene representation. However, existing approaches still face challenges in perceptual consistency and precise view-dependent effects. We present PEP-GS, a novel framework that enhances structured 3D Gaussians through three key innovations: (1) a Local-Enhanced Multi-head Self-Attention (LEMSA) mechanism that replaces spherical harmonics for more accurate view-dependent color decoding, and (2) Kolmogorov-Arnold Networks (KAN) that optimize Gaussian opacity and covariance functions for enhanced interpretability and splatting precision. (3) a Neural Laplacian Pyramid Decomposition (NLPD) that improves perceptual similarity across views. Our comprehensive evaluation across multiple datasets indicates that, compared to the current state-of-the-art methods, these improvements are particularly evident in challenging scenarios such as view-dependent effects, specular reflections, fine-scale details and false geometry generation.



### Poze: Sports Technique Feedback under Data Constraints
- **Arxiv ID**: http://arxiv.org/abs/2411.05734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05734v1)
- **Published**: 2024-11-08 17:48:20+00:00
- **Updated**: 2024-11-08 17:48:20+00:00
- **Authors**: Agamdeep Singh, Sujit PB, Mayank Vatsa
- **Comment**: None
- **Journal**: None
- **Summary**: Access to expert coaching is essential for developing technique in sports, yet economic barriers often place it out of reach for many enthusiasts. To bridge this gap, we introduce Poze, an innovative video processing framework that provides feedback on human motion, emulating the insights of a professional coach. Poze combines pose estimation with sequence comparison and is optimized to function effectively with minimal data. Poze surpasses state-of-the-art vision-language models in video question-answering frameworks, achieving 70% and 196% increase in accuracy over GPT4V and LLaVAv1.6 7b, respectively.



### StdGEN: Semantic-Decomposed 3D Character Generation from Single Images
- **Arxiv ID**: http://arxiv.org/abs/2411.05738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05738v1)
- **Published**: 2024-11-08 17:54:18+00:00
- **Updated**: 2024-11-08 17:54:18+00:00
- **Authors**: Yuze He, Yanning Zhou, Wang Zhao, Zhongkai Wu, Kaiwen Xiao, Wei Yang, Yong-Jin Liu, Xiao Han
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: https://stdgen.github.io



### WavShadow: Wavelet Based Shadow Segmentation and Removal
- **Arxiv ID**: http://arxiv.org/abs/2411.05747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.05747v1)
- **Published**: 2024-11-08 18:08:33+00:00
- **Updated**: 2024-11-08 18:08:33+00:00
- **Authors**: Shreyans Jain, Aadya Arora, Viraj Vekaria, Karan Gandhi
- **Comment**: ICVGIP’24, December 2024, Bangaluru, India
- **Journal**: None
- **Summary**: Shadow removal and segmentation remain challenging tasks in computer vision, particularly in complex real-world scenarios. This study presents a novel approach that enhances the ShadowFormer model by incorporating Masked Autoencoder (MAE) priors and Fast Fourier Convolution (FFC) blocks, leading to significantly faster convergence and improved performance. We introduce key innovations: (1) integration of MAE priors trained on Places2 dataset for better context understanding, (2) adoption of Haar wavelet features for enhanced edge detection and multi-scale analysis, and (3) implementation of a modified SAM Adapter for robust shadow segmentation. Extensive experiments on the challenging DESOBA dataset demonstrate that our approach achieves state-of-the-art results, with notable improvements in both convergence speed and shadow removal quality.



### FisherMask: Enhancing Neural Network Labeling Efficiency in Image Classification Using Fisher Information
- **Arxiv ID**: http://arxiv.org/abs/2411.05752v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2411.05752v1)
- **Published**: 2024-11-08 18:10:46+00:00
- **Updated**: 2024-11-08 18:10:46+00:00
- **Authors**: Shreen Gul, Mohamed Elmahallawy, Sanjay Madria, Ardhendu Tripathy
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) models are popular across various domains due to their remarkable performance and efficiency. However, their effectiveness relies heavily on large amounts of labeled data, which are often time-consuming and labor-intensive to generate manually. To overcome this challenge, it is essential to develop strategies that reduce reliance on extensive labeled data while preserving model performance. In this paper, we propose FisherMask, a Fisher information-based active learning (AL) approach that identifies key network parameters by masking them based on their Fisher information values. FisherMask enhances batch AL by using Fisher information to select the most critical parameters, allowing the identification of the most impactful samples during AL training. Moreover, Fisher information possesses favorable statistical properties, offering valuable insights into model behavior and providing a better understanding of the performance characteristics within the AL pipeline. Our extensive experiments demonstrate that FisherMask significantly outperforms state-of-the-art methods on diverse datasets, including CIFAR-10 and FashionMNIST, especially under imbalanced settings. These improvements lead to substantial gains in labeling efficiency. Hence serving as an effective tool to measure the sensitivity of model parameters to data samples. Our code is available on \url{https://github.com/sgchr273/FisherMask}.



### End-to-End Navigation with Vision Language Models: Transforming Spatial Reasoning into Question-Answering
- **Arxiv ID**: http://arxiv.org/abs/2411.05755v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2411.05755v1)
- **Published**: 2024-11-08 18:16:58+00:00
- **Updated**: 2024-11-08 18:16:58+00:00
- **Authors**: Dylan Goetting, Himanshu Gaurav Singh, Antonio Loquercio
- **Comment**: None
- **Journal**: None
- **Summary**: We present VLMnav, an embodied framework to transform a Vision-Language Model (VLM) into an end-to-end navigation policy. In contrast to prior work, we do not rely on a separation between perception, planning, and control; instead, we use a VLM to directly select actions in one step. Surprisingly, we find that a VLM can be used as an end-to-end policy zero-shot, i.e., without any fine-tuning or exposure to navigation data. This makes our approach open-ended and generalizable to any downstream navigation task. We run an extensive study to evaluate the performance of our approach in comparison to baseline prompting methods. In addition, we perform a design analysis to understand the most impactful design decisions. Visual examples and code for our project can be found at https://jirl-upenn.github.io/VLMnav/



### Sketched Equivariant Imaging Regularization and Deep Internal Learning for Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/2411.05771v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2411.05771v1)
- **Published**: 2024-11-08 18:33:03+00:00
- **Updated**: 2024-11-08 18:33:03+00:00
- **Authors**: Guixian Xu, Jinglai Li, Junqi Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Equivariant Imaging (EI) regularization has become the de-facto technique for unsupervised training of deep imaging networks, without any need of ground-truth data. Observing that the EI-based unsupervised training paradigm currently has significant computational redundancy leading to inefficiency in high-dimensional applications, we propose a sketched EI regularization which leverages the randomized sketching techniques for acceleration. We then extend our sketched EI regularization to develop an accelerated deep internal learning framework -- Sketched Equivariant Deep Image Prior (Sk.EI-DIP), which can be efficiently applied for single-image and task-adapted reconstruction. Our numerical study on X-ray CT image reconstruction tasks demonstrate that our approach can achieve order-of-magnitude computational acceleration over standard EI-based counterpart in single-input setting, and network adaptation at test time.



### Curriculum Learning for Few-Shot Domain Adaptation in CT-based Airway Tree Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2411.05779v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05779v1)
- **Published**: 2024-11-08 18:46:40+00:00
- **Updated**: 2024-11-08 18:46:40+00:00
- **Authors**: Maxime Jacovella, Ali Keshavarzi, Elsa Angelini
- **Comment**: Under review for 22nd IEEE International Symposium on Biomedical
  Imaging (ISBI), Houston, TX, USA
- **Journal**: None
- **Summary**: Despite advances with deep learning (DL), automated airway segmentation from chest CT scans continues to face challenges in segmentation quality and generalization across cohorts. To address these, we propose integrating Curriculum Learning (CL) into airway segmentation networks, distributing the training set into batches according to ad-hoc complexity scores derived from CT scans and corresponding ground-truth tree features. We specifically investigate few-shot domain adaptation, targeting scenarios where manual annotation of a full fine-tuning dataset is prohibitively expensive. Results are reported on two large open-cohorts (ATM22 and AIIB23) with high performance using CL for full training (Source domain) and few-shot fine-tuning (Target domain), but with also some insights on potential detrimental effects if using a classic Bootstrapping scoring function or if not using proper scan sequencing.



### GazeSearch: Radiology Findings Search Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2411.05780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2411.05780v1)
- **Published**: 2024-11-08 18:47:08+00:00
- **Updated**: 2024-11-08 18:47:08+00:00
- **Authors**: Trong Thang Pham, Tien-Phat Nguyen, Yuki Ikebe, Akash Awasthi, Zhigang Deng, Carol C. Wu, Hien Nguyen, Ngan Le
- **Comment**: Aceepted WACV 2025
- **Journal**: None
- **Summary**: Medical eye-tracking data is an important information source for understanding how radiologists visually interpret medical images. This information not only improves the accuracy of deep learning models for X-ray analysis but also their interpretability, enhancing transparency in decision-making. However, the current eye-tracking data is dispersed, unprocessed, and ambiguous, making it difficult to derive meaningful insights. Therefore, there is a need to create a new dataset with more focus and purposeful eyetracking data, improving its utility for diagnostic applications. In this work, we propose a refinement method inspired by the target-present visual search challenge: there is a specific finding and fixations are guided to locate it. After refining the existing eye-tracking datasets, we transform them into a curated visual search dataset, called GazeSearch, specifically for radiology findings, where each fixation sequence is purposefully aligned to the task of locating a particular finding. Subsequently, we introduce a scan path prediction baseline, called ChestSearch, specifically tailored to GazeSearch. Finally, we employ the newly introduced GazeSearch as a benchmark to evaluate the performance of current state-of-the-art methods, offering a comprehensive assessment for visual search in the medical imaging domain.



### ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles
- **Arxiv ID**: http://arxiv.org/abs/2411.05783v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2411.05783v1)
- **Published**: 2024-11-08 18:50:37+00:00
- **Updated**: 2024-11-08 18:50:37+00:00
- **Authors**: Kayo Yin, Chinmay Singh, Fyodor O. Minakov, Vanessa Milan, Hal Daumé III, Cyril Zhang, Alex X. Lu, Danielle Bragg
- **Comment**: Accepted to EMNLP 2024
- **Journal**: None
- **Summary**: Deaf and hard-of-hearing (DHH) students face significant barriers in accessing science, technology, engineering, and mathematics (STEM) education, notably due to the scarcity of STEM resources in signed languages. To help address this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia articles on STEM topics in English, interpreted into over 300 hours of American Sign Language (ASL). ASL STEM Wiki is the first continuous signing dataset focused on STEM, facilitating the development of AI resources for STEM education in ASL. We identify several use cases of ASL STEM Wiki with human-centered applications. For example, because this dataset highlights the frequent use of fingerspelling for technical concepts, which inhibits DHH students' ability to learn, we develop models to identify fingerspelled words -- which can later be used to query for appropriate ASL signs to suggest to interpreters.




# Arxiv Papers in cs.CV on 2015-12-02
### Rethinking the Inception Architecture for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1512.00567v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.00567v3)
- **Published**: 2015-12-02 03:44:38+00:00
- **Updated**: 2015-12-11 20:27:50+00:00
- **Authors**: Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.



### Attribute2Image: Conditional Image Generation from Visual Attributes
- **Arxiv ID**: http://arxiv.org/abs/1512.00570v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1512.00570v2)
- **Published**: 2015-12-02 04:07:28+00:00
- **Updated**: 2016-10-08 08:55:32+00:00
- **Authors**: Xinchen Yan, Jimei Yang, Kihyuk Sohn, Honglak Lee
- **Comment**: 19 pages, accepted by ECCV 2016, The 14th European Conference on
  Computer Vision (2016)
- **Journal**: None
- **Summary**: This paper investigates a novel problem of generating images from visual attributes. We model the image as a composite of foreground and background and develop a layered generative model with disentangled latent variables that can be learned end-to-end using a variational auto-encoder. We experiment with natural images of faces and birds and demonstrate that the proposed models are capable of generating realistic and diverse samples with disentangled latent representations. We use a general energy minimization algorithm for posterior inference of latent variables given novel images. Therefore, the learned generative models show excellent quantitative and visual results in the tasks of attribute-conditioned image reconstruction and completion.



### The MegaFace Benchmark: 1 Million Faces for Recognition at Scale
- **Arxiv ID**: http://arxiv.org/abs/1512.00596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.00596v1)
- **Published**: 2015-12-02 07:17:54+00:00
- **Updated**: 2015-12-02 07:17:54+00:00
- **Authors**: Ira Kemelmacher-Shlizerman, Steve Seitz, Daniel Miller, Evan Brossard
- **Comment**: None
- **Journal**: None
- **Summary**: Recent face recognition experiments on a major benchmark LFW show stunning performance--a number of algorithms achieve near to perfect score, surpassing human recognition rates. In this paper, we advocate evaluations at the million scale (LFW includes only 13K photos of 5K people). To this end, we have assembled the MegaFace dataset and created the first MegaFace challenge. Our dataset includes One Million photos that capture more than 690K different individuals. The challenge evaluates performance of algorithms with increasing numbers of distractors (going from 10 to 1M) in the gallery set. We present both identification and verification performance, evaluate performance with respect to pose and a person's age, and compare as a function of training data size (number of photos and people). We report results of state of the art and baseline algorithms. Our key observations are that testing at the million scale reveals big performance differences (of algorithms that perform similarly well on smaller scale) and that age invariant recognition as well as pose are still challenging for most. The MegaFace dataset, baseline code, and evaluation scripts, are all publicly released for further experimentations at: megaface.cs.washington.edu.



### Double Sparse Multi-Frame Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1512.00607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.00607v1)
- **Published**: 2015-12-02 08:25:23+00:00
- **Updated**: 2015-12-02 08:25:23+00:00
- **Authors**: Toshiyuki Kato, Hideitsu Hino, Noboru Murata
- **Comment**: None
- **Journal**: None
- **Summary**: A large number of image super resolution algorithms based on the sparse coding are proposed, and some algorithms realize the multi-frame super resolution. In multi-frame super resolution based on the sparse coding, both accurate image registration and sparse coding are required. Previous study on multi-frame super resolution based on sparse coding firstly apply block matching for image registration, followed by sparse coding to enhance the image resolution. In this paper, these two problems are solved by optimizing a single objective function. The results of numerical experiments support the effectiveness of the proposed approch.



### Continuous and Simultaneous Gesture and Posture Recognition for Commanding a Robotic Wheelchair; Towards Spotting the Signal Patterns
- **Arxiv ID**: http://arxiv.org/abs/1512.00622v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1512.00622v1)
- **Published**: 2015-12-02 09:25:52+00:00
- **Updated**: 2015-12-02 09:25:52+00:00
- **Authors**: Ali Boyali, Naohisa Hashimoto, Manolya Kavakli
- **Comment**: None
- **Journal**: None
- **Summary**: Spotting signal patterns with varying lengths has been still an open problem in the literature. In this study, we describe a signal pattern recognition approach for continuous and simultaneous classification of a tracked hand's posture and gestures and map them to steering commands for control of a robotic wheelchair. The developed methodology not only affords 100\% recognition accuracy on a streaming signal for continuous recognition, but also brings about a new perspective for building a training dictionary which eliminates human intervention to spot the gesture or postures on a training signal. In the training phase we employ a state of art subspace clustering method to find the most representative state samples. The recognition and training framework reveal boundaries of the patterns on the streaming signal with a successive decision tree structure intrinsically. We make use of the Collaborative ans Block Sparse Representation based classification methods for continuous gesture and posture recognition.



### MMSE Estimation for Poisson Noise Removal in Images
- **Arxiv ID**: http://arxiv.org/abs/1512.00717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1512.00717v1)
- **Published**: 2015-12-02 14:49:12+00:00
- **Updated**: 2015-12-02 14:49:12+00:00
- **Authors**: Stanislav Pyatykh, Jürgen Hesser
- **Comment**: None
- **Journal**: None
- **Summary**: Poisson noise suppression is an important preprocessing step in several applications, such as medical imaging, microscopy, and astronomical imaging. In this work, we propose a novel patch-wise Poisson noise removal strategy, in which the MMSE estimator is utilized in order to produce the denoising result for each image patch. Fast and accurate computation of the MMSE estimator is carried out using k-d tree search followed by search in the K-nearest neighbor graph. Our experiments show that the proposed method is the preferable choice for low signal-to-noise ratios.



### Recognizing Semantic Features in Faces using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1512.00743v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1512.00743v2)
- **Published**: 2015-12-02 15:46:26+00:00
- **Updated**: 2016-10-19 13:33:44+00:00
- **Authors**: Amogh Gudi
- **Comment**: Thesis, M.Sc. Artificial Intelligence, University of Amsterdam, 2014
- **Journal**: None
- **Summary**: The human face constantly conveys information, both consciously and subconsciously. However, as basic as it is for humans to visually interpret this information, it is quite a big challenge for machines. Conventional semantic facial feature recognition and analysis techniques are already in use and are based on physiological heuristics, but they suffer from lack of robustness and high computation time. This thesis aims to explore ways for machines to learn to interpret semantic information available in faces in an automated manner without requiring manual design of feature detectors, using the approach of Deep Learning. This thesis provides a study of the effects of various factors and hyper-parameters of deep neural networks in the process of determining an optimal network configuration for the task of semantic facial feature recognition. This thesis explores the effectiveness of the system to recognize the various semantic features (like emotions, age, gender, ethnicity etc.) present in faces. Furthermore, the relation between the effect of high-level concepts on low level features is explored through an analysis of the similarities in low-level descriptors of different semantic features. This thesis also demonstrates a novel idea of using a deep network to generate 3-D Active Appearance Models of faces from real-world 2-D images.   For a more detailed report on this work, please see [arXiv:1512.00743v1].



### Active Learning for Delineation of Curvilinear Structures
- **Arxiv ID**: http://arxiv.org/abs/1512.00747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.00747v1)
- **Published**: 2015-12-02 15:57:59+00:00
- **Updated**: 2015-12-02 15:57:59+00:00
- **Authors**: Agata Mosinska, Raphael Sznitman, Przemysław Głowacki, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Many recent delineation techniques owe much of their increased effectiveness to path classification algorithms that make it possible to distinguish promising paths from others. The downside of this development is that they require annotated training data, which is tedious to produce.   In this paper, we propose an Active Learning approach that considerably speeds up the annotation process. Unlike standard ones, it takes advantage of the specificities of the delineation problem. It operates on a graph and can reduce the training set size by up to 80% without compromising the reconstruction quality.   We will show that our approach outperforms conventional ones on various biomedical and natural image datasets, thus showing that it is broadly applicable.



### Actions ~ Transformations
- **Arxiv ID**: http://arxiv.org/abs/1512.00795v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.00795v2)
- **Published**: 2015-12-02 18:17:32+00:00
- **Updated**: 2016-07-26 04:51:49+00:00
- **Authors**: Xiaolong Wang, Ali Farhadi, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: What defines an action like "kicking ball"? We argue that the true meaning of an action lies in the change or transformation an action brings to the environment. In this paper, we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens (precondition) to the state after the action (effect). Motivated by recent advancements of video representation using deep learning, we design a Siamese network which models the action as a transformation on a high-level feature space. We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51. More importantly, our approach is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset.



### Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos
- **Arxiv ID**: http://arxiv.org/abs/1512.00818v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1512.00818v2)
- **Published**: 2015-12-02 19:34:00+00:00
- **Updated**: 2015-12-16 00:58:49+00:00
- **Authors**: Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet Sawhney, Ahmed Elgammal
- **Comment**: To appear in AAAI 2016
- **Journal**: None
- **Summary**: We propose a new zero-shot Event Detection method by Multi-modal Distributional Semantic embedding of videos. Our model embeds object and action concepts as well as other available modalities from videos into a distributional semantic space. To our knowledge, this is the first Zero-Shot event detection model that is built on top of distributional semantics and extends it in the following directions: (a) semantic embedding of multimodal information in videos (with focus on the visual modalities), (b) automatically determining relevance of concepts/attributes to a free text query, which could be useful for other applications, and (c) retrieving videos by free text event query (e.g., "changing a vehicle tire") based on their content. We embed videos into a distributional semantic space and then measure the similarity between videos and the event query in a free text form. We validated our method on the large TRECVID MED (Multimedia Event Detection) challenge. Using only the event title as a query, our method outperformed the state-of-the-art that uses big descriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC metric. It is also an order of magnitude faster.



### Compressive hyperspectral imaging via adaptive sampling and dictionary learning
- **Arxiv ID**: http://arxiv.org/abs/1512.00901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1512.00901v1)
- **Published**: 2015-12-02 23:13:04+00:00
- **Updated**: 2015-12-02 23:13:04+00:00
- **Authors**: Mingrui Yang, Frank de Hoog, Yuqi Fan, Wen Hu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new sampling strategy for hyperspectral signals that is based on dictionary learning and singular value decomposition (SVD). Specifically, we first learn a sparsifying dictionary from training spectral data using dictionary learning. We then perform an SVD on the dictionary and use the first few left singular vectors as the rows of the measurement matrix to obtain the compressive measurements for reconstruction. The proposed method provides significant improvement over the conventional compressive sensing approaches. The reconstruction performance is further improved by reconditioning the sensing matrix using matrix balancing. We also demonstrate that the combination of dictionary learning and SVD is robust by applying them to different datasets.



### Innovation Pursuit: A New Approach to Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1512.00907v5
- **DOI**: 10.1109/TSP.2017.2749206
- **Categories**: **cs.CV**, cs.IR, cs.IT, cs.LG, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1512.00907v5)
- **Published**: 2015-12-02 23:52:43+00:00
- **Updated**: 2017-11-26 15:24:33+00:00
- **Authors**: Mostafa Rahmani, George Atia
- **Comment**: None
- **Journal**: IEEE Transactions on Signal Processing ( Volume: 65, Issue: 23,
  Dec.1, 1 2017 )
- **Summary**: In subspace clustering, a group of data points belonging to a union of subspaces are assigned membership to their respective subspaces. This paper presents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of subspace clustering using a new geometrical idea whereby subspaces are identified based on their relative novelties. We present two frameworks in which the idea of innovation pursuit is used to distinguish the subspaces. Underlying the first framework is an iterative method that finds the subspaces consecutively by solving a series of simple linear optimization problems, each searching for a direction of innovation in the span of the data potentially orthogonal to all subspaces except for the one to be identified in one step of the algorithm. A detailed mathematical analysis is provided establishing sufficient conditions for iPursuit to correctly cluster the data. The proposed approach can provably yield exact clustering even when the subspaces have significant intersections. It is shown that the complexity of the iterative approach scales only linearly in the number of data points and subspaces, and quadratically in the dimension of the subspaces. The second framework integrates iPursuit with spectral clustering to yield a new variant of spectral-clustering-based algorithms. The numerical simulations with both real and synthetic data demonstrate that iPursuit can often outperform the state-of-the-art subspace clustering algorithms, more so for subspaces with significant intersections, and that it significantly improves the state-of-the-art result for subspace-segmentation-based face clustering.




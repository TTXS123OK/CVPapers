# Arxiv Papers in cs.CV on 2015-11-22
### Gradual DropIn of Layers to Train Very Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.06951v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06951v1)
- **Published**: 2015-11-22 02:33:08+00:00
- **Updated**: 2015-11-22 02:33:08+00:00
- **Authors**: Leslie N. Smith, Emily M. Hand, Timothy Doster
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the concept of dynamically growing a neural network during training. In particular, an untrainable deep network starts as a trainable shallow network and newly added layers are slowly, organically added during training, thereby increasing the network's depth. This is accomplished by a new layer, which we call DropIn. The DropIn layer starts by passing the output from a previous layer (effectively skipping over the newly added layers), then increasingly including units from the new layers for both feedforward and backpropagation. We show that deep networks, which are untrainable with conventional methods, will converge with DropIn layers interspersed in the architecture. In addition, we demonstrate that DropIn provides regularization during training in an analogous way as dropout. Experiments are described with the MNIST dataset and various expanded LeNet architectures, CIFAR-10 dataset with its architecture expanded from 3 to 11 layers, and on the ImageNet dataset with the AlexNet architecture expanded to 13 layers and the VGG 16-layer architecture.



### Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources
- **Arxiv ID**: http://arxiv.org/abs/1511.06973v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06973v2)
- **Published**: 2015-11-22 07:08:14+00:00
- **Updated**: 2016-04-14 08:09:08+00:00
- **Authors**: Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, Anton van den Hengel
- **Comment**: Accepted to IEEE Conf. Computer Vision and Pattern Recognition
- **Journal**: None
- **Summary**: We propose a method for visual question answering which combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. This allows more complex questions to be answered using the predominant neural network-based approach than has previously been possible. It particularly allows questions to be asked about the contents of an image, even when the image itself does not contain the whole answer. The method constructs a textual representation of the semantic content of an image, and merges it with textual information sourced from a knowledge base, to develop a deeper understanding of the scene viewed. Priming a recurrent neural network with this combined information, and the submitted question, leads to a very flexible visual question answering approach. We are specifically able to answer questions posed in natural language, that refer to information not contained in the image. We demonstrate the effectiveness of our model on two publicly available datasets, Toronto COCO-QA and MS COCO-VQA and show that it produces the best reported results in both cases.



### End-to-end Learning of Action Detection from Frame Glimpses in Videos
- **Arxiv ID**: http://arxiv.org/abs/1511.06984v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06984v2)
- **Published**: 2015-11-22 09:41:50+00:00
- **Updated**: 2017-03-13 07:33:15+00:00
- **Authors**: Serena Yeung, Olga Russakovsky, Greg Mori, Li Fei-Fei
- **Comment**: Update to version in CVPR 2016 proceedings
- **Journal**: None
- **Summary**: In this work we introduce a fully end-to-end approach for action detection in videos that learns to directly predict the temporal bounds of actions. Our intuition is that the process of detecting actions is naturally one of observation and refinement: observing moments in video, and refining hypotheses about when an action is occurring. Based on this insight, we formulate our model as a recurrent neural network-based agent that interacts with a video over time. The agent observes video frames and decides both where to look next and when to emit a prediction. Since backpropagation is not adequate in this non-differentiable setting, we use REINFORCE to learn the agent's decision policy. Our model achieves state-of-the-art results on the THUMOS'14 and ActivityNet datasets while observing only a fraction (2% or less) of the video frames.



### Learning High-level Prior with Convolutional Neural Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1511.06988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06988v1)
- **Published**: 2015-11-22 10:25:02+00:00
- **Updated**: 2015-11-22 10:25:02+00:00
- **Authors**: Haitian Zheng, Yebin Liu, Mengqi Ji, Feng Wu, Lu Fang
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: This paper proposes a convolutional neural network that can fuse high-level prior for semantic image segmentation. Motivated by humans' vision recognition system, our key design is a three-layer generative structure consisting of high-level coding, middle-level segmentation and low-level image to introduce global prior for semantic segmentation. Based on this structure, we proposed a generative model called conditional variational auto-encoder (CVAE) that can build up the links behind these three layers. These important links include an image encoder that extracts high level info from image, a segmentation encoder that extracts high level info from segmentation, and a hybrid decoder that outputs semantic segmentation from the high level prior and input image. We theoretically derive the semantic segmentation as an optimization problem parameterized by these links. Finally, the optimization problem enables us to take advantage of state-of-the-art fully convolutional network structure for the implementation of the above encoders and decoder. Experimental results on several representative datasets demonstrate our supreme performance for semantic segmentation.



### SceneNet: Understanding Real World Indoor Scenes With Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1511.07041v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07041v2)
- **Published**: 2015-11-22 17:59:49+00:00
- **Updated**: 2015-11-26 22:09:09+00:00
- **Authors**: Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: Scene understanding is a prerequisite to many high level tasks for any automated intelligent machine operating in real world environments. Recent attempts with supervised learning have shown promise in this direction but also highlighted the need for enormous quantity of supervised data --- performance increases in proportion to the amount of data used. However, this quickly becomes prohibitive when considering the manual labour needed to collect such data. In this work, we focus our attention on depth based semantic per-pixel labelling as a scene understanding problem and show the potential of computer graphics to generate virtually unlimited labelled data from synthetic 3D scenes. By carefully synthesizing training data with appropriate noise models we show comparable performance to state-of-the-art RGBD systems on NYUv2 dataset despite using only depth data as input and set a benchmark on depth-based segmentation on SUN RGB-D dataset. Additionally, we offer a route to generating synthesized frame or video data, and understanding of different factors influencing performance gains.



### ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1511.07053v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.07053v3)
- **Published**: 2015-11-22 19:25:27+00:00
- **Updated**: 2016-05-24 15:55:41+00:00
- **Authors**: Francesco Visin, Marco Ciccone, Adriana Romero, Kyle Kastner, Kyunghyun Cho, Yoshua Bengio, Matteo Matteucci, Aaron Courville
- **Comment**: In CVPR Deep Vision Workshop, 2016
- **Journal**: None
- **Summary**: We propose a structured prediction architecture, which exploits the local generic features extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed architecture, called ReSeg, is based on the recently introduced ReNet model for image classification. We modify and extend it to perform the more challenging task of semantic segmentation. Each ReNet layer is composed of four RNN that sweep the image horizontally and vertically in both directions, encoding patches or activations, and providing relevant global information. Moreover, ReNet layers are stacked on top of pre-trained convolutional layers, benefiting from generic local features. Upsampling layers follow ReNet layers to recover the original image resolution in the final predictions. The proposed ReSeg architecture is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse, Oxford Flower, and CamVid; achieving state-of-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applications in other structured prediction problems. The source code and model hyperparameters are available on https://github.com/fvisin/reseg.



### Fine-grained pose prediction, normalization, and recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.07063v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07063v1)
- **Published**: 2015-11-22 20:32:45+00:00
- **Updated**: 2015-11-22 20:32:45+00:00
- **Authors**: Ning Zhang, Evan Shelhamer, Yang Gao, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Pose variation and subtle differences in appearance are key challenges to fine-grained classification. While deep networks have markedly improved general recognition, many approaches to fine-grained recognition rely on anchoring networks to parts for better accuracy. Identifying parts to find correspondence discounts pose variation so that features can be tuned to appearance. To this end previous methods have examined how to find parts and extract pose-normalized features. These methods have generally separated fine-grained recognition into stages which first localize parts using hand-engineered and coarsely-localized proposal features, and then separately learn deep descriptors centered on inferred part positions. We unify these steps in an end-to-end trainable network supervised by keypoint locations and class labels that localizes parts by a fully convolutional network to focus the learning of feature representations for the fine-grained classification task. Experiments on the popular CUB200 dataset show that our method is state-of-the-art and suggest a continuing role for strong supervision.



### Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes
- **Arxiv ID**: http://arxiv.org/abs/1511.07067v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1511.07067v2)
- **Published**: 2015-11-22 20:46:42+00:00
- **Updated**: 2016-06-29 18:15:25+00:00
- **Authors**: Satwik Kottur, Ramakrishna Vedantam, Jos√© M. F. Moura, Devi Parikh
- **Comment**: 15 pages, 11 figures
- **Journal**: None
- **Summary**: We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, although "eats" and "stares at" seem unrelated in text, they share semantics visually. When people are eating something, they also tend to stare at the food. Grounding diverse relations like "eats" and "stares at" into vision remains challenging, despite recent progress in vision. We note that the visual grounding of words depends on semantics, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained, visually grounded notions of semantic relatedness. We show improvements over text-only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets are available online.



### Auxiliary Image Regularization for Deep CNNs with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1511.07069v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07069v2)
- **Published**: 2015-11-22 20:59:19+00:00
- **Updated**: 2016-03-02 07:21:08+00:00
- **Authors**: Samaneh Azadi, Jiashi Feng, Stefanie Jegelka, Trevor Darrell
- **Comment**: Published as a conference paper at ICLR 2016
- **Journal**: None
- **Summary**: Precisely-labeled data sets with sufficient amount of samples are very important for training deep convolutional neural networks (CNNs). However, many of the available real-world data sets contain erroneously labeled samples and those errors substantially hinder the learning of very accurate CNN models. In this work, we consider the problem of training a deep CNN model for image classification with mislabeled training samples - an issue that is common in real image data sets with tags supplied by amateur users. To solve this problem, we propose an auxiliary image regularization technique, optimized by the stochastic Alternating Direction Method of Multipliers (ADMM) algorithm, that automatically exploits the mutual context information among training images and encourages the model to select reliable images to robustify the learning process. Comprehensive experiments on benchmark data sets clearly demonstrate our proposed regularized CNN model is resistant to label noise in training data.




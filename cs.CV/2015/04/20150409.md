# Arxiv Papers in cs.CV on 2015-04-09
### Linearly Supporting Feature Extraction For Automated Estimation Of Stellar Atmospheric Parameters
- **Arxiv ID**: http://arxiv.org/abs/1504.02164v2
- **DOI**: 10.1088/0067-0049/218/1/3
- **Categories**: **astro-ph.SR**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1504.02164v2)
- **Published**: 2015-04-09 01:09:13+00:00
- **Updated**: 2015-04-10 01:17:04+00:00
- **Authors**: Xiangru Li, Yu Lu, Georges Comte, Ali Luo, Yongheng Zhao, Yongjun Wang
- **Comment**: 21 pages, 7 figures, 8 tables, The Astrophysical Journal Supplement
  Series (accepted for publication)
- **Journal**: ApJS, 2015, 218(1): 3
- **Summary**: We describe a scheme to extract linearly supporting (LSU) features from stellar spectra to automatically estimate the atmospheric parameters $T_{eff}$, log$~g$, and [Fe/H]. "Linearly supporting" means that the atmospheric parameters can be accurately estimated from the extracted features through a linear model. The successive steps of the process are as follow: first, decompose the spectrum using a wavelet packet (WP) and represent it by the derived decomposition coefficients; second, detect representative spectral features from the decomposition coefficients using the proposed method Least Absolute Shrinkage and Selection Operator (LARS)$_{bs}$; third, estimate the atmospheric parameters $T_{eff}$, log$~g$, and [Fe/H] from the detected features using a linear regression method. One prominent characteristic of this scheme is its ability to evaluate quantitatively the contribution of each detected feature to the atmospheric parameter estimate and also to trace back the physical significance of that feature. This work also shows that the usefulness of a component depends on both wavelength and frequency. The proposed scheme has been evaluated on both real spectra from the Sloan Digital Sky Survey (SDSS)/SEGUE and synthetic spectra calculated from Kurucz's NEWODF models. On real spectra, we extracted 23 features to estimate $T_{eff}$, 62 features for log$~g$, and 68 features for [Fe/H]. Test consistencies between our estimates and those provided by the Spectroscopic Sarameter Pipeline of SDSS show that the mean absolute errors (MAEs) are 0.0062 dex for log$~T_{eff}$ (83 K for $T_{eff}$), 0.2345 dex for log$~g$, and 0.1564 dex for [Fe/H]. For the synthetic spectra, the MAE test accuracies are 0.0022 dex for log$~T_{eff}$ (32 K for $T_{eff}$), 0.0337 dex for log$~g$, and 0.0268 dex for [Fe/H].



### Connectivity Preserving Multivalued Functions in Digital Topology
- **Arxiv ID**: http://arxiv.org/abs/1504.02174v2
- **DOI**: None
- **Categories**: **cs.CV**, math.GN, I.4.m
- **Links**: [PDF](http://arxiv.org/pdf/1504.02174v2)
- **Published**: 2015-04-09 02:05:02+00:00
- **Updated**: 2015-12-23 15:48:27+00:00
- **Authors**: Laurence Boxer, P. Christopher Staecker
- **Comment**: small changes
- **Journal**: None
- **Summary**: We study connectivity preserving multivalued functions between digital images. This notion generalizes that of continuous multivalued functions studied mostly in the setting of the digital plane $Z^2$. We show that connectivity preserving multivalued functions, like continuous multivalued functions, are appropriate models for digital morpholological operations. Connectivity preservation, unlike continuity, is preserved by compositions, and generalizes easily to higher dimensions and arbitrary adjacency relations.



### A Multiphase Image Segmentation Based on Fuzzy Membership Functions and L1-norm Fidelity
- **Arxiv ID**: http://arxiv.org/abs/1504.02206v2
- **DOI**: 10.1007/s10915-016-0183-z
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1504.02206v2)
- **Published**: 2015-04-09 07:21:02+00:00
- **Updated**: 2016-02-12 21:06:37+00:00
- **Authors**: Fang Li, Stanley Osher, Jing Qin, Ming Yan
- **Comment**: 28 pages, 8 figures, 3 tables
- **Journal**: Journal of Scientific Computing, 69 (2016), 82-106
- **Summary**: In this paper, we propose a variational multiphase image segmentation model based on fuzzy membership functions and L1-norm fidelity. Then we apply the alternating direction method of multipliers to solve an equivalent problem. All the subproblems can be solved efficiently. Specifically, we propose a fast method to calculate the fuzzy median. Experimental results and comparisons show that the L1-norm based method is more robust to outliers such as impulse noise and keeps better contrast than its L2-norm counterpart. Theoretically, we prove the existence of the minimizer and analyze the convergence of the algorithm.



### Extraction of Protein Sequence Motif Information using PSO K-Means
- **Arxiv ID**: http://arxiv.org/abs/1504.02235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.02235v1)
- **Published**: 2015-04-09 09:53:44+00:00
- **Updated**: 2015-04-09 09:53:44+00:00
- **Authors**: R. Gowri, R. Rathipriya
- **Comment**: None
- **Journal**: None
- **Summary**: The main objective of the paper is to find the motif information.The functionalities of the proteins are ideally found from their motif information which is extracted using various techniques like clustering with k-means, hybrid k-means, self-organising maps, etc., in the literature. In this work protein sequence information is extracted using optimised k-means algorithm. The particle swarm optimisation technique is one of the frequently used optimisation method. In the current work the PSO k-means is used for motif information extraction. This paper also deals with the comparison between the motif information obtained from clusters and biclustersusing PSO k-means algorithm. The motif information acquired is based on the structure homogeneity of the protein sequence.



### Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1504.02340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.02340v1)
- **Published**: 2015-04-09 14:57:32+00:00
- **Updated**: 2015-04-09 14:57:32+00:00
- **Authors**: Wongun Choi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on the two key aspects of multiple target tracking problem: 1) designing an accurate affinity measure to associate detections and 2) implementing an efficient and accurate (near) online multiple target tracking algorithm. As the first contribution, we introduce a novel Aggregated Local Flow Descriptor (ALFD) that encodes the relative motion pattern between a pair of temporally distant detections using long term interest point trajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust affinity measure for estimating the likelihood of matching detections regardless of the application scenarios. As another contribution, we present a Near-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is formulated as a data-association between targets and detections in a temporal window, that is performed repeatedly at every frame. While being efficient, NOMT achieves robustness via integrating multiple cues including ALFD metric, target dynamics, appearance similarity, and long term trajectory regularization into the model. Our ablative analysis verifies the superiority of the ALFD metric over the other conventional affinity metrics. We run a comprehensive experimental evaluation on two challenging tracking datasets, KITTI and MOT datasets. The NOMT method combined with ALFD metric achieves the best accuracy in both datasets with significant margins (about 10% higher MOTA) over the state-of-the-arts.



### When Face Recognition Meets with Deep Learning: an Evaluation of Convolutional Neural Networks for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1504.02351v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1504.02351v1)
- **Published**: 2015-04-09 15:27:49+00:00
- **Updated**: 2015-04-09 15:27:49+00:00
- **Authors**: Guosheng Hu, Yongxin Yang, Dong Yi, Josef Kittler, William Christmas, Stan Z. Li, Timothy Hospedales
- **Comment**: 7 pages, 4 figures, 7 tables
- **Journal**: None
- **Summary**: Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluate the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available.



### Exploring EEG for Object Detection and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1504.02356v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.IR, H.1.2; H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/1504.02356v1)
- **Published**: 2015-04-09 15:43:52+00:00
- **Updated**: 2015-04-09 15:43:52+00:00
- **Authors**: Eva Mohedano, Amaia Salvador, Sergi Porta, Xavier Gir칩-i-Nieto, Graham Healy, Kevin McGuinness, Noel O'Connor, Alan F. Smeaton
- **Comment**: This preprint is the full version of a short paper accepted in the
  ACM International Conference on Multimedia Retrieval (ICMR) 2015 (Shanghai,
  China)
- **Journal**: None
- **Summary**: This paper explores the potential for using Brain Computer Interfaces (BCI) as a relevance feedback mechanism in content-based image retrieval. We investigate if it is possible to capture useful EEG signals to detect if relevant objects are present in a dataset of realistic and complex images. We perform several experiments using a rapid serial visual presentation (RSVP) of images at different rates (5Hz and 10Hz) on 8 users with different degrees of familiarization with BCI and the dataset. We then use the feedback from the BCI and mouse-based interfaces to retrieve localized objects in a subset of TRECVid images. We show that it is indeed possible to detect such objects in complex images and, also, that users with previous knowledge on the dataset or experience with the RSVP outperform others. When the users have limited time to annotate the images (100 seconds in our experiments) both interfaces are comparable in performance. Comparing our best users in a retrieval task, we found that EEG-based relevance feedback outperforms mouse-based feedback. The realistic and complex image dataset differentiates our work from previous studies on EEG for image retrieval.



### Real-time Monocular Object SLAM
- **Arxiv ID**: http://arxiv.org/abs/1504.02398v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1504.02398v1)
- **Published**: 2015-04-09 17:46:19+00:00
- **Updated**: 2015-04-09 17:46:19+00:00
- **Authors**: Dorian G치lvez-L칩pez, Marta Salas, Juan D. Tard칩s, J. M. M. Montiel
- **Comment**: None
- **Journal**: None
- **Summary**: We present a real-time object-based SLAM system that leverages the largest object database to date. Our approach comprises two main components: 1) a monocular SLAM algorithm that exploits object rigidity constraints to improve the map and find its real scale, and 2) a novel object recognition algorithm based on bags of binary words, which provides live detections with a database of 500 3D objects. The two components work together and benefit each other: the SLAM algorithm accumulates information from the observations of the objects, anchors object features to especial map landmarks and sets constrains on the optimization. At the same time, objects partially or fully located within the map are used as a prior to guide the recognition algorithm, achieving higher recall. We evaluate our proposal on five real environments showing improvements on the accuracy of the map and efficiency with respect to other state-of-the-art techniques.



### Predicting Complete 3D Models of Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1504.02437v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.02437v3)
- **Published**: 2015-04-09 19:25:33+00:00
- **Updated**: 2017-08-18 01:55:57+00:00
- **Authors**: Ruiqi Guo, Chuhang Zou, Derek Hoiem
- **Comment**: None
- **Journal**: None
- **Summary**: One major goal of vision is to infer physical models of objects, surfaces, and their layout from sensors. In this paper, we aim to interpret indoor scenes from one RGBD image. Our representation encodes the layout of walls, which must conform to a Manhattan structure but is otherwise flexible, and the layout and extent of objects, modeled with CAD-like 3D shapes. We represent both the visible and occluded portions of the scene, producing a complete 3D parse. Such a scene interpretation is useful for robotics and visual reasoning, but difficult to produce due to the well-known challenge of segmentation, the high degree of occlusion, and the diversity of objects in indoor scene. We take a data-driven approach, generating sets of potential object regions, matching to regions in training images, and transferring and aligning associated 3D models while encouraging fit to observations and overall consistency. We demonstrate encouraging results on the NYU v2 dataset and highlight a variety of interesting directions for future work.



### What Do Deep CNNs Learn About Objects?
- **Arxiv ID**: http://arxiv.org/abs/1504.02485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.02485v1)
- **Published**: 2015-04-09 20:02:03+00:00
- **Updated**: 2015-04-09 20:02:03+00:00
- **Authors**: Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko
- **Comment**: 2 pages workshop paper. arXiv admin note: substantial text overlap
  with arXiv:1412.7122
- **Journal**: None
- **Summary**: Deep convolutional neural networks learn extremely powerful image representations, yet most of that power is hidden in the millions of deep-layer parameters. What exactly do these parameters represent? Recent work has started to analyse CNN representations, finding that, e.g., they are invariant to some 2D transformations Fischer et al. (2014), but are confused by particular types of image noise Nguyen et al. (2014). In this work, we delve deeper and ask: how invariant are CNNs to object-class variations caused by 3D shape, pose, and photorealism?



### Unsupervised Feature Learning from Temporal Data
- **Arxiv ID**: http://arxiv.org/abs/1504.02518v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1504.02518v2)
- **Published**: 2015-04-09 23:26:26+00:00
- **Updated**: 2015-04-15 23:08:30+00:00
- **Authors**: Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1412.6056
- **Journal**: None
- **Summary**: Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.




# Arxiv Papers in cs.CV on 2015-07-29
### Adapted sampling for 3D X-ray computed tomography
- **Arxiv ID**: http://arxiv.org/abs/1507.08030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.08030v1)
- **Published**: 2015-07-29 06:30:04+00:00
- **Updated**: 2015-07-29 06:30:04+00:00
- **Authors**: Anthony Cazasnoves, Fanny Buyens, Sylvie Sevestre
- **Comment**: The 13th International Meeting on Fully Three-Dimensional Image
  Reconstruction in Radiology and Nuclear Medicine
- **Journal**: None
- **Summary**: In this paper, we introduce a method to build an adapted mesh representation of a 3D object for X-Ray tomography reconstruction. Using this representation, we provide means to reduce the computational cost of reconstruction by way of iterative algorithms. The adapted sampling of the reconstruction space is directly obtained from the projection dataset and prior to any reconstruction. It is built following two stages : firstly, 2D structural information is extracted from the projection images and is secondly merged in 3D to obtain a 3D pointcloud sampling the interfaces of the object. A relevant mesh is then built from this cloud by way of tetrahedralization. Critical parameters selections have been automatized through a statistical framework, thus avoiding dependence on users expertise. Applying this approach on geometrical shapes and on a 3D Shepp-Logan phantom, we show the relevance of such a sampling - obtained in a few seconds - and the drastic decrease in cells number to be estimated during reconstruction when compared to the usual regular voxel lattice. A first iterative reconstruction of the Shepp-Logan using this kind of sampling shows the relevant advantages in terms of low dose or sparse acquisition sampling contexts. The method can also prove useful for other applications such as finite element method computations.



### Collaborative Representation Classification Ensemble for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1507.08064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.08064v1)
- **Published**: 2015-07-29 08:47:49+00:00
- **Updated**: 2015-07-29 08:47:49+00:00
- **Authors**: Xiaochao Qu, Suah Kim, Run Cui, Hyoung Joong Kim
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Collaborative Representation Classification (CRC) for face recognition attracts a lot attention recently due to its good recognition performance and fast speed. Compared to Sparse Representation Classification (SRC), CRC achieves a comparable recognition performance with 10-1000 times faster speed. In this paper, we propose to ensemble several CRC models to promote the recognition rate, where each CRC model uses different and divergent randomly generated biologically-inspired features as the face representation. The proposed ensemble algorithm calculates an ensemble weight for each CRC model that guided by the underlying classification rule of CRC. The obtained weights reflect the confidences of those CRC models where the more confident CRC models have larger weights. The proposed weighted ensemble method proves to be very effective and improves the performance of each CRC model significantly. Extensive experiments are conducted to show the superior performance of the proposed method.



### Cross-pose Face Recognition by Canonical Correlation Analysis
- **Arxiv ID**: http://arxiv.org/abs/1507.08076v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.08076v1)
- **Published**: 2015-07-29 09:28:11+00:00
- **Updated**: 2015-07-29 09:28:11+00:00
- **Authors**: Annan Li, Shiguang Shan, Xilin Chen, Bingpeng Ma, Shuicheng Yan, Wen Gao
- **Comment**: None
- **Journal**: None
- **Summary**: The pose problem is one of the bottlenecks in automatic face recognition. We argue that one of the diffculties in this problem is the severe misalignment in face images or feature vectors with different poses. In this paper, we propose that this problem can be statistically solved or at least mitigated by maximizing the intra-subject across-pose correlations via canonical correlation analysis (CCA). In our method, based on the data set with coupled face images of the same identities and across two different poses, CCA learns simultaneously two linear transforms, each for one pose. In the transformed subspace, the intra-subject correlations between the different poses are maximized, which implies pose-invariance or pose-robustness is achieved. The experimental results show that our approach could considerably improve the recognition performance. And if further enhanced with holistic+local feature representation, the performance could be comparable to the state-of-the-art.



### Tracking Randomly Moving Objects on Edge Box Proposals
- **Arxiv ID**: http://arxiv.org/abs/1507.08085v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.08085v2)
- **Published**: 2015-07-29 09:59:42+00:00
- **Updated**: 2015-11-29 23:51:02+00:00
- **Authors**: Gao Zhu, Fatih Porikli, Hongdong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Most tracking-by-detection methods employ a local search window around the predicted object location in the current frame assuming the previous location is accurate, the trajectory is smooth, and the computational capacity permits a search radius that can accommodate the maximum speed yet small enough to reduce mismatches. These, however, may not be valid always, in particular for fast and irregularly moving objects. Here, we present an object tracker that is not limited to a local search window and has ability to probe efficiently the entire frame. Our method generates a small number of "high-quality" proposals by a novel instance-specific objectness measure and evaluates them against the object model that can be adopted from an existing tracking-by-detection approach as a core tracker. During the tracking process, we update the object model concentrating on hard false-positives supplied by the proposals, which help suppressing distractors caused by difficult background clutters, and learn how to re-rank proposals according to the object model. Since we reduce significantly the number of hypotheses the core tracker evaluates, we can use richer object descriptors and stronger detector. Our method outperforms most recent state-of-the-art trackers on popular tracking benchmarks, and provides improved robustness for fast moving objects as well as for ultra low-frame-rate videos.



### IT-Dendrogram: A New Member of the In-Tree (IT) Clustering Family
- **Arxiv ID**: http://arxiv.org/abs/1507.08155v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1507.08155v1)
- **Published**: 2015-07-29 14:22:13+00:00
- **Updated**: 2015-07-29 14:22:13+00:00
- **Authors**: Teng Qiu, Yongjie Li
- **Comment**: 13 pages, 6 figures. IT-Dendrogram: An Effective Method to Visualize
  the In-Tree structure by Dendrogram
- **Journal**: None
- **Summary**: Previously, we proposed a physically-inspired method to construct data points into an effective in-tree (IT) structure, in which the underlying cluster structure in the dataset is well revealed. Although there are some edges in the IT structure requiring to be removed, such undesired edges are generally distinguishable from other edges and thus are easy to be determined. For instance, when the IT structures for the 2-dimensional (2D) datasets are graphically presented, those undesired edges can be easily spotted and interactively determined. However, in practice, there are many datasets that do not lie in the 2D Euclidean space, thus their IT structures cannot be graphically presented. But if we can effectively map those IT structures into a visualized space in which the salient features of those undesired edges are preserved, then the undesired edges in the IT structures can still be visually determined in a visualization environment. Previously, this purpose was reached by our method called IT-map. The outstanding advantage of IT-map is that clusters can still be found even with the so-called crowding problem in the embedding.   In this paper, we propose another method, called IT-Dendrogram, to achieve the same goal through an effective combination of the IT structure and the single link hierarchical clustering (SLHC) method. Like IT-map, IT-Dendrogram can also effectively represent the IT structures in a visualization environment, whereas using another form, called the Dendrogram. IT-Dendrogram can serve as another visualization method to determine the undesired edges in the IT structures and thus benefit the IT-based clustering analysis. This was demonstrated on several datasets with different shapes, dimensions, and attributes. Unlike IT-map, IT-Dendrogram can always avoid the crowding problem, which could help users make more reliable cluster analysis in certain problems.



### Fast Robust PCA on Graphs
- **Arxiv ID**: http://arxiv.org/abs/1507.08173v2
- **DOI**: 10.1109/JSTSP.2016.2555239
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.08173v2)
- **Published**: 2015-07-29 14:53:33+00:00
- **Updated**: 2016-01-25 20:29:57+00:00
- **Authors**: Nauman Shahid, Nathanael Perraudin, Vassilis Kalofolias, Gilles Puy, Pierre Vandergheynst
- **Comment**: None
- **Journal**: None
- **Summary**: Mining useful clusters from high dimensional data has received significant attention of the computer vision and pattern recognition community in the recent years. Linear and non-linear dimensionality reduction has played an important role to overcome the curse of dimensionality. However, often such methods are accompanied with three different problems: high computational complexity (usually associated with the nuclear norm minimization), non-convexity (for matrix factorization methods) and susceptibility to gross corruptions in the data. In this paper we propose a principal component analysis (PCA) based solution that overcomes these three issues and approximates a low-rank recovery method for high dimensional datasets. We target the low-rank recovery by enforcing two types of graph smoothness assumptions, one on the data samples and the other on the features by designing a convex optimization problem. The resulting algorithm is fast, efficient and scalable for huge datasets with O(nlog(n)) computational complexity in the number of data samples. It is also robust to gross corruptions in the dataset as well as to the model parameters. Clustering experiments on 7 benchmark datasets with different types of corruptions and background separation experiments on 3 video datasets show that our proposed model outperforms 10 state-of-the-art dimensionality reduction models. Our theoretical analysis proves that the proposed model is able to recover approximate low-rank representations with a bounded error for clusterable data.



### Beamforming through regularized inverse problems in ultrasound medical imaging
- **Arxiv ID**: http://arxiv.org/abs/1507.08184v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.08184v2)
- **Published**: 2015-07-29 15:29:09+00:00
- **Updated**: 2016-05-18 22:34:36+00:00
- **Authors**: Teodora Szasz, Adrian Basarab, Denis Kouam√©
- **Comment**: None
- **Journal**: None
- **Summary**: Beamforming in ultrasound imaging has significant impact on the quality of the final image, controlling its resolution and contrast. Despite its low spatial resolution and contrast, delay-and-sum is still extensively used nowadays in clinical applications, due to its real-time capabilities. The most common alternatives are minimum variance method and its variants, which overcome the drawbacks of delay-and-sum, at the cost of higher computational complexity that limits its utilization in real-time applications.   In this paper, we propose to perform beamforming in ultrasound imaging through a regularized inverse problem based on a linear model relating the reflected echoes to the signal to be recovered. Our approach presents two major advantages: i) its flexibility in the choice of statistical assumptions on the signal to be beamformed (Laplacian and Gaussian statistics are tested herein) and ii) its robustness to a reduced number of pulse emissions. The proposed framework is flexible and allows for choosing the right trade-off between noise suppression and sharpness of the resulted image. We illustrate the performance of our approach on both simulated and experimental data, with \textit{in vivo} examples of carotid and thyroid. Compared to delay-and-sum, minimimum variance and two other recently published beamforming techniques, our method offers better spatial resolution, respectively contrast, when using Laplacian and Gaussian priors.



### Deep Learning for Single-View Instance Recognition
- **Arxiv ID**: http://arxiv.org/abs/1507.08286v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1507.08286v1)
- **Published**: 2015-07-29 20:11:12+00:00
- **Updated**: 2015-07-29 20:11:12+00:00
- **Authors**: David Held, Sebastian Thrun, Silvio Savarese
- **Comment**: 16 pages, 15 figures
- **Journal**: None
- **Summary**: Deep learning methods have typically been trained on large datasets in which many training examples are available. However, many real-world product datasets have only a small number of images available for each product. We explore the use of deep learning methods for recognizing object instances when we have only a single training example per class. We show that feedforward neural networks outperform state-of-the-art methods for recognizing objects from novel viewpoints even when trained from just a single image per object. To further improve our performance on this task, we propose to take advantage of a supplementary dataset in which we observe a separate set of objects from multiple viewpoints. We introduce a new approach for training deep learning methods for instance recognition with limited training data, in which we use an auxiliary multi-view dataset to train our network to be robust to viewpoint changes. We find that this approach leads to a more robust classifier for recognizing objects from novel viewpoints, outperforming previous state-of-the-art approaches including keypoint-matching, template-based techniques, and sparse coding.




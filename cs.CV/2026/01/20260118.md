# Arxiv Papers in cs.CV on 2026-01-18
### Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion
- **Arxiv ID**: http://arxiv.org/abs/2601.12224v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.12224v1)
- **Published**: 2026-01-18 02:14:08+00:00
- **Updated**: 2026-01-18 02:14:08+00:00
- **Authors**: Meng Wei, Kun Yuan, Shi Li, Yue Zhou, Long Bai, Nassir Navab, Hongliang Ren, Hong Joo Lee, Tom Vercauteren, Nicolas Padoy
- **Comment**: None
- **Journal**: AAAI 2026
- **Summary**: Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.



### DiffusionQC: Artifact Detection in Histopathology via Diffusion Model
- **Arxiv ID**: http://arxiv.org/abs/2601.12233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12233v1)
- **Published**: 2026-01-18 02:59:26+00:00
- **Updated**: 2026-01-18 02:59:26+00:00
- **Authors**: Zhenzhen Wang, Zhongliang Zhou, Zhuoyu Wen, Jeong Hwan Kook, John B Wojcik, John Kang
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Digital pathology plays a vital role across modern medicine, offering critical insights for disease diagnosis, prognosis, and treatment. However, histopathology images often contain artifacts introduced during slide preparation and digitization. Detecting and excluding them is essential to ensure reliable downstream analysis. Traditional supervised models typically require large annotated datasets, which is resource-intensive and not generalizable to novel artifact types. To address this, we propose DiffusionQC, which detects artifacts as outliers among clean images using a diffusion model. It requires only a set of clean images for training rather than pixel-level artifact annotations and predefined artifact types. Furthermore, we introduce a contrastive learning module to explicitly enlarge the distribution separation between artifact and clean images, yielding an enhanced version of our method. Empirical results demonstrate superior performance to state-of-the-art and offer cross-stain generalization capacity, with significantly less data and annotations.



### Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models
- **Arxiv ID**: http://arxiv.org/abs/2601.12234v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.12234v1)
- **Published**: 2026-01-18 03:08:08+00:00
- **Updated**: 2026-01-18 03:08:08+00:00
- **Authors**: Fadlullah Raji, Stefano Petrangeli, Matheus Gadelha, Yu Shen, Uttaran Bhattacharya, Gang Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Generating 3D models has traditionally been a complex task requiring specialized expertise. While recent advances in generative AI have sought to automate this process, existing methods produce non-editable representation, such as meshes or point clouds, limiting their adaptability for iterative design. In this paper, we introduce Proc3D, a system designed to generate editable 3D models while enabling real-time modifications. At its core, Proc3D introduces procedural compact graph (PCG), a graph representation of 3D models, that encodes the algorithmic rules and structures necessary for generating the model. This representation exposes key parameters, allowing intuitive manual adjustments via sliders and checkboxes, as well as real-time, automated modifications through natural language prompts using Large Language Models (LLMs). We demonstrate Proc3D's capabilities using two generative approaches: GPT-4o with in-context learning (ICL) and a fine-tuned LLAMA-3 model. Experimental results show that Proc3D outperforms existing methods in editing efficiency, achieving more than 400x speedup over conventional approaches that require full regeneration for each modification. Additionally, Proc3D improves ULIP scores by 28%, a metric that evaluates the alignment between generated 3D models and text prompts. By enabling text-aligned 3D model generation along with precise, real-time parametric edits, Proc3D facilitates highly accurate text-based image editing applications.



### Less is More: Label-Guided Summarization of Procedural and Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/2601.12243v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.12243v1)
- **Published**: 2026-01-18 03:41:48+00:00
- **Updated**: 2026-01-18 03:41:48+00:00
- **Authors**: Shreya Rajpal, Michal Golovanesky, Carsten Eickhoff
- **Comment**: 22 pages, 6 figures
- **Journal**: None
- **Summary**: Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.



### An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2601.12249v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.12249v1)
- **Published**: 2026-01-18 03:55:33+00:00
- **Updated**: 2026-01-18 03:55:33+00:00
- **Authors**: Ehsan Sadeghi Pour, Mahdi Esmaeili, Morteza Romoozi
- **Comment**: 13 page
- **Journal**: None
- **Summary**: Breast cancer is one of the most common cancers among women worldwide, and its accurate and timely diagnosis plays a critical role in improving treatment outcomes. This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating the Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency. In this study, a comprehensive dataset of breast cancer images from INbreast, MIAS, and DDSM was preprocessed through data augmentation and contrast enhancement and resized to 227x227 pixels for model training. Leveraging the Transformer's ability to manage long-range dependencies with Self-Attention mechanisms, the proposed model achieved high accuracy in detecting cancerous masses, outperforming foundational models such as BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the proposed model include an accuracy of 98.5\%, sensitivity of 97.8\%, specificity of 96.3\%, F1-score of 98.2\%, and overall precision of 97.9\%. These metrics demonstrate a significant improvement over traditional methods and confirm the model's effectiveness in identifying cancerous masses in complex scenarios and large datasets. This model shows potential as a reliable and efficient tool for breast cancer diagnosis and can be effectively integrated into medical diagnostic systems.



### Breaking Coordinate Overfitting: Geometry-Aware WiFi Sensing for Cross-Layout 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2601.12252v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.12252v1)
- **Published**: 2026-01-18 04:13:09+00:00
- **Updated**: 2026-01-18 04:13:09+00:00
- **Authors**: Songming Jia, Yan Lu, Bin Liu, Xiang Zhang, Peng Zhao, Xinmeng Tang, Yelin Wei, Jinyang Huang, Huan Yan, Zhi Liu
- **Comment**: Accpeted by AMC Mobicom 2026
- **Journal**: None
- **Summary**: WiFi-based 3D human pose estimation offers a low-cost and privacy-preserving alternative to vision-based systems for smart interaction. However, existing approaches rely on visual 3D poses as supervision and directly regress CSI to a camera-based coordinate system. We find that this practice leads to coordinate overfitting: models memorize deployment-specific WiFi transceiver layouts rather than only learning activity-relevant representations, resulting in severe generalization failures. To address this challenge, we present PerceptAlign, the first geometry-conditioned framework for WiFi-based cross-layout pose estimation. PerceptAlign introduces a lightweight coordinate unification procedure that aligns WiFi and vision measurements in a shared 3D space using only two checkerboards and a few photos. Within this unified space, it encodes calibrated transceiver positions into high-dimensional embeddings and fuses them with CSI features, making the model explicitly aware of device geometry as a conditional variable. This design forces the network to disentangle human motion from deployment layouts, enabling robust and, for the first time, layout-invariant WiFi pose estimation. To support systematic evaluation, we construct the largest cross-domain 3D WiFi pose estimation dataset to date, comprising 21 subjects, 5 scenes, 18 actions, and 7 device layouts. Experiments show that PerceptAlign reduces in-domain error by 12.3% and cross-domain error by more than 60% compared to state-of-the-art baselines. These results establish geometry-conditioned learning as a viable path toward scalable and practical WiFi sensing.



### Federated Joint Learning for Domain and Class Generalization
- **Arxiv ID**: http://arxiv.org/abs/2601.12253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](https://arxiv.org/pdf/2601.12253v1)
- **Published**: 2026-01-18 04:24:11+00:00
- **Updated**: 2026-01-18 04:24:11+00:00
- **Authors**: Haoran Xu, Jiaze Li, Jianzhong Ju, Zhenbo Luo
- **Comment**: ICASSP 2026
- **Journal**: None
- **Summary**: Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization, termed \textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.



### DeepRAHT: Learning Predictive RAHT for Point Cloud Attribute Compression
- **Arxiv ID**: http://arxiv.org/abs/2601.12255v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IT, cs.MM
- **Links**: [PDF](https://arxiv.org/pdf/2601.12255v1)
- **Published**: 2026-01-18 04:33:45+00:00
- **Updated**: 2026-01-18 04:33:45+00:00
- **Authors**: Chunyang Fu, Tai Qin, Shiqi Wang, Zhu Li
- **Comment**: Accepted by AAAI 2026
- **Journal**: None
- **Summary**: Regional Adaptive Hierarchical Transform (RAHT) is an effective point cloud attribute compression (PCAC) method. However, its application in deep learning lacks research. In this paper, we propose an end-to-end RAHT framework for lossy PCAC based on the sparse tensor, called DeepRAHT. The RAHT transform is performed within the learning reconstruction process, without requiring manual RAHT for preprocessing. We also introduce the predictive RAHT to reduce bitrates and design a learning-based prediction model to enhance performance. Moreover, we devise a bitrate proxy that applies run-length coding to entropy model, achieving seamless variable-rate coding and improving robustness. DeepRAHT is a reversible and distortion-controllable framework, ensuring its lower bound performance and offering significant application potential. The experiments demonstrate that DeepRAHT is a high-performance, faster, and more robust solution than the baseline methods. Project Page: https://github.com/zb12138/DeepRAHT.



### Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy
- **Arxiv ID**: http://arxiv.org/abs/2601.12257v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG, cs.GR
- **Links**: [PDF](https://arxiv.org/pdf/2601.12257v1)
- **Published**: 2026-01-18 04:40:00+00:00
- **Updated**: 2026-01-18 04:40:00+00:00
- **Authors**: Fadlullah Raji, John Murray-Bruce
- **Comment**: None
- **Journal**: European Conference on Computer Vision (ECCV 2024)
- **Summary**: Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \textit{light-occluding} and \textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.



### DALD-PCAC: Density-Adaptive Learning Descriptor for Point Cloud Lossless Attribute Compression
- **Arxiv ID**: http://arxiv.org/abs/2601.12261v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.12261v1)
- **Published**: 2026-01-18 04:54:46+00:00
- **Updated**: 2026-01-18 04:54:46+00:00
- **Authors**: Chunyang Fu, Ge Li, Wei Gao, Shiqi Wang, Zhu Li, Shan Liu
- **Comment**: Accepted by TOMM
- **Journal**: None
- **Summary**: Recently, deep learning has significantly advanced the performance of point cloud geometry compression. However, the learning-based lossless attribute compression of point clouds with varying densities is under-explored. In this paper, we develop a learning-based framework, namely DALD-PCAC that leverages Levels of Detail (LoD) to tailor for point cloud lossless attribute compression. We develop a point-wise attention model using a permutation-invariant Transformer to tackle the challenges of sparsity and irregularity of point clouds during context modeling. We also propose a Density-Adaptive Learning Descriptor (DALD) capable of capturing structure and correlations among points across a large range of neighbors. In addition, we develop a prior-guided block partitioning to reduce the attribute variance within blocks and enhance the performance. Experiments on LiDAR and object point clouds show that DALD-PCAC achieves the state-of-the-art performance on most data. Our method boosts the compression performance and is robust to the varying densities of point clouds. Moreover, it guarantees a good trade-off between performance and complexity, exhibiting great potential in real-world applications. The source code is available at https://github.com/zb12138/DALD_PCAC.



### AgenticPruner: MAC-Constrained Neural Network Compression via LLM-Driven Strategy Search
- **Arxiv ID**: http://arxiv.org/abs/2601.12272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12272v1)
- **Published**: 2026-01-18 06:07:29+00:00
- **Updated**: 2026-01-18 06:07:29+00:00
- **Authors**: Shahrzad Esmat, Mahdi Banisharif, Ali Jannesari
- **Comment**: 38 pages, 2 figures, 14 tables
- **Journal**: None
- **Summary**: Neural network pruning remains essential for deploying deep learning models on resource-constrained devices, yet existing approaches primarily target parameter reduction without directly controlling computational cost. This yields unpredictable inference latency in deployment scenarios where strict Multiply-Accumulate (MAC) operation budgets must be met. We propose AgenticPruner, a framework utilizing large language models to achieve MAC-constrained optimization through iterative strategy learning. Our approach coordinates three specialized agents: a Profiling Agent that analyzes model architecture and MAC distributions, a Master Agent that orchestrates the workflow with divergence monitoring, and an Analysis Agent powered by Claude 3.5 Sonnet that learns optimal strategies from historical attempts. Through in-context learning, the Analysis Agent improves convergence success rate from 48% to 71% compared to grid search. Building upon isomorphic pruning's graph-based structural grouping, our method adds context-aware adaptation by analyzing patterns across pruning iterations, enabling automatic convergence to target MAC budgets within user-defined tolerance bands.   We validate our framework on ImageNet-1K across ResNet, ConvNeXt, and DeiT architectures. On CNNs, our approach achieves MAC targeting while maintaining or improving accuracy: ResNet-50 reaches 1.77G MACs with 77.04% accuracy (+0.91% vs baseline); ResNet-101 achieves 4.22G MACs with 78.94% accuracy (+1.56% vs baseline). For ConvNeXt-Small, pruning to 8.17G MACs yields 1.41x GPU and 1.07x CPU speedup with 45% parameter reduction. On Vision Transformers, we demonstrate MAC-budget compliance within user-defined tolerance bands (typically +1% to +5% overshoot, -5% to -15% undershoot), establishing feasibility for deployment scenarios requiring strict computational guarantees.



### CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2601.12282v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.12282v1)
- **Published**: 2026-01-18 06:42:24+00:00
- **Updated**: 2026-01-18 06:42:24+00:00
- **Authors**: Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam
- **Comment**: None
- **Journal**: None
- **Summary**: The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.



### SDiT: Semantic Region-Adaptive for Diffusion Transformers
- **Arxiv ID**: http://arxiv.org/abs/2601.12283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12283v1)
- **Published**: 2026-01-18 06:43:36+00:00
- **Updated**: 2026-01-18 06:43:36+00:00
- **Authors**: Bowen Lin, Fanjiang Ye, Yihua Liu, Zhenghui Guo, Boyuan Zhang, Weijian Zheng, Yufan Xu, Tiancheng Xing, Yuke Wang, Chengming Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion Transformers (DiTs) achieve state-of-the-art performance in text-to-image synthesis but remain computationally expensive due to the iterative nature of denoising and the quadratic cost of global attention. In this work, we observe that denoising dynamics are spatially non-uniform-background regions converge rapidly while edges and textured areas evolve much more actively. Building on this insight, we propose SDiT, a Semantic Region-Adaptive Diffusion Transformer that allocates computation according to regional complexity. SDiT introduces a training-free framework combining (1) semantic-aware clustering via fast Quickshift-based segmentation, (2) complexity-driven regional scheduling to selectively update informative areas, and (3) boundary-aware refinement to maintain spatial coherence. Without any model retraining or architectural modification, SDiT achieves up to 3.0x acceleration while preserving nearly identical perceptual and semantic quality to full-attention inference.



### LegacyAvatars: Volumetric Face Avatars For Traditional Graphics Pipelines
- **Arxiv ID**: http://arxiv.org/abs/2601.12285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12285v1)
- **Published**: 2026-01-18 06:46:05+00:00
- **Updated**: 2026-01-18 06:46:05+00:00
- **Authors**: Safa C. Medin, Gengyan Li, Ziqian Bai, Ruofei Du, Leonhard Helminger, Yinda Zhang, Stephan J. Garbin, Philip L. Davidson, Gregory W. Wornell, Thabo Beeler, Abhimitra Meka
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel representation for efficient classical rendering of photorealistic 3D face avatars. Leveraging recent advances in radiance fields anchored to parametric face models, our approach achieves controllable volumetric rendering of complex facial features, including hair, skin, and eyes. At enrollment time, we learn a set of radiance manifolds in 3D space to extract an explicit layered mesh, along with appearance and warp textures. During deployment, this allows us to control and animate the face through simple linear blending and alpha compositing of textures over a static mesh. This explicit representation also enables the generated avatar to be efficiently streamed online and then rendered using classical mesh and shader-based rendering on legacy graphics platforms, eliminating the need for any custom engineering or integration.



### OpenNavMap: Structure-Free Topometric Mapping via Large-Scale Collaborative Localization
- **Arxiv ID**: http://arxiv.org/abs/2601.12291v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.12291v1)
- **Published**: 2026-01-18 07:24:46+00:00
- **Updated**: 2026-01-18 07:24:46+00:00
- **Authors**: Jianhao Jiao, Changkun Liu, Jingwen Yu, Boyi Liu, Qianyi Zhang, Yue Wang, Dimitrios Kanoulas
- **Comment**: 21 pages, 20 figures
- **Journal**: None
- **Summary**: Scalable and maintainable map representations are fundamental to enabling large-scale visual navigation and facilitating the deployment of robots in real-world environments. While collaborative localization across multi-session mapping enhances efficiency, traditional structure-based methods struggle with high maintenance costs and fail in feature-less environments or under significant viewpoint changes typical of crowd-sourced data. To address this, we propose OPENNAVMAP, a lightweight, structure-free topometric system leveraging 3D geometric foundation models for on-demand reconstruction. Our method unifies dynamic programming-based sequence matching, geometric verification, and confidence-calibrated optimization to robust, coarse-to-fine submap alignment without requiring pre-built 3D models. Evaluations on the Map-Free benchmark demonstrate superior accuracy over structure-from-motion and regression baselines, achieving an average translation error of 0.62m. Furthermore, the system maintains global consistency across 15km of multi-session data with an absolute trajectory error below 3m for map merging. Finally, we validate practical utility through 12 successful autonomous image-goal navigation tasks on simulated and physical robots. Code and datasets will be publicly available in https://rpl-cs-ucl.github.io/OpenNavMap_page.



### Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2601.12303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12303v1)
- **Published**: 2026-01-18 08:01:44+00:00
- **Updated**: 2026-01-18 08:01:44+00:00
- **Authors**: Shizhan Gong, Xiaofan Zhang, Qi Dou
- **Comment**: AAAI 2026
- **Journal**: None
- **Summary**: Deep learning has achieved remarkable success in image recognition, yet their inherent opacity poses challenges for deployment in critical domains. Concept-based interpretations aim to address this by explaining model reasoning through human-understandable concepts. However, existing post-hoc methods and ante-hoc concept bottleneck models (CBMs), suffer from limitations such as unreliable concept relevance, non-visual or labor-intensive concept definitions, and model or data-agnostic assumptions. This paper introduces Post-hoc Concept Bottleneck Model via Representation Decomposition (PCBM-ReD), a novel pipeline that retrofits interpretability onto pretrained opaque models. PCBM-ReD automatically extracts visual concepts from a pre-trained encoder, employs multimodal large language models (MLLMs) to label and filter concepts based on visual identifiability and task relevance, and selects an independent subset via reconstruction-guided optimization. Leveraging CLIP's visual-text alignment, it decomposes image representations into linear combination of concept embeddings to fit into the CBMs abstraction. Extensive experiments across 11 image classification tasks show PCBM-ReD achieves state-of-the-art accuracy, narrows the performance gap with end-to-end models, and exhibits better interpretability.



### A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models
- **Arxiv ID**: http://arxiv.org/abs/2601.12304v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.12304v1)
- **Published**: 2026-01-18 08:05:33+00:00
- **Updated**: 2026-01-18 08:05:33+00:00
- **Authors**: Wutao Chen, Huaqin Zou, Chen Wan, Lifeng Huang
- **Comment**: Accepted to ICASSP 2026
- **Journal**: None
- **Summary**: Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.



### Adaptive Multi-Scale Correlation Meta-Network for Few-Shot Remote Sensing Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2601.12308v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](https://arxiv.org/pdf/2601.12308v1)
- **Published**: 2026-01-18 08:21:51+00:00
- **Updated**: 2026-01-18 08:21:51+00:00
- **Authors**: Anurag Kaushish, Ayan Sar, Sampurna Roy, Sudeshna Chakraborty, Prashant Trivedi, Tanupriya Choudhury, Kanav Gupta
- **Comment**: Accepted in IEEE ICASSP 2026
- **Journal**: None
- **Summary**: Few-shot learning in remote sensing remains challenging due to three factors: the scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects. To address these issues, we introduce Adaptive Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful framework with three key innovations: (i) correlation-guided feature pyramids for capturing scale-invariant patterns, (ii) an adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, and (iii) correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. Unlike prior approaches that rely on heavy pre-trained models or transformers, AMC-MetaNet is trained from scratch with only $\sim600K$ parameters, offering $20\times$ fewer parameters than ResNet-18 while maintaining high efficiency ($<50$ms per image inference). AMC-MetaNet achieves up to 86.65\% accuracy in 5-way 5-shot classification on various remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID. Our results establish AMC-MetaNet as a computationally efficient, scale-aware framework for real-world few-shot remote sensing.



### CurConMix+: A Unified Spatio-Temporal Framework for Hierarchical Surgical Workflow Understanding
- **Arxiv ID**: http://arxiv.org/abs/2601.12312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12312v1)
- **Published**: 2026-01-18 08:42:10+00:00
- **Updated**: 2026-01-18 08:42:10+00:00
- **Authors**: Yongjun Jeon, Jongmin Shin, Kanggil Park, Seonmin Park, Soyoung Lim, Jung Yong Kim, Jinsoo Rhu, Jongman Kim, Gyu-Seong Choi, Namkee Oh, Kyu-Hwan Jung
- **Comment**: None
- **Journal**: None
- **Summary**: Surgical action triplet recognition aims to understand fine-grained surgical behaviors by modeling the interactions among instruments, actions, and anatomical targets. Despite its clinical importance for workflow analysis and skill assessment, progress has been hindered by severe class imbalance, subtle visual variations, and the semantic interdependence among triplet components. Existing approaches often address only a subset of these challenges rather than tackling them jointly, which limits their ability to form a holistic understanding. This study builds upon CurConMix, a spatial representation framework. At its core, a curriculum-guided contrastive learning strategy learns discriminative and progressively correlated features, further enhanced by structured hard-pair sampling and feature-level mixup. Its temporal extension, CurConMix+, integrates a Multi-Resolution Temporal Transformer (MRTT) that achieves robust, context-aware understanding by adaptively fusing multi-scale temporal features and dynamically balancing spatio-temporal cues. Furthermore, we introduce LLS48, a new, hierarchically annotated benchmark for complex laparoscopic left lateral sectionectomy, providing step-, task-, and action-level annotations. Extensive experiments on CholecT45 and LLS48 demonstrate that CurConMix+ not only outperforms state-of-the-art approaches in triplet recognition, but also exhibits strong cross-level generalization, as its fine-grained features effectively transfer to higher-level phase and step recognition tasks. Together, the framework and dataset provide a unified foundation for hierarchy-aware, reproducible, and interpretable surgical workflow understanding. The code and dataset will be publicly released on GitHub to facilitate reproducibility and further research.



### S^2F-Net:A Robust Spatial-Spectral Fusion Framework for Cross-Model AIGC Detection
- **Arxiv ID**: http://arxiv.org/abs/2601.12313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12313v1)
- **Published**: 2026-01-18 08:43:27+00:00
- **Updated**: 2026-01-18 08:43:27+00:00
- **Authors**: Xiangyu Hu, Yicheng Hong, Hongchuang Zheng, Wenjun Zeng, Bingyao Liu
- **Comment**: 27pages 9figures
- **Journal**: None
- **Summary**: The rapid development of generative models has imposed an urgent demand for detection schemes with strong generalization capabilities. However, existing detection methods generally suffer from overfitting to specific source models, leading to significant performance degradation when confronted with unseen generative architectures. To address these challenges, this paper proposes a cross-model detection framework called S 2 F-Net, whose core lies in exploring and leveraging the inherent spectral discrepancies between real and synthetic textures. Considering that upsampling operations leave unique and distinguishable frequency fingerprints in both texture-poor and texture-rich regions, we focus our research on the detection of frequency-domain artifacts, aiming to fundamentally improve the generalization performance of the model. Specifically, we introduce a learnable frequency attention module that adaptively weights and enhances discriminative frequency bands by synergizing spatial texture analysis and spectral dependencies.On the AIGCDetectBenchmark, which includes 17 categories of generative models, S 2 F-Net achieves a detection accuracy of 90.49%, significantly outperforming various existing baseline methods in cross-domain detection scenarios.



### GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer
- **Arxiv ID**: http://arxiv.org/abs/2601.12316v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.12316v1)
- **Published**: 2026-01-18 08:54:02+00:00
- **Updated**: 2026-01-18 08:54:02+00:00
- **Authors**: Xinyuan Zhao, Xianrui Chen, Ahmad Chaddad
- **Comment**: accepted at ICASSP 2026
- **Journal**: None
- **Summary**: We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49째, 3.22째, 10.16째, and 1.44째, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.



### Multi-Sensor Matching with HyperNetworks
- **Arxiv ID**: http://arxiv.org/abs/2601.12325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12325v1)
- **Published**: 2026-01-18 09:19:33+00:00
- **Updated**: 2026-01-18 09:19:33+00:00
- **Authors**: Eli Passov, Nathan S. Netanyahu, Yosi Keller
- **Comment**: None
- **Journal**: None
- **Summary**: Hypernetworks are models that generate or modulate the weights of another network. They provide a flexible mechanism for injecting context and task conditioning and have proven broadly useful across diverse applications without significant increases in model size. We leverage hypernetworks to improve multimodal patch matching by introducing a lightweight descriptor-learning architecture that augments a Siamese CNN with (i) hypernetwork modules that compute adaptive, per-channel scaling and shifting and (ii) conditional instance normalization that provides modality-specific adaptation (e.g., visible vs. infrared, VIS-IR) in shallow layers. This combination preserves the efficiency of descriptor-based methods during inference while increasing robustness to appearance shifts. Trained with a triplet loss and hard-negative mining, our approach achieves state-of-the-art results on VIS-NIR and other VIS-IR benchmarks and matches or surpasses prior methods on additional datasets, despite their higher inference cost. To spur progress on domain shift, we also release GAP-VIR, a cross-platform (ground/aerial) VIS-IR patch dataset with 500K pairs, enabling rigorous evaluation of cross-domain generalization and adaptation.



### EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation
- **Arxiv ID**: http://arxiv.org/abs/2601.12326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12326v1)
- **Published**: 2026-01-18 09:20:09+00:00
- **Updated**: 2026-01-18 09:20:09+00:00
- **Authors**: Jing Zhang, Bingjie Fan
- **Comment**: 11pages,10figures
- **Journal**: None
- **Summary**: Existing image emotion editing methods struggle to disentangle emotional cues from latent content representations, often yielding weak emotional expression and distorted visual structures. To bridge this gap, we propose EmoKGEdit, a novel training-free framework for precise and structure-preserving image emotion editing. Specifically, we construct a Multimodal Sentiment Association Knowledge Graph (MSA-KG) to disentangle the intricate relationships among objects, scenes, attributes, visual clues and emotion. MSA-KG explicitly encode the causal chain among object-attribute-emotion, and as external knowledge to support chain of thought reasoning, guiding the multimodal large model to infer plausible emotion-related visual cues and generate coherent instructions. In addition, based on MSA-KG, we design a disentangled structure-emotion editing module that explicitly separates emotional attributes from layout features within the latent space, which ensures that the target emotion is effectively injected while strictly maintaining visual spatial coherence. Extensive experiments demonstrate that EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, and outperforms the state-of-the-art methods.



### FlowIID: Single-Step Intrinsic Image Decomposition via Latent Flow Matching
- **Arxiv ID**: http://arxiv.org/abs/2601.12329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12329v1)
- **Published**: 2026-01-18 09:29:13+00:00
- **Updated**: 2026-01-18 09:29:13+00:00
- **Authors**: Mithlesh Singla, Seema Kumari, Shanmuganathan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: Intrinsic Image Decomposition (IID) separates an image into albedo and shading components. It is a core step in many real-world applications, such as relighting and material editing. Existing IID models achieve good results, but often use a large number of parameters. This makes them costly to combine with other models in real-world settings. To address this problem, we propose a flow matching-based solution. For this, we design a novel architecture, FlowIID, based on latent flow matching. FlowIID combines a VAE-guided latent space with a flow matching module, enabling a stable decomposition of albedo and shading. FlowIID is not only parameter-efficient, but also produces results in a single inference step. Despite its compact design, FlowIID delivers competitive and superior results compared to existing models across various benchmarks. This makes it well-suited for deployment in resource-constrained and real-time vision applications.



### Turbo-GoDec: Exploiting the Cluster Sparsity Prior for Hyperspectral Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2601.12337v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12337v1)
- **Published**: 2026-01-18 09:49:58+00:00
- **Updated**: 2026-01-18 09:49:58+00:00
- **Authors**: Jiahui Sheng, Xiaorun Li, Shuhan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: As a key task in hyperspectral image processing, hyperspectral anomaly detection has garnered significant attention and undergone extensive research. Existing methods primarily relt on two prior assumption: low-rank background and sparse anomaly, along with additional spatial assumptions of the background. However, most methods only utilize the sparsity prior assumption for anomalies and rarely expand on this hypothesis. From observations of hyperspectral images, we find that anomalous pixels exhibit certain spatial distribution characteristics: they often manifest as small, clustered groups in space, which we refer to as cluster sparsity of anomalies. Then, we combined the cluster sparsity prior with the classical GoDec algorithm, incorporating the cluster sparsity prior into the S-step of GoDec. This resulted in a new hyperspectral anomaly detection method, which we called Turbo-GoDec. In this approach, we modeled the cluster sparsity prior of anomalies using a Markov random field and computed the marginal probabilities of anomalies through message passing on a factor graph. Locations with high anomalous probabilities were treated as the sparse component in the Turbo-GoDec. Experiments are conducted on three real hyperspectral image (HSI) datasets which demonstrate the superior performance of the proposed Turbo-GoDec method in detecting small-size anomalies comparing with the vanilla GoDec (LSMAD) and state-of-the-art anomaly detection methods. The code is available at https://github.com/jiahuisheng/Turbo-GoDec.



### MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents
- **Arxiv ID**: http://arxiv.org/abs/2601.12346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12346v1)
- **Published**: 2026-01-18 10:41:33+00:00
- **Updated**: 2026-01-18 10:41:33+00:00
- **Authors**: Peizhou Huang, Zixuan Zhong, Zhongwei Wan, Donghao Zhou, Samiul Alam, Xin Wang, Zexin Li, Zhihao Dou, Li Zhu, Jing Xiong, Chaofan Tao, Yan Xu, Dimitrios Dimitriadis, Tuo Zhang, Mi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.



### SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2601.12357v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.12357v1)
- **Published**: 2026-01-18 11:31:46+00:00
- **Updated**: 2026-01-18 11:31:46+00:00
- **Authors**: Hailing Jin, Huiying Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.



### From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2601.12358v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](https://arxiv.org/pdf/2601.12358v1)
- **Published**: 2026-01-18 11:32:29+00:00
- **Updated**: 2026-01-18 11:32:29+00:00
- **Authors**: Omar Y. Goba, Ahmed Y. Gado, Catherine M. Elias, Ahmed Hussein
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.



### DepthCropSeg++: Scaling a Crop Segmentation Foundation Model With Depth-Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/2601.12366v1
- **DOI**: 10.1109/JSTSP.2026.3654362
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12366v1)
- **Published**: 2026-01-18 11:51:09+00:00
- **Updated**: 2026-01-18 11:51:09+00:00
- **Authors**: Jiafei Zhang, Songliang Cao, Binghui Xu, Yanan Li, Weiwei Jia, Tingting Wu, Hao Lu, Weijuan Hu, Zhiguo Han
- **Comment**: 13 pages, 15 figures and 7 tables
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, 2026
- **Summary**: DepthCropSeg++: a foundation model for crop segmentation, capable of segmenting different crop species under open in-field environment. Crop segmentation is a fundamental task for modern agriculture, which closely relates to many downstream tasks such as plant phenotyping, density estimation, and weed control. In the era of foundation models, a number of generic large language and vision models have been developed. These models have demonstrated remarkable real world generalization due to significant model capacity and largescale datasets. However, current crop segmentation models mostly learn from limited data due to expensive pixel-level labelling cost, often performing well only under specific crop types or controlled environment. In this work, we follow the vein of our previous work DepthCropSeg, an almost unsupervised approach to crop segmentation, to scale up a cross-species and crossscene crop segmentation dataset, with 28,406 images across 30+ species and 15 environmental conditions. We also build upon a state-of-the-art semantic segmentation architecture ViT-Adapter architecture, enhance it with dynamic upsampling for improved detail awareness, and train the model with a two-stage selftraining pipeline. To systematically validate model performance, we conduct comprehensive experiments to justify the effectiveness and generalization capabilities across multiple crop datasets. Results demonstrate that DepthCropSeg++ achieves 93.11% mIoU on a comprehensive testing set, outperforming both supervised baselines and general-purpose vision foundation models like Segmentation Anything Model (SAM) by significant margins (+0.36% and +48.57% respectively). The model particularly excels in challenging scenarios including night-time environment (86.90% mIoU), high-density canopies (90.09% mIoU), and unseen crop varieties (90.09% mIoU), indicating a new state of the art for crop segmentation.



### CD-TWINSAFE: A ROS-enabled Digital Twin for Scene Understanding and Safety Emerging V2I Technology
- **Arxiv ID**: http://arxiv.org/abs/2601.12373v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.RO
- **Links**: [PDF](https://arxiv.org/pdf/2601.12373v1)
- **Published**: 2026-01-18 12:07:26+00:00
- **Updated**: 2026-01-18 12:07:26+00:00
- **Authors**: Amro Khaled, Farah Khaled, Omar Riad, Catherine M. Elias
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, the CD-TWINSAFE is introduced, a V2I-based digital twin for Autonomous Vehicles. The proposed architecture is composed of two stacks running simultaneously, an on-board driving stack that includes a stereo camera for scene understanding, and a digital twin stack that runs an Unreal Engine 5 replica of the scene viewed by the camera as well as returning safety alerts to the cockpit. The on-board stack is implemented on the vehicle side including 2 main autonomous modules; localization and perception. The position and orientation of the ego vehicle are obtained using on-board sensors. Furthermore, the perception module is responsible for processing 20-fps images from stereo camera and understands the scene through two complementary pipelines. The pipeline are working on object detection and feature extraction including object velocity, yaw and the safety metrics time-to-collision and time-headway. The collected data form the driving stack are sent to the infrastructure side through the ROS-enabled architecture in the form of custom ROS2 messages and sent over UDP links that ride a 4G modem for V2I communication. The environment is monitored via the digital twin through the shared messages which update the information of the spawned ego vehicle and detected objects based on the real-time localization and perception data. Several tests with different driving scenarios to confirm the validity and real-time response of the proposed architecture.



### Utilizing the Score of Data Distribution for Hyperspectral Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2601.12379v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12379v1)
- **Published**: 2026-01-18 12:14:48+00:00
- **Updated**: 2026-01-18 12:14:48+00:00
- **Authors**: Jiahui Sheng, Yidan Shi, Shu Xiang, Xiaorun Li, Shuhan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral images (HSIs) are a type of image that contains abundant spectral information. As a type of real-world data, the high-dimensional spectra in hyperspectral images are actually determined by only a few factors, such as chemical composition and illumination. Thus, spectra in hyperspectral images are highly likely to satisfy the manifold hypothesis. Based on the hyperspectral manifold hypothesis, we propose a novel hyperspectral anomaly detection method (named ScoreAD) that leverages the time-dependent gradient field of the data distribution (i.e., the score), as learned by a score-based generative model (SGM). Our method first trains the SGM on the entire set of spectra from the hyperspectral image. At test time, each spectrum is passed through a perturbation kernel, and the resulting perturbed spectrum is fed into the trained SGM to obtain the estimated score. The manifold hypothesis of HSIs posits that background spectra reside on one or more low-dimensional manifolds. Conversely, anomalous spectra, owing to their unique spectral signatures, are considered outliers that do not conform to the background manifold. Based on this fundamental discrepancy in their manifold distributions, we leverage a generative SGM to achieve hyperspectral anomaly detection. Experiments on the four hyperspectral datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/jiahuisheng/ScoreAD.



### A Hierarchical Benchmark of Foundation Models for Dermatology
- **Arxiv ID**: http://arxiv.org/abs/2601.12382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12382v1)
- **Published**: 2026-01-18 12:38:15+00:00
- **Updated**: 2026-01-18 12:38:15+00:00
- **Authors**: Furkan Yuceyalcin, Abdurrahim Yilmaz, Burak Temelkuran
- **Comment**: None
- **Journal**: None
- **Summary**: Foundation models have transformed medical image analysis by providing robust feature representations that reduce the need for large-scale task-specific training. However, current benchmarks in dermatology often reduce the complex diagnostic taxonomy to flat, binary classification tasks, such as distinguishing melanoma from benign nevi. This oversimplification obscures a model's ability to perform fine-grained differential diagnoses, which is critical for clinical workflow integration. This study evaluates the utility of embeddings derived from ten foundation models, spanning general computer vision, general medical imaging, and dermatology-specific domains, for hierarchical skin lesion classification. Using the DERM12345 dataset, which comprises 40 lesion subclasses, we calculated frozen embeddings and trained lightweight adapter models using a five-fold cross-validation. We introduce a hierarchical evaluation framework that assesses performance across four levels of clinical granularity: 40 Subclasses, 15 Main Classes, 2 and 4 Superclasses, and Binary Malignancy. Our results reveal a "granularity gap" in model capabilities: MedImageInsights achieved the strongest overall performance (97.52% weighted F1-Score on Binary Malignancy detection) but declined to 65.50% on fine-grained 40-class subtype classification. Conversely, MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained 40-class subtype discrimination while achieving lower overall performance than MedImageInsights on broader classification tasks. Our findings suggest that while general medical foundation models are highly effective for high-level screening, specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems.



### Class-Partitioned VQ-VAE and Latent Flow Matching for Point Cloud Scene Generation
- **Arxiv ID**: http://arxiv.org/abs/2601.12391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12391v1)
- **Published**: 2026-01-18 13:05:35+00:00
- **Updated**: 2026-01-18 13:05:35+00:00
- **Authors**: Dasith de Silva Edirimuni, Ajmal Saeed Mian
- **Comment**: Accepted to AAAI 2026, Main Technical Track
- **Journal**: None
- **Summary**: Most 3D scene generation methods are limited to only generating object bounding box parameters while newer diffusion methods also generate class labels and latent features. Using object size or latent feature, they then retrieve objects from a predefined database. For complex scenes of varied, multi-categorical objects, diffusion-based latents cannot be effectively decoded by current autoencoders into the correct point cloud objects which agree with target classes. We introduce a Class-Partitioned Vector Quantized Variational Autoencoder (CPVQ-VAE) that is trained to effectively decode object latent features, by employing a pioneering $\textit{class-partitioned codebook}$ where codevectors are labeled by class. To address the problem of $\textit{codebook collapse}$, we propose a $\textit{class-aware}$ running average update which reinitializes dead codevectors within each partition. During inference, object features and class labels, both generated by a Latent-space Flow Matching Model (LFMM) designed specifically for scene generation, are consumed by the CPVQ-VAE. The CPVQ-VAE's class-aware inverse look-up then maps generated latents to codebook entries that are decoded to class-specific point cloud shapes. Thereby, we achieve pure point cloud generation without relying on an external objects database for retrieval. Extensive experiments reveal that our method reliably recovers plausible point cloud scenes, with up to 70.4% and 72.3% reduction in Chamfer and Point2Mesh errors on complex living room scenes.



### Weaknesses of Facial Emotion Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2601.12402v1
- **DOI**: 10.1007/978-3-032-05802-7_1
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.12402v1)
- **Published**: 2026-01-18 13:27:01+00:00
- **Updated**: 2026-01-18 13:27:01+00:00
- **Authors**: Aleksandra Jamr처z, Patrycja Wysocka, Piotr Garbat
- **Comment**: None
- **Journal**: Proc. 12th Machine Intelligence and Digital Interaction Conf. (MIDI 2024), Warsaw, Poland, Dec. 2024 (14-22)
- **Summary**: Emotion detection from faces is one of the machine learning problems needed for human-computer interaction. The variety of methods used is enormous, which motivated an in-depth review of articles and scientific studies. Three of the most interesting and best solutions are selected, followed by the selection of three datasets that stood out for the diversity and number of images in them. The selected neural networks are trained, and then a series of experiments are performed to compare their performance, including testing on different datasets than a model was trained on. This reveals weaknesses in existing solutions, including differences between datasets, unequal levels of difficulty in recognizing certain emotions and the challenges in differentiating between closely related emotions.



### HOT-POT: Optimal Transport for Sparse Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2601.12423v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](https://arxiv.org/pdf/2601.12423v1)
- **Published**: 2026-01-18 14:10:45+00:00
- **Updated**: 2026-01-18 14:10:45+00:00
- **Authors**: Antonin Clerc, Michael Quellmalz, Moritz Piening, Philipp Flotho, Gregor Kornhardt, Gabriele Steidl
- **Comment**: 18 pages, 10 figures, 6 tables
- **Journal**: None
- **Summary**: Stereo vision between images faces a range of challenges, including occlusions, motion, and camera distortions, across applications in autonomous driving, robotics, and face analysis. Due to parameter sensitivity, further complications arise for stereo matching with sparse features, such as facial landmarks. To overcome this ill-posedness and enable unsupervised sparse matching, we consider line constraints of the camera geometry from an optimal transport (OT) viewpoint. Formulating camera-projected points as (half)lines, we propose the use of the classical epipolar distance as well as a 3D ray distance to quantify matching quality. Employing these distances as a cost function of a (partial) OT problem, we arrive at efficiently solvable assignment problems. Moreover, we extend our approach to unsupervised object matching by formulating it as a hierarchical OT problem. The resulting algorithms allow for efficient feature and object matching, as demonstrated in our numerical experiments. Here, we focus on applications in facial analysis, where we aim to match distinct landmarking conventions.



### ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models
- **Arxiv ID**: http://arxiv.org/abs/2601.12428v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.12428v1)
- **Published**: 2026-01-18 14:27:10+00:00
- **Updated**: 2026-01-18 14:27:10+00:00
- **Authors**: Baorui Peng, Wenyao Zhang, Liang Xu, Zekun Qi, Jiazhao Zhang, Hongsi Liu, Wenjun Zeng, Xin Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, video-based world models that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied world models with physical realism, task completion capability, embodiment plausibility and visual quality. Specifically, we first construct a large-scale (~235K) video preference dataset and employ it to train a hierarchical reward model designed to capture multi-dimensional reward consistent with human preferences. We further propose a practical alignment algorithm that post-trains flow-based world models using this reward through a computationally efficient PPO-style algorithm. Comprehensive experiments and theoretical analysis demonstrate that ReWorld significantly improves the physical fidelity, logical coherence, embodiment and visual quality of generated rollouts, outperforming previous methods.



### SkeFi: Cross-Modal Knowledge Transfer for Wireless Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2601.12432v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](https://arxiv.org/pdf/2601.12432v1)
- **Published**: 2026-01-18 14:39:02+00:00
- **Updated**: 2026-01-18 14:39:02+00:00
- **Authors**: Shunyu Huang, Yunjiao Zhou, Jianfei Yang
- **Comment**: Published in IEEE Internet of Things Journal
- **Journal**: None
- **Summary**: Skeleton-based action recognition leverages human pose keypoints to categorize human actions, which shows superior generalization and interoperability compared to regular end-to-end action recognition. Existing solutions use RGB cameras to annotate skeletal keypoints, but their performance declines in dark environments and raises privacy concerns, limiting their use in smart homes and hospitals. This paper explores non-invasive wireless sensors, i.e., LiDAR and mmWave, to mitigate these challenges as a feasible alternative. Two problems are addressed: (1) insufficient data on wireless sensor modality to train an accurate skeleton estimation model, and (2) skeletal keypoints derived from wireless sensors are noisier than RGB, causing great difficulties for subsequent action recognition models. Our work, SkeFi, overcomes these gaps through a novel cross-modal knowledge transfer method acquired from the data-rich RGB modality. We propose the enhanced Temporal Correlation Adaptive Graph Convolution (TC-AGC) with frame interactive enhancement to overcome the noise from missing or inconsecutive frames. Additionally, our research underscores the effectiveness of enhancing multiscale temporal modeling through dual temporal convolution. By integrating TC-AGC with temporal modeling for cross-modal transfer, our framework can extract accurate poses and actions from noisy wireless sensors. Experiments demonstrate that SkeFi realizes state-of-the-art performances on mmWave and LiDAR. The code is available at https://github.com/Huang0035/Skefi.



### Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery
- **Arxiv ID**: http://arxiv.org/abs/2601.12442v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.12442v1)
- **Published**: 2026-01-18 14:57:35+00:00
- **Updated**: 2026-01-18 14:57:35+00:00
- **Authors**: Shahnawaz Alam, Mohammed Mudassir Uddin, Mohammed Kaif Pasha
- **Comment**: None
- **Journal**: None
- **Summary**: Scientific Artificial Intelligence (AI) applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. We introduce the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. The architecture comprises three components: automated constraint extraction from scientific literature, probabilistic neural backbone with variational inference, and differentiable constraint satisfaction layer ensuring physical consistency. Experiments on Materials Project (140,000+ materials), QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline simultaneously addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.



### Adversarial Defense in Vision-Language Models: An Overview
- **Arxiv ID**: http://arxiv.org/abs/2601.12443v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.12443v1)
- **Published**: 2026-01-18 14:57:51+00:00
- **Updated**: 2026-01-18 14:57:51+00:00
- **Authors**: Xiaowei Fu, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.



### Privacy-Preserving Federated Learning with Verifiable Fairness Guarantees
- **Arxiv ID**: http://arxiv.org/abs/2601.12447v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CL, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.12447v1)
- **Published**: 2026-01-18 15:06:30+00:00
- **Updated**: 2026-01-18 15:06:30+00:00
- **Authors**: Mohammed Himayath Ali, Mohammed Aqib Abdullah, Syed Muneer Hussin, Mohammed Mudassir Uddin, Shahnawaz Alam
- **Comment**: None
- **Journal**: None
- **Summary**: Federated learning enables collaborative model training across distributed institutions without centralizing sensitive data; however, ensuring algorithmic fairness across heterogeneous data distributions while preserving privacy remains fundamentally unresolved. This paper introduces CryptoFair-FL, a novel cryptographic framework providing the first verifiable fairness guarantees for federated learning systems under formal security definitions. The proposed approach combines additively homomorphic encryption with secure multi-party computation to enable privacy-preserving verification of demographic parity and equalized odds metrics without revealing protected attribute distributions or individual predictions. A novel batched verification protocol reduces computational complexity from BigO(n^2) to BigO(n \log n) while maintaining (\dparam, \deltap)-differential privacy with dparam = 0.5 and deltap = 10^{-6}. Theoretical analysis establishes information-theoretic lower bounds on the privacy cost of fairness verification, demonstrating that the proposed protocol achieves near-optimal privacy-fairness tradeoffs. Comprehensive experiments across four benchmark datasets (MIMIC-IV healthcare records, Adult Income, CelebA, and a novel FedFair-100 benchmark) demonstrate that CryptoFair-FL reduces fairness violations from 0.231 to 0.031 demographic parity difference while incurring only 2.3 times computational overhead compared to standard federated averaging. The framework successfully defends against attribute inference attacks, maintaining adversarial success probability below 0.05 across all tested configurations. These results establish a practical pathway for deploying fairness-aware federated learning in regulated industries requiring both privacy protection and algorithmic accountability.



### Large-scale EM Benchmark for Multi-Organelle Instance Segmentation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2601.12464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12464v1)
- **Published**: 2026-01-18 16:09:27+00:00
- **Updated**: 2026-01-18 16:09:27+00:00
- **Authors**: Yanrui Lu, Danyang Chen, Haowen Xiao, Jiarui Zhu, Fukang Ge, Binqian Zou, Jiali Guan, Jiayin Liang, Yuting Wang, Ziqian Guan, Xiangcheng Bao, Jinhao Bi, Lin Gu, Jun He, Yingying Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate instance-level segmentation of organelles in electron microscopy (EM) is critical for quantitative analysis of subcellular morphology and inter-organelle interactions. However, current benchmarks, based on small, curated datasets, fail to capture the inherent heterogeneity and large spatial context of in-the-wild EM data, imposing fundamental limitations on current patch-based methods. To address these limitations, we developed a large-scale, multi-source benchmark for multi-organelle instance segmentation, comprising over 100,000 2D EM images across variety cell types and five organelle classes that capture real-world variability. Dataset annotations were generated by our designed connectivity-aware Label Propagation Algorithm (3D LPA) with expert refinement. We further benchmarked several state-of-the-art models, including U-Net, SAM variants, and Mask2Former. Our results show several limitations: current models struggle to generalize across heterogeneous EM data and perform poorly on organelles with global, distributed morphologies (e.g., Endoplasmic Reticulum). These findings underscore the fundamental mismatch between local-context models and the challenge of modeling long-range structural continuity in the presence of real-world variability. The benchmark dataset and labeling tool will be publicly released soon.



### DCAC: Dynamic Class-Aware Cache Creates Stronger Out-of-Distribution Detectors
- **Arxiv ID**: http://arxiv.org/abs/2601.12468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12468v1)
- **Published**: 2026-01-18 16:16:31+00:00
- **Updated**: 2026-01-18 16:16:31+00:00
- **Authors**: Yanqi Wu, Qichao Chen, Runhe Lai, Xinhua Lu, Jia-Xin Zhuang, Zhilin Zhao, Wei-Shi Zheng, Ruixuan Wang
- **Comment**: 9 pages, 9 figures, Accepted by AAAI2026
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) detection remains a fundamental challenge for deep neural networks, particularly due to overconfident predictions on unseen OOD samples during testing. We reveal a key insight: OOD samples predicted as the same class, or given high probabilities for it, are visually more similar to each other than to the true in-distribution (ID) samples. Motivated by this class-specific observation, we propose DCAC (Dynamic Class-Aware Cache), a training-free, test-time calibration module that maintains separate caches for each ID class to collect high-entropy samples and calibrate the raw predictions of input samples. DCAC leverages cached visual features and predicted probabilities through a lightweight two-layer module to mitigate overconfident predictions on OOD samples. This module can be seamlessly integrated with various existing OOD detection methods across both unimodal and vision-language models while introducing minimal computational overhead. Extensive experiments on multiple OOD benchmarks demonstrate that DCAC significantly enhances existing methods, achieving substantial improvements, i.e., reducing FPR95 by 6.55% when integrated with ASH-S on ImageNet OOD benchmark.



### NeuralFur: Animal Fur Reconstruction From Multi-View Images
- **Arxiv ID**: http://arxiv.org/abs/2601.12481v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](https://arxiv.org/pdf/2601.12481v1)
- **Published**: 2026-01-18 16:46:38+00:00
- **Updated**: 2026-01-18 16:46:38+00:00
- **Authors**: Vanessa Sklyarova, Berna Kabadayi, Anastasios Yiannakidis, Giorgio Becherini, Michael J. Black, Justus Thies
- **Comment**: For additional results and code, please refer to https://neuralfur.is.tue.mpg.de
- **Journal**: None
- **Summary**: Reconstructing realistic animal fur geometry from images is a challenging task due to the fine-scale details, self-occlusion, and view-dependent appearance of fur. In contrast to human hairstyle reconstruction, there are also no datasets that can be leveraged to learn a fur prior for different animals. In this work, we present a first multi-view-based method for high-fidelity 3D fur modeling of animals using a strand-based representation, leveraging the general knowledge of a vision language model. Given multi-view RGB images, we first reconstruct a coarse surface geometry using traditional multi-view stereo techniques. We then use a vision language model (VLM) system to retrieve information about the realistic length structure of the fur for each part of the body. We use this knowledge to construct the animal's furless geometry and grow strands atop it. The fur reconstruction is supervised with both geometric and photometric losses computed from multi-view images. To mitigate orientation ambiguities stemming from the Gabor filters that are applied to the input images, we additionally utilize the VLM to guide the strands' growth direction and their relation to the gravity vector that we incorporate as a loss. With this new schema of using a VLM to guide 3D reconstruction from multi-view inputs, we show generalization across a variety of animals with different fur types. For additional results and code, please refer to https://neuralfur.is.tue.mpg.de.



### Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2601.12493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12493v1)
- **Published**: 2026-01-18 17:06:09+00:00
- **Updated**: 2026-01-18 17:06:09+00:00
- **Authors**: Mehrdad Noori, Gustavo Adolfo Vargas Hakim, David Osowiechi, Fereshteh Shakeri, Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Ismail Ben Ayed, Christian Desrosiers
- **Comment**: Accepted to WACV 2026
- **Journal**: None
- **Summary**: Medical Vision-language models (VLMs) have shown remarkable performances in various medical imaging domains such as histo\-pathology by leveraging pre-trained, contrastive models that exploit visual and textual information. However, histopathology images may exhibit severe domain shifts, such as staining, contamination, blurring, and noise, which may severely degrade the VLM's downstream performance. In this work, we introduce Histopath-C, a new benchmark with realistic synthetic corruptions designed to mimic real-world distribution shifts observed in digital histopathology. Our framework dynamically applies corruptions to any available dataset and evaluates Test-Time Adaptation (TTA) mechanisms on the fly. We then propose LATTE, a transductive, low-rank adaptation strategy that exploits multiple text templates, mitigating the sensitivity of histopathology VLMs to diverse text inputs. Our approach outperforms state-of-the-art TTA methods originally designed for natural images across a breadth of histopathology datasets, demonstrating the effectiveness of our proposed design for robust adaptation in histopathology images. Code and data are available at https://github.com/Mehrdad-Noori/Histopath-C.



### Video Individual Counting and Tracking from Moving Drones: A Benchmark and Methods
- **Arxiv ID**: http://arxiv.org/abs/2601.12500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12500v1)
- **Published**: 2026-01-18 17:17:31+00:00
- **Updated**: 2026-01-18 17:17:31+00:00
- **Authors**: Yaowu Fan, Jia Wan, Tao Han, Andy J. Ma, Antoni B. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: Counting and tracking dense crowds in large-scale scenes is highly challenging, yet existing methods mainly rely on datasets captured by fixed cameras, which provide limited spatial coverage and are inadequate for large-scale dense crowd analysis. To address this limitation, we propose a flexible solution using moving drones to capture videos and perform video-level crowd counting and tracking of unique pedestrians across entire scenes. We introduce MovingDroneCrowd++, the largest video-level dataset for dense crowd counting and tracking captured by moving drones, covering diverse and complex conditions with varying flight altitudes, camera angles, and illumination. Existing methods fail to achieve satisfactory performance on this dataset. To this end, we propose GD3A (Global Density Map Decomposition via Descriptor Association), a density map-based video individual counting method that avoids explicit localization. GD3A establishes pixel-level correspondences between pedestrian descriptors across consecutive frames via optimal transport with an adaptive dustbin score, enabling the decomposition of global density maps into shared, inflow, and outflow components. Building on this framework, we further introduce DVTrack, which converts descriptor-level matching into instance-level associations through a descriptor voting mechanism for pedestrian tracking. Experimental results show that our methods significantly outperform existing approaches under dense crowds and complex motion, reducing counting error by 47.4 percent and improving tracking performance by 39.2 percent.



### SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote Sensing Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2601.12507v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](https://arxiv.org/pdf/2601.12507v1)
- **Published**: 2026-01-18 17:36:48+00:00
- **Updated**: 2026-01-18 17:36:48+00:00
- **Authors**: Ruo Qi, Linhui Dai, Yusong Qin, Chaolei Yang, Yanshan Li
- **Comment**: None
- **Journal**: None
- **Summary**: In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at https://github.com/qiruo-ya/SDCoNet.



### Fine-Tuning Cycle-GAN for Domain Adaptation of MRI Images
- **Arxiv ID**: http://arxiv.org/abs/2601.12512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12512v1)
- **Published**: 2026-01-18 17:50:00+00:00
- **Updated**: 2026-01-18 17:50:00+00:00
- **Authors**: Mohd Usama, Belal Ahmad, Faleh Menawer R Althiyabi
- **Comment**: 14 pages, 9 figures, 2 tables
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) scans acquired from different scanners or institutions often suffer from domain shifts owing to variations in hardware, protocols, and acquisition parameters. This discrepancy degrades the performance of deep learning models trained on source domain data when applied to target domain images. In this study, we propose a Cycle-GAN-based model for unsupervised medical-image domain adaptation. Leveraging CycleGANs, our model learns bidirectional mappings between the source and target domains without paired training data, preserving the anatomical content of the images. By leveraging Cycle-GAN capabilities with content and disparity loss for adaptation tasks, we ensured image-domain adaptation while maintaining image integrity. Several experiments on MRI datasets demonstrated the efficacy of our model in bidirectional domain adaptation without labelled data. Furthermore, research offers promising avenues for improving the diagnostic accuracy of healthcare. The statistical results confirm that our approach improves model performance and reduces domain-related variability, thus contributing to more precise and consistent medical image analysis.



### Deep Feature Deformation Weights
- **Arxiv ID**: http://arxiv.org/abs/2601.12527v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](https://arxiv.org/pdf/2601.12527v1)
- **Published**: 2026-01-18 18:23:03+00:00
- **Updated**: 2026-01-18 18:23:03+00:00
- **Authors**: Richard Liu, Itai Lang, Rana Hanocka
- **Comment**: None
- **Journal**: None
- **Summary**: Handle-based mesh deformation has been a long-standing paradigm in computer graphics, enabling intuitive shape edits from sparse controls. Classic techniques offer precise and rapid deformation control. However, they solve an optimization problem with constraints defined by control handle placement, requiring a user to know apriori the ideal distribution of handles on the shape to accomplish the desired edit. The mapping from handle set to deformation behavior is often unintuitive and, importantly, non-semantic. Modern data-driven methods, on the other hand, leverage a data prior to obtain semantic edits, but are slow and imprecise. We propose a technique that fuses the semantic prior of data with the precise control and speed of traditional frameworks. Our approach is surprisingly simple yet effective: deep feature proximity makes for smooth and semantic deformation weights, with no need for additional regularization. The weights can be computed in real-time for any surface point, whereas prior methods require optimization for new handles. Moreover, the semantic prior from deep features enables co-deformation of semantic parts. We introduce an improved feature distillation pipeline, barycentric feature distillation, which efficiently uses the visual signal from shape renders to minimize distillation cost. This allows our weights to be computed for high resolution meshes in under a minute, in contrast to potentially hours for both classical and neural methods. We preserve and extend properties of classical methods through feature space constraints and locality weighting. Our field representation allows for automatic detection of semantic symmetries, which we use to produce symmetry-preserving deformations. We show a proof-of-concept application which can produce deformations for meshes up to 1 million faces in real-time on a consumer-grade machine.



### XRefine: Attention-Guided Keypoint Match Refinement
- **Arxiv ID**: http://arxiv.org/abs/2601.12530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12530v1)
- **Published**: 2026-01-18 18:32:53+00:00
- **Updated**: 2026-01-18 18:32:53+00:00
- **Authors**: Jan Fabian Schmid, Annika Hagemann
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse keypoint matching is crucial for 3D vision tasks, yet current keypoint detectors often produce spatially inaccurate matches. Existing refinement methods mitigate this issue through alignment of matched keypoint locations, but they are typically detector-specific, requiring retraining for each keypoint detector. We introduce XRefine, a novel, detector-agnostic approach for sub-pixel keypoint refinement that operates solely on image patches centered at matched keypoints. Our cross-attention-based architecture learns to predict refined keypoint coordinates without relying on internal detector representations, enabling generalization across detectors. Furthermore, XRefine can be extended to handle multi-view feature tracks. Experiments on MegaDepth, KITTI, and ScanNet demonstrate that the approach consistently improves geometric estimation accuracy, achieving superior performance compared to existing refinement methods while maintaining runtime efficiency. Our code and trained models can be found at https://github.com/boschresearch/xrefine.



### BirdsEye-RU: A Dataset For Detecting Faces from Overhead Images
- **Arxiv ID**: http://arxiv.org/abs/2601.12533v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12533v2)
- **Published**: 2026-01-18 18:36:03+00:00
- **Updated**: 2026-01-21 03:24:02+00:00
- **Authors**: Md. Ahanaf Arif Khan, Ariful Islam, Sangeeta Biswas, Md. Iqbal Aziz Khan, Subrata Pramanik, Sanjoy Kumar Chakravarty, Bimal Kumar Pramanik
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting faces in overhead images remains a significant challenge due to extreme scale variations and environmental clutter. To address this, we created the BirdsEye-RU dataset, a comprehensive collection of 2,978 images containing over eight thousand annotated faces. This dataset is specifically designed to capture small and distant faces across diverse environments, containing both drone images and smartphone-captured images from high altitude. We present a detailed description of the BirdsEye-RU dataset in this paper. We made our dataset freely available to the public, and it can be accessed at https://www.kaggle.com/datasets/mdahanafarifkhan/birdseye-ru.



### Encoding Emotion Through Self-Supervised Eye Movement Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2601.12534v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.12534v2)
- **Published**: 2026-01-18 18:37:41+00:00
- **Updated**: 2026-01-21 03:08:38+00:00
- **Authors**: Marcus Ma, Jordan Prescott, Emily Zhou, Tiantian Feng, Kleanthis Avramidis, Gabor Mihaly Toth, Shrikanth Narayanan
- **Comment**: None
- **Journal**: None
- **Summary**: The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.



### PISE: Physics-Anchored Semantically-Enhanced Deep Computational Ghost Imaging for Robust Low-Bandwidth Machine Perception
- **Arxiv ID**: http://arxiv.org/abs/2601.12551v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](https://arxiv.org/pdf/2601.12551v1)
- **Published**: 2026-01-18 19:31:24+00:00
- **Updated**: 2026-01-18 19:31:24+00:00
- **Authors**: Tong Wu
- **Comment**: 4 pages, 4 figures, 3 tables. Submitted to IEICE Transactions
- **Journal**: None
- **Summary**: We propose PISE, a physics-informed deep ghost imaging framework for low-bandwidth edge perception. By combining adjoint operator initialization with semantic guidance, PISE improves classification accuracy by 2.57% and reduces variance by 9x at 5% sampling.



### Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory
- **Arxiv ID**: http://arxiv.org/abs/2601.12557v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.12557v1)
- **Published**: 2026-01-18 19:43:48+00:00
- **Updated**: 2026-01-18 19:43:48+00:00
- **Authors**: Mark Moussa, Amber V. Young, Brianna Isola, Vasuda Trehan, Michael D. Himes, Nicholas Wogan, Giada Arney
- **Comment**: 8 pages, 4 figures. Submitted and accepted in AAAI-26 (IAAI Emerging Applications track)
- **Journal**: None
- **Summary**: Future direct-imaging flagship missions, such as NASA's Habitable Worlds Observatory (HWO), face critical decisions in prioritizing observations due to extremely stringent time and resource constraints. In this paper, we introduce two advanced machine-learning architectures tailored for predicting biosignature species fluxes from exoplanetary reflected-light spectra: a Bayesian Convolutional Neural Network (BCNN) and our novel model architecture, the Spectral Query Adaptive Transformer (SQuAT). The BCNN robustly quantifies both epistemic and aleatoric uncertainties, offering reliable predictions under diverse observational conditions, whereas SQuAT employs query-driven attention mechanisms to enhance interpretability by explicitly associating spectral features with specific biosignature species. We demonstrate that both models achieve comparably high predictive accuracy on an augmented dataset spanning a wide range of exoplanetary conditions, while highlighting their distinct advantages in uncertainty quantification and spectral interpretability. These capabilities position our methods as promising tools for accelerating target triage, optimizing observation schedules, and maximizing scientific return for upcoming flagship missions such as HWO.



### Camera Pose Revisited
- **Arxiv ID**: http://arxiv.org/abs/2601.12567v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12567v1)
- **Published**: 2026-01-18 20:10:34+00:00
- **Updated**: 2026-01-18 20:10:34+00:00
- **Authors**: Wadysaw Skarbek, Micha Salomonowicz, Micha Kr처l
- **Comment**: 30 pages, 9 figures, 9 tables
- **Journal**: None
- **Summary**: Estimating the position and orientation of a camera with respect to an observed scene is one of the central problems in computer vision, particularly in the context of camera calibration and multi-sensor systems. This paper addresses the planar Perspective--$n$--Point problem, with special emphasis on the initial estimation of the pose of a calibration object. As a solution, we propose the \texttt{PnP-ProCay78} algorithm, which combines the classical quadratic formulation of the reconstruction error with a Cayley parameterization of rotations and least-squares optimization. The key component of the method is a deterministic selection of starting points based on an analysis of the reconstruction error for two canonical vectors, allowing costly solution-space search procedures to be avoided. Experimental validation is performed using data acquired also from high-resolution RGB cameras and very low-resolution thermal cameras in an integrated RGB--IR setup. The results demonstrate that the proposed algorithm achieves practically the same projection accuracy as optimal \texttt{SQPnP} and slightly higher than \texttt{IPPE}, both prominent \texttt{PnP-OpenCV} procedures. However, \texttt{PnP-ProCay78} maintains a significantly simpler algorithmic structure. Moreover, the analysis of optimization trajectories in Cayley space provides an intuitive insight into the convergence process, making the method attractive also from a didactic perspective. Unlike existing PnP solvers, the proposed \texttt{PnP-ProCay78} algorithm combines projection error minimization with an analytically eliminated reconstruction-error surrogate for translation, yielding a hybrid cost formulation that is both geometrically transparent and computationally efficient.



### Towards Robust Universal Perturbation Attacks: A Float-Coded, Penalty-Driven Evolutionary Approach
- **Arxiv ID**: http://arxiv.org/abs/2601.12624v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.12624v1)
- **Published**: 2026-01-18 23:31:12+00:00
- **Updated**: 2026-01-18 23:31:12+00:00
- **Authors**: Shiqi Wang, Mahdi Khosravy, Neeraj Gupta, Olaf Witkowski
- **Comment**: None
- **Journal**: None
- **Summary**: Universal adversarial perturbations (UAPs) have garnered significant attention due to their ability to undermine deep neural networks across multiple inputs using a single noise pattern. Evolutionary algorithms offer a promising approach to generating such perturbations due to their ability to navigate non-convex, gradient-free landscapes. In this work, we introduce a float-coded, penalty-driven single-objective evolutionary framework for UAP generation that achieves lower visibility perturbations while enhancing attack success rates. Our approach leverages continuous gene representations aligned with contemporary deep learning scales, incorporates dynamic evolutionary operators with adaptive scheduling, and utilizes a modular PyTorch implementation for seamless integration with modern architectures. Additionally, we ensure the universality of the generated perturbations by testing across diverse models and by periodically switching batches to prevent overfitting. Experimental results on the ImageNet dataset demonstrate that our framework consistently produces perturbations with smaller norms, higher misclassification effectiveness, and faster convergence compared to existing evolutionary-based methods. These findings highlight the robustness and scalability of our approach for universal adversarial attacks across various deep learning architectures.



### Linear Mechanisms for Spatiotemporal Reasoning in Vision Language Models
- **Arxiv ID**: http://arxiv.org/abs/2601.12626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.12626v1)
- **Published**: 2026-01-18 23:48:38+00:00
- **Updated**: 2026-01-18 23:48:38+00:00
- **Authors**: Raphi Kang, Hongqiao Chen, Georgia Gkioxari, Pietro Perona
- **Comment**: None
- **Journal**: None
- **Summary**: Spatio-temporal reasoning is a remarkable capability of Vision Language Models (VLMs), but the underlying mechanisms of such abilities remain largely opaque. We postulate that visual/geometrical and textual representations of spatial structure must be combined at some point in VLM computations. We search for such confluence, and ask whether the identified representation can causally explain aspects of input-output model behavior through a linear model. We show empirically that VLMs encode object locations by linearly binding \textit{spatial IDs} to textual activations, then perform reasoning via language tokens. Through rigorous causal interventions we demonstrate that these IDs, which are ubiquitous across the model, can systematically mediate model beliefs at intermediate VLM layers. Additionally, we find that spatial IDs serve as a diagnostic tool for identifying limitations in existing VLMs, and as a valuable learning signal. We extend our analysis to video VLMs and identify an analogous linear temporal ID mechanism. By characterizing our proposed spatiotemporal ID mechanism, we elucidate a previously underexplored internal reasoning process in VLMs, toward improved interpretability and the principled design of more aligned and capable models. We release our code for reproducibility: https://github.com/Raphoo/linear-mech-vlms.




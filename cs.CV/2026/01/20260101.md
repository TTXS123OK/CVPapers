# Arxiv Papers in cs.CV on 2026-01-01
### Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection
- **Arxiv ID**: http://arxiv.org/abs/2601.00141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00141v1)
- **Published**: 2026-01-01 00:00:07+00:00
- **Updated**: 2026-01-01 00:00:07+00:00
- **Authors**: Lawrence Han
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.



### FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications
- **Arxiv ID**: http://arxiv.org/abs/2601.00150v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CE, cs.MM
- **Links**: [PDF](https://arxiv.org/pdf/2601.00150v1)
- **Published**: 2026-01-01 00:42:54+00:00
- **Updated**: 2026-01-01 00:42:54+00:00
- **Authors**: Yehui Yang, Dalu Yang, Wenshuo Zhou, Fangxin Shang, Yifan Liu, Jie Ren, Haojun Fei, Qing Yang, Tao Chen
- **Comment**: None
- **Journal**: None
- **Summary**: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.



### Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions
- **Arxiv ID**: http://arxiv.org/abs/2601.00156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00156v1)
- **Published**: 2026-01-01 01:14:55+00:00
- **Updated**: 2026-01-01 01:14:55+00:00
- **Authors**: Kaiwen Zheng, Junchen Fu, Songpei Xu, Yaoqing He, Joemon M. Jose, Han Hu, Xuri Ge
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.



### Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework
- **Arxiv ID**: http://arxiv.org/abs/2601.00192v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.00192v1)
- **Published**: 2026-01-01 03:44:42+00:00
- **Updated**: 2026-01-01 03:44:42+00:00
- **Authors**: Moirangthem Tiken Singh, Manibhushan Yaikhom
- **Comment**: None
- **Journal**: None
- **Summary**: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $Î¼$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.



### DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2601.00194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00194v1)
- **Published**: 2026-01-01 04:03:30+00:00
- **Updated**: 2026-01-01 04:03:30+00:00
- **Authors**: Salma Gonzalez-Sabbagh, Antonio Robles-Kelly, Shang Gao
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Recovering the in-air colours of seafloor from satellite imagery is a challenging task due to the exponential attenuation of light with depth in the water column. In this study, we present DichroGAN, a conditional generative adversarial network (cGAN) designed for this purpose. DichroGAN employs a two-steps simultaneous training: first, two generators utilise a hyperspectral image cube to estimate diffuse and specular reflections, thereby obtaining atmospheric scene radiance. Next, a third generator receives as input the generated scene radiance containing the features of each spectral band, while a fourth generator estimates the underwater light transmission. These generators work together to remove the effects of light absorption and scattering, restoring the in-air colours of seafloor based on the underwater image formation equation. DichroGAN is trained on a compact dataset derived from PRISMA satellite imagery, comprising RGB images paired with their corresponding spectral bands and masks. Extensive experiments on both satellite and underwater datasets demonstrate that DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques.



### MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing
- **Arxiv ID**: http://arxiv.org/abs/2601.00204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00204v1)
- **Published**: 2026-01-01 04:42:59+00:00
- **Updated**: 2026-01-01 04:42:59+00:00
- **Authors**: Xiaokun Sun, Zeyu Cai, Hao Tang, Ying Tai, Jian Yang, Zhenyu Zhang
- **Comment**: Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/
- **Journal**: None
- **Summary**: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.



### CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting
- **Arxiv ID**: http://arxiv.org/abs/2601.00207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](https://arxiv.org/pdf/2601.00207v1)
- **Published**: 2026-01-01 04:51:02+00:00
- **Updated**: 2026-01-01 04:51:02+00:00
- **Authors**: Md Ahmed Al Muzaddid, William J. Beksi
- **Comment**: 8 pages, 10 figures, and 2 tables
- **Journal**: None
- **Summary**: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.



### IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2601.00212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00212v1)
- **Published**: 2026-01-01 05:04:36+00:00
- **Updated**: 2026-01-01 05:04:36+00:00
- **Authors**: Han Liu, Yubo Fan, Hao Li, Dewei Hu, Daniel Moyer, Zhoubing Xu, Benoit M. Dawant, Ipek Oguz
- **Comment**: Extension of our 1st place solution for the CrossMoDA 2023 challenge
- **Journal**: None
- **Summary**: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.



### From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2601.00215v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](https://arxiv.org/pdf/2601.00215v1)
- **Published**: 2026-01-01 05:19:28+00:00
- **Updated**: 2026-01-01 05:19:28+00:00
- **Authors**: Omar Sharif, Eftekhar Hossain, Patrick Ng
- **Comment**: 23 pages, 15 Figures, 10 Tables
- **Journal**: None
- **Summary**: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.   To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.



### LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization
- **Arxiv ID**: http://arxiv.org/abs/2601.00222v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00222v1)
- **Published**: 2026-01-01 06:05:05+00:00
- **Updated**: 2026-01-01 06:05:05+00:00
- **Authors**: Jie Li, Kwan-Yee K. Wong, Kai Han
- **Comment**: The IEEE/CVF Winter Conference on Applications of Computer Vision 2026
- **Journal**: None
- **Summary**: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.



### Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions
- **Arxiv ID**: http://arxiv.org/abs/2601.00225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00225v1)
- **Published**: 2026-01-01 06:11:16+00:00
- **Updated**: 2026-01-01 06:11:16+00:00
- **Authors**: Aobo Li, Jinjian Wu, Yongxu Liu, Leida Li, Weisheng Dong
- **Comment**: Accepted by NIPS 2025
- **Journal**: None
- **Summary**: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.



### Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2601.00237v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](https://arxiv.org/pdf/2601.00237v1)
- **Published**: 2026-01-01 07:01:47+00:00
- **Updated**: 2026-01-01 07:01:47+00:00
- **Authors**: Chao Yang, Haoyuan Zheng, Yue Ma
- **Comment**: 8 pages,8 figures
- **Journal**: None
- **Summary**: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.



### Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture
- **Arxiv ID**: http://arxiv.org/abs/2601.00243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00243v1)
- **Published**: 2026-01-01 07:33:09+00:00
- **Updated**: 2026-01-01 07:33:09+00:00
- **Authors**: Anirudha Ghosh, Ritam Sarkar, Debaditya Barman
- **Comment**: Submitted to the 3rd International Conference on Nonlinear Dynamics and Applications (ICNDA 2026), 12 pages, 7 figures
- **Journal**: None
- **Summary**: Effective pest management is crucial for enhancing agricultural productivity, especially for crops such as sugarcane and wheat that are highly vulnerable to pest infestations. Traditional pest management methods depend heavily on manual field inspections and the use of chemical pesticides. These approaches are often costly, time-consuming, labor-intensive, and can have a negative impact on the environment. To overcome these challenges, this study presents a lightweight framework for pest detection and pesticide recommendation, designed for low-resource devices such as smartphones and drones, making it suitable for use by small and marginal farmers.   The proposed framework includes two main components. The first is a Pest Detection Module that uses a compact, lightweight convolutional neural network (CNN) combined with prototypical meta-learning to accurately identify pests even when only a few training samples are available. The second is a Pesticide Recommendation Module that incorporates environmental factors like crop type and growth stage to suggest safe and eco-friendly pesticide recommendations. To train and evaluate our framework, a comprehensive pest image dataset was developed by combining multiple publicly available datasets. The final dataset contains samples with different viewing angles, pest sizes, and background conditions to ensure strong generalization.   Experimental results show that the proposed lightweight CNN achieves high accuracy, comparable to state-of-the-art models, while significantly reducing computational complexity. The Decision Support System additionally improves pest management by reducing dependence on traditional chemical pesticides and encouraging sustainable practices, demonstrating its potential for real-time applications in precision agriculture.



### Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective
- **Arxiv ID**: http://arxiv.org/abs/2601.00257v1
- **DOI**: 10.1109/MWC.2025.3630130
- **Categories**: **eess.SY**, cs.AI, cs.CV, cs.MA, cs.NI
- **Links**: [PDF](https://arxiv.org/pdf/2601.00257v1)
- **Published**: 2026-01-01 08:22:38+00:00
- **Updated**: 2026-01-01 08:22:38+00:00
- **Authors**: Aly Sabri Abdalla, Vuk Marojevic
- **Comment**: This article has been accepted for publication in the IEEE Wireless Communications Magazine
- **Journal**: None
- **Summary**: Despite the growing interest in low-altitude economy (LAE) applications, including UAV-based logistics and emergency response, fundamental challenges remain in orchestrating such missions over complex, signal-constrained environments. These include the absence of real-time, resilient, and context-aware orchestration of aerial nodes with limited integration of artificial intelligence (AI) specialized for LAE missions. This paper introduces an open radio access network (O-RAN)-enabled LAE framework that leverages seamless coordination between the disaggregated RAN architecture, open interfaces, and RAN intelligent controllers (RICs) to facilitate closed-loop, AI-optimized, and mission-critical LAE operations. We evaluate the feasibility and performance of the proposed architecture via a semantic-aware rApp that acts as a terrain interpreter, offering semantic guidance to a reinforcement learning-enabled xApp, which performs real-time trajectory planning for LAE swarm nodes. We survey the capabilities of UAV testbeds that can be leveraged for LAE research, and present critical research challenges and standardization needs.



### TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models
- **Arxiv ID**: http://arxiv.org/abs/2601.00260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00260v1)
- **Published**: 2026-01-01 08:27:01+00:00
- **Updated**: 2026-01-01 08:27:01+00:00
- **Authors**: Kohei Yamamoto, Tomohiro Kikuchi
- **Comment**: None
- **Journal**: None
- **Summary**: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.



### S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding
- **Arxiv ID**: http://arxiv.org/abs/2601.00264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00264v1)
- **Published**: 2026-01-01 08:54:51+00:00
- **Updated**: 2026-01-01 08:54:51+00:00
- **Authors**: He Wang, Longteng Guo, Pengkang Huo, Xuanxu Lin, Yichen Yuan, Jie Jiang, Jing Liu
- **Comment**: 12 pages, 5 figures. Dataset available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign
- **Journal**: None
- **Summary**: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.



### ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching
- **Arxiv ID**: http://arxiv.org/abs/2601.00267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00267v1)
- **Published**: 2026-01-01 09:11:09+00:00
- **Updated**: 2026-01-01 09:11:09+00:00
- **Authors**: Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, Bin Chen, Xuan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.



### FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2601.00269v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.00269v1)
- **Published**: 2026-01-01 09:19:39+00:00
- **Updated**: 2026-01-01 09:19:39+00:00
- **Authors**: Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu
- **Comment**: 14 pages, 9 figures, 5 tables
- **Journal**: None
- **Summary**: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.



### Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification
- **Arxiv ID**: http://arxiv.org/abs/2601.00278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00278v1)
- **Published**: 2026-01-01 09:28:56+00:00
- **Updated**: 2026-01-01 09:28:56+00:00
- **Authors**: Chi Ding, Junxiao Xue, Xinyi Yin, Shi Chen, Yunyun Shi, Yiduo Wang, Fengjian Xue, Xuecheng Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Long-Tailed distributions are pervasive in remote sensing due to the inherently imbalanced occurrence of grounded objects. However, a critical challenge remains largely overlooked, i.e., disentangling hard tail data samples from noisy ambiguous ones. Conventional methods often indiscriminately emphasize all low-confidence samples, leading to overfitting on noisy data. To bridge this gap, building upon Evidential Deep Learning, we propose a model-agnostic uncertainty-aware framework termed DUAL, which dynamically disentangles prediction uncertainty into Epistemic Uncertainty (EU) and Aleatoric Uncertainty (AU). Specifically, we introduce EU as an indicator of sample scarcity to guide a reweighting strategy for hard-to-learn tail samples, while leveraging AU to quantify data ambiguity, employing an adaptive label smoothing mechanism to suppress the impact of noise. Extensive experiments on multiple datasets across various backbones demonstrate the effectiveness and generalization of our framework, surpassing strong baselines such as TGN and SADE. Ablation studies provide further insights into the crucial choices of our design.



### SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting
- **Arxiv ID**: http://arxiv.org/abs/2601.00285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00285v1)
- **Published**: 2026-01-01 09:53:03+00:00
- **Updated**: 2026-01-01 09:53:03+00:00
- **Authors**: Jun-Jee Chao, Volkan Isler
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.



### Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies
- **Arxiv ID**: http://arxiv.org/abs/2601.00286v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.00286v1)
- **Published**: 2026-01-01 09:53:44+00:00
- **Updated**: 2026-01-01 09:53:44+00:00
- **Authors**: Ali Anaissi, Ali Braytee, Weidong Huang, Junaid Akram, Alaa Farhat, Jie Hua
- **Comment**: The 23rd Australasian Data Science and Machine Learning Conference (AusDM'25)
- **Journal**: None
- **Summary**: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.



### TimeColor: Flexible Reference Colorization via Temporal Concatenation
- **Arxiv ID**: http://arxiv.org/abs/2601.00296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00296v1)
- **Published**: 2026-01-01 10:30:38+00:00
- **Updated**: 2026-01-01 10:30:38+00:00
- **Authors**: Bryan Constantine Sadihin, Yihao Meng, Michael Hua Wang, Matteo Jiahao Chen, Hang Su
- **Comment**: Demo samples are available at: https://bconstantine.github.io/TimeColor/
- **Journal**: None
- **Summary**: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.



### VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2601.00307v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.00307v1)
- **Published**: 2026-01-01 11:06:11+00:00
- **Updated**: 2026-01-01 11:06:11+00:00
- **Authors**: Anns Ijaz, Muhammad Azeem Javed
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.



### ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition
- **Arxiv ID**: http://arxiv.org/abs/2601.00311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00311v1)
- **Published**: 2026-01-01 11:20:19+00:00
- **Updated**: 2026-01-01 11:20:19+00:00
- **Authors**: Feng-Qi Cui, Jinyang Huang, Sirui Zhao, Jinglong Guo, Qifan Cai, Xin Yan, Zhi Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Video behavior recognition demands stable and discriminative representations under complex spatiotemporal variations. However, prevailing data augmentation strategies for videos remain largely perturbation-driven, often introducing uncontrolled variations that amplify non-discriminative factors, which finally weaken intra-class distributional structure and representation drift with inconsistent gains across temporal scales. To address these problems, we propose Representation-aware Mixing Augmentation (ReMA), a plug-and-play augmentation strategy that formulates mixing as a controlled replacement process to expand representations while preserving class-conditional stability. ReMA integrates two complementary mechanisms. Firstly, the Representation Alignment Mechanism (RAM) performs structured intra-class mixing under distributional alignment constraints, suppressing irrelevant intra-class drift while enhancing statistical reliability. Then, the Dynamic Selection Mechanism (DSM) generates motion-aware spatiotemporal masks to localize perturbations, guiding them away from discrimination-sensitive regions and promoting temporal coherence. By jointly controlling how and where mixing is applied, ReMA improves representation robustness without additional supervision or trainable parameters. Extensive experiments on diverse video behavior benchmarks demonstrate that ReMA consistently enhances generalization and robustness across different spatiotemporal granularities.



### Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation
- **Arxiv ID**: http://arxiv.org/abs/2601.00322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00322v1)
- **Published**: 2026-01-01 12:30:53+00:00
- **Updated**: 2026-01-01 12:30:53+00:00
- **Authors**: Siyan Fang, Long Peng, Yuntao Wang, Ruonan Wei, Yuehuan Wang
- **Comment**: This paper has been accepted by AAAI 2026
- **Journal**: None
- **Summary**: Image reflection separation aims to disentangle the transmission layer and the reflection layer from a blended image. Existing methods rely on limited information from a single image, tending to confuse the two layers when their contrasts are similar, a challenge more severe at night. To address this issue, we propose the Depth-Memory Decoupling Network (DMDNet). It employs the Depth-Aware Scanning (DAScan) to guide Mamba toward salient structures, promoting information flow along semantic coherence to construct stable states. Working in synergy with DAScan, the Depth-Synergized State-Space Model (DS-SSM) modulates the sensitivity of state activations by depth, suppressing the spread of ambiguous features that interfere with layer disentanglement. Furthermore, we introduce the Memory Expert Compensation Module (MECM), leveraging cross-image historical knowledge to guide experts in providing layer-specific compensation. To address the lack of datasets for nighttime reflection separation, we construct the Nighttime Image Reflection Separation (NightIRS) dataset. Extensive experiments demonstrate that DMDNet outperforms state-of-the-art methods in both daytime and nighttime.



### HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2601.00327v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.00327v1)
- **Published**: 2026-01-01 12:45:45+00:00
- **Updated**: 2026-01-01 12:45:45+00:00
- **Authors**: Naiqi Zhang, Chuancheng Shi, Jingtong Dou, Wenhua Wu, Fei Shen, Jianhua Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.



### Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2601.00328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00328v1)
- **Published**: 2026-01-01 12:48:56+00:00
- **Updated**: 2026-01-01 12:48:56+00:00
- **Authors**: Yingzhi Tang, Qijian Zhang, Junhui Hou
- **Comment**: None
- **Journal**: None
- **Summary**: Achieving consistent and high-fidelity geometry and appearance reconstruction of 3D digital humans from a single RGB image is inherently a challenging task. Existing studies typically resort to decoupled pipelines for geometry estimation and appearance synthesis, often hindering unified reconstruction and causing inconsistencies. This paper introduces \textbf{JGA-LBD}, a novel framework that unifies the modeling of geometry and appearance into a joint latent representation and formulates the generation process as bridge diffusion. Observing that directly integrating heterogeneous input conditions (e.g., depth maps, SMPL models) leads to substantial training difficulties, we unify all conditions into the 3D Gaussian representations, which can be further compressed into a unified latent space through a shared sparse variational autoencoder (VAE). Subsequently, the specialized form of bridge diffusion enables to start with a partial observation of the target latent code and solely focuses on inferring the missing components. Finally, a dedicated decoding module extracts the complete 3D human geometric structure and renders novel views from the inferred latent representation. Experiments demonstrate that JGA-LBD outperforms current state-of-the-art approaches in terms of both geometry fidelity and appearance quality, including challenging in-the-wild scenarios. Our code will be made publicly available at https://github.com/haiantyz/JGA-LBD.



### Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation
- **Arxiv ID**: http://arxiv.org/abs/2601.00344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00344v1)
- **Published**: 2026-01-01 13:54:29+00:00
- **Updated**: 2026-01-01 13:54:29+00:00
- **Authors**: Bruce Mugizi, Sudi Murindanyi, Olivia Nakacwa, Andrew Katumba
- **Comment**: None
- **Journal**: None
- **Summary**: Speeding is a major contributor to road fatalities, particularly in developing countries such as Uganda, where road safety infrastructure is limited. This study proposes a real-time intelligent traffic surveillance system tailored to such regions, using computer vision techniques to address vehicle detection, license plate recognition, and speed estimation. The study collected a rich dataset using a speed gun, a Canon Camera, and a mobile phone to train the models. License plate detection using YOLOv8 achieved a mean average precision (mAP) of 97.9%. For character recognition of the detected license plate, the CNN model got a character error rate (CER) of 3.85%, while the transformer model significantly reduced the CER to 1.79%. Speed estimation used source and target regions of interest, yielding a good performance of 10 km/h margin of error. Additionally, a database was established to correlate user information with vehicle detection data, enabling automated ticket issuance via SMS via Africa's Talking API. This system addresses critical traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated traffic enforcement in developing countries where such interventions are urgently needed.



### OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning
- **Arxiv ID**: http://arxiv.org/abs/2601.00352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00352v1)
- **Published**: 2026-01-01 14:11:49+00:00
- **Updated**: 2026-01-01 14:11:49+00:00
- **Authors**: Liuxiang Qiu, Hui Da, Yuzhen Niu, Tiesong Zhao, Yang Cao, Zheng-Jun Zha
- **Comment**: None
- **Journal**: None
- **Summary**: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.



### The Impact of Lesion Focus on the Performance of AI-Based Melanoma Classification
- **Arxiv ID**: http://arxiv.org/abs/2601.00355v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.00355v1)
- **Published**: 2026-01-01 14:17:32+00:00
- **Updated**: 2026-01-01 14:17:32+00:00
- **Authors**: Tanay Donde
- **Comment**: None
- **Journal**: None
- **Summary**: Melanoma is the most lethal subtype of skin cancer, and early and accurate detection of this disease can greatly improve patients' outcomes. Although machine learning models, especially convolutional neural networks (CNNs), have shown great potential in automating melanoma classification, their diagnostic reliability still suffers due to inconsistent focus on lesion areas. In this study, we analyze the relationship between lesion attention and diagnostic performance, involving masked images, bounding box detection, and transfer learning. We used multiple explainability and sensitivity analysis approaches to investigate how well models aligned their attention with lesion areas and how this alignment correlated with precision, recall, and F1-score. Results showed that models with a higher focus on lesion areas achieved better diagnostic performance, suggesting the potential of interpretable AI in medical diagnostics. This study provides a foundation for developing more accurate and trustworthy melanoma classification models in the future.



### Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers
- **Arxiv ID**: http://arxiv.org/abs/2601.00359v1
- **DOI**: 10.1109/IROS60139.2025.11245809
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](https://arxiv.org/pdf/2601.00359v1)
- **Published**: 2026-01-01 14:29:31+00:00
- **Updated**: 2026-01-01 14:29:31+00:00
- **Authors**: SÃ¶hnke Benedikt Fischedick, Daniel Seichter, Benedict Stephan, Robin Schmidt, Horst-Michael Gross
- **Comment**: Published in Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)
- **Journal**: Proc. IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2025, pp. 2400-2407
- **Summary**: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.



### Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2601.00368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00368v1)
- **Published**: 2026-01-01 15:11:55+00:00
- **Updated**: 2026-01-01 15:11:55+00:00
- **Authors**: Aarya Sumuk
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: We present a lightweight two-stage framework for joint geometry and color inpainting of damaged 3D objects, motivated by the digital restoration of cultural heritage artifacts. The pipeline separates damage localization from reconstruction. In the first stage, a 2D convolutional network predicts damage masks on RGB slices extracted from a voxelized object, and these predictions are aggregated into a volumetric mask. In the second stage, a diffusion-based 3D U-Net performs mask-conditioned inpainting directly on voxel grids, reconstructing geometry and color while preserving observed regions. The model jointly predicts occupancy and color using a composite objective that combines occupancy reconstruction with masked color reconstruction and perceptual regularization. We evaluate the approach on a curated set of textured artifacts with synthetically generated damage using standard geometric and color metrics. Compared to symmetry-based baselines, our method produces more complete geometry and more coherent color reconstructions at a fixed 32^3 resolution. Overall, the results indicate that explicit mask conditioning is a practical way to guide volumetric diffusion models for joint 3D geometry and color inpainting.



### BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2601.00369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00369v1)
- **Published**: 2026-01-01 15:13:11+00:00
- **Updated**: 2026-01-01 15:13:11+00:00
- **Authors**: Seungyeon Cho, Tae-kyun Kim
- **Comment**: 16 pages; 8 figures. Extension of previous conference paper. Project page: https://github.com/VinnyCSY/BHaRNet
- **Journal**: None
- **Summary**: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.



### NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/2601.00393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00393v1)
- **Published**: 2026-01-01 17:07:30+00:00
- **Updated**: 2026-01-01 17:07:30+00:00
- **Authors**: Yuxue Yang, Lue Fan, Ziqi Shi, Junran Peng, Feng Wang, Zhaoxiang Zhang
- **Comment**: Project Page: https://neoverse-4d.github.io
- **Journal**: None
- **Summary**: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io



### RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection
- **Arxiv ID**: http://arxiv.org/abs/2601.00398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00398v1)
- **Published**: 2026-01-01 17:22:44+00:00
- **Updated**: 2026-01-01 17:22:44+00:00
- **Authors**: Tao Wu, Qing Xu, Xiangjian He, Oakleigh Weekes, James Brown, Wenting Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.



### ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis
- **Arxiv ID**: http://arxiv.org/abs/2601.00416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00416v1)
- **Published**: 2026-01-01 18:10:11+00:00
- **Updated**: 2026-01-01 18:10:11+00:00
- **Authors**: Tyler Ward, Abdullah Imran
- **Comment**: 21 pages, 10 figures, 8 tables
- **Journal**: None
- **Summary**: Functional connectivity (FC) analysis, a valuable tool for computer-aided brain disorder diagnosis, traditionally relies on atlas-based parcellation. However, issues relating to selection bias and a lack of regard for subject specificity can arise as a result of such parcellations. Addressing this, we propose ABFR-KAN, a transformer-based classification network that incorporates novel advanced brain function representation components with the power of Kolmogorov-Arnold Networks (KANs) to mitigate structural bias, improve anatomical conformity, and enhance the reliability of FC estimation. Extensive experiments on the ABIDE I dataset, including cross-site evaluation and ablation studies across varying model backbones and KAN configurations, demonstrate that ABFR-KAN consistently outperforms state-of-the-art baselines for autism spectrum distorder (ASD) classification. Our code is available at https://github.com/tbwa233/ABFR-KAN.



### Deep Delta Learning
- **Arxiv ID**: http://arxiv.org/abs/2601.00417v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.00417v1)
- **Published**: 2026-01-01 18:11:38+00:00
- **Updated**: 2026-01-01 18:11:38+00:00
- **Authors**: Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu
- **Comment**: Project Page: https://github.com/yifanzhang-pro/deep-delta-learning
- **Journal**: None
- **Summary**: The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\mathbf{k}(\mathbf{X})$ and a gating scalar $Î²(\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $Î²(\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.



### Robust Assembly Progress Estimation via Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2601.00422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00422v1)
- **Published**: 2026-01-01 18:26:17+00:00
- **Updated**: 2026-01-01 18:26:17+00:00
- **Authors**: Kazuma Miura, Sarthak Pathak, Kazunori Umeda
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the advancement of AI technologies has accelerated the development of smart factories. In particular, the automatic monitoring of product assembly progress is crucial for improving operational efficiency, minimizing the cost of discarded parts, and maximizing factory productivity. However, in cases where assembly tasks are performed manually over multiple days, implementing smart factory systems remains a challenge. Previous work has proposed Anomaly Triplet-Net, which estimates assembly progress by applying deep metric learning to the visual features of products. Nevertheless, when visual changes between consecutive tasks are subtle, misclassification often occurs. To address this issue, this paper proposes a robust system for estimating assembly progress, even in cases of occlusion or minimal visual change, using a small-scale dataset. Our method leverages a Quadruplet Loss-based learning approach for anomaly images and introduces a custom data loader that strategically selects training samples to enhance estimation accuracy. We evaluated our approach using a image datasets: captured during desktop PC assembly. The proposed Anomaly Quadruplet-Net outperformed existing methods on the dataset. Specifically, it improved the estimation accuracy by 1.3% and reduced misclassification between adjacent tasks by 1.9% in the desktop PC dataset and demonstrating the effectiveness of the proposed method.



### E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models
- **Arxiv ID**: http://arxiv.org/abs/2601.00423v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.00423v1)
- **Published**: 2026-01-01 18:27:32+00:00
- **Updated**: 2026-01-01 18:27:32+00:00
- **Authors**: Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan
- **Comment**: Code: https://github.com/shengjun-zhang/VisualGRPO
- **Journal**: None
- **Summary**: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.



### CPPO: Contrastive Perception for Vision Language Policy Optimization
- **Arxiv ID**: http://arxiv.org/abs/2601.00501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2601.00501v1)
- **Published**: 2026-01-01 22:48:26+00:00
- **Updated**: 2026-01-01 22:48:26+00:00
- **Authors**: Ahmad Rezaei, Mohsen Gholami, Saeed Ranjbar Alvar, Kevin Cannons, Mohammad Asiful Hossain, Zhou Weimin, Shunbo Zhou, Yong Zhang, Mohammad Akbari
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.



### MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation
- **Arxiv ID**: http://arxiv.org/abs/2601.00504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](https://arxiv.org/pdf/2601.00504v1)
- **Published**: 2026-01-01 22:56:37+00:00
- **Updated**: 2026-01-01 22:56:37+00:00
- **Authors**: Miaowei Wang, Jakub ZadroÅ¼ny, Oisin Mac Aodha, Amir Vaxman
- **Comment**: AAAI2026 Accepted
- **Journal**: None
- **Summary**: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.




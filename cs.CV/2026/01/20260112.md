# Arxiv Papers in cs.CV on 2026-01-12
### MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2601.07107v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.07107v1)
- **Published**: 2026-01-12 00:11:10+00:00
- **Updated**: 2026-01-12 00:11:10+00:00
- **Authors**: Meng Lu, Yuxing Lu, Yuchen Zhuang, Megan Mullins, Yang Xie, Guanghua Xiao, Charles Fleming, Wenqi Shi, Xuan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Vision language models (VLMs) achieve strong performance on general image understanding but struggle to think with medical images, especially when performing multi-step reasoning through iterative visual interaction. Medical VLMs often rely on static visual embeddings and single-pass inference, preventing models from re-examining, verifying, or refining visual evidence during reasoning. While tool-integrated reasoning offers a promising path forward, open-source VLMs lack the training infrastructure to learn effective tool selection, invocation, and coordination in multi-modal medical reasoning. We introduce MedVistaGym, a scalable and interactive training environment that incentivizes tool-integrated visual reasoning for medical image analysis. MedVistaGym equips VLMs to determine when and which tools to invoke, localize task-relevant image regions, and integrate single or multiple sub-image evidence into interleaved multimodal reasoning within a unified, executable interface for agentic training. Using MedVistaGym, we train MedVistaGym-R1 to interleave tool use with agentic reasoning through trajectory sampling and end-to-end reinforcement learning. Across six medical VQA benchmarks, MedVistaGym-R1-8B exceeds comparably sized tool-augmented baselines by 19.10% to 24.21%, demonstrating that structured agentic training--not tool access alone--unlocks effective tool-integrated reasoning for medical image analysis.



### Few-shot Class-Incremental Learning via Generative Co-Memory Regularization
- **Arxiv ID**: http://arxiv.org/abs/2601.07117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2601.07117v1)
- **Published**: 2026-01-12 01:10:44+00:00
- **Updated**: 2026-01-12 01:10:44+00:00
- **Authors**: Kexin Bao, Yong Li, Dan Zeng, Shiming Ge
- **Comment**: Accepted by International Journal on Computer Vision (IJCV)
- **Journal**: None
- **Summary**: Few-shot class-incremental learning (FSCIL) aims to incrementally learn models from a small amount of novel data, which requires strong representation and adaptation ability of models learned under few-example supervision to avoid catastrophic forgetting on old classes and overfitting to novel classes. This work proposes a generative co-memory regularization approach to facilitate FSCIL. In the approach, the base learning leverages generative domain adaptation finetuning to finetune a pretrained generative encoder on a few examples of base classes by jointly incorporating a masked autoencoder (MAE) decoder for feature reconstruction and a fully-connected classifier for feature classification, which enables the model to efficiently capture general and adaptable representations. Using the finetuned encoder and learned classifier, we construct two class-wise memories: representation memory for storing the mean features for each class, and weight memory for storing the classifier weights. After that, the memory-regularized incremental learning is performed to train the classifier dynamically on the examples of few-shot classes in each incremental session by simultaneously optimizing feature classification and co-memory regularization. The memories are updated in a class-incremental manner and they collaboratively regularize the incremental learning. In this way, the learned models improve recognition accuracy, while mitigating catastrophic forgetting over old classes and overfitting to novel classes. Extensive experiments on popular benchmarks clearly demonstrate that our approach outperforms the state-of-the-arts.



### SC-MII: Infrastructure LiDAR-based 3D Object Detection on Edge Devices for Split Computing with Multiple Intermediate Outputs Integration
- **Arxiv ID**: http://arxiv.org/abs/2601.07119v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.07119v1)
- **Published**: 2026-01-12 01:17:01+00:00
- **Updated**: 2026-01-12 01:17:01+00:00
- **Authors**: Taisuke Noguchi, Takayuki Nishio, Takuya Azumi
- **Comment**: 6 pages. This version includes minor lstlisting configuration adjustments for successful compilation. No changes to content or layout. Originally published at IEEE CCNC 2026
- **Journal**: None
- **Summary**: 3D object detection using LiDAR-based point cloud data and deep neural networks is essential in autonomous driving technology. However, deploying state-of-the-art models on edge devices present challenges due to high computational demands and energy consumption. Additionally, single LiDAR setups suffer from blind spots. This paper proposes SC-MII, multiple infrastructure LiDAR-based 3D object detection on edge devices for Split Computing with Multiple Intermediate outputs Integration. In SC-MII, edge devices process local point clouds through the initial DNN layers and send intermediate outputs to an edge server. The server integrates these features and completes inference, reducing both latency and device load while improving privacy. Experimental results on a real-world dataset show a 2.19x speed-up and a 71.6% reduction in edge device processing time, with at most a 1.09% drop in accuracy.



### ReinPool: Reinforcement Learning Pooling Multi-Vector Embeddings for Retrieval System
- **Arxiv ID**: http://arxiv.org/abs/2601.07125v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CL, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2601.07125v1)
- **Published**: 2026-01-12 01:37:21+00:00
- **Updated**: 2026-01-12 01:37:21+00:00
- **Authors**: Sungguk Cha, DongWook Kim, Mintae Kim, Youngsub Han, Byoung-Ki Jeon, Sangyeob Lee
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Multi-vector embedding models have emerged as a powerful paradigm for document retrieval, preserving fine-grained visual and textual details through token-level representations. However, this expressiveness comes at a staggering cost: storing embeddings for every token inflates index sizes by over $1000\times$ compared to single-vector approaches, severely limiting scalability. We introduce \textbf{ReinPool}, a reinforcement learning framework that learns to dynamically filter and pool multi-vector embeddings into compact, retrieval-optimized representations. By training with an inverse retrieval objective and NDCG-based rewards, ReinPool identifies and retains only the most discriminative vectors without requiring manual importance annotations. On the Vidore V2 benchmark across three vision-language embedding models, ReinPool compresses multi-vector representations by $746$--$1249\times$ into single vectors while recovering 76--81\% of full multi-vector retrieval performance. Compared to static mean pooling baselines, ReinPool achieves 22--33\% absolute NDCG@3 improvement, demonstrating that learned selection significantly outperforms heuristic aggregation.




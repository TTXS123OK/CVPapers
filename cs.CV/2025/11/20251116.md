# Arxiv Papers in cs.CV on 2025-11-16
### DEMIST: \underline{DE}coupled \underline{M}ulti-stream latent d\underline{I}ffusion for Quantitative Myelin Map \underline{S}yn\underline{T}hesis
- **Arxiv ID**: http://arxiv.org/abs/2511.12396v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2511.12396v1)
- **Published**: 2025-11-16 00:19:48+00:00
- **Updated**: 2025-11-16 00:19:48+00:00
- **Authors**: Jiacheng Wang, Hao Li, Xing Yao, Ahmad Toubasi, Taegan Vinarsky, Caroline Gheen, Joy Derwenskus, Chaoyang Jin, Richard Dortch, Junzhong Xu, Francesca Bagnato, Ipek Oguz
- **Comment**: None
- **Journal**: None
- **Summary**: Quantitative magnetization transfer (qMT) imaging provides myelin-sensitive biomarkers, such as the pool size ratio (PSR), which is valuable for multiple sclerosis (MS) assessment. However, qMT requires specialized 20-30 minute scans. We propose DEMIST to synthesize PSR maps from standard T1w and FLAIR images using a 3D latent diffusion model with three complementary conditioning mechanisms. Our approach has two stages: first, we train separate autoencoders for PSR and anatomical images to learn aligned latent representations. Second, we train a conditional diffusion model in this latent space on top of a frozen diffusion foundation backbone. Conditioning is decoupled into: (i) \textbf{semantic} tokens via cross-attention, (ii) \textbf{spatial} per-scale residual hints via a 3D ControlNet branch, and (iii) \textbf{adaptive} LoRA-modulated attention. We include edge-aware loss terms to preserve lesion boundaries and alignment losses to maintain quantitative consistency, while keeping the number of trainable parameters low and retaining the inductive bias of the pretrained model. We evaluate on 163 scans from 99 subjects using 5-fold cross-validation. Our method outperforms VAE, GAN and diffusion baselines on multiple metrics, producing sharper boundaries and better quantitative agreement with ground truth. Our code is publicly available at https://github.com/MedICL-VU/MS-Synthesis-3DcLDM.



### MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting
- **Arxiv ID**: http://arxiv.org/abs/2511.12400v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2511.12400v1)
- **Published**: 2025-11-16 00:35:37+00:00
- **Updated**: 2025-11-16 00:35:37+00:00
- **Authors**: Xu Yang, Gady Agam
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather   than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision   transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and   ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly   modulates spatial and channel attention. The two components are fused through pointwise multiplication and   a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained   weights frozen.   Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,   detection, and segmentation tasks with roughly less than 5\% of backbone parameters.   The design further enables stable optimization, fast convergence, and strong cross-architecture   generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach   for efficient adaptation of frozen vision backbones.



### VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2511.12405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12405v1)
- **Published**: 2025-11-16 00:55:28+00:00
- **Updated**: 2025-11-16 00:55:28+00:00
- **Authors**: Hyunki Seong, Seongwoo Moon, Hojin Ahn, Jehun Kang, David Hyunchul Shim
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.



### Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection
- **Arxiv ID**: http://arxiv.org/abs/2511.12410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12410v1)
- **Published**: 2025-11-16 01:28:45+00:00
- **Updated**: 2025-11-16 01:28:45+00:00
- **Authors**: Xi Xiao, Zhuxuanzi Wang, Mingqiao Mo, Chen Liu, Chenrui Ma, Yanshu Li, Smita Krishnaswamy, Xiao Wang, Tianyang Wang
- **Comment**: Accepted by WACV 2026
- **Journal**: None
- **Summary**: The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \ours, a self-supervised framework that \emph{visually probes} target domains without labels. \ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: https://github.com/xixiaouab/PROBE/tree/main



### Towards Rotation-only Imaging Geometry: Rotation Estimation
- **Arxiv ID**: http://arxiv.org/abs/2511.12415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12415v1)
- **Published**: 2025-11-16 02:04:32+00:00
- **Updated**: 2025-11-16 02:04:32+00:00
- **Authors**: Xinrui Li, Qi Cai, Yuanxin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.



### Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance
- **Arxiv ID**: http://arxiv.org/abs/2511.12419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12419v1)
- **Published**: 2025-11-16 02:16:14+00:00
- **Updated**: 2025-11-16 02:16:14+00:00
- **Authors**: Wenjie Li, Jinglei Shi, Jin Han, Heng Guo, Zhanyu Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.



### MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation
- **Arxiv ID**: http://arxiv.org/abs/2511.12422v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2511.12422v1)
- **Published**: 2025-11-16 02:33:50+00:00
- **Updated**: 2025-11-16 02:33:50+00:00
- **Authors**: Nuolin Sun, Linyuan Wang, Haonan Wei, Lei Li, Bin Yan
- **Comment**: None
- **Journal**: None
- **Summary**: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.



### RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning
- **Arxiv ID**: http://arxiv.org/abs/2511.12428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12428v1)
- **Published**: 2025-11-16 03:11:52+00:00
- **Updated**: 2025-11-16 03:11:52+00:00
- **Authors**: Jingqi Xu, Jingxi Lu, Chenghao Li, Sreetama Sarkar, Souvik Kundu, Peter A. Beerel
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.



### Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/2511.12432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12432v1)
- **Published**: 2025-11-16 03:22:33+00:00
- **Updated**: 2025-11-16 03:22:33+00:00
- **Authors**: Xilai Li, Xiaosong Li, Weijun Jiang
- **Comment**: Accepted at AAAI 2026
- **Journal**: None
- **Summary**: Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.



### Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2511.12438v1
- **DOI**: 10.1109/AIBThings63359.2024.10861885
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](https://arxiv.org/pdf/2511.12438v1)
- **Published**: 2025-11-16 03:39:17+00:00
- **Updated**: 2025-11-16 03:39:17+00:00
- **Authors**: ANK Zaman, Prosenjit Chatterjee, Rajat Sharma
- **Comment**: None
- **Journal**: 2024 2nd International Conference on Artificial Intelligence, Blockchain, and Internet of Things (AIBThings)
- **Summary**: A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.



### CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training
- **Arxiv ID**: http://arxiv.org/abs/2511.12446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12446v1)
- **Published**: 2025-11-16 04:19:22+00:00
- **Updated**: 2025-11-16 04:19:22+00:00
- **Authors**: Jiahe Qian, Yuhao Shen, Zhangtianyi Chen, Juexiao Zhou, Peisong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.



### MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding
- **Arxiv ID**: http://arxiv.org/abs/2511.12449v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG
- **Links**: [PDF](https://arxiv.org/pdf/2511.12449v1)
- **Published**: 2025-11-16 04:29:35+00:00
- **Updated**: 2025-11-16 04:29:35+00:00
- **Authors**: Zhanheng Nie, Chenghan Fu, Daoze Zhang, Junxian Wu, Wanxian Guan, Pengjie Wang, Jian Xu, Bo Zheng
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.



### DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2511.12452v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](https://arxiv.org/pdf/2511.12452v1)
- **Published**: 2025-11-16 04:46:06+00:00
- **Updated**: 2025-11-16 04:46:06+00:00
- **Authors**: Xiaoyu Lin, Aniket Ghorpade, Hansheng Zhu, Justin Qiu, Dea Rrozhani, Monica Lama, Mick Yang, Zixuan Bian, Ruohan Ren, Alan B. Hong, Jiatao Gu, Chris Callison-Burch
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.



### Co-Layout: LLM-driven Co-optimization for Interior Layout
- **Arxiv ID**: http://arxiv.org/abs/2511.12474v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.GR
- **Links**: [PDF](https://arxiv.org/pdf/2511.12474v1)
- **Published**: 2025-11-16 06:20:55+00:00
- **Updated**: 2025-11-16 06:20:55+00:00
- **Authors**: Chucheng Xiang, Ruchao Bao, Biyin Feng, Wenzheng Wu, Zhongyuan Liu, Yirui Guan, Ligang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.



### MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2511.12480v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](https://arxiv.org/pdf/2511.12480v1)
- **Published**: 2025-11-16 07:11:33+00:00
- **Updated**: 2025-11-16 07:11:33+00:00
- **Authors**: Jingshan Hong, Haigen Hu, Huihuang Zhang, Qianwei Zhou, Zhao Li
- **Comment**: None
- **Journal**: None
- **Summary**: In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.



### Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/2511.12498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12498v1)
- **Published**: 2025-11-16 08:16:25+00:00
- **Updated**: 2025-11-16 08:16:25+00:00
- **Authors**: Jongseong Bae, Junwoo Ha, Jinnyeong Heo, Yeongin Lee, Ha Young Kim
- **Comment**: Accepted to AAAI 2026
- **Journal**: None
- **Summary**: Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.



### BSO: Binary Spiking Online Optimization Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2511.12502v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](https://arxiv.org/pdf/2511.12502v1)
- **Published**: 2025-11-16 08:30:45+00:00
- **Updated**: 2025-11-16 08:30:45+00:00
- **Authors**: Yu Liang, Yu Yang, Wenjie Wei, Ammar Belatreche, Shuai Wang, Malu Zhang, Yang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.



### Visible Structure Retrieval for Lightweight Image-Based Relocalisation
- **Arxiv ID**: http://arxiv.org/abs/2511.12503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12503v1)
- **Published**: 2025-11-16 08:32:18+00:00
- **Updated**: 2025-11-16 08:32:18+00:00
- **Authors**: Fereidoon Zangeneh, Leonard Bruns, Amit Dekel, Alessandro Pieropan, Patric Jensfelt
- **Comment**: Accepted at BMVC 2025
- **Journal**: None
- **Summary**: Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.



### DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection
- **Arxiv ID**: http://arxiv.org/abs/2511.12511v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](https://arxiv.org/pdf/2511.12511v1)
- **Published**: 2025-11-16 08:54:00+00:00
- **Updated**: 2025-11-16 08:54:00+00:00
- **Authors**: Jialiang Shen, Jiyang Zheng, Yunqi Xue, Huajie Chen, Yu Yao, Hui Kang, Ruiqi Liu, Helin Gong, Yang Yang, Dadong Wang, Tongliang Liu
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.



### MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics
- **Arxiv ID**: http://arxiv.org/abs/2511.12525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12525v1)
- **Published**: 2025-11-16 09:43:12+00:00
- **Updated**: 2025-11-16 09:43:12+00:00
- **Authors**: Jing Li, Yifan Wang, Jiafeng Yan, Renlong Zhang, Bin Yang
- **Comment**: 10 pages, 7 figures. Accepted by AAAI 2026
- **Journal**: None
- **Summary**: Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.



### D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2511.12528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12528v1)
- **Published**: 2025-11-16 09:47:45+00:00
- **Updated**: 2025-11-16 09:47:45+00:00
- **Authors**: Zheyuan Zhang, Jiwei Zhang, Boyu Zhou, Linzhimeng Duan, Hong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.



### ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2511.12530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12530v1)
- **Published**: 2025-11-16 09:56:57+00:00
- **Updated**: 2025-11-16 09:56:57+00:00
- **Authors**: Yuan Zhou, Litao Hua, Shilong Jin, Wentao Huang, Haoran Duan
- **Comment**: Accepted to AAAI 2026. Code is available at: https://github.com/robin-hlt/AAAI26-ReaSon
- **Journal**: None
- **Summary**: Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.



### HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2511.12547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12547v1)
- **Published**: 2025-11-16 10:46:16+00:00
- **Updated**: 2025-11-16 10:46:16+00:00
- **Authors**: Zhiguang Lu, Qianqian Xu, Peisong Wen, Siran Da, Qingming Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.



### EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis
- **Arxiv ID**: http://arxiv.org/abs/2511.12554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](https://arxiv.org/pdf/2511.12554v1)
- **Published**: 2025-11-16 11:16:50+00:00
- **Updated**: 2025-11-16 11:16:50+00:00
- **Authors**: Yijie Guo, Dexiang Hong, Weidong Chen, Zihan She, Cheng Ye, Xiaojun Chang, Zhendong Mao
- **Comment**: 11 pages, 7 figures. This is a preprint version of a paper submitted to CVPR 2026
- **Journal**: None
- **Summary**: Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.



